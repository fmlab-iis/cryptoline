(* frege: -v -isafety -isafety_timeout 14400 -jobs 24 -slicing -no_carry_constraint __asm_final_map.cl
Parsing Cryptoline file:                [OK]            0.565823 seconds
Checking well-formedness:               [OK]            0.142000 seconds
Transforming to SSA form:               [OK]            0.030817 seconds
Normalizing specification:              [OK]            0.001321 seconds
Rewriting assignments:                  [OK]            0.023318 seconds
Verifying program safety:               [OK]            657.638363 seconds
Verifying range assertions:             [OK]            3.879925 seconds
Verifying range specification:          [OK]            6073.400216 seconds
Rewriting value-preserved casting:      [OK]            0.000884 seconds
Verifying algebraic assertions:         [OK]            150.859373 seconds
Verifying algebraic specification:      [OK]            694.034763 seconds
Verification result:                    [OK]            7580.597569 seconds
*)

(* quine: -isafety -jobs 24 -slicing -no_carry_constraint -v __asm_final_map.cl
Parsing Cryptoline file:                [OK]            0.774267 seconds
Checking well-formedness:               [OK]            0.224364 seconds
Transforming to SSA form:               [OK]            0.051276 seconds
Rewriting assignments:                  [OK]            0.045866 seconds
Verifying program safety:               [OK]            987.814237 seconds
Verifying range assertions:             [OK]            0.558105 seconds
Verifying range specification:          [OK]            10237.074002 seconds
Rewriting value-preserved casting:      [OK]            0.001446 seconds
Verifying algebraic assertions:         [OK]            178.371755 seconds
Verifying algebraic specification:      [OK]            1611.939324 seconds
Verification result:                    [OK]            13016.884281 seconds
*)

proc main (
sint32 L0x2001afc8, sint32 L0x2001afcc, sint32 L0x2001afd0, sint32 L0x2001afd4,
sint32 L0x2001afd8, sint32 L0x2001afdc, sint32 L0x2001afe0, sint32 L0x2001afe4,
sint32 L0x2001afe8, sint32 L0x2001afec, sint32 L0x2001aff0, sint32 L0x2001aff4,
sint32 L0x2001aff8, sint32 L0x2001affc, sint32 L0x2001b000, sint32 L0x2001b004,
sint32 L0x2001b008, sint32 L0x2001b00c, sint32 L0x2001b010, sint32 L0x2001b014,
sint32 L0x2001b018, sint32 L0x2001b01c, sint32 L0x2001b020, sint32 L0x2001b024,
sint32 L0x2001b028, sint32 L0x2001b02c, sint32 L0x2001b030, sint32 L0x2001b034,
sint32 L0x2001b038, sint32 L0x2001b03c, sint32 L0x2001b040, sint32 L0x2001b044,
sint32 L0x2001b048, sint32 L0x2001b04c, sint32 L0x2001b050, sint32 L0x2001b054,
sint32 L0x2001b058, sint32 L0x2001b05c, sint32 L0x2001b060, sint32 L0x2001b064,
sint32 L0x2001b068, sint32 L0x2001b06c, sint32 L0x2001b070, sint32 L0x2001b074,
sint32 L0x2001b078, sint32 L0x2001b07c, sint32 L0x2001b080, sint32 L0x2001b084,
sint32 L0x2001b088, sint32 L0x2001b08c, sint32 L0x2001b090, sint32 L0x2001b094,
sint32 L0x2001b098, sint32 L0x2001b09c, sint32 L0x2001b0a0, sint32 L0x2001b0a4,
sint32 L0x2001b0a8, sint32 L0x2001b0ac, sint32 L0x2001b0b0, sint32 L0x2001b0b4,
sint32 L0x2001b0b8, sint32 L0x2001b0bc, sint32 L0x2001b0c0, sint32 L0x2001b0c4,
sint32 L0x2001b0c8, sint32 L0x2001b0cc, sint32 L0x2001b0d0, sint32 L0x2001b0d4,
sint32 L0x2001b0d8, sint32 L0x2001b0dc, sint32 L0x2001b0e0, sint32 L0x2001b0e4,
sint32 L0x2001b0e8, sint32 L0x2001b0ec, sint32 L0x2001b0f0, sint32 L0x2001b0f4,
sint32 L0x2001b0f8, sint32 L0x2001b0fc, sint32 L0x2001b100, sint32 L0x2001b104,
sint32 L0x2001b108, sint32 L0x2001b10c, sint32 L0x2001b110, sint32 L0x2001b114,
sint32 L0x2001b118, sint32 L0x2001b11c, sint32 L0x2001b120, sint32 L0x2001b124,
sint32 L0x2001b128, sint32 L0x2001b12c, sint32 L0x2001b130, sint32 L0x2001b134,
sint32 L0x2001b138, sint32 L0x2001b13c, sint32 L0x2001b140, sint32 L0x2001b144,
sint32 L0x2001b148, sint32 L0x2001b14c, sint32 L0x2001b150, sint32 L0x2001b154,
sint32 L0x2001b158, sint32 L0x2001b15c, sint32 L0x2001b160, sint32 L0x2001b164,
sint32 L0x2001b168, sint32 L0x2001b16c, sint32 L0x2001b170, sint32 L0x2001b174,
sint32 L0x2001b178, sint32 L0x2001b17c, sint32 L0x2001b180, sint32 L0x2001b184,
sint32 L0x2001b188, sint32 L0x2001b18c, sint32 L0x2001b190, sint32 L0x2001b194,
sint32 L0x2001b198, sint32 L0x2001b19c, sint32 L0x2001b1a0, sint32 L0x2001b1a4,
sint32 L0x2001b1a8, sint32 L0x2001b1ac, sint32 L0x2001b1b0, sint32 L0x2001b1b4,
sint32 L0x2001b1b8, sint32 L0x2001b1bc, sint32 L0x2001b1c0, sint32 L0x2001b1c4,
sint32 L0x2001b1c8, sint32 L0x2001b1cc, sint32 L0x2001b1d0, sint32 L0x2001b1d4,
sint32 L0x2001b1d8, sint32 L0x2001b1dc, sint32 L0x2001b1e0, sint32 L0x2001b1e4,
sint32 L0x2001b1e8, sint32 L0x2001b1ec, sint32 L0x2001b1f0, sint32 L0x2001b1f4,
sint32 L0x2001b1f8, sint32 L0x2001b1fc, sint32 L0x2001b200, sint32 L0x2001b204,
sint32 L0x2001b208, sint32 L0x2001b20c, sint32 L0x2001b210, sint32 L0x2001b214,
sint32 L0x2001b218, sint32 L0x2001b21c, sint32 L0x2001b220, sint32 L0x2001b224,
sint32 L0x2001b228, sint32 L0x2001b22c, sint32 L0x2001b230, sint32 L0x2001b234,
sint32 L0x2001b238, sint32 L0x2001b23c, sint32 L0x2001b240, sint32 L0x2001b244,
sint32 L0x2001b248, sint32 L0x2001b24c, sint32 L0x2001b250, sint32 L0x2001b254,
sint32 L0x2001b258, sint32 L0x2001b25c, sint32 L0x2001b260, sint32 L0x2001b264,
sint32 L0x2001b268, sint32 L0x2001b26c, sint32 L0x2001b270, sint32 L0x2001b274,
sint32 L0x2001b278, sint32 L0x2001b27c, sint32 L0x2001b280, sint32 L0x2001b284,
sint32 L0x2001b288, sint32 L0x2001b28c, sint32 L0x2001b290, sint32 L0x2001b294,
sint32 L0x2001b298, sint32 L0x2001b29c, sint32 L0x2001b2a0, sint32 L0x2001b2a4,
sint32 L0x2001b2a8, sint32 L0x2001b2ac, sint32 L0x2001b2b0, sint32 L0x2001b2b4,
sint32 L0x2001b2b8, sint32 L0x2001b2bc, sint32 L0x2001b2c0, sint32 L0x2001b2c4,
sint32 L0x2001b2c8, sint32 L0x2001b2cc, sint32 L0x2001b2d0, sint32 L0x2001b2d4,
sint32 L0x2001b2d8, sint32 L0x2001b2dc, sint32 L0x2001b2e0, sint32 L0x2001b2e4,
sint32 L0x2001b2e8, sint32 L0x2001b2ec, sint32 L0x2001b2f0, sint32 L0x2001b2f4,
sint32 L0x2001b2f8, sint32 L0x2001b2fc, sint32 L0x2001b300, sint32 L0x2001b304,
sint32 L0x2001b308, sint32 L0x2001b30c, sint32 L0x2001b310, sint32 L0x2001b314,
sint32 L0x2001b318, sint32 L0x2001b31c, sint32 L0x2001b320, sint32 L0x2001b324,
sint32 L0x2001b328, sint32 L0x2001b32c, sint32 L0x2001b330, sint32 L0x2001b334,
sint32 L0x2001b338, sint32 L0x2001b33c, sint32 L0x2001b340, sint32 L0x2001b344,
sint32 L0x2001b348, sint32 L0x2001b34c, sint32 L0x2001b350, sint32 L0x2001b354,
sint32 L0x2001b358, sint32 L0x2001b35c, sint32 L0x2001b360, sint32 L0x2001b364,
sint32 L0x2001b368, sint32 L0x2001b36c, sint32 L0x2001b370, sint32 L0x2001b374,
sint32 L0x2001b378, sint32 L0x2001b37c, sint32 L0x2001b380, sint32 L0x2001b384,
sint32 L0x2001b388, sint32 L0x2001b38c, sint32 L0x2001b390, sint32 L0x2001b394,
sint32 L0x2001b398, sint32 L0x2001b39c, sint32 L0x2001b3a0, sint32 L0x2001b3a4,
sint32 L0x2001b3a8, sint32 L0x2001b3ac, sint32 L0x2001b3b0, sint32 L0x2001b3b4,
sint32 L0x2001b3b8, sint32 L0x2001b3bc, sint32 L0x2001b3c0, sint32 L0x2001b3c4,
sint32 L0x2001b3c8, sint32 L0x2001b3cc, sint32 L0x2001b3d0, sint32 L0x2001b3d4,
sint32 L0x2001b3d8, sint32 L0x2001b3dc, sint32 L0x2001b3e0, sint32 L0x2001b3e4,
sint32 L0x2001b3e8, sint32 L0x2001b3ec, sint32 L0x2001b3f0, sint32 L0x2001b3f4,
sint32 L0x2001b3f8, sint32 L0x2001b3fc, sint32 L0x2001b400, sint32 L0x2001b404,
sint32 L0x2001b408, sint32 L0x2001b40c, sint32 L0x2001b410, sint32 L0x2001b414,
sint32 L0x2001b418, sint32 L0x2001b41c, sint32 L0x2001b420, sint32 L0x2001b424,
sint32 L0x2001b428, sint32 L0x2001b42c, sint32 L0x2001b430, sint32 L0x2001b434,
sint32 L0x2001b438, sint32 L0x2001b43c, sint32 L0x2001b440, sint32 L0x2001b444,
sint32 L0x2001b448, sint32 L0x2001b44c, sint32 L0x2001b450, sint32 L0x2001b454,
sint32 L0x2001b458, sint32 L0x2001b45c, sint32 L0x2001b460, sint32 L0x2001b464,
sint32 L0x2001b468, sint32 L0x2001b46c, sint32 L0x2001b470, sint32 L0x2001b474,
sint32 L0x2001b478, sint32 L0x2001b47c, sint32 L0x2001b480, sint32 L0x2001b484,
sint32 L0x2001b488, sint32 L0x2001b48c, sint32 L0x2001b490, sint32 L0x2001b494,
sint32 L0x2001b498, sint32 L0x2001b49c, sint32 L0x2001b4a0, sint32 L0x2001b4a4,
sint32 L0x2001b4a8, sint32 L0x2001b4ac, sint32 L0x2001b4b0, sint32 L0x2001b4b4,
sint32 L0x2001b4b8, sint32 L0x2001b4bc, sint32 L0x2001b4c0, sint32 L0x2001b4c4,
sint32 L0x2001b4c8, sint32 L0x2001b4cc, sint32 L0x2001b4d0, sint32 L0x2001b4d4,
sint32 L0x2001b4d8, sint32 L0x2001b4dc, sint32 L0x2001b4e0, sint32 L0x2001b4e4,
sint32 L0x2001b4e8, sint32 L0x2001b4ec, sint32 L0x2001b4f0, sint32 L0x2001b4f4,
sint32 L0x2001b4f8, sint32 L0x2001b4fc, sint32 L0x2001b500, sint32 L0x2001b504,
sint32 L0x2001b508, sint32 L0x2001b50c, sint32 L0x2001b510, sint32 L0x2001b514,
sint32 L0x2001b518, sint32 L0x2001b51c, sint32 L0x2001b520, sint32 L0x2001b524,
sint32 L0x2001b528, sint32 L0x2001b52c, sint32 L0x2001b530, sint32 L0x2001b534,
sint32 L0x2001b538, sint32 L0x2001b53c, sint32 L0x2001b540, sint32 L0x2001b544,
sint32 L0x2001b548, sint32 L0x2001b54c, sint32 L0x2001b550, sint32 L0x2001b554,
sint32 L0x2001b558, sint32 L0x2001b55c, sint32 L0x2001b560, sint32 L0x2001b564,
sint32 L0x2001b568, sint32 L0x2001b56c, sint32 L0x2001b570, sint32 L0x2001b574,
sint32 L0x2001b578, sint32 L0x2001b57c, sint32 L0x2001b580, sint32 L0x2001b584,
sint32 L0x2001b588, sint32 L0x2001b58c, sint32 L0x2001b590, sint32 L0x2001b594,
sint32 L0x2001b598, sint32 L0x2001b59c, sint32 L0x2001b5a0, sint32 L0x2001b5a4,
sint32 L0x2001b5a8, sint32 L0x2001b5ac, sint32 L0x2001b5b0, sint32 L0x2001b5b4,
sint32 L0x2001b5b8, sint32 L0x2001b5bc, sint32 L0x2001b5c0, sint32 L0x2001b5c4,
sint32 L0x2001b5c8, sint32 L0x2001b5cc, sint32 L0x2001b5d0, sint32 L0x2001b5d4,
sint32 L0x2001b5d8, sint32 L0x2001b5dc, sint32 L0x2001b5e0, sint32 L0x2001b5e4,
sint32 L0x2001b5e8, sint32 L0x2001b5ec, sint32 L0x2001b5f0, sint32 L0x2001b5f4,
sint32 L0x2001b5f8, sint32 L0x2001b5fc, sint32 L0x2001b600, sint32 L0x2001b604,
sint32 L0x2001b608, sint32 L0x2001b60c, sint32 L0x2001b610, sint32 L0x2001b614,
sint32 L0x2001b618, sint32 L0x2001b61c, sint32 L0x2001b620, sint32 L0x2001b624,
sint32 L0x2001b628, sint32 L0x2001b62c, sint32 L0x2001b630, sint32 L0x2001b634,
sint32 L0x2001b638, sint32 L0x2001b63c, sint32 L0x2001b640, sint32 L0x2001b644,
sint32 L0x2001b648, sint32 L0x2001b64c, sint32 L0x2001b650, sint32 L0x2001b654,
sint32 L0x2001b658, sint32 L0x2001b65c, sint32 L0x2001b660, sint32 L0x2001b664,
sint32 L0x2001b668, sint32 L0x2001b66c, sint32 L0x2001b670, sint32 L0x2001b674,
sint32 L0x2001b678, sint32 L0x2001b67c, sint32 L0x2001b680, sint32 L0x2001b684,
sint32 L0x2001b688, sint32 L0x2001b68c, sint32 L0x2001b690, sint32 L0x2001b694,
sint32 L0x2001b698, sint32 L0x2001b69c, sint32 L0x2001b6a0, sint32 L0x2001b6a4,
sint32 L0x2001b6a8, sint32 L0x2001b6ac, sint32 L0x2001b6b0, sint32 L0x2001b6b4,
sint32 L0x2001b6b8, sint32 L0x2001b6bc, sint32 L0x2001b6c0, sint32 L0x2001b6c4,
sint32 L0x2001b6c8, sint32 L0x2001b6cc, sint32 L0x2001b6d0, sint32 L0x2001b6d4,
sint32 L0x2001b6d8, sint32 L0x2001b6dc, sint32 L0x2001b6e0, sint32 L0x2001b6e4,
sint32 L0x2001b6e8, sint32 L0x2001b6ec, sint32 L0x2001b6f0, sint32 L0x2001b6f4,
sint32 L0x2001b6f8, sint32 L0x2001b6fc, sint32 L0x2001b700, sint32 L0x2001b704,
sint32 L0x2001b708, sint32 L0x2001b70c, sint32 L0x2001b710, sint32 L0x2001b714,
sint32 L0x2001b718, sint32 L0x2001b71c, sint32 L0x2001b720, sint32 L0x2001b724,
sint32 L0x2001b728, sint32 L0x2001b72c, sint32 L0x2001b730, sint32 L0x2001b734,
sint32 L0x2001b738, sint32 L0x2001b73c, sint32 L0x2001b740, sint32 L0x2001b744,
sint32 L0x2001b748, sint32 L0x2001b74c, sint32 L0x2001b750, sint32 L0x2001b754,
sint32 L0x2001b758, sint32 L0x2001b75c, sint32 L0x2001b760, sint32 L0x2001b764,
sint32 L0x2001b768, sint32 L0x2001b76c, sint32 L0x2001b770, sint32 L0x2001b774,
sint32 L0x2001b778, sint32 L0x2001b77c, sint32 L0x2001b780, sint32 L0x2001b784,
sint32 L0x2001b788, sint32 L0x2001b78c, sint32 L0x2001b790, sint32 L0x2001b794,
sint32 L0x2001b798, sint32 L0x2001b79c, sint32 L0x2001b7a0, sint32 L0x2001b7a4,
sint32 L0x2001b7a8, sint32 L0x2001b7ac, sint32 L0x2001b7b0, sint32 L0x2001b7b4,
sint32 L0x2001b7b8, sint32 L0x2001b7bc, sint32 L0x2001b7c0, sint32 L0x2001b7c4,
sint32 L0x2001b7c8, sint32 L0x2001b7cc, sint32 L0x2001b7d0, sint32 L0x2001b7d4,
sint32 L0x2001b7d8, sint32 L0x2001b7dc, sint32 L0x2001b7e0, sint32 L0x2001b7e4,
sint32 L0x2001b7e8, sint32 L0x2001b7ec, sint32 L0x2001b7f0, sint32 L0x2001b7f4,
sint32 L0x2001b7f8, sint32 L0x2001b7fc, sint32 L0x2001b800, sint32 L0x2001b804,
sint32 L0x2001b808, sint32 L0x2001b80c, sint32 L0x2001b810, sint32 L0x2001b814,
sint32 L0x2001b818, sint32 L0x2001b81c, sint32 L0x2001b820, sint32 L0x2001b824,
sint32 L0x2001b828, sint32 L0x2001b82c, sint32 L0x2001b830, sint32 L0x2001b834,
sint32 L0x2001b838, sint32 L0x2001b83c, sint32 L0x2001b840, sint32 L0x2001b844,
sint32 L0x2001b848, sint32 L0x2001b84c, sint32 L0x2001b850, sint32 L0x2001b854,
sint32 L0x2001b858, sint32 L0x2001b85c, sint32 L0x2001b860, sint32 L0x2001b864,
sint32 L0x2001b868, sint32 L0x2001b86c, sint32 L0x2001b870, sint32 L0x2001b874,
sint32 L0x2001b878, sint32 L0x2001b87c, sint32 L0x2001b880, sint32 L0x2001b884,
sint32 L0x2001b888, sint32 L0x2001b88c, sint32 L0x2001b890, sint32 L0x2001b894,
sint32 L0x2001b898, sint32 L0x2001b89c, sint32 L0x2001b8a0, sint32 L0x2001b8a4,
sint32 L0x2001b8a8, sint32 L0x2001b8ac, sint32 L0x2001b8b0, sint32 L0x2001b8b4,
sint32 L0x2001b8b8, sint32 L0x2001b8bc, sint32 L0x2001b8c0, sint32 L0x2001b8c4,
sint32 L0x2001b8c8, sint32 L0x2001b8cc, sint32 L0x2001b8d0, sint32 L0x2001b8d4,
sint32 L0x2001b8d8, sint32 L0x2001b8dc, sint32 L0x2001b8e0, sint32 L0x2001b8e4,
sint32 L0x2001b8e8, sint32 L0x2001b8ec, sint32 L0x2001b8f0, sint32 L0x2001b8f4,
sint32 L0x2001b8f8, sint32 L0x2001b8fc, sint32 L0x2001b900, sint32 L0x2001b904,
sint32 L0x2001b908, sint32 L0x2001b90c, sint32 L0x2001b910, sint32 L0x2001b914,
sint32 L0x2001b918, sint32 L0x2001b91c, sint32 L0x2001b920, sint32 L0x2001b924,
sint32 L0x2001b928, sint32 L0x2001b92c, sint32 L0x2001b930, sint32 L0x2001b934,
sint32 L0x2001b938, sint32 L0x2001b93c, sint32 L0x2001b940, sint32 L0x2001b944,
sint32 L0x2001b948, sint32 L0x2001b94c, sint32 L0x2001b950, sint32 L0x2001b954,
sint32 L0x2001b958, sint32 L0x2001b95c, sint32 L0x2001b960, sint32 L0x2001b964,
sint32 L0x2001b968, sint32 L0x2001b96c, sint32 L0x2001b970, sint32 L0x2001b974,
sint32 L0x2001b978, sint32 L0x2001b97c, sint32 L0x2001b980, sint32 L0x2001b984,
sint32 L0x2001b988, sint32 L0x2001b98c, sint32 L0x2001b990, sint32 L0x2001b994,
sint32 L0x2001b998, sint32 L0x2001b99c, sint32 L0x2001b9a0, sint32 L0x2001b9a4,
sint32 L0x2001b9a8, sint32 L0x2001b9ac, sint32 L0x2001b9b0, sint32 L0x2001b9b4,
sint32 L0x2001b9b8, sint32 L0x2001b9bc, sint32 L0x2001b9c0, sint32 L0x2001b9c4,
sint32 L0x2001b9c8, sint32 L0x2001b9cc, sint32 L0x2001b9d0, sint32 L0x2001b9d4,
sint32 L0x2001b9d8, sint32 L0x2001b9dc, sint32 L0x2001b9e0, sint32 L0x2001b9e4,
sint32 L0x2001b9e8, sint32 L0x2001b9ec, sint32 L0x2001b9f0, sint32 L0x2001b9f4,
sint32 L0x2001b9f8, sint32 L0x2001b9fc, sint32 L0x2001ba00, sint32 L0x2001ba04,
sint32 L0x2001ba08, sint32 L0x2001ba0c, sint32 L0x2001ba10, sint32 L0x2001ba14,
sint32 L0x2001ba18, sint32 L0x2001ba1c, sint32 L0x2001ba20, sint32 L0x2001ba24,
sint32 L0x2001ba28, sint32 L0x2001ba2c, sint32 L0x2001ba30, sint32 L0x2001ba34,
sint32 L0x2001ba38, sint32 L0x2001ba3c, sint32 L0x2001ba40, sint32 L0x2001ba44,
sint32 L0x2001ba48, sint32 L0x2001ba4c, sint32 L0x2001ba50, sint32 L0x2001ba54,
sint32 L0x2001ba58, sint32 L0x2001ba5c, sint32 L0x2001ba60, sint32 L0x2001ba64,
sint32 L0x2001ba68, sint32 L0x2001ba6c, sint32 L0x2001ba70, sint32 L0x2001ba74,
sint32 L0x2001ba78, sint32 L0x2001ba7c, sint32 L0x2001ba80, sint32 L0x2001ba84,
sint32 L0x2001ba88, sint32 L0x2001ba8c, sint32 L0x2001ba90, sint32 L0x2001ba94,
sint32 L0x2001ba98, sint32 L0x2001ba9c, sint32 L0x2001baa0, sint32 L0x2001baa4,
sint32 L0x2001baa8, sint32 L0x2001baac, sint32 L0x2001bab0, sint32 L0x2001bab4,
sint32 L0x2001bab8, sint32 L0x2001babc, sint32 L0x2001bac0, sint32 L0x2001bac4,
sint32 L0x2001bac8, sint32 L0x2001bacc, sint32 L0x2001bad0, sint32 L0x2001bad4,
sint32 L0x2001bad8, sint32 L0x2001badc, sint32 L0x2001bae0, sint32 L0x2001bae4,
sint32 L0x2001bae8, sint32 L0x2001baec, sint32 L0x2001baf0, sint32 L0x2001baf4,
sint32 L0x2001baf8, sint32 L0x2001bafc, sint32 L0x2001bb00, sint32 L0x2001bb04,
sint32 L0x2001bb08, sint32 L0x2001bb0c, sint32 L0x2001bb10, sint32 L0x2001bb14,
sint32 L0x2001bb18, sint32 L0x2001bb1c, sint32 L0x2001bb20, sint32 L0x2001bb24,
sint32 L0x2001bb28, sint32 L0x2001bb2c, sint32 L0x2001bb30, sint32 L0x2001bb34,
sint32 L0x2001bb38, sint32 L0x2001bb3c, sint32 L0x2001bb40, sint32 L0x2001bb44,
sint32 L0x2001bb48, sint32 L0x2001bb4c, sint32 L0x2001bb50, sint32 L0x2001bb54,
sint32 L0x2001bb58, sint32 L0x2001bb5c, sint32 L0x2001bb60, sint32 L0x2001bb64,
sint32 L0x2001bb68, sint32 L0x2001bb6c, sint32 L0x2001bb70, sint32 L0x2001bb74,
sint32 L0x2001bb78, sint32 L0x2001bb7c, sint32 L0x2001bb80, sint32 L0x2001bb84,
sint32 L0x2001bb88, sint32 L0x2001bb8c, sint32 L0x2001bb90, sint32 L0x2001bb94,
sint32 L0x2001bb98, sint32 L0x2001bb9c, sint32 L0x2001bba0, sint32 L0x2001bba4,
sint32 L0x2001bba8, sint32 L0x2001bbac, sint32 L0x2001bbb0, sint32 L0x2001bbb4,
sint32 L0x2001bbb8, sint32 L0x2001bbbc, sint32 L0x2001bbc0, sint32 L0x2001bbc4,
sint32 L0x2001bbc8, sint32 L0x2001bbcc, sint32 L0x2001bbd0, sint32 L0x2001bbd4,
sint32 L0x2001bbd8, sint32 L0x2001bbdc, sint32 L0x2001bbe0, sint32 L0x2001bbe4,
sint32 L0x2001bbe8, sint32 L0x2001bbec, sint32 L0x2001bbf0, sint32 L0x2001bbf4,
sint32 L0x2001bbf8, sint32 L0x2001bbfc, sint32 L0x2001bc00, sint32 L0x2001bc04,
sint32 L0x2001bc08, sint32 L0x2001bc0c, sint32 L0x2001bc10, sint32 L0x2001bc14,
sint32 L0x2001bc18, sint32 L0x2001bc1c, sint32 L0x2001bc20, sint32 L0x2001bc24,
sint32 L0x2001bc28, sint32 L0x2001bc2c, sint32 L0x2001bc30, sint32 L0x2001bc34,
sint32 L0x2001bc38, sint32 L0x2001bc3c, sint32 L0x2001bc40, sint32 L0x2001bc44,
sint32 L0x2001bc48, sint32 L0x2001bc4c, sint32 L0x2001bc50, sint32 L0x2001bc54,
sint32 L0x2001bc58, sint32 L0x2001bc5c, sint32 L0x2001bc60, sint32 L0x2001bc64,
sint32 L0x2001bc68, sint32 L0x2001bc6c, sint32 L0x2001bc70, sint32 L0x2001bc74,
sint32 L0x2001bc78, sint32 L0x2001bc7c, sint32 L0x2001bc80, sint32 L0x2001bc84,
sint32 L0x2001bc88, sint32 L0x2001bc8c, sint32 L0x2001bc90, sint32 L0x2001bc94,
sint32 L0x2001bc98, sint32 L0x2001bc9c, sint32 L0x2001bca0, sint32 L0x2001bca4,
sint32 L0x2001bca8, sint32 L0x2001bcac, sint32 L0x2001bcb0, sint32 L0x2001bcb4,
sint32 L0x2001bcb8, sint32 L0x2001bcbc, sint32 L0x2001bcc0, sint32 L0x2001bcc4,
sint32 L0x2001bcc8, sint32 L0x2001bccc, sint32 L0x2001bcd0, sint32 L0x2001bcd4,
sint32 L0x2001bcd8, sint32 L0x2001bcdc, sint32 L0x2001bce0, sint32 L0x2001bce4,
sint32 L0x2001bce8, sint32 L0x2001bcec, sint32 L0x2001bcf0, sint32 L0x2001bcf4,
sint32 L0x2001bcf8, sint32 L0x2001bcfc, sint32 L0x2001bd00, sint32 L0x2001bd04,
sint32 L0x2001bd08, sint32 L0x2001bd0c, sint32 L0x2001bd10, sint32 L0x2001bd14,
sint32 L0x2001bd18, sint32 L0x2001bd1c, sint32 L0x2001bd20, sint32 L0x2001bd24,
sint32 L0x2001bd28, sint32 L0x2001bd2c, sint32 L0x2001bd30, sint32 L0x2001bd34,
sint32 L0x2001bd38, sint32 L0x2001bd3c, sint32 L0x2001bd40, sint32 L0x2001bd44,
sint32 L0x2001bd48, sint32 L0x2001bd4c, sint32 L0x2001bd50, sint32 L0x2001bd54,
sint32 L0x2001bd58, sint32 L0x2001bd5c, sint32 L0x2001bd60, sint32 L0x2001bd64,
sint32 L0x2001bd68, sint32 L0x2001bd6c, sint32 L0x2001bd70, sint32 L0x2001bd74,
sint32 L0x2001bd78, sint32 L0x2001bd7c, sint32 L0x2001bd80, sint32 L0x2001bd84,
sint32 L0x2001bd88, sint32 L0x2001bd8c, sint32 L0x2001bd90, sint32 L0x2001bd94,
sint32 L0x2001bd98, sint32 L0x2001bd9c, sint32 L0x2001bda0, sint32 L0x2001bda4,
sint32 L0x2001bda8, sint32 L0x2001bdac, sint32 L0x2001bdb0, sint32 L0x2001bdb4,
sint32 L0x2001bdb8, sint32 L0x2001bdbc, sint32 L0x2001bdc0, sint32 L0x2001bdc4,
sint32 L0x2001bdc8, sint32 L0x2001bdcc, sint32 L0x2001bdd0, sint32 L0x2001bdd4,
sint32 L0x2001bdd8, sint32 L0x2001bddc, sint32 L0x2001bde0, sint32 L0x2001bde4,
sint32 L0x2001bde8, sint32 L0x2001bdec, sint32 L0x2001bdf0, sint32 L0x2001bdf4,
sint32 L0x2001bdf8, sint32 L0x2001bdfc, sint32 L0x2001be00, sint32 L0x2001be04,
sint32 L0x2001be08, sint32 L0x2001be0c, sint32 L0x2001be10, sint32 L0x2001be14,
sint32 L0x2001be18, sint32 L0x2001be1c, sint32 L0x2001be20, sint32 L0x2001be24,
sint32 L0x2001be28, sint32 L0x2001be2c, sint32 L0x2001be30, sint32 L0x2001be34,
sint32 L0x2001be38, sint32 L0x2001be3c, sint32 L0x2001be40, sint32 L0x2001be44,
sint32 L0x2001be48, sint32 L0x2001be4c, sint32 L0x2001be50, sint32 L0x2001be54,
sint32 L0x2001be58, sint32 L0x2001be5c, sint32 L0x2001be60, sint32 L0x2001be64,
sint32 L0x2001be68, sint32 L0x2001be6c, sint32 L0x2001be70, sint32 L0x2001be74,
sint32 L0x2001be78, sint32 L0x2001be7c, sint32 L0x2001be80, sint32 L0x2001be84,
sint32 L0x2001be88, sint32 L0x2001be8c, sint32 L0x2001be90, sint32 L0x2001be94,
sint32 L0x2001be98, sint32 L0x2001be9c, sint32 L0x2001bea0, sint32 L0x2001bea4,
sint32 L0x2001bea8, sint32 L0x2001beac, sint32 L0x2001beb0, sint32 L0x2001beb4,
sint32 L0x2001beb8, sint32 L0x2001bebc, sint32 L0x2001bec0, sint32 L0x2001bec4,
sint32 L0x2001bec8, sint32 L0x2001becc, sint32 L0x2001bed0, sint32 L0x2001bed4,
sint32 L0x2001bed8, sint32 L0x2001bedc, sint32 L0x2001bee0, sint32 L0x2001bee4,
sint32 L0x2001bee8, sint32 L0x2001beec, sint32 L0x2001bef0, sint32 L0x2001bef4,
sint32 L0x2001bef8, sint32 L0x2001befc, sint32 L0x2001bf00, sint32 L0x2001bf04,
sint32 L0x2001bf08, sint32 L0x2001bf0c, sint32 L0x2001bf10, sint32 L0x2001bf14,
sint32 L0x2001bf18, sint32 L0x2001bf1c, sint32 L0x2001bf20, sint32 L0x2001bf24,
sint32 L0x2001bf28, sint32 L0x2001bf2c, sint32 L0x2001bf30, sint32 L0x2001bf34,
sint32 L0x2001bf38, sint32 L0x2001bf3c, sint32 L0x2001bf40, sint32 L0x2001bf44,
sint32 L0x2001bf48, sint32 L0x2001bf4c, sint32 L0x2001bf50, sint32 L0x2001bf54,
sint32 L0x2001bf58, sint32 L0x2001bf5c, sint32 L0x2001bf60, sint32 L0x2001bf64,
sint32 L0x2001bf68, sint32 L0x2001bf6c, sint32 L0x2001bf70, sint32 L0x2001bf74,
sint32 L0x2001bf78, sint32 L0x2001bf7c, sint32 L0x2001bf80, sint32 L0x2001bf84,
sint32 L0x2001bf88, sint32 L0x2001bf8c, sint32 L0x2001bf90, sint32 L0x2001bf94,
sint32 L0x2001bf98, sint32 L0x2001bf9c, sint32 L0x2001bfa0, sint32 L0x2001bfa4,
sint32 L0x2001bfa8, sint32 L0x2001bfac, sint32 L0x2001bfb0, sint32 L0x2001bfb4,
sint32 L0x2001bfb8, sint32 L0x2001bfbc, sint32 L0x2001bfc0, sint32 L0x2001bfc4,
bit x,
bit inp_poly00, bit inp_poly01, bit inp_poly02, bit inp_poly03,
bit inp_poly04, bit inp_poly05, bit inp_poly06, bit inp_poly07,
bit inp_poly08, bit inp_poly09, bit inp_poly0a, bit inp_poly0b,
bit inp_poly0c, bit inp_poly0d, bit inp_poly0e, bit inp_poly0f,
bit inp_poly10, bit inp_poly11, bit inp_poly12, bit inp_poly13,
bit inp_poly14, bit inp_poly15, bit inp_poly16, bit inp_poly17,
bit inp_poly18, bit inp_poly19, bit inp_poly1a, bit inp_poly1b,
bit inp_poly1c, bit inp_poly1d, bit inp_poly1e, bit inp_poly1f,
bit inp_poly20, bit inp_poly21, bit inp_poly22, bit inp_poly23,
bit inp_poly24, bit inp_poly25, bit inp_poly26, bit inp_poly27,
bit inp_poly28, bit inp_poly29, bit inp_poly2a, bit inp_poly2b,
bit inp_poly2c, bit inp_poly2d, bit inp_poly2e, bit inp_poly2f,
bit inp_poly30, bit inp_poly31, bit inp_poly32, bit inp_poly33,
bit inp_poly34, bit inp_poly35, bit inp_poly36, bit inp_poly37,
bit inp_poly38, bit inp_poly39, bit inp_poly3a, bit inp_poly3b,
bit inp_poly3c, bit inp_poly3d, bit inp_poly3e, bit inp_poly3f,
bit inp_poly40, bit inp_poly41, bit inp_poly42, bit inp_poly43,
bit inp_poly44, bit inp_poly45, bit inp_poly46, bit inp_poly47,
bit inp_poly48, bit inp_poly49, bit inp_poly4a, bit inp_poly4b,
bit inp_poly4c, bit inp_poly4d, bit inp_poly4e, bit inp_poly4f,
bit inp_poly50, bit inp_poly51, bit inp_poly52, bit inp_poly53,
bit inp_poly54, bit inp_poly55, bit inp_poly56, bit inp_poly57,
bit inp_poly58, bit inp_poly59, bit inp_poly5a, bit inp_poly5b,
bit inp_poly5c, bit inp_poly5d, bit inp_poly5e, bit inp_poly5f,
bit inp_poly60, bit inp_poly61, bit inp_poly62, bit inp_poly63,
bit inp_poly64, bit inp_poly65, bit inp_poly66, bit inp_poly67,
bit inp_poly68, bit inp_poly69, bit inp_poly6a, bit inp_poly6b,
bit inp_poly6c, bit inp_poly6d, bit inp_poly6e, bit inp_poly6f,
bit inp_poly70, bit inp_poly71, bit inp_poly72, bit inp_poly73,
bit inp_poly74, bit inp_poly75, bit inp_poly76, bit inp_poly77,
bit inp_poly78, bit inp_poly79, bit inp_poly7a, bit inp_poly7b,
bit inp_poly7c, bit inp_poly7d, bit inp_poly7e, bit inp_poly7f,
bit inp_poly80, bit inp_poly81, bit inp_poly82, bit inp_poly83,
bit inp_poly84, bit inp_poly85, bit inp_poly86, bit inp_poly87,
bit inp_poly88, bit inp_poly89, bit inp_poly8a, bit inp_poly8b,
bit inp_poly8c, bit inp_poly8d, bit inp_poly8e, bit inp_poly8f,
bit inp_poly90, bit inp_poly91, bit inp_poly92, bit inp_poly93,
bit inp_poly94, bit inp_poly95, bit inp_poly96, bit inp_poly97,
bit inp_poly98, bit inp_poly99, bit inp_poly9a, bit inp_poly9b,
bit inp_poly9c, bit inp_poly9d, bit inp_poly9e, bit inp_poly9f,
bit inp_polya0, bit inp_polya1, bit inp_polya2, bit inp_polya3,
bit inp_polya4, bit inp_polya5, bit inp_polya6, bit inp_polya7,
bit inp_polya8, bit inp_polya9, bit inp_polyaa, bit inp_polyab,
bit inp_polyac, bit inp_polyad, bit inp_polyae, bit inp_polyaf,
bit inp_polyb0, bit inp_polyb1, bit inp_polyb2, bit inp_polyb3,
bit inp_polyb4, bit inp_polyb5, bit inp_polyb6, bit inp_polyb7,
bit inp_polyb8, bit inp_polyb9, bit inp_polyba, bit inp_polybb,
bit inp_polybc, bit inp_polybd, bit inp_polybe, bit inp_polybf,
bit inp_polyc0, bit inp_polyc1, bit inp_polyc2, bit inp_polyc3,
bit inp_polyc4, bit inp_polyc5, bit inp_polyc6, bit inp_polyc7,
bit inp_polyc8, bit inp_polyc9, bit inp_polyca, bit inp_polycb,
bit inp_polycc, bit inp_polycd, bit inp_polyce, bit inp_polycf,
bit inp_polyd0, bit inp_polyd1, bit inp_polyd2, bit inp_polyd3,
bit inp_polyd4, bit inp_polyd5, bit inp_polyd6, bit inp_polyd7,
bit inp_polyd8, bit inp_polyd9, bit inp_polyda, bit inp_polydb,
bit inp_polydc, bit inp_polydd, bit inp_polyde, bit inp_polydf,
bit inp_polye0, bit inp_polye1, bit inp_polye2, bit inp_polye3,
bit inp_polye4, bit inp_polye5, bit inp_polye6, bit inp_polye7,
bit inp_polye8, bit inp_polye9, bit inp_polyea, bit inp_polyeb,
bit inp_polyec, bit inp_polyed, bit inp_polyee, bit inp_polyef,
bit inp_polyf0, bit inp_polyf1, bit inp_polyf2, bit inp_polyf3,
bit inp_polyf4, bit inp_polyf5, bit inp_polyf6, bit inp_polyf7,
bit inp_polyf8, bit inp_polyf9, bit inp_polyfa, bit inp_polyfb,
bit inp_polyfc, bit inp_polyfd, bit inp_polyfe, bit inp_polyff
,
bit segment0,
bit segment00, bit segment01, bit segment02, bit segment03,
bit segment04, bit segment05, bit segment06, bit segment07,
bit segment08, bit segment09, bit segment0a, bit segment0b,
bit segment0c, bit segment0d, bit segment0e, bit segment0f,
bit segment1,
bit segment10, bit segment11, bit segment12, bit segment13,
bit segment14, bit segment15, bit segment16, bit segment17,
bit segment18, bit segment19, bit segment1a, bit segment1b,
bit segment1c, bit segment1d, bit segment1e, bit segment1f,
bit segment2,
bit segment20, bit segment21, bit segment22, bit segment23,
bit segment24, bit segment25, bit segment26, bit segment27,
bit segment28, bit segment29, bit segment2a, bit segment2b,
bit segment2c, bit segment2d, bit segment2e, bit segment2f,
bit segment3,
bit segment30, bit segment31, bit segment32, bit segment33,
bit segment34, bit segment35, bit segment36, bit segment37,
bit segment38, bit segment39, bit segment3a, bit segment3b,
bit segment3c, bit segment3d, bit segment3e, bit segment3f
) =
{
and [
segment00**2 = 
L0x2001afc8*x** 0+L0x2001afcc*x** 1+L0x2001afd0*x** 2+L0x2001afd4*x** 3+
L0x2001afd8*x** 4+L0x2001afdc*x** 5+L0x2001afe0*x** 6+L0x2001afe4*x** 7+
L0x2001afe8*x** 8+L0x2001afec*x** 9+L0x2001aff0*x**10+L0x2001aff4*x**11+
L0x2001aff8*x**12+L0x2001affc*x**13+L0x2001b000*x**14+L0x2001b004*x**15,
segment01**2 = 
L0x2001b008*x** 0+L0x2001b00c*x** 1+L0x2001b010*x** 2+L0x2001b014*x** 3+
L0x2001b018*x** 4+L0x2001b01c*x** 5+L0x2001b020*x** 6+L0x2001b024*x** 7+
L0x2001b028*x** 8+L0x2001b02c*x** 9+L0x2001b030*x**10+L0x2001b034*x**11+
L0x2001b038*x**12+L0x2001b03c*x**13+L0x2001b040*x**14+L0x2001b044*x**15,
segment02**2 = 
L0x2001b048*x** 0+L0x2001b04c*x** 1+L0x2001b050*x** 2+L0x2001b054*x** 3+
L0x2001b058*x** 4+L0x2001b05c*x** 5+L0x2001b060*x** 6+L0x2001b064*x** 7+
L0x2001b068*x** 8+L0x2001b06c*x** 9+L0x2001b070*x**10+L0x2001b074*x**11+
L0x2001b078*x**12+L0x2001b07c*x**13+L0x2001b080*x**14+L0x2001b084*x**15,
segment03**2 = 
L0x2001b088*x** 0+L0x2001b08c*x** 1+L0x2001b090*x** 2+L0x2001b094*x** 3+
L0x2001b098*x** 4+L0x2001b09c*x** 5+L0x2001b0a0*x** 6+L0x2001b0a4*x** 7+
L0x2001b0a8*x** 8+L0x2001b0ac*x** 9+L0x2001b0b0*x**10+L0x2001b0b4*x**11+
L0x2001b0b8*x**12+L0x2001b0bc*x**13+L0x2001b0c0*x**14+L0x2001b0c4*x**15,
segment04**2 = 
L0x2001b0c8*x** 0+L0x2001b0cc*x** 1+L0x2001b0d0*x** 2+L0x2001b0d4*x** 3+
L0x2001b0d8*x** 4+L0x2001b0dc*x** 5+L0x2001b0e0*x** 6+L0x2001b0e4*x** 7+
L0x2001b0e8*x** 8+L0x2001b0ec*x** 9+L0x2001b0f0*x**10+L0x2001b0f4*x**11+
L0x2001b0f8*x**12+L0x2001b0fc*x**13+L0x2001b100*x**14+L0x2001b104*x**15,
segment05**2 = 
L0x2001b108*x** 0+L0x2001b10c*x** 1+L0x2001b110*x** 2+L0x2001b114*x** 3+
L0x2001b118*x** 4+L0x2001b11c*x** 5+L0x2001b120*x** 6+L0x2001b124*x** 7+
L0x2001b128*x** 8+L0x2001b12c*x** 9+L0x2001b130*x**10+L0x2001b134*x**11+
L0x2001b138*x**12+L0x2001b13c*x**13+L0x2001b140*x**14+L0x2001b144*x**15,
segment06**2 = 
L0x2001b148*x** 0+L0x2001b14c*x** 1+L0x2001b150*x** 2+L0x2001b154*x** 3+
L0x2001b158*x** 4+L0x2001b15c*x** 5+L0x2001b160*x** 6+L0x2001b164*x** 7+
L0x2001b168*x** 8+L0x2001b16c*x** 9+L0x2001b170*x**10+L0x2001b174*x**11+
L0x2001b178*x**12+L0x2001b17c*x**13+L0x2001b180*x**14+L0x2001b184*x**15,
segment07**2 = 
L0x2001b188*x** 0+L0x2001b18c*x** 1+L0x2001b190*x** 2+L0x2001b194*x** 3+
L0x2001b198*x** 4+L0x2001b19c*x** 5+L0x2001b1a0*x** 6+L0x2001b1a4*x** 7+
L0x2001b1a8*x** 8+L0x2001b1ac*x** 9+L0x2001b1b0*x**10+L0x2001b1b4*x**11+
L0x2001b1b8*x**12+L0x2001b1bc*x**13+L0x2001b1c0*x**14+L0x2001b1c4*x**15,
segment08**2 = 
L0x2001b1c8*x** 0+L0x2001b1cc*x** 1+L0x2001b1d0*x** 2+L0x2001b1d4*x** 3+
L0x2001b1d8*x** 4+L0x2001b1dc*x** 5+L0x2001b1e0*x** 6+L0x2001b1e4*x** 7+
L0x2001b1e8*x** 8+L0x2001b1ec*x** 9+L0x2001b1f0*x**10+L0x2001b1f4*x**11+
L0x2001b1f8*x**12+L0x2001b1fc*x**13+L0x2001b200*x**14+L0x2001b204*x**15,
segment09**2 = 
L0x2001b208*x** 0+L0x2001b20c*x** 1+L0x2001b210*x** 2+L0x2001b214*x** 3+
L0x2001b218*x** 4+L0x2001b21c*x** 5+L0x2001b220*x** 6+L0x2001b224*x** 7+
L0x2001b228*x** 8+L0x2001b22c*x** 9+L0x2001b230*x**10+L0x2001b234*x**11+
L0x2001b238*x**12+L0x2001b23c*x**13+L0x2001b240*x**14+L0x2001b244*x**15,
segment0a**2 = 
L0x2001b248*x** 0+L0x2001b24c*x** 1+L0x2001b250*x** 2+L0x2001b254*x** 3+
L0x2001b258*x** 4+L0x2001b25c*x** 5+L0x2001b260*x** 6+L0x2001b264*x** 7+
L0x2001b268*x** 8+L0x2001b26c*x** 9+L0x2001b270*x**10+L0x2001b274*x**11+
L0x2001b278*x**12+L0x2001b27c*x**13+L0x2001b280*x**14+L0x2001b284*x**15,
segment0b**2 = 
L0x2001b288*x** 0+L0x2001b28c*x** 1+L0x2001b290*x** 2+L0x2001b294*x** 3+
L0x2001b298*x** 4+L0x2001b29c*x** 5+L0x2001b2a0*x** 6+L0x2001b2a4*x** 7+
L0x2001b2a8*x** 8+L0x2001b2ac*x** 9+L0x2001b2b0*x**10+L0x2001b2b4*x**11+
L0x2001b2b8*x**12+L0x2001b2bc*x**13+L0x2001b2c0*x**14+L0x2001b2c4*x**15,
segment0c**2 = 
L0x2001b2c8*x** 0+L0x2001b2cc*x** 1+L0x2001b2d0*x** 2+L0x2001b2d4*x** 3+
L0x2001b2d8*x** 4+L0x2001b2dc*x** 5+L0x2001b2e0*x** 6+L0x2001b2e4*x** 7+
L0x2001b2e8*x** 8+L0x2001b2ec*x** 9+L0x2001b2f0*x**10+L0x2001b2f4*x**11+
L0x2001b2f8*x**12+L0x2001b2fc*x**13+L0x2001b300*x**14+L0x2001b304*x**15,
segment0d**2 = 
L0x2001b308*x** 0+L0x2001b30c*x** 1+L0x2001b310*x** 2+L0x2001b314*x** 3+
L0x2001b318*x** 4+L0x2001b31c*x** 5+L0x2001b320*x** 6+L0x2001b324*x** 7+
L0x2001b328*x** 8+L0x2001b32c*x** 9+L0x2001b330*x**10+L0x2001b334*x**11+
L0x2001b338*x**12+L0x2001b33c*x**13+L0x2001b340*x**14+L0x2001b344*x**15,
segment0e**2 = 
L0x2001b348*x** 0+L0x2001b34c*x** 1+L0x2001b350*x** 2+L0x2001b354*x** 3+
L0x2001b358*x** 4+L0x2001b35c*x** 5+L0x2001b360*x** 6+L0x2001b364*x** 7+
L0x2001b368*x** 8+L0x2001b36c*x** 9+L0x2001b370*x**10+L0x2001b374*x**11+
L0x2001b378*x**12+L0x2001b37c*x**13+L0x2001b380*x**14+L0x2001b384*x**15,
segment0f**2 = 
L0x2001b388*x** 0+L0x2001b38c*x** 1+L0x2001b390*x** 2+L0x2001b394*x** 3+
L0x2001b398*x** 4+L0x2001b39c*x** 5+L0x2001b3a0*x** 6+L0x2001b3a4*x** 7+
L0x2001b3a8*x** 8+L0x2001b3ac*x** 9+L0x2001b3b0*x**10+L0x2001b3b4*x**11+
L0x2001b3b8*x**12+L0x2001b3bc*x**13+L0x2001b3c0*x**14+L0x2001b3c4*x**15,
segment0 = 
(segment00**2)*x** 0+(segment01**2)*x**16+(segment02**2)*x**32+
(segment03**2)*x**48+(segment04**2)*x**64+(segment05**2)*x**80+
(segment06**2)*x**96+(segment07**2)*x**112+(segment08**2)*x**128+
(segment09**2)*x**144+(segment0a**2)*x**160+(segment0b**2)*x**176+
(segment0c**2)*x**192+(segment0d**2)*x**208+(segment0e**2)*x**224+
(segment0f**2)*x**240
,
segment10**2 = 
L0x2001b3c8*x** 0+L0x2001b3cc*x** 1+L0x2001b3d0*x** 2+L0x2001b3d4*x** 3+
L0x2001b3d8*x** 4+L0x2001b3dc*x** 5+L0x2001b3e0*x** 6+L0x2001b3e4*x** 7+
L0x2001b3e8*x** 8+L0x2001b3ec*x** 9+L0x2001b3f0*x**10+L0x2001b3f4*x**11+
L0x2001b3f8*x**12+L0x2001b3fc*x**13+L0x2001b400*x**14+L0x2001b404*x**15,
segment11**2 = 
L0x2001b408*x** 0+L0x2001b40c*x** 1+L0x2001b410*x** 2+L0x2001b414*x** 3+
L0x2001b418*x** 4+L0x2001b41c*x** 5+L0x2001b420*x** 6+L0x2001b424*x** 7+
L0x2001b428*x** 8+L0x2001b42c*x** 9+L0x2001b430*x**10+L0x2001b434*x**11+
L0x2001b438*x**12+L0x2001b43c*x**13+L0x2001b440*x**14+L0x2001b444*x**15,
segment12**2 = 
L0x2001b448*x** 0+L0x2001b44c*x** 1+L0x2001b450*x** 2+L0x2001b454*x** 3+
L0x2001b458*x** 4+L0x2001b45c*x** 5+L0x2001b460*x** 6+L0x2001b464*x** 7+
L0x2001b468*x** 8+L0x2001b46c*x** 9+L0x2001b470*x**10+L0x2001b474*x**11+
L0x2001b478*x**12+L0x2001b47c*x**13+L0x2001b480*x**14+L0x2001b484*x**15,
segment13**2 = 
L0x2001b488*x** 0+L0x2001b48c*x** 1+L0x2001b490*x** 2+L0x2001b494*x** 3+
L0x2001b498*x** 4+L0x2001b49c*x** 5+L0x2001b4a0*x** 6+L0x2001b4a4*x** 7+
L0x2001b4a8*x** 8+L0x2001b4ac*x** 9+L0x2001b4b0*x**10+L0x2001b4b4*x**11+
L0x2001b4b8*x**12+L0x2001b4bc*x**13+L0x2001b4c0*x**14+L0x2001b4c4*x**15,
segment14**2 = 
L0x2001b4c8*x** 0+L0x2001b4cc*x** 1+L0x2001b4d0*x** 2+L0x2001b4d4*x** 3+
L0x2001b4d8*x** 4+L0x2001b4dc*x** 5+L0x2001b4e0*x** 6+L0x2001b4e4*x** 7+
L0x2001b4e8*x** 8+L0x2001b4ec*x** 9+L0x2001b4f0*x**10+L0x2001b4f4*x**11+
L0x2001b4f8*x**12+L0x2001b4fc*x**13+L0x2001b500*x**14+L0x2001b504*x**15,
segment15**2 = 
L0x2001b508*x** 0+L0x2001b50c*x** 1+L0x2001b510*x** 2+L0x2001b514*x** 3+
L0x2001b518*x** 4+L0x2001b51c*x** 5+L0x2001b520*x** 6+L0x2001b524*x** 7+
L0x2001b528*x** 8+L0x2001b52c*x** 9+L0x2001b530*x**10+L0x2001b534*x**11+
L0x2001b538*x**12+L0x2001b53c*x**13+L0x2001b540*x**14+L0x2001b544*x**15,
segment16**2 = 
L0x2001b548*x** 0+L0x2001b54c*x** 1+L0x2001b550*x** 2+L0x2001b554*x** 3+
L0x2001b558*x** 4+L0x2001b55c*x** 5+L0x2001b560*x** 6+L0x2001b564*x** 7+
L0x2001b568*x** 8+L0x2001b56c*x** 9+L0x2001b570*x**10+L0x2001b574*x**11+
L0x2001b578*x**12+L0x2001b57c*x**13+L0x2001b580*x**14+L0x2001b584*x**15,
segment17**2 = 
L0x2001b588*x** 0+L0x2001b58c*x** 1+L0x2001b590*x** 2+L0x2001b594*x** 3+
L0x2001b598*x** 4+L0x2001b59c*x** 5+L0x2001b5a0*x** 6+L0x2001b5a4*x** 7+
L0x2001b5a8*x** 8+L0x2001b5ac*x** 9+L0x2001b5b0*x**10+L0x2001b5b4*x**11+
L0x2001b5b8*x**12+L0x2001b5bc*x**13+L0x2001b5c0*x**14+L0x2001b5c4*x**15,
segment18**2 = 
L0x2001b5c8*x** 0+L0x2001b5cc*x** 1+L0x2001b5d0*x** 2+L0x2001b5d4*x** 3+
L0x2001b5d8*x** 4+L0x2001b5dc*x** 5+L0x2001b5e0*x** 6+L0x2001b5e4*x** 7+
L0x2001b5e8*x** 8+L0x2001b5ec*x** 9+L0x2001b5f0*x**10+L0x2001b5f4*x**11+
L0x2001b5f8*x**12+L0x2001b5fc*x**13+L0x2001b600*x**14+L0x2001b604*x**15,
segment19**2 = 
L0x2001b608*x** 0+L0x2001b60c*x** 1+L0x2001b610*x** 2+L0x2001b614*x** 3+
L0x2001b618*x** 4+L0x2001b61c*x** 5+L0x2001b620*x** 6+L0x2001b624*x** 7+
L0x2001b628*x** 8+L0x2001b62c*x** 9+L0x2001b630*x**10+L0x2001b634*x**11+
L0x2001b638*x**12+L0x2001b63c*x**13+L0x2001b640*x**14+L0x2001b644*x**15,
segment1a**2 = 
L0x2001b648*x** 0+L0x2001b64c*x** 1+L0x2001b650*x** 2+L0x2001b654*x** 3+
L0x2001b658*x** 4+L0x2001b65c*x** 5+L0x2001b660*x** 6+L0x2001b664*x** 7+
L0x2001b668*x** 8+L0x2001b66c*x** 9+L0x2001b670*x**10+L0x2001b674*x**11+
L0x2001b678*x**12+L0x2001b67c*x**13+L0x2001b680*x**14+L0x2001b684*x**15,
segment1b**2 = 
L0x2001b688*x** 0+L0x2001b68c*x** 1+L0x2001b690*x** 2+L0x2001b694*x** 3+
L0x2001b698*x** 4+L0x2001b69c*x** 5+L0x2001b6a0*x** 6+L0x2001b6a4*x** 7+
L0x2001b6a8*x** 8+L0x2001b6ac*x** 9+L0x2001b6b0*x**10+L0x2001b6b4*x**11+
L0x2001b6b8*x**12+L0x2001b6bc*x**13+L0x2001b6c0*x**14+L0x2001b6c4*x**15,
segment1c**2 = 
L0x2001b6c8*x** 0+L0x2001b6cc*x** 1+L0x2001b6d0*x** 2+L0x2001b6d4*x** 3+
L0x2001b6d8*x** 4+L0x2001b6dc*x** 5+L0x2001b6e0*x** 6+L0x2001b6e4*x** 7+
L0x2001b6e8*x** 8+L0x2001b6ec*x** 9+L0x2001b6f0*x**10+L0x2001b6f4*x**11+
L0x2001b6f8*x**12+L0x2001b6fc*x**13+L0x2001b700*x**14+L0x2001b704*x**15,
segment1d**2 = 
L0x2001b708*x** 0+L0x2001b70c*x** 1+L0x2001b710*x** 2+L0x2001b714*x** 3+
L0x2001b718*x** 4+L0x2001b71c*x** 5+L0x2001b720*x** 6+L0x2001b724*x** 7+
L0x2001b728*x** 8+L0x2001b72c*x** 9+L0x2001b730*x**10+L0x2001b734*x**11+
L0x2001b738*x**12+L0x2001b73c*x**13+L0x2001b740*x**14+L0x2001b744*x**15,
segment1e**2 = 
L0x2001b748*x** 0+L0x2001b74c*x** 1+L0x2001b750*x** 2+L0x2001b754*x** 3+
L0x2001b758*x** 4+L0x2001b75c*x** 5+L0x2001b760*x** 6+L0x2001b764*x** 7+
L0x2001b768*x** 8+L0x2001b76c*x** 9+L0x2001b770*x**10+L0x2001b774*x**11+
L0x2001b778*x**12+L0x2001b77c*x**13+L0x2001b780*x**14+L0x2001b784*x**15,
segment1f**2 = 
L0x2001b788*x** 0+L0x2001b78c*x** 1+L0x2001b790*x** 2+L0x2001b794*x** 3+
L0x2001b798*x** 4+L0x2001b79c*x** 5+L0x2001b7a0*x** 6+L0x2001b7a4*x** 7+
L0x2001b7a8*x** 8+L0x2001b7ac*x** 9+L0x2001b7b0*x**10+L0x2001b7b4*x**11+
L0x2001b7b8*x**12+L0x2001b7bc*x**13+L0x2001b7c0*x**14+L0x2001b7c4*x**15,
segment1 = 
(segment10**2)*x** 0+(segment11**2)*x**16+(segment12**2)*x**32+
(segment13**2)*x**48+(segment14**2)*x**64+(segment15**2)*x**80+
(segment16**2)*x**96+(segment17**2)*x**112+(segment18**2)*x**128+
(segment19**2)*x**144+(segment1a**2)*x**160+(segment1b**2)*x**176+
(segment1c**2)*x**192+(segment1d**2)*x**208+(segment1e**2)*x**224+
(segment1f**2)*x**240
,
segment20**2 = 
L0x2001b7c8*x** 0+L0x2001b7cc*x** 1+L0x2001b7d0*x** 2+L0x2001b7d4*x** 3+
L0x2001b7d8*x** 4+L0x2001b7dc*x** 5+L0x2001b7e0*x** 6+L0x2001b7e4*x** 7+
L0x2001b7e8*x** 8+L0x2001b7ec*x** 9+L0x2001b7f0*x**10+L0x2001b7f4*x**11+
L0x2001b7f8*x**12+L0x2001b7fc*x**13+L0x2001b800*x**14+L0x2001b804*x**15,
segment21**2 = 
L0x2001b808*x** 0+L0x2001b80c*x** 1+L0x2001b810*x** 2+L0x2001b814*x** 3+
L0x2001b818*x** 4+L0x2001b81c*x** 5+L0x2001b820*x** 6+L0x2001b824*x** 7+
L0x2001b828*x** 8+L0x2001b82c*x** 9+L0x2001b830*x**10+L0x2001b834*x**11+
L0x2001b838*x**12+L0x2001b83c*x**13+L0x2001b840*x**14+L0x2001b844*x**15,
segment22**2 = 
L0x2001b848*x** 0+L0x2001b84c*x** 1+L0x2001b850*x** 2+L0x2001b854*x** 3+
L0x2001b858*x** 4+L0x2001b85c*x** 5+L0x2001b860*x** 6+L0x2001b864*x** 7+
L0x2001b868*x** 8+L0x2001b86c*x** 9+L0x2001b870*x**10+L0x2001b874*x**11+
L0x2001b878*x**12+L0x2001b87c*x**13+L0x2001b880*x**14+L0x2001b884*x**15,
segment23**2 = 
L0x2001b888*x** 0+L0x2001b88c*x** 1+L0x2001b890*x** 2+L0x2001b894*x** 3+
L0x2001b898*x** 4+L0x2001b89c*x** 5+L0x2001b8a0*x** 6+L0x2001b8a4*x** 7+
L0x2001b8a8*x** 8+L0x2001b8ac*x** 9+L0x2001b8b0*x**10+L0x2001b8b4*x**11+
L0x2001b8b8*x**12+L0x2001b8bc*x**13+L0x2001b8c0*x**14+L0x2001b8c4*x**15,
segment24**2 = 
L0x2001b8c8*x** 0+L0x2001b8cc*x** 1+L0x2001b8d0*x** 2+L0x2001b8d4*x** 3+
L0x2001b8d8*x** 4+L0x2001b8dc*x** 5+L0x2001b8e0*x** 6+L0x2001b8e4*x** 7+
L0x2001b8e8*x** 8+L0x2001b8ec*x** 9+L0x2001b8f0*x**10+L0x2001b8f4*x**11+
L0x2001b8f8*x**12+L0x2001b8fc*x**13+L0x2001b900*x**14+L0x2001b904*x**15,
segment25**2 = 
L0x2001b908*x** 0+L0x2001b90c*x** 1+L0x2001b910*x** 2+L0x2001b914*x** 3+
L0x2001b918*x** 4+L0x2001b91c*x** 5+L0x2001b920*x** 6+L0x2001b924*x** 7+
L0x2001b928*x** 8+L0x2001b92c*x** 9+L0x2001b930*x**10+L0x2001b934*x**11+
L0x2001b938*x**12+L0x2001b93c*x**13+L0x2001b940*x**14+L0x2001b944*x**15,
segment26**2 = 
L0x2001b948*x** 0+L0x2001b94c*x** 1+L0x2001b950*x** 2+L0x2001b954*x** 3+
L0x2001b958*x** 4+L0x2001b95c*x** 5+L0x2001b960*x** 6+L0x2001b964*x** 7+
L0x2001b968*x** 8+L0x2001b96c*x** 9+L0x2001b970*x**10+L0x2001b974*x**11+
L0x2001b978*x**12+L0x2001b97c*x**13+L0x2001b980*x**14+L0x2001b984*x**15,
segment27**2 = 
L0x2001b988*x** 0+L0x2001b98c*x** 1+L0x2001b990*x** 2+L0x2001b994*x** 3+
L0x2001b998*x** 4+L0x2001b99c*x** 5+L0x2001b9a0*x** 6+L0x2001b9a4*x** 7+
L0x2001b9a8*x** 8+L0x2001b9ac*x** 9+L0x2001b9b0*x**10+L0x2001b9b4*x**11+
L0x2001b9b8*x**12+L0x2001b9bc*x**13+L0x2001b9c0*x**14+L0x2001b9c4*x**15,
segment28**2 = 
L0x2001b9c8*x** 0+L0x2001b9cc*x** 1+L0x2001b9d0*x** 2+L0x2001b9d4*x** 3+
L0x2001b9d8*x** 4+L0x2001b9dc*x** 5+L0x2001b9e0*x** 6+L0x2001b9e4*x** 7+
L0x2001b9e8*x** 8+L0x2001b9ec*x** 9+L0x2001b9f0*x**10+L0x2001b9f4*x**11+
L0x2001b9f8*x**12+L0x2001b9fc*x**13+L0x2001ba00*x**14+L0x2001ba04*x**15,
segment29**2 = 
L0x2001ba08*x** 0+L0x2001ba0c*x** 1+L0x2001ba10*x** 2+L0x2001ba14*x** 3+
L0x2001ba18*x** 4+L0x2001ba1c*x** 5+L0x2001ba20*x** 6+L0x2001ba24*x** 7+
L0x2001ba28*x** 8+L0x2001ba2c*x** 9+L0x2001ba30*x**10+L0x2001ba34*x**11+
L0x2001ba38*x**12+L0x2001ba3c*x**13+L0x2001ba40*x**14+L0x2001ba44*x**15,
segment2a**2 = 
L0x2001ba48*x** 0+L0x2001ba4c*x** 1+L0x2001ba50*x** 2+L0x2001ba54*x** 3+
L0x2001ba58*x** 4+L0x2001ba5c*x** 5+L0x2001ba60*x** 6+L0x2001ba64*x** 7+
L0x2001ba68*x** 8+L0x2001ba6c*x** 9+L0x2001ba70*x**10+L0x2001ba74*x**11+
L0x2001ba78*x**12+L0x2001ba7c*x**13+L0x2001ba80*x**14+L0x2001ba84*x**15,
segment2b**2 = 
L0x2001ba88*x** 0+L0x2001ba8c*x** 1+L0x2001ba90*x** 2+L0x2001ba94*x** 3+
L0x2001ba98*x** 4+L0x2001ba9c*x** 5+L0x2001baa0*x** 6+L0x2001baa4*x** 7+
L0x2001baa8*x** 8+L0x2001baac*x** 9+L0x2001bab0*x**10+L0x2001bab4*x**11+
L0x2001bab8*x**12+L0x2001babc*x**13+L0x2001bac0*x**14+L0x2001bac4*x**15,
segment2c**2 = 
L0x2001bac8*x** 0+L0x2001bacc*x** 1+L0x2001bad0*x** 2+L0x2001bad4*x** 3+
L0x2001bad8*x** 4+L0x2001badc*x** 5+L0x2001bae0*x** 6+L0x2001bae4*x** 7+
L0x2001bae8*x** 8+L0x2001baec*x** 9+L0x2001baf0*x**10+L0x2001baf4*x**11+
L0x2001baf8*x**12+L0x2001bafc*x**13+L0x2001bb00*x**14+L0x2001bb04*x**15,
segment2d**2 = 
L0x2001bb08*x** 0+L0x2001bb0c*x** 1+L0x2001bb10*x** 2+L0x2001bb14*x** 3+
L0x2001bb18*x** 4+L0x2001bb1c*x** 5+L0x2001bb20*x** 6+L0x2001bb24*x** 7+
L0x2001bb28*x** 8+L0x2001bb2c*x** 9+L0x2001bb30*x**10+L0x2001bb34*x**11+
L0x2001bb38*x**12+L0x2001bb3c*x**13+L0x2001bb40*x**14+L0x2001bb44*x**15,
segment2e**2 = 
L0x2001bb48*x** 0+L0x2001bb4c*x** 1+L0x2001bb50*x** 2+L0x2001bb54*x** 3+
L0x2001bb58*x** 4+L0x2001bb5c*x** 5+L0x2001bb60*x** 6+L0x2001bb64*x** 7+
L0x2001bb68*x** 8+L0x2001bb6c*x** 9+L0x2001bb70*x**10+L0x2001bb74*x**11+
L0x2001bb78*x**12+L0x2001bb7c*x**13+L0x2001bb80*x**14+L0x2001bb84*x**15,
segment2f**2 = 
L0x2001bb88*x** 0+L0x2001bb8c*x** 1+L0x2001bb90*x** 2+L0x2001bb94*x** 3+
L0x2001bb98*x** 4+L0x2001bb9c*x** 5+L0x2001bba0*x** 6+L0x2001bba4*x** 7+
L0x2001bba8*x** 8+L0x2001bbac*x** 9+L0x2001bbb0*x**10+L0x2001bbb4*x**11+
L0x2001bbb8*x**12+L0x2001bbbc*x**13+L0x2001bbc0*x**14+L0x2001bbc4*x**15,
segment2 = 
(segment20**2)*x** 0+(segment21**2)*x**16+(segment22**2)*x**32+
(segment23**2)*x**48+(segment24**2)*x**64+(segment25**2)*x**80+
(segment26**2)*x**96+(segment27**2)*x**112+(segment28**2)*x**128+
(segment29**2)*x**144+(segment2a**2)*x**160+(segment2b**2)*x**176+
(segment2c**2)*x**192+(segment2d**2)*x**208+(segment2e**2)*x**224+
(segment2f**2)*x**240
,
segment30**2 = 
L0x2001bbc8*x** 0+L0x2001bbcc*x** 1+L0x2001bbd0*x** 2+L0x2001bbd4*x** 3+
L0x2001bbd8*x** 4+L0x2001bbdc*x** 5+L0x2001bbe0*x** 6+L0x2001bbe4*x** 7+
L0x2001bbe8*x** 8+L0x2001bbec*x** 9+L0x2001bbf0*x**10+L0x2001bbf4*x**11+
L0x2001bbf8*x**12+L0x2001bbfc*x**13+L0x2001bc00*x**14+L0x2001bc04*x**15,
segment31**2 = 
L0x2001bc08*x** 0+L0x2001bc0c*x** 1+L0x2001bc10*x** 2+L0x2001bc14*x** 3+
L0x2001bc18*x** 4+L0x2001bc1c*x** 5+L0x2001bc20*x** 6+L0x2001bc24*x** 7+
L0x2001bc28*x** 8+L0x2001bc2c*x** 9+L0x2001bc30*x**10+L0x2001bc34*x**11+
L0x2001bc38*x**12+L0x2001bc3c*x**13+L0x2001bc40*x**14+L0x2001bc44*x**15,
segment32**2 = 
L0x2001bc48*x** 0+L0x2001bc4c*x** 1+L0x2001bc50*x** 2+L0x2001bc54*x** 3+
L0x2001bc58*x** 4+L0x2001bc5c*x** 5+L0x2001bc60*x** 6+L0x2001bc64*x** 7+
L0x2001bc68*x** 8+L0x2001bc6c*x** 9+L0x2001bc70*x**10+L0x2001bc74*x**11+
L0x2001bc78*x**12+L0x2001bc7c*x**13+L0x2001bc80*x**14+L0x2001bc84*x**15,
segment33**2 = 
L0x2001bc88*x** 0+L0x2001bc8c*x** 1+L0x2001bc90*x** 2+L0x2001bc94*x** 3+
L0x2001bc98*x** 4+L0x2001bc9c*x** 5+L0x2001bca0*x** 6+L0x2001bca4*x** 7+
L0x2001bca8*x** 8+L0x2001bcac*x** 9+L0x2001bcb0*x**10+L0x2001bcb4*x**11+
L0x2001bcb8*x**12+L0x2001bcbc*x**13+L0x2001bcc0*x**14+L0x2001bcc4*x**15,
segment34**2 = 
L0x2001bcc8*x** 0+L0x2001bccc*x** 1+L0x2001bcd0*x** 2+L0x2001bcd4*x** 3+
L0x2001bcd8*x** 4+L0x2001bcdc*x** 5+L0x2001bce0*x** 6+L0x2001bce4*x** 7+
L0x2001bce8*x** 8+L0x2001bcec*x** 9+L0x2001bcf0*x**10+L0x2001bcf4*x**11+
L0x2001bcf8*x**12+L0x2001bcfc*x**13+L0x2001bd00*x**14+L0x2001bd04*x**15,
segment35**2 = 
L0x2001bd08*x** 0+L0x2001bd0c*x** 1+L0x2001bd10*x** 2+L0x2001bd14*x** 3+
L0x2001bd18*x** 4+L0x2001bd1c*x** 5+L0x2001bd20*x** 6+L0x2001bd24*x** 7+
L0x2001bd28*x** 8+L0x2001bd2c*x** 9+L0x2001bd30*x**10+L0x2001bd34*x**11+
L0x2001bd38*x**12+L0x2001bd3c*x**13+L0x2001bd40*x**14+L0x2001bd44*x**15,
segment36**2 = 
L0x2001bd48*x** 0+L0x2001bd4c*x** 1+L0x2001bd50*x** 2+L0x2001bd54*x** 3+
L0x2001bd58*x** 4+L0x2001bd5c*x** 5+L0x2001bd60*x** 6+L0x2001bd64*x** 7+
L0x2001bd68*x** 8+L0x2001bd6c*x** 9+L0x2001bd70*x**10+L0x2001bd74*x**11+
L0x2001bd78*x**12+L0x2001bd7c*x**13+L0x2001bd80*x**14+L0x2001bd84*x**15,
segment37**2 = 
L0x2001bd88*x** 0+L0x2001bd8c*x** 1+L0x2001bd90*x** 2+L0x2001bd94*x** 3+
L0x2001bd98*x** 4+L0x2001bd9c*x** 5+L0x2001bda0*x** 6+L0x2001bda4*x** 7+
L0x2001bda8*x** 8+L0x2001bdac*x** 9+L0x2001bdb0*x**10+L0x2001bdb4*x**11+
L0x2001bdb8*x**12+L0x2001bdbc*x**13+L0x2001bdc0*x**14+L0x2001bdc4*x**15,
segment38**2 = 
L0x2001bdc8*x** 0+L0x2001bdcc*x** 1+L0x2001bdd0*x** 2+L0x2001bdd4*x** 3+
L0x2001bdd8*x** 4+L0x2001bddc*x** 5+L0x2001bde0*x** 6+L0x2001bde4*x** 7+
L0x2001bde8*x** 8+L0x2001bdec*x** 9+L0x2001bdf0*x**10+L0x2001bdf4*x**11+
L0x2001bdf8*x**12+L0x2001bdfc*x**13+L0x2001be00*x**14+L0x2001be04*x**15,
segment39**2 = 
L0x2001be08*x** 0+L0x2001be0c*x** 1+L0x2001be10*x** 2+L0x2001be14*x** 3+
L0x2001be18*x** 4+L0x2001be1c*x** 5+L0x2001be20*x** 6+L0x2001be24*x** 7+
L0x2001be28*x** 8+L0x2001be2c*x** 9+L0x2001be30*x**10+L0x2001be34*x**11+
L0x2001be38*x**12+L0x2001be3c*x**13+L0x2001be40*x**14+L0x2001be44*x**15,
segment3a**2 = 
L0x2001be48*x** 0+L0x2001be4c*x** 1+L0x2001be50*x** 2+L0x2001be54*x** 3+
L0x2001be58*x** 4+L0x2001be5c*x** 5+L0x2001be60*x** 6+L0x2001be64*x** 7+
L0x2001be68*x** 8+L0x2001be6c*x** 9+L0x2001be70*x**10+L0x2001be74*x**11+
L0x2001be78*x**12+L0x2001be7c*x**13+L0x2001be80*x**14+L0x2001be84*x**15,
segment3b**2 = 
L0x2001be88*x** 0+L0x2001be8c*x** 1+L0x2001be90*x** 2+L0x2001be94*x** 3+
L0x2001be98*x** 4+L0x2001be9c*x** 5+L0x2001bea0*x** 6+L0x2001bea4*x** 7+
L0x2001bea8*x** 8+L0x2001beac*x** 9+L0x2001beb0*x**10+L0x2001beb4*x**11+
L0x2001beb8*x**12+L0x2001bebc*x**13+L0x2001bec0*x**14+L0x2001bec4*x**15,
segment3c**2 = 
L0x2001bec8*x** 0+L0x2001becc*x** 1+L0x2001bed0*x** 2+L0x2001bed4*x** 3+
L0x2001bed8*x** 4+L0x2001bedc*x** 5+L0x2001bee0*x** 6+L0x2001bee4*x** 7+
L0x2001bee8*x** 8+L0x2001beec*x** 9+L0x2001bef0*x**10+L0x2001bef4*x**11+
L0x2001bef8*x**12+L0x2001befc*x**13+L0x2001bf00*x**14+L0x2001bf04*x**15,
segment3d**2 = 
L0x2001bf08*x** 0+L0x2001bf0c*x** 1+L0x2001bf10*x** 2+L0x2001bf14*x** 3+
L0x2001bf18*x** 4+L0x2001bf1c*x** 5+L0x2001bf20*x** 6+L0x2001bf24*x** 7+
L0x2001bf28*x** 8+L0x2001bf2c*x** 9+L0x2001bf30*x**10+L0x2001bf34*x**11+
L0x2001bf38*x**12+L0x2001bf3c*x**13+L0x2001bf40*x**14+L0x2001bf44*x**15,
segment3e**2 = 
L0x2001bf48*x** 0+L0x2001bf4c*x** 1+L0x2001bf50*x** 2+L0x2001bf54*x** 3+
L0x2001bf58*x** 4+L0x2001bf5c*x** 5+L0x2001bf60*x** 6+L0x2001bf64*x** 7+
L0x2001bf68*x** 8+L0x2001bf6c*x** 9+L0x2001bf70*x**10+L0x2001bf74*x**11+
L0x2001bf78*x**12+L0x2001bf7c*x**13+L0x2001bf80*x**14+L0x2001bf84*x**15,
segment3f**2 = 
L0x2001bf88*x** 0+L0x2001bf8c*x** 1+L0x2001bf90*x** 2+L0x2001bf94*x** 3+
L0x2001bf98*x** 4+L0x2001bf9c*x** 5+L0x2001bfa0*x** 6+L0x2001bfa4*x** 7+
L0x2001bfa8*x** 8+L0x2001bfac*x** 9+L0x2001bfb0*x**10+L0x2001bfb4*x**11+
L0x2001bfb8*x**12+L0x2001bfbc*x**13+L0x2001bfc0*x**14+L0x2001bfc4*x**15,
segment3 = 
(segment30**2)*x** 0+(segment31**2)*x**16+(segment32**2)*x**32+
(segment33**2)*x**48+(segment34**2)*x**64+(segment35**2)*x**80+
(segment36**2)*x**96+(segment37**2)*x**112+(segment38**2)*x**128+
(segment39**2)*x**144+(segment3a**2)*x**160+(segment3b**2)*x**176+
(segment3c**2)*x**192+(segment3d**2)*x**208+(segment3e**2)*x**224+
(segment3f**2)*x**240
,
eqmod (64*inp_poly00**2) (segment0) [1043969, x**4 - 1],
eqmod (64*inp_poly01**2) (segment0) [1043969, x**4 - 1043968],
eqmod (64*inp_poly02**2) (segment0) [1043969, x**4 - 554923],
eqmod (64*inp_poly03**2) (segment0) [1043969, x**4 - 489046],
eqmod (64*inp_poly04**2) (segment0) [1043969, x**4 - 287998],
eqmod (64*inp_poly05**2) (segment0) [1043969, x**4 - 755971],
eqmod (64*inp_poly06**2) (segment0) [1043969, x**4 - 719789],
eqmod (64*inp_poly07**2) (segment0) [1043969, x**4 - 324180],
eqmod (64*inp_poly08**2) (segment0) [1043969, x**4 - 29512],
eqmod (64*inp_poly09**2) (segment0) [1043969, x**4 - 1014457],
eqmod (64*inp_poly0a**2) (segment0) [1043969, x**4 - 145873],
eqmod (64*inp_poly0b**2) (segment0) [1043969, x**4 - 898096],
eqmod (64*inp_poly0c**2) (segment0) [1043969, x**4 - 445347],
eqmod (64*inp_poly0d**2) (segment0) [1043969, x**4 - 598622],
eqmod (64*inp_poly0e**2) (segment0) [1043969, x**4 - 775725],
eqmod (64*inp_poly0f**2) (segment0) [1043969, x**4 - 268244],
eqmod (64*inp_poly10**2) (segment0) [1043969, x**4 - 754540],
eqmod (64*inp_poly11**2) (segment0) [1043969, x**4 - 289429],
eqmod (64*inp_poly12**2) (segment0) [1043969, x**4 - 689776],
eqmod (64*inp_poly13**2) (segment0) [1043969, x**4 - 354193],
eqmod (64*inp_poly14**2) (segment0) [1043969, x**4 - 731663],
eqmod (64*inp_poly15**2) (segment0) [1043969, x**4 - 312306],
eqmod (64*inp_poly16**2) (segment0) [1043969, x**4 - 379345],
eqmod (64*inp_poly17**2) (segment0) [1043969, x**4 - 664624],
eqmod (64*inp_poly18**2) (segment0) [1043969, x**4 - 125710],
eqmod (64*inp_poly19**2) (segment0) [1043969, x**4 - 918259],
eqmod (64*inp_poly1a**2) (segment0) [1043969, x**4 - 317781],
eqmod (64*inp_poly1b**2) (segment0) [1043969, x**4 - 726188],
eqmod (64*inp_poly1c**2) (segment0) [1043969, x**4 - 427629],
eqmod (64*inp_poly1d**2) (segment0) [1043969, x**4 - 616340],
eqmod (64*inp_poly1e**2) (segment0) [1043969, x**4 - 750053],
eqmod (64*inp_poly1f**2) (segment0) [1043969, x**4 - 293916],
eqmod (64*inp_poly20**2) (segment0) [1043969, x**4 - 587782],
eqmod (64*inp_poly21**2) (segment0) [1043969, x**4 - 456187],
eqmod (64*inp_poly22**2) (segment0) [1043969, x**4 - 252302],
eqmod (64*inp_poly23**2) (segment0) [1043969, x**4 - 791667],
eqmod (64*inp_poly24**2) (segment0) [1043969, x**4 - 467086],
eqmod (64*inp_poly25**2) (segment0) [1043969, x**4 - 576883],
eqmod (64*inp_poly26**2) (segment0) [1043969, x**4 - 141058],
eqmod (64*inp_poly27**2) (segment0) [1043969, x**4 - 902911],
eqmod (64*inp_poly28**2) (segment0) [1043969, x**4 - 33480],
eqmod (64*inp_poly29**2) (segment0) [1043969, x**4 - 1010489],
eqmod (64*inp_poly2a**2) (segment0) [1043969, x**4 - 349716],
eqmod (64*inp_poly2b**2) (segment0) [1043969, x**4 - 694253],
eqmod (64*inp_poly2c**2) (segment0) [1043969, x**4 - 75356],
eqmod (64*inp_poly2d**2) (segment0) [1043969, x**4 - 968613],
eqmod (64*inp_poly2e**2) (segment0) [1043969, x**4 - 599293],
eqmod (64*inp_poly2f**2) (segment0) [1043969, x**4 - 444676],
eqmod (64*inp_poly30**2) (segment0) [1043969, x**4 - 899855],
eqmod (64*inp_poly31**2) (segment0) [1043969, x**4 - 144114],
eqmod (64*inp_poly32**2) (segment0) [1043969, x**4 - 28054],
eqmod (64*inp_poly33**2) (segment0) [1043969, x**4 - 1015915],
eqmod (64*inp_poly34**2) (segment0) [1043969, x**4 - 531761],
eqmod (64*inp_poly35**2) (segment0) [1043969, x**4 - 512208],
eqmod (64*inp_poly36**2) (segment0) [1043969, x**4 - 219801],
eqmod (64*inp_poly37**2) (segment0) [1043969, x**4 - 824168],
eqmod (64*inp_poly38**2) (segment0) [1043969, x**4 - 37338],
eqmod (64*inp_poly39**2) (segment0) [1043969, x**4 - 1006631],
eqmod (64*inp_poly3a**2) (segment0) [1043969, x**4 - 62231],
eqmod (64*inp_poly3b**2) (segment0) [1043969, x**4 - 981738],
eqmod (64*inp_poly3c**2) (segment0) [1043969, x**4 - 388624],
eqmod (64*inp_poly3d**2) (segment0) [1043969, x**4 - 655345],
eqmod (64*inp_poly3e**2) (segment0) [1043969, x**4 - 587715],
eqmod (64*inp_poly3f**2) (segment0) [1043969, x**4 - 456254]
,
eqmod (64*inp_poly40**2) (segment1) [1043969, x**4 - 1013205],
eqmod (64*inp_poly41**2) (segment1) [1043969, x**4 - 30764],
eqmod (64*inp_poly42**2) (segment1) [1043969, x**4 - 373885],
eqmod (64*inp_poly43**2) (segment1) [1043969, x**4 - 670084],
eqmod (64*inp_poly44**2) (segment1) [1043969, x**4 - 194431],
eqmod (64*inp_poly45**2) (segment1) [1043969, x**4 - 849538],
eqmod (64*inp_poly46**2) (segment1) [1043969, x**4 - 37663],
eqmod (64*inp_poly47**2) (segment1) [1043969, x**4 - 1006306],
eqmod (64*inp_poly48**2) (segment1) [1043969, x**4 - 345862],
eqmod (64*inp_poly49**2) (segment1) [1043969, x**4 - 698107],
eqmod (64*inp_poly4a**2) (segment1) [1043969, x**4 - 385759],
eqmod (64*inp_poly4b**2) (segment1) [1043969, x**4 - 658210],
eqmod (64*inp_poly4c**2) (segment1) [1043969, x**4 - 394048],
eqmod (64*inp_poly4d**2) (segment1) [1043969, x**4 - 649921],
eqmod (64*inp_poly4e**2) (segment1) [1043969, x**4 - 727440],
eqmod (64*inp_poly4f**2) (segment1) [1043969, x**4 - 316529],
eqmod (64*inp_poly50**2) (segment1) [1043969, x**4 - 1026124],
eqmod (64*inp_poly51**2) (segment1) [1043969, x**4 - 17845],
eqmod (64*inp_poly52**2) (segment1) [1043969, x**4 - 488999],
eqmod (64*inp_poly53**2) (segment1) [1043969, x**4 - 554970],
eqmod (64*inp_poly54**2) (segment1) [1043969, x**4 - 135077],
eqmod (64*inp_poly55**2) (segment1) [1043969, x**4 - 908892],
eqmod (64*inp_poly56**2) (segment1) [1043969, x**4 - 359871],
eqmod (64*inp_poly57**2) (segment1) [1043969, x**4 - 684098],
eqmod (64*inp_poly58**2) (segment1) [1043969, x**4 - 562705],
eqmod (64*inp_poly59**2) (segment1) [1043969, x**4 - 481264],
eqmod (64*inp_poly5a**2) (segment1) [1043969, x**4 - 555001],
eqmod (64*inp_poly5b**2) (segment1) [1043969, x**4 - 488968],
eqmod (64*inp_poly5c**2) (segment1) [1043969, x**4 - 518782],
eqmod (64*inp_poly5d**2) (segment1) [1043969, x**4 - 525187],
eqmod (64*inp_poly5e**2) (segment1) [1043969, x**4 - 216315],
eqmod (64*inp_poly5f**2) (segment1) [1043969, x**4 - 827654],
eqmod (64*inp_poly60**2) (segment1) [1043969, x**4 - 61601],
eqmod (64*inp_poly61**2) (segment1) [1043969, x**4 - 982368],
eqmod (64*inp_poly62**2) (segment1) [1043969, x**4 - 90787],
eqmod (64*inp_poly63**2) (segment1) [1043969, x**4 - 953182],
eqmod (64*inp_poly64**2) (segment1) [1043969, x**4 - 799581],
eqmod (64*inp_poly65**2) (segment1) [1043969, x**4 - 244388],
eqmod (64*inp_poly66**2) (segment1) [1043969, x**4 - 270821],
eqmod (64*inp_poly67**2) (segment1) [1043969, x**4 - 773148],
eqmod (64*inp_poly68**2) (segment1) [1043969, x**4 - 418683],
eqmod (64*inp_poly69**2) (segment1) [1043969, x**4 - 625286],
eqmod (64*inp_poly6a**2) (segment1) [1043969, x**4 - 481490],
eqmod (64*inp_poly6b**2) (segment1) [1043969, x**4 - 562479],
eqmod (64*inp_poly6c**2) (segment1) [1043969, x**4 - 403165],
eqmod (64*inp_poly6d**2) (segment1) [1043969, x**4 - 640804],
eqmod (64*inp_poly6e**2) (segment1) [1043969, x**4 - 886657],
eqmod (64*inp_poly6f**2) (segment1) [1043969, x**4 - 157312],
eqmod (64*inp_poly70**2) (segment1) [1043969, x**4 - 830722],
eqmod (64*inp_poly71**2) (segment1) [1043969, x**4 - 213247],
eqmod (64*inp_poly72**2) (segment1) [1043969, x**4 - 309107],
eqmod (64*inp_poly73**2) (segment1) [1043969, x**4 - 734862],
eqmod (64*inp_poly74**2) (segment1) [1043969, x**4 - 942795],
eqmod (64*inp_poly75**2) (segment1) [1043969, x**4 - 101174],
eqmod (64*inp_poly76**2) (segment1) [1043969, x**4 - 873218],
eqmod (64*inp_poly77**2) (segment1) [1043969, x**4 - 170751],
eqmod (64*inp_poly78**2) (segment1) [1043969, x**4 - 743637],
eqmod (64*inp_poly79**2) (segment1) [1043969, x**4 - 300332],
eqmod (64*inp_poly7a**2) (segment1) [1043969, x**4 - 164662],
eqmod (64*inp_poly7b**2) (segment1) [1043969, x**4 - 879307],
eqmod (64*inp_poly7c**2) (segment1) [1043969, x**4 - 948221],
eqmod (64*inp_poly7d**2) (segment1) [1043969, x**4 - 95748],
eqmod (64*inp_poly7e**2) (segment1) [1043969, x**4 - 34851],
eqmod (64*inp_poly7f**2) (segment1) [1043969, x**4 - 1009118]
,
eqmod (64*inp_poly80**2) (segment2) [1043969, x**4 - 941631],
eqmod (64*inp_poly81**2) (segment2) [1043969, x**4 - 102338],
eqmod (64*inp_poly82**2) (segment2) [1043969, x**4 - 115688],
eqmod (64*inp_poly83**2) (segment2) [1043969, x**4 - 928281],
eqmod (64*inp_poly84**2) (segment2) [1043969, x**4 - 193484],
eqmod (64*inp_poly85**2) (segment2) [1043969, x**4 - 850485],
eqmod (64*inp_poly86**2) (segment2) [1043969, x**4 - 685958],
eqmod (64*inp_poly87**2) (segment2) [1043969, x**4 - 358011],
eqmod (64*inp_poly88**2) (segment2) [1043969, x**4 - 3261],
eqmod (64*inp_poly89**2) (segment2) [1043969, x**4 - 1040708],
eqmod (64*inp_poly8a**2) (segment2) [1043969, x**4 - 405626],
eqmod (64*inp_poly8b**2) (segment2) [1043969, x**4 - 638343],
eqmod (64*inp_poly8c**2) (segment2) [1043969, x**4 - 633347],
eqmod (64*inp_poly8d**2) (segment2) [1043969, x**4 - 410622],
eqmod (64*inp_poly8e**2) (segment2) [1043969, x**4 - 389617],
eqmod (64*inp_poly8f**2) (segment2) [1043969, x**4 - 654352],
eqmod (64*inp_poly90**2) (segment2) [1043969, x**4 - 96534],
eqmod (64*inp_poly91**2) (segment2) [1043969, x**4 - 947435],
eqmod (64*inp_poly92**2) (segment2) [1043969, x**4 - 799554],
eqmod (64*inp_poly93**2) (segment2) [1043969, x**4 - 244415],
eqmod (64*inp_poly94**2) (segment2) [1043969, x**4 - 704462],
eqmod (64*inp_poly95**2) (segment2) [1043969, x**4 - 339507],
eqmod (64*inp_poly96**2) (segment2) [1043969, x**4 - 666593],
eqmod (64*inp_poly97**2) (segment2) [1043969, x**4 - 377376],
eqmod (64*inp_poly98**2) (segment2) [1043969, x**4 - 963976],
eqmod (64*inp_poly99**2) (segment2) [1043969, x**4 - 79993],
eqmod (64*inp_poly9a**2) (segment2) [1043969, x**4 - 650310],
eqmod (64*inp_poly9b**2) (segment2) [1043969, x**4 - 393659],
eqmod (64*inp_poly9c**2) (segment2) [1043969, x**4 - 483878],
eqmod (64*inp_poly9d**2) (segment2) [1043969, x**4 - 560091],
eqmod (64*inp_poly9e**2) (segment2) [1043969, x**4 - 984749],
eqmod (64*inp_poly9f**2) (segment2) [1043969, x**4 - 59220],
eqmod (64*inp_polya0**2) (segment2) [1043969, x**4 - 15495],
eqmod (64*inp_polya1**2) (segment2) [1043969, x**4 - 1028474],
eqmod (64*inp_polya2**2) (segment2) [1043969, x**4 - 403201],
eqmod (64*inp_polya3**2) (segment2) [1043969, x**4 - 640768],
eqmod (64*inp_polya4**2) (segment2) [1043969, x**4 - 605504],
eqmod (64*inp_polya5**2) (segment2) [1043969, x**4 - 438465],
eqmod (64*inp_polya6**2) (segment2) [1043969, x**4 - 409728],
eqmod (64*inp_polya7**2) (segment2) [1043969, x**4 - 634241],
eqmod (64*inp_polya8**2) (segment2) [1043969, x**4 - 30018],
eqmod (64*inp_polya9**2) (segment2) [1043969, x**4 - 1013951],
eqmod (64*inp_polyaa**2) (segment2) [1043969, x**4 - 109250],
eqmod (64*inp_polyab**2) (segment2) [1043969, x**4 - 934719],
eqmod (64*inp_polyac**2) (segment2) [1043969, x**4 - 16675],
eqmod (64*inp_polyad**2) (segment2) [1043969, x**4 - 1027294],
eqmod (64*inp_polyae**2) (segment2) [1043969, x**4 - 643778],
eqmod (64*inp_polyaf**2) (segment2) [1043969, x**4 - 400191],
eqmod (64*inp_polyb0**2) (segment2) [1043969, x**4 - 188469],
eqmod (64*inp_polyb1**2) (segment2) [1043969, x**4 - 855500],
eqmod (64*inp_polyb2**2) (segment2) [1043969, x**4 - 968467],
eqmod (64*inp_polyb3**2) (segment2) [1043969, x**4 - 75502],
eqmod (64*inp_polyb4**2) (segment2) [1043969, x**4 - 658814],
eqmod (64*inp_polyb5**2) (segment2) [1043969, x**4 - 385155],
eqmod (64*inp_polyb6**2) (segment2) [1043969, x**4 - 405305],
eqmod (64*inp_polyb7**2) (segment2) [1043969, x**4 - 638664],
eqmod (64*inp_polyb8**2) (segment2) [1043969, x**4 - 874265],
eqmod (64*inp_polyb9**2) (segment2) [1043969, x**4 - 169704],
eqmod (64*inp_polyba**2) (segment2) [1043969, x**4 - 658791],
eqmod (64*inp_polybb**2) (segment2) [1043969, x**4 - 385178],
eqmod (64*inp_polybc**2) (segment2) [1043969, x**4 - 40112],
eqmod (64*inp_polybd**2) (segment2) [1043969, x**4 - 1003857],
eqmod (64*inp_polybe**2) (segment2) [1043969, x**4 - 608327],
eqmod (64*inp_polybf**2) (segment2) [1043969, x**4 - 435642]
,
eqmod (64*inp_polyc0**2) (segment3) [1043969, x**4 - 759697],
eqmod (64*inp_polyc1**2) (segment3) [1043969, x**4 - 284272],
eqmod (64*inp_polyc2**2) (segment3) [1043969, x**4 - 908658],
eqmod (64*inp_polyc3**2) (segment3) [1043969, x**4 - 135311],
eqmod (64*inp_polyc4**2) (segment3) [1043969, x**4 - 369462],
eqmod (64*inp_polyc5**2) (segment3) [1043969, x**4 - 674507],
eqmod (64*inp_polyc6**2) (segment3) [1043969, x**4 - 1021423],
eqmod (64*inp_polyc7**2) (segment3) [1043969, x**4 - 22546],
eqmod (64*inp_polyc8**2) (segment3) [1043969, x**4 - 943589],
eqmod (64*inp_polyc9**2) (segment3) [1043969, x**4 - 100380],
eqmod (64*inp_polyca**2) (segment3) [1043969, x**4 - 927162],
eqmod (64*inp_polycb**2) (segment3) [1043969, x**4 - 116807],
eqmod (64*inp_polycc**2) (segment3) [1043969, x**4 - 350308],
eqmod (64*inp_polycd**2) (segment3) [1043969, x**4 - 693661],
eqmod (64*inp_polyce**2) (segment3) [1043969, x**4 - 674670],
eqmod (64*inp_polycf**2) (segment3) [1043969, x**4 - 369299],
eqmod (64*inp_polyd0**2) (segment3) [1043969, x**4 - 319829],
eqmod (64*inp_polyd1**2) (segment3) [1043969, x**4 - 724140],
eqmod (64*inp_polyd2**2) (segment3) [1043969, x**4 - 518322],
eqmod (64*inp_polyd3**2) (segment3) [1043969, x**4 - 525647],
eqmod (64*inp_polyd4**2) (segment3) [1043969, x**4 - 727472],
eqmod (64*inp_polyd5**2) (segment3) [1043969, x**4 - 316497],
eqmod (64*inp_polyd6**2) (segment3) [1043969, x**4 - 659984],
eqmod (64*inp_polyd7**2) (segment3) [1043969, x**4 - 383985],
eqmod (64*inp_polyd8**2) (segment3) [1043969, x**4 - 269719],
eqmod (64*inp_polyd9**2) (segment3) [1043969, x**4 - 774250],
eqmod (64*inp_polyda**2) (segment3) [1043969, x**4 - 485076],
eqmod (64*inp_polydb**2) (segment3) [1043969, x**4 - 558893],
eqmod (64*inp_polydc**2) (segment3) [1043969, x**4 - 975148],
eqmod (64*inp_polydd**2) (segment3) [1043969, x**4 - 68821],
eqmod (64*inp_polyde**2) (segment3) [1043969, x**4 - 118175],
eqmod (64*inp_polydf**2) (segment3) [1043969, x**4 - 925794],
eqmod (64*inp_polye0**2) (segment3) [1043969, x**4 - 405653],
eqmod (64*inp_polye1**2) (segment3) [1043969, x**4 - 638316],
eqmod (64*inp_polye2**2) (segment3) [1043969, x**4 - 364094],
eqmod (64*inp_polye3**2) (segment3) [1043969, x**4 - 679875],
eqmod (64*inp_polye4**2) (segment3) [1043969, x**4 - 857780],
eqmod (64*inp_polye5**2) (segment3) [1043969, x**4 - 186189],
eqmod (64*inp_polye6**2) (segment3) [1043969, x**4 - 9514],
eqmod (64*inp_polye7**2) (segment3) [1043969, x**4 - 1034455],
eqmod (64*inp_polye8**2) (segment3) [1043969, x**4 - 438813],
eqmod (64*inp_polye9**2) (segment3) [1043969, x**4 - 605156],
eqmod (64*inp_polyea**2) (segment3) [1043969, x**4 - 613180],
eqmod (64*inp_polyeb**2) (segment3) [1043969, x**4 - 430789],
eqmod (64*inp_polyec**2) (segment3) [1043969, x**4 - 643048],
eqmod (64*inp_polyed**2) (segment3) [1043969, x**4 - 400921],
eqmod (64*inp_polyee**2) (segment3) [1043969, x**4 - 993476],
eqmod (64*inp_polyef**2) (segment3) [1043969, x**4 - 50493],
eqmod (64*inp_polyf0**2) (segment3) [1043969, x**4 - 143510],
eqmod (64*inp_polyf1**2) (segment3) [1043969, x**4 - 900459],
eqmod (64*inp_polyf2**2) (segment3) [1043969, x**4 - 956472],
eqmod (64*inp_polyf3**2) (segment3) [1043969, x**4 - 87497],
eqmod (64*inp_polyf4**2) (segment3) [1043969, x**4 - 904239],
eqmod (64*inp_polyf5**2) (segment3) [1043969, x**4 - 139730],
eqmod (64*inp_polyf6**2) (segment3) [1043969, x**4 - 362716],
eqmod (64*inp_polyf7**2) (segment3) [1043969, x**4 - 681253],
eqmod (64*inp_polyf8**2) (segment3) [1043969, x**4 - 928856],
eqmod (64*inp_polyf9**2) (segment3) [1043969, x**4 - 115113],
eqmod (64*inp_polyfa**2) (segment3) [1043969, x**4 - 567842],
eqmod (64*inp_polyfb**2) (segment3) [1043969, x**4 - 476127],
eqmod (64*inp_polyfc**2) (segment3) [1043969, x**4 - 1009759],
eqmod (64*inp_polyfd**2) (segment3) [1043969, x**4 - 34210],
eqmod (64*inp_polyfe**2) (segment3) [1043969, x**4 - 660435],
eqmod (64*inp_polyff**2) (segment3) [1043969, x**4 - 383534]
] && and [
(-64)@32*1043969@32 <=s L0x2001afc8, L0x2001afc8 <=s 64@32*1043969@32,
(-64)@32*1043969@32 <=s L0x2001afcc, L0x2001afcc <=s 64@32*1043969@32,
(-64)@32*1043969@32 <=s L0x2001afd0, L0x2001afd0 <=s 64@32*1043969@32,
(-64)@32*1043969@32 <=s L0x2001afd4, L0x2001afd4 <=s 64@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001afd8, L0x2001afd8 <=s 32@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001afdc, L0x2001afdc <=s 32@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001afe0, L0x2001afe0 <=s 32@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001afe4, L0x2001afe4 <=s 32@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001afe8, L0x2001afe8 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001afec, L0x2001afec <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001aff0, L0x2001aff0 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001aff4, L0x2001aff4 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001aff8, L0x2001aff8 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001affc, L0x2001affc <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b000, L0x2001b000 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b004, L0x2001b004 <=s 16@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b008, L0x2001b008 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b00c, L0x2001b00c <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b010, L0x2001b010 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b014, L0x2001b014 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b018, L0x2001b018 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b01c, L0x2001b01c <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b020, L0x2001b020 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b024, L0x2001b024 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b028, L0x2001b028 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b02c, L0x2001b02c <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b030, L0x2001b030 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b034, L0x2001b034 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b038, L0x2001b038 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b03c, L0x2001b03c <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b040, L0x2001b040 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b044, L0x2001b044 <=s 8@32*1043969@32,
(-40)@32*1043969@32 <=s L0x2001b048, L0x2001b048 <=s 40@32*1043969@32,
(-40)@32*1043969@32 <=s L0x2001b04c, L0x2001b04c <=s 40@32*1043969@32,
(-40)@32*1043969@32 <=s L0x2001b050, L0x2001b050 <=s 40@32*1043969@32,
(-40)@32*1043969@32 <=s L0x2001b054, L0x2001b054 <=s 40@32*1043969@32,
(-20)@32*1043969@32 <=s L0x2001b058, L0x2001b058 <=s 20@32*1043969@32,
(-20)@32*1043969@32 <=s L0x2001b05c, L0x2001b05c <=s 20@32*1043969@32,
(-20)@32*1043969@32 <=s L0x2001b060, L0x2001b060 <=s 20@32*1043969@32,
(-20)@32*1043969@32 <=s L0x2001b064, L0x2001b064 <=s 20@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b068, L0x2001b068 <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b06c, L0x2001b06c <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b070, L0x2001b070 <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b074, L0x2001b074 <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b078, L0x2001b078 <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b07c, L0x2001b07c <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b080, L0x2001b080 <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b084, L0x2001b084 <=s 10@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b088, L0x2001b088 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b08c, L0x2001b08c <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b090, L0x2001b090 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b094, L0x2001b094 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b098, L0x2001b098 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b09c, L0x2001b09c <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b0a0, L0x2001b0a0 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b0a4, L0x2001b0a4 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b0a8, L0x2001b0a8 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b0ac, L0x2001b0ac <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b0b0, L0x2001b0b0 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b0b4, L0x2001b0b4 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b0b8, L0x2001b0b8 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b0bc, L0x2001b0bc <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b0c0, L0x2001b0c0 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b0c4, L0x2001b0c4 <=s 5@32*1043969@32,
(-40)@32*1043969@32 <=s L0x2001b0c8, L0x2001b0c8 <=s 40@32*1043969@32,
(-40)@32*1043969@32 <=s L0x2001b0cc, L0x2001b0cc <=s 40@32*1043969@32,
(-40)@32*1043969@32 <=s L0x2001b0d0, L0x2001b0d0 <=s 40@32*1043969@32,
(-40)@32*1043969@32 <=s L0x2001b0d4, L0x2001b0d4 <=s 40@32*1043969@32,
(-20)@32*1043969@32 <=s L0x2001b0d8, L0x2001b0d8 <=s 20@32*1043969@32,
(-20)@32*1043969@32 <=s L0x2001b0dc, L0x2001b0dc <=s 20@32*1043969@32,
(-20)@32*1043969@32 <=s L0x2001b0e0, L0x2001b0e0 <=s 20@32*1043969@32,
(-20)@32*1043969@32 <=s L0x2001b0e4, L0x2001b0e4 <=s 20@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b0e8, L0x2001b0e8 <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b0ec, L0x2001b0ec <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b0f0, L0x2001b0f0 <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b0f4, L0x2001b0f4 <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b0f8, L0x2001b0f8 <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b0fc, L0x2001b0fc <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b100, L0x2001b100 <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b104, L0x2001b104 <=s 10@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b108, L0x2001b108 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b10c, L0x2001b10c <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b110, L0x2001b110 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b114, L0x2001b114 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b118, L0x2001b118 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b11c, L0x2001b11c <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b120, L0x2001b120 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b124, L0x2001b124 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b128, L0x2001b128 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b12c, L0x2001b12c <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b130, L0x2001b130 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b134, L0x2001b134 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b138, L0x2001b138 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b13c, L0x2001b13c <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b140, L0x2001b140 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b144, L0x2001b144 <=s 5@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001b148, L0x2001b148 <=s 32@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001b14c, L0x2001b14c <=s 32@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001b150, L0x2001b150 <=s 32@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001b154, L0x2001b154 <=s 32@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b158, L0x2001b158 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b15c, L0x2001b15c <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b160, L0x2001b160 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b164, L0x2001b164 <=s 16@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b168, L0x2001b168 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b16c, L0x2001b16c <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b170, L0x2001b170 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b174, L0x2001b174 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b178, L0x2001b178 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b17c, L0x2001b17c <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b180, L0x2001b180 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b184, L0x2001b184 <=s 8@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b188, L0x2001b188 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b18c, L0x2001b18c <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b190, L0x2001b190 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b194, L0x2001b194 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b198, L0x2001b198 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b19c, L0x2001b19c <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b1a0, L0x2001b1a0 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b1a4, L0x2001b1a4 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b1a8, L0x2001b1a8 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b1ac, L0x2001b1ac <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b1b0, L0x2001b1b0 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b1b4, L0x2001b1b4 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b1b8, L0x2001b1b8 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b1bc, L0x2001b1bc <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b1c0, L0x2001b1c0 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b1c4, L0x2001b1c4 <=s 4@32*1043969@32,
(-64)@32*1043969@32 <=s L0x2001b1c8, L0x2001b1c8 <=s 64@32*1043969@32,
(-64)@32*1043969@32 <=s L0x2001b1cc, L0x2001b1cc <=s 64@32*1043969@32,
(-64)@32*1043969@32 <=s L0x2001b1d0, L0x2001b1d0 <=s 64@32*1043969@32,
(-64)@32*1043969@32 <=s L0x2001b1d4, L0x2001b1d4 <=s 64@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001b1d8, L0x2001b1d8 <=s 32@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001b1dc, L0x2001b1dc <=s 32@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001b1e0, L0x2001b1e0 <=s 32@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001b1e4, L0x2001b1e4 <=s 32@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b1e8, L0x2001b1e8 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b1ec, L0x2001b1ec <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b1f0, L0x2001b1f0 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b1f4, L0x2001b1f4 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b1f8, L0x2001b1f8 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b1fc, L0x2001b1fc <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b200, L0x2001b200 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b204, L0x2001b204 <=s 16@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b208, L0x2001b208 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b20c, L0x2001b20c <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b210, L0x2001b210 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b214, L0x2001b214 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b218, L0x2001b218 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b21c, L0x2001b21c <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b220, L0x2001b220 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b224, L0x2001b224 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b228, L0x2001b228 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b22c, L0x2001b22c <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b230, L0x2001b230 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b234, L0x2001b234 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b238, L0x2001b238 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b23c, L0x2001b23c <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b240, L0x2001b240 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b244, L0x2001b244 <=s 8@32*1043969@32,
(-40)@32*1043969@32 <=s L0x2001b248, L0x2001b248 <=s 40@32*1043969@32,
(-40)@32*1043969@32 <=s L0x2001b24c, L0x2001b24c <=s 40@32*1043969@32,
(-40)@32*1043969@32 <=s L0x2001b250, L0x2001b250 <=s 40@32*1043969@32,
(-40)@32*1043969@32 <=s L0x2001b254, L0x2001b254 <=s 40@32*1043969@32,
(-20)@32*1043969@32 <=s L0x2001b258, L0x2001b258 <=s 20@32*1043969@32,
(-20)@32*1043969@32 <=s L0x2001b25c, L0x2001b25c <=s 20@32*1043969@32,
(-20)@32*1043969@32 <=s L0x2001b260, L0x2001b260 <=s 20@32*1043969@32,
(-20)@32*1043969@32 <=s L0x2001b264, L0x2001b264 <=s 20@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b268, L0x2001b268 <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b26c, L0x2001b26c <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b270, L0x2001b270 <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b274, L0x2001b274 <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b278, L0x2001b278 <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b27c, L0x2001b27c <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b280, L0x2001b280 <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b284, L0x2001b284 <=s 10@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b288, L0x2001b288 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b28c, L0x2001b28c <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b290, L0x2001b290 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b294, L0x2001b294 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b298, L0x2001b298 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b29c, L0x2001b29c <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b2a0, L0x2001b2a0 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b2a4, L0x2001b2a4 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b2a8, L0x2001b2a8 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b2ac, L0x2001b2ac <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b2b0, L0x2001b2b0 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b2b4, L0x2001b2b4 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b2b8, L0x2001b2b8 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b2bc, L0x2001b2bc <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b2c0, L0x2001b2c0 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b2c4, L0x2001b2c4 <=s 5@32*1043969@32,
(-40)@32*1043969@32 <=s L0x2001b2c8, L0x2001b2c8 <=s 40@32*1043969@32,
(-40)@32*1043969@32 <=s L0x2001b2cc, L0x2001b2cc <=s 40@32*1043969@32,
(-40)@32*1043969@32 <=s L0x2001b2d0, L0x2001b2d0 <=s 40@32*1043969@32,
(-40)@32*1043969@32 <=s L0x2001b2d4, L0x2001b2d4 <=s 40@32*1043969@32,
(-20)@32*1043969@32 <=s L0x2001b2d8, L0x2001b2d8 <=s 20@32*1043969@32,
(-20)@32*1043969@32 <=s L0x2001b2dc, L0x2001b2dc <=s 20@32*1043969@32,
(-20)@32*1043969@32 <=s L0x2001b2e0, L0x2001b2e0 <=s 20@32*1043969@32,
(-20)@32*1043969@32 <=s L0x2001b2e4, L0x2001b2e4 <=s 20@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b2e8, L0x2001b2e8 <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b2ec, L0x2001b2ec <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b2f0, L0x2001b2f0 <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b2f4, L0x2001b2f4 <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b2f8, L0x2001b2f8 <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b2fc, L0x2001b2fc <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b300, L0x2001b300 <=s 10@32*1043969@32,
(-10)@32*1043969@32 <=s L0x2001b304, L0x2001b304 <=s 10@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b308, L0x2001b308 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b30c, L0x2001b30c <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b310, L0x2001b310 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b314, L0x2001b314 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b318, L0x2001b318 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b31c, L0x2001b31c <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b320, L0x2001b320 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b324, L0x2001b324 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b328, L0x2001b328 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b32c, L0x2001b32c <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b330, L0x2001b330 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b334, L0x2001b334 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b338, L0x2001b338 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b33c, L0x2001b33c <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b340, L0x2001b340 <=s 5@32*1043969@32,
(-5)@32*1043969@32 <=s L0x2001b344, L0x2001b344 <=s 5@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001b348, L0x2001b348 <=s 32@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001b34c, L0x2001b34c <=s 32@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001b350, L0x2001b350 <=s 32@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001b354, L0x2001b354 <=s 32@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b358, L0x2001b358 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b35c, L0x2001b35c <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b360, L0x2001b360 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b364, L0x2001b364 <=s 16@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b368, L0x2001b368 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b36c, L0x2001b36c <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b370, L0x2001b370 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b374, L0x2001b374 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b378, L0x2001b378 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b37c, L0x2001b37c <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b380, L0x2001b380 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b384, L0x2001b384 <=s 8@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b388, L0x2001b388 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b38c, L0x2001b38c <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b390, L0x2001b390 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b394, L0x2001b394 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b398, L0x2001b398 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b39c, L0x2001b39c <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b3a0, L0x2001b3a0 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b3a4, L0x2001b3a4 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b3a8, L0x2001b3a8 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b3ac, L0x2001b3ac <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b3b0, L0x2001b3b0 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b3b4, L0x2001b3b4 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b3b8, L0x2001b3b8 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b3bc, L0x2001b3bc <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b3c0, L0x2001b3c0 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b3c4, L0x2001b3c4 <=s 4@32*1043969@32,
(-64)@32*1043969@32 <=s L0x2001b3c8, L0x2001b3c8 <=s 64@32*1043969@32,
(-64)@32*1043969@32 <=s L0x2001b3cc, L0x2001b3cc <=s 64@32*1043969@32,
(-64)@32*1043969@32 <=s L0x2001b3d0, L0x2001b3d0 <=s 64@32*1043969@32,
(-64)@32*1043969@32 <=s L0x2001b3d4, L0x2001b3d4 <=s 64@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001b3d8, L0x2001b3d8 <=s 32@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001b3dc, L0x2001b3dc <=s 32@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001b3e0, L0x2001b3e0 <=s 32@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001b3e4, L0x2001b3e4 <=s 32@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b3e8, L0x2001b3e8 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b3ec, L0x2001b3ec <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b3f0, L0x2001b3f0 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b3f4, L0x2001b3f4 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b3f8, L0x2001b3f8 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b3fc, L0x2001b3fc <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b400, L0x2001b400 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b404, L0x2001b404 <=s 16@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b408, L0x2001b408 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b40c, L0x2001b40c <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b410, L0x2001b410 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b414, L0x2001b414 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b418, L0x2001b418 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b41c, L0x2001b41c <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b420, L0x2001b420 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b424, L0x2001b424 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b428, L0x2001b428 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b42c, L0x2001b42c <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b430, L0x2001b430 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b434, L0x2001b434 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b438, L0x2001b438 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b43c, L0x2001b43c <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b440, L0x2001b440 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b444, L0x2001b444 <=s 8@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b448, L0x2001b448 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b44c, L0x2001b44c <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b450, L0x2001b450 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b454, L0x2001b454 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b458, L0x2001b458 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b45c, L0x2001b45c <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b460, L0x2001b460 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b464, L0x2001b464 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b468, L0x2001b468 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b46c, L0x2001b46c <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b470, L0x2001b470 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b474, L0x2001b474 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b478, L0x2001b478 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b47c, L0x2001b47c <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b480, L0x2001b480 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b484, L0x2001b484 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b488, L0x2001b488 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b48c, L0x2001b48c <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b490, L0x2001b490 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b494, L0x2001b494 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b498, L0x2001b498 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b49c, L0x2001b49c <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b4a0, L0x2001b4a0 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b4a4, L0x2001b4a4 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b4a8, L0x2001b4a8 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b4ac, L0x2001b4ac <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b4b0, L0x2001b4b0 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b4b4, L0x2001b4b4 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b4b8, L0x2001b4b8 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b4bc, L0x2001b4bc <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b4c0, L0x2001b4c0 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b4c4, L0x2001b4c4 <=s 4@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b4c8, L0x2001b4c8 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b4cc, L0x2001b4cc <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b4d0, L0x2001b4d0 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b4d4, L0x2001b4d4 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b4d8, L0x2001b4d8 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b4dc, L0x2001b4dc <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b4e0, L0x2001b4e0 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b4e4, L0x2001b4e4 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b4e8, L0x2001b4e8 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b4ec, L0x2001b4ec <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b4f0, L0x2001b4f0 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b4f4, L0x2001b4f4 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b4f8, L0x2001b4f8 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b4fc, L0x2001b4fc <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b500, L0x2001b500 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b504, L0x2001b504 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b508, L0x2001b508 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b50c, L0x2001b50c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b510, L0x2001b510 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b514, L0x2001b514 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b518, L0x2001b518 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b51c, L0x2001b51c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b520, L0x2001b520 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b524, L0x2001b524 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b528, L0x2001b528 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b52c, L0x2001b52c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b530, L0x2001b530 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b534, L0x2001b534 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b538, L0x2001b538 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b53c, L0x2001b53c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b540, L0x2001b540 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b544, L0x2001b544 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b548, L0x2001b548 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b54c, L0x2001b54c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b550, L0x2001b550 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b554, L0x2001b554 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b558, L0x2001b558 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b55c, L0x2001b55c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b560, L0x2001b560 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b564, L0x2001b564 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b568, L0x2001b568 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b56c, L0x2001b56c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b570, L0x2001b570 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b574, L0x2001b574 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b578, L0x2001b578 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b57c, L0x2001b57c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b580, L0x2001b580 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b584, L0x2001b584 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b588, L0x2001b588 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b58c, L0x2001b58c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b590, L0x2001b590 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b594, L0x2001b594 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b598, L0x2001b598 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b59c, L0x2001b59c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b5a0, L0x2001b5a0 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b5a4, L0x2001b5a4 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b5a8, L0x2001b5a8 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b5ac, L0x2001b5ac <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b5b0, L0x2001b5b0 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b5b4, L0x2001b5b4 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b5b8, L0x2001b5b8 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b5bc, L0x2001b5bc <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b5c0, L0x2001b5c0 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b5c4, L0x2001b5c4 <=s 2@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b5c8, L0x2001b5c8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b5cc, L0x2001b5cc <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b5d0, L0x2001b5d0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b5d4, L0x2001b5d4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b5d8, L0x2001b5d8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b5dc, L0x2001b5dc <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b5e0, L0x2001b5e0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b5e4, L0x2001b5e4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b5e8, L0x2001b5e8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b5ec, L0x2001b5ec <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b5f0, L0x2001b5f0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b5f4, L0x2001b5f4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b5f8, L0x2001b5f8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b5fc, L0x2001b5fc <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b600, L0x2001b600 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b604, L0x2001b604 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b608, L0x2001b608 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b60c, L0x2001b60c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b610, L0x2001b610 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b614, L0x2001b614 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b618, L0x2001b618 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b61c, L0x2001b61c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b620, L0x2001b620 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b624, L0x2001b624 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b628, L0x2001b628 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b62c, L0x2001b62c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b630, L0x2001b630 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b634, L0x2001b634 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b638, L0x2001b638 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b63c, L0x2001b63c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b640, L0x2001b640 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b644, L0x2001b644 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b648, L0x2001b648 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b64c, L0x2001b64c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b650, L0x2001b650 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b654, L0x2001b654 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b658, L0x2001b658 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b65c, L0x2001b65c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b660, L0x2001b660 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b664, L0x2001b664 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b668, L0x2001b668 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b66c, L0x2001b66c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b670, L0x2001b670 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b674, L0x2001b674 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b678, L0x2001b678 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b67c, L0x2001b67c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b680, L0x2001b680 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b684, L0x2001b684 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b688, L0x2001b688 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b68c, L0x2001b68c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b690, L0x2001b690 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b694, L0x2001b694 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b698, L0x2001b698 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b69c, L0x2001b69c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b6a0, L0x2001b6a0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b6a4, L0x2001b6a4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b6a8, L0x2001b6a8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b6ac, L0x2001b6ac <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b6b0, L0x2001b6b0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b6b4, L0x2001b6b4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b6b8, L0x2001b6b8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b6bc, L0x2001b6bc <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b6c0, L0x2001b6c0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b6c4, L0x2001b6c4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b6c8, L0x2001b6c8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b6cc, L0x2001b6cc <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b6d0, L0x2001b6d0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b6d4, L0x2001b6d4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b6d8, L0x2001b6d8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b6dc, L0x2001b6dc <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b6e0, L0x2001b6e0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b6e4, L0x2001b6e4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b6e8, L0x2001b6e8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b6ec, L0x2001b6ec <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b6f0, L0x2001b6f0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b6f4, L0x2001b6f4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b6f8, L0x2001b6f8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b6fc, L0x2001b6fc <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b700, L0x2001b700 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b704, L0x2001b704 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b708, L0x2001b708 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b70c, L0x2001b70c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b710, L0x2001b710 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b714, L0x2001b714 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b718, L0x2001b718 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b71c, L0x2001b71c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b720, L0x2001b720 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b724, L0x2001b724 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b728, L0x2001b728 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b72c, L0x2001b72c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b730, L0x2001b730 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b734, L0x2001b734 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b738, L0x2001b738 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b73c, L0x2001b73c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b740, L0x2001b740 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b744, L0x2001b744 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b748, L0x2001b748 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b74c, L0x2001b74c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b750, L0x2001b750 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b754, L0x2001b754 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b758, L0x2001b758 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b75c, L0x2001b75c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b760, L0x2001b760 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b764, L0x2001b764 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b768, L0x2001b768 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b76c, L0x2001b76c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b770, L0x2001b770 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b774, L0x2001b774 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b778, L0x2001b778 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b77c, L0x2001b77c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b780, L0x2001b780 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b784, L0x2001b784 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b788, L0x2001b788 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b78c, L0x2001b78c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b790, L0x2001b790 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b794, L0x2001b794 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b798, L0x2001b798 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b79c, L0x2001b79c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b7a0, L0x2001b7a0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b7a4, L0x2001b7a4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b7a8, L0x2001b7a8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b7ac, L0x2001b7ac <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b7b0, L0x2001b7b0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b7b4, L0x2001b7b4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b7b8, L0x2001b7b8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b7bc, L0x2001b7bc <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b7c0, L0x2001b7c0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b7c4, L0x2001b7c4 <=s 1@32*1043969@32,
(-64)@32*1043969@32 <=s L0x2001b7c8, L0x2001b7c8 <=s 64@32*1043969@32,
(-64)@32*1043969@32 <=s L0x2001b7cc, L0x2001b7cc <=s 64@32*1043969@32,
(-64)@32*1043969@32 <=s L0x2001b7d0, L0x2001b7d0 <=s 64@32*1043969@32,
(-64)@32*1043969@32 <=s L0x2001b7d4, L0x2001b7d4 <=s 64@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001b7d8, L0x2001b7d8 <=s 32@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001b7dc, L0x2001b7dc <=s 32@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001b7e0, L0x2001b7e0 <=s 32@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001b7e4, L0x2001b7e4 <=s 32@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b7e8, L0x2001b7e8 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b7ec, L0x2001b7ec <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b7f0, L0x2001b7f0 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b7f4, L0x2001b7f4 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b7f8, L0x2001b7f8 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b7fc, L0x2001b7fc <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b800, L0x2001b800 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001b804, L0x2001b804 <=s 16@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b808, L0x2001b808 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b80c, L0x2001b80c <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b810, L0x2001b810 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b814, L0x2001b814 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b818, L0x2001b818 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b81c, L0x2001b81c <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b820, L0x2001b820 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b824, L0x2001b824 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b828, L0x2001b828 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b82c, L0x2001b82c <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b830, L0x2001b830 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b834, L0x2001b834 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b838, L0x2001b838 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b83c, L0x2001b83c <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b840, L0x2001b840 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001b844, L0x2001b844 <=s 8@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b848, L0x2001b848 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b84c, L0x2001b84c <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b850, L0x2001b850 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b854, L0x2001b854 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b858, L0x2001b858 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b85c, L0x2001b85c <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b860, L0x2001b860 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b864, L0x2001b864 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b868, L0x2001b868 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b86c, L0x2001b86c <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b870, L0x2001b870 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b874, L0x2001b874 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b878, L0x2001b878 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b87c, L0x2001b87c <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b880, L0x2001b880 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b884, L0x2001b884 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b888, L0x2001b888 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b88c, L0x2001b88c <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b890, L0x2001b890 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b894, L0x2001b894 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b898, L0x2001b898 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b89c, L0x2001b89c <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b8a0, L0x2001b8a0 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b8a4, L0x2001b8a4 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b8a8, L0x2001b8a8 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b8ac, L0x2001b8ac <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b8b0, L0x2001b8b0 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b8b4, L0x2001b8b4 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b8b8, L0x2001b8b8 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b8bc, L0x2001b8bc <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b8c0, L0x2001b8c0 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001b8c4, L0x2001b8c4 <=s 4@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b8c8, L0x2001b8c8 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b8cc, L0x2001b8cc <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b8d0, L0x2001b8d0 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b8d4, L0x2001b8d4 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b8d8, L0x2001b8d8 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b8dc, L0x2001b8dc <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b8e0, L0x2001b8e0 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b8e4, L0x2001b8e4 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b8e8, L0x2001b8e8 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b8ec, L0x2001b8ec <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b8f0, L0x2001b8f0 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b8f4, L0x2001b8f4 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b8f8, L0x2001b8f8 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b8fc, L0x2001b8fc <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b900, L0x2001b900 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b904, L0x2001b904 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b908, L0x2001b908 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b90c, L0x2001b90c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b910, L0x2001b910 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b914, L0x2001b914 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b918, L0x2001b918 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b91c, L0x2001b91c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b920, L0x2001b920 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b924, L0x2001b924 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b928, L0x2001b928 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b92c, L0x2001b92c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b930, L0x2001b930 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b934, L0x2001b934 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b938, L0x2001b938 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b93c, L0x2001b93c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b940, L0x2001b940 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b944, L0x2001b944 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b948, L0x2001b948 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b94c, L0x2001b94c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b950, L0x2001b950 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b954, L0x2001b954 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b958, L0x2001b958 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b95c, L0x2001b95c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b960, L0x2001b960 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b964, L0x2001b964 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b968, L0x2001b968 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b96c, L0x2001b96c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b970, L0x2001b970 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b974, L0x2001b974 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b978, L0x2001b978 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b97c, L0x2001b97c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b980, L0x2001b980 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b984, L0x2001b984 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b988, L0x2001b988 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b98c, L0x2001b98c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b990, L0x2001b990 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b994, L0x2001b994 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b998, L0x2001b998 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b99c, L0x2001b99c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b9a0, L0x2001b9a0 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b9a4, L0x2001b9a4 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b9a8, L0x2001b9a8 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b9ac, L0x2001b9ac <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b9b0, L0x2001b9b0 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b9b4, L0x2001b9b4 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b9b8, L0x2001b9b8 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b9bc, L0x2001b9bc <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b9c0, L0x2001b9c0 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001b9c4, L0x2001b9c4 <=s 2@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b9c8, L0x2001b9c8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b9cc, L0x2001b9cc <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b9d0, L0x2001b9d0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b9d4, L0x2001b9d4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b9d8, L0x2001b9d8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b9dc, L0x2001b9dc <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b9e0, L0x2001b9e0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b9e4, L0x2001b9e4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b9e8, L0x2001b9e8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b9ec, L0x2001b9ec <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b9f0, L0x2001b9f0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b9f4, L0x2001b9f4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b9f8, L0x2001b9f8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001b9fc, L0x2001b9fc <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba00, L0x2001ba00 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba04, L0x2001ba04 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba08, L0x2001ba08 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba0c, L0x2001ba0c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba10, L0x2001ba10 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba14, L0x2001ba14 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba18, L0x2001ba18 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba1c, L0x2001ba1c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba20, L0x2001ba20 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba24, L0x2001ba24 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba28, L0x2001ba28 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba2c, L0x2001ba2c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba30, L0x2001ba30 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba34, L0x2001ba34 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba38, L0x2001ba38 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba3c, L0x2001ba3c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba40, L0x2001ba40 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba44, L0x2001ba44 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba48, L0x2001ba48 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba4c, L0x2001ba4c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba50, L0x2001ba50 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba54, L0x2001ba54 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba58, L0x2001ba58 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba5c, L0x2001ba5c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba60, L0x2001ba60 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba64, L0x2001ba64 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba68, L0x2001ba68 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba6c, L0x2001ba6c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba70, L0x2001ba70 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba74, L0x2001ba74 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba78, L0x2001ba78 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba7c, L0x2001ba7c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba80, L0x2001ba80 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba84, L0x2001ba84 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba88, L0x2001ba88 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba8c, L0x2001ba8c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba90, L0x2001ba90 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba94, L0x2001ba94 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba98, L0x2001ba98 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001ba9c, L0x2001ba9c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001baa0, L0x2001baa0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001baa4, L0x2001baa4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001baa8, L0x2001baa8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001baac, L0x2001baac <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bab0, L0x2001bab0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bab4, L0x2001bab4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bab8, L0x2001bab8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001babc, L0x2001babc <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bac0, L0x2001bac0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bac4, L0x2001bac4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bac8, L0x2001bac8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bacc, L0x2001bacc <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bad0, L0x2001bad0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bad4, L0x2001bad4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bad8, L0x2001bad8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001badc, L0x2001badc <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bae0, L0x2001bae0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bae4, L0x2001bae4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bae8, L0x2001bae8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001baec, L0x2001baec <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001baf0, L0x2001baf0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001baf4, L0x2001baf4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001baf8, L0x2001baf8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bafc, L0x2001bafc <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb00, L0x2001bb00 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb04, L0x2001bb04 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb08, L0x2001bb08 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb0c, L0x2001bb0c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb10, L0x2001bb10 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb14, L0x2001bb14 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb18, L0x2001bb18 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb1c, L0x2001bb1c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb20, L0x2001bb20 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb24, L0x2001bb24 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb28, L0x2001bb28 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb2c, L0x2001bb2c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb30, L0x2001bb30 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb34, L0x2001bb34 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb38, L0x2001bb38 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb3c, L0x2001bb3c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb40, L0x2001bb40 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb44, L0x2001bb44 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb48, L0x2001bb48 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb4c, L0x2001bb4c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb50, L0x2001bb50 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb54, L0x2001bb54 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb58, L0x2001bb58 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb5c, L0x2001bb5c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb60, L0x2001bb60 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb64, L0x2001bb64 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb68, L0x2001bb68 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb6c, L0x2001bb6c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb70, L0x2001bb70 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb74, L0x2001bb74 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb78, L0x2001bb78 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb7c, L0x2001bb7c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb80, L0x2001bb80 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb84, L0x2001bb84 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb88, L0x2001bb88 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb8c, L0x2001bb8c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb90, L0x2001bb90 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb94, L0x2001bb94 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb98, L0x2001bb98 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bb9c, L0x2001bb9c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bba0, L0x2001bba0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bba4, L0x2001bba4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bba8, L0x2001bba8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bbac, L0x2001bbac <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bbb0, L0x2001bbb0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bbb4, L0x2001bbb4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bbb8, L0x2001bbb8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bbbc, L0x2001bbbc <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bbc0, L0x2001bbc0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bbc4, L0x2001bbc4 <=s 1@32*1043969@32,
(-64)@32*1043969@32 <=s L0x2001bbc8, L0x2001bbc8 <=s 64@32*1043969@32,
(-64)@32*1043969@32 <=s L0x2001bbcc, L0x2001bbcc <=s 64@32*1043969@32,
(-64)@32*1043969@32 <=s L0x2001bbd0, L0x2001bbd0 <=s 64@32*1043969@32,
(-64)@32*1043969@32 <=s L0x2001bbd4, L0x2001bbd4 <=s 64@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001bbd8, L0x2001bbd8 <=s 32@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001bbdc, L0x2001bbdc <=s 32@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001bbe0, L0x2001bbe0 <=s 32@32*1043969@32,
(-32)@32*1043969@32 <=s L0x2001bbe4, L0x2001bbe4 <=s 32@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001bbe8, L0x2001bbe8 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001bbec, L0x2001bbec <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001bbf0, L0x2001bbf0 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001bbf4, L0x2001bbf4 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001bbf8, L0x2001bbf8 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001bbfc, L0x2001bbfc <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001bc00, L0x2001bc00 <=s 16@32*1043969@32,
(-16)@32*1043969@32 <=s L0x2001bc04, L0x2001bc04 <=s 16@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001bc08, L0x2001bc08 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001bc0c, L0x2001bc0c <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001bc10, L0x2001bc10 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001bc14, L0x2001bc14 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001bc18, L0x2001bc18 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001bc1c, L0x2001bc1c <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001bc20, L0x2001bc20 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001bc24, L0x2001bc24 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001bc28, L0x2001bc28 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001bc2c, L0x2001bc2c <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001bc30, L0x2001bc30 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001bc34, L0x2001bc34 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001bc38, L0x2001bc38 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001bc3c, L0x2001bc3c <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001bc40, L0x2001bc40 <=s 8@32*1043969@32,
(-8)@32*1043969@32 <=s L0x2001bc44, L0x2001bc44 <=s 8@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bc48, L0x2001bc48 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bc4c, L0x2001bc4c <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bc50, L0x2001bc50 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bc54, L0x2001bc54 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bc58, L0x2001bc58 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bc5c, L0x2001bc5c <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bc60, L0x2001bc60 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bc64, L0x2001bc64 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bc68, L0x2001bc68 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bc6c, L0x2001bc6c <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bc70, L0x2001bc70 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bc74, L0x2001bc74 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bc78, L0x2001bc78 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bc7c, L0x2001bc7c <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bc80, L0x2001bc80 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bc84, L0x2001bc84 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bc88, L0x2001bc88 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bc8c, L0x2001bc8c <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bc90, L0x2001bc90 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bc94, L0x2001bc94 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bc98, L0x2001bc98 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bc9c, L0x2001bc9c <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bca0, L0x2001bca0 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bca4, L0x2001bca4 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bca8, L0x2001bca8 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bcac, L0x2001bcac <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bcb0, L0x2001bcb0 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bcb4, L0x2001bcb4 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bcb8, L0x2001bcb8 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bcbc, L0x2001bcbc <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bcc0, L0x2001bcc0 <=s 4@32*1043969@32,
(-4)@32*1043969@32 <=s L0x2001bcc4, L0x2001bcc4 <=s 4@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bcc8, L0x2001bcc8 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bccc, L0x2001bccc <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bcd0, L0x2001bcd0 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bcd4, L0x2001bcd4 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bcd8, L0x2001bcd8 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bcdc, L0x2001bcdc <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bce0, L0x2001bce0 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bce4, L0x2001bce4 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bce8, L0x2001bce8 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bcec, L0x2001bcec <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bcf0, L0x2001bcf0 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bcf4, L0x2001bcf4 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bcf8, L0x2001bcf8 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bcfc, L0x2001bcfc <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd00, L0x2001bd00 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd04, L0x2001bd04 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd08, L0x2001bd08 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd0c, L0x2001bd0c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd10, L0x2001bd10 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd14, L0x2001bd14 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd18, L0x2001bd18 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd1c, L0x2001bd1c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd20, L0x2001bd20 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd24, L0x2001bd24 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd28, L0x2001bd28 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd2c, L0x2001bd2c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd30, L0x2001bd30 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd34, L0x2001bd34 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd38, L0x2001bd38 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd3c, L0x2001bd3c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd40, L0x2001bd40 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd44, L0x2001bd44 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd48, L0x2001bd48 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd4c, L0x2001bd4c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd50, L0x2001bd50 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd54, L0x2001bd54 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd58, L0x2001bd58 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd5c, L0x2001bd5c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd60, L0x2001bd60 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd64, L0x2001bd64 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd68, L0x2001bd68 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd6c, L0x2001bd6c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd70, L0x2001bd70 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd74, L0x2001bd74 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd78, L0x2001bd78 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd7c, L0x2001bd7c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd80, L0x2001bd80 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd84, L0x2001bd84 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd88, L0x2001bd88 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd8c, L0x2001bd8c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd90, L0x2001bd90 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd94, L0x2001bd94 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd98, L0x2001bd98 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bd9c, L0x2001bd9c <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bda0, L0x2001bda0 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bda4, L0x2001bda4 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bda8, L0x2001bda8 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bdac, L0x2001bdac <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bdb0, L0x2001bdb0 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bdb4, L0x2001bdb4 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bdb8, L0x2001bdb8 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bdbc, L0x2001bdbc <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bdc0, L0x2001bdc0 <=s 2@32*1043969@32,
(-2)@32*1043969@32 <=s L0x2001bdc4, L0x2001bdc4 <=s 2@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bdc8, L0x2001bdc8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bdcc, L0x2001bdcc <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bdd0, L0x2001bdd0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bdd4, L0x2001bdd4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bdd8, L0x2001bdd8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bddc, L0x2001bddc <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bde0, L0x2001bde0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bde4, L0x2001bde4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bde8, L0x2001bde8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bdec, L0x2001bdec <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bdf0, L0x2001bdf0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bdf4, L0x2001bdf4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bdf8, L0x2001bdf8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bdfc, L0x2001bdfc <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be00, L0x2001be00 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be04, L0x2001be04 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be08, L0x2001be08 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be0c, L0x2001be0c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be10, L0x2001be10 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be14, L0x2001be14 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be18, L0x2001be18 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be1c, L0x2001be1c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be20, L0x2001be20 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be24, L0x2001be24 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be28, L0x2001be28 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be2c, L0x2001be2c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be30, L0x2001be30 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be34, L0x2001be34 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be38, L0x2001be38 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be3c, L0x2001be3c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be40, L0x2001be40 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be44, L0x2001be44 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be48, L0x2001be48 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be4c, L0x2001be4c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be50, L0x2001be50 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be54, L0x2001be54 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be58, L0x2001be58 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be5c, L0x2001be5c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be60, L0x2001be60 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be64, L0x2001be64 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be68, L0x2001be68 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be6c, L0x2001be6c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be70, L0x2001be70 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be74, L0x2001be74 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be78, L0x2001be78 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be7c, L0x2001be7c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be80, L0x2001be80 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be84, L0x2001be84 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be88, L0x2001be88 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be8c, L0x2001be8c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be90, L0x2001be90 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be94, L0x2001be94 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be98, L0x2001be98 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001be9c, L0x2001be9c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bea0, L0x2001bea0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bea4, L0x2001bea4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bea8, L0x2001bea8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001beac, L0x2001beac <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001beb0, L0x2001beb0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001beb4, L0x2001beb4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001beb8, L0x2001beb8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bebc, L0x2001bebc <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bec0, L0x2001bec0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bec4, L0x2001bec4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bec8, L0x2001bec8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001becc, L0x2001becc <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bed0, L0x2001bed0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bed4, L0x2001bed4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bed8, L0x2001bed8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bedc, L0x2001bedc <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bee0, L0x2001bee0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bee4, L0x2001bee4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bee8, L0x2001bee8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001beec, L0x2001beec <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bef0, L0x2001bef0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bef4, L0x2001bef4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bef8, L0x2001bef8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001befc, L0x2001befc <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf00, L0x2001bf00 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf04, L0x2001bf04 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf08, L0x2001bf08 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf0c, L0x2001bf0c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf10, L0x2001bf10 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf14, L0x2001bf14 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf18, L0x2001bf18 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf1c, L0x2001bf1c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf20, L0x2001bf20 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf24, L0x2001bf24 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf28, L0x2001bf28 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf2c, L0x2001bf2c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf30, L0x2001bf30 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf34, L0x2001bf34 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf38, L0x2001bf38 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf3c, L0x2001bf3c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf40, L0x2001bf40 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf44, L0x2001bf44 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf48, L0x2001bf48 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf4c, L0x2001bf4c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf50, L0x2001bf50 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf54, L0x2001bf54 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf58, L0x2001bf58 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf5c, L0x2001bf5c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf60, L0x2001bf60 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf64, L0x2001bf64 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf68, L0x2001bf68 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf6c, L0x2001bf6c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf70, L0x2001bf70 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf74, L0x2001bf74 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf78, L0x2001bf78 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf7c, L0x2001bf7c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf80, L0x2001bf80 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf84, L0x2001bf84 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf88, L0x2001bf88 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf8c, L0x2001bf8c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf90, L0x2001bf90 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf94, L0x2001bf94 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf98, L0x2001bf98 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bf9c, L0x2001bf9c <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bfa0, L0x2001bfa0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bfa4, L0x2001bfa4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bfa8, L0x2001bfa8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bfac, L0x2001bfac <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bfb0, L0x2001bfb0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bfb4, L0x2001bfb4 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bfb8, L0x2001bfb8 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bfbc, L0x2001bfbc <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bfc0, L0x2001bfc0 <=s 1@32*1043969@32,
(-1)@32*1043969@32 <=s L0x2001bfc4, L0x2001bfc4 <=s 1@32*1043969@32
]
}

(**************** CUT    0 ****************)

ecut and [
segment00**2 = 
L0x2001afc8*x** 0+L0x2001afcc*x** 1+L0x2001afd0*x** 2+L0x2001afd4*x** 3+
L0x2001afd8*x** 4+L0x2001afdc*x** 5+L0x2001afe0*x** 6+L0x2001afe4*x** 7+
L0x2001afe8*x** 8+L0x2001afec*x** 9+L0x2001aff0*x**10+L0x2001aff4*x**11+
L0x2001aff8*x**12+L0x2001affc*x**13+L0x2001b000*x**14+L0x2001b004*x**15,
segment01**2 = 
L0x2001b008*x** 0+L0x2001b00c*x** 1+L0x2001b010*x** 2+L0x2001b014*x** 3+
L0x2001b018*x** 4+L0x2001b01c*x** 5+L0x2001b020*x** 6+L0x2001b024*x** 7+
L0x2001b028*x** 8+L0x2001b02c*x** 9+L0x2001b030*x**10+L0x2001b034*x**11+
L0x2001b038*x**12+L0x2001b03c*x**13+L0x2001b040*x**14+L0x2001b044*x**15,
segment02**2 = 
L0x2001b048*x** 0+L0x2001b04c*x** 1+L0x2001b050*x** 2+L0x2001b054*x** 3+
L0x2001b058*x** 4+L0x2001b05c*x** 5+L0x2001b060*x** 6+L0x2001b064*x** 7+
L0x2001b068*x** 8+L0x2001b06c*x** 9+L0x2001b070*x**10+L0x2001b074*x**11+
L0x2001b078*x**12+L0x2001b07c*x**13+L0x2001b080*x**14+L0x2001b084*x**15,
segment03**2 = 
L0x2001b088*x** 0+L0x2001b08c*x** 1+L0x2001b090*x** 2+L0x2001b094*x** 3+
L0x2001b098*x** 4+L0x2001b09c*x** 5+L0x2001b0a0*x** 6+L0x2001b0a4*x** 7+
L0x2001b0a8*x** 8+L0x2001b0ac*x** 9+L0x2001b0b0*x**10+L0x2001b0b4*x**11+
L0x2001b0b8*x**12+L0x2001b0bc*x**13+L0x2001b0c0*x**14+L0x2001b0c4*x**15,
segment04**2 = 
L0x2001b0c8*x** 0+L0x2001b0cc*x** 1+L0x2001b0d0*x** 2+L0x2001b0d4*x** 3+
L0x2001b0d8*x** 4+L0x2001b0dc*x** 5+L0x2001b0e0*x** 6+L0x2001b0e4*x** 7+
L0x2001b0e8*x** 8+L0x2001b0ec*x** 9+L0x2001b0f0*x**10+L0x2001b0f4*x**11+
L0x2001b0f8*x**12+L0x2001b0fc*x**13+L0x2001b100*x**14+L0x2001b104*x**15,
segment05**2 = 
L0x2001b108*x** 0+L0x2001b10c*x** 1+L0x2001b110*x** 2+L0x2001b114*x** 3+
L0x2001b118*x** 4+L0x2001b11c*x** 5+L0x2001b120*x** 6+L0x2001b124*x** 7+
L0x2001b128*x** 8+L0x2001b12c*x** 9+L0x2001b130*x**10+L0x2001b134*x**11+
L0x2001b138*x**12+L0x2001b13c*x**13+L0x2001b140*x**14+L0x2001b144*x**15,
segment06**2 = 
L0x2001b148*x** 0+L0x2001b14c*x** 1+L0x2001b150*x** 2+L0x2001b154*x** 3+
L0x2001b158*x** 4+L0x2001b15c*x** 5+L0x2001b160*x** 6+L0x2001b164*x** 7+
L0x2001b168*x** 8+L0x2001b16c*x** 9+L0x2001b170*x**10+L0x2001b174*x**11+
L0x2001b178*x**12+L0x2001b17c*x**13+L0x2001b180*x**14+L0x2001b184*x**15,
segment07**2 = 
L0x2001b188*x** 0+L0x2001b18c*x** 1+L0x2001b190*x** 2+L0x2001b194*x** 3+
L0x2001b198*x** 4+L0x2001b19c*x** 5+L0x2001b1a0*x** 6+L0x2001b1a4*x** 7+
L0x2001b1a8*x** 8+L0x2001b1ac*x** 9+L0x2001b1b0*x**10+L0x2001b1b4*x**11+
L0x2001b1b8*x**12+L0x2001b1bc*x**13+L0x2001b1c0*x**14+L0x2001b1c4*x**15,
segment08**2 = 
L0x2001b1c8*x** 0+L0x2001b1cc*x** 1+L0x2001b1d0*x** 2+L0x2001b1d4*x** 3+
L0x2001b1d8*x** 4+L0x2001b1dc*x** 5+L0x2001b1e0*x** 6+L0x2001b1e4*x** 7+
L0x2001b1e8*x** 8+L0x2001b1ec*x** 9+L0x2001b1f0*x**10+L0x2001b1f4*x**11+
L0x2001b1f8*x**12+L0x2001b1fc*x**13+L0x2001b200*x**14+L0x2001b204*x**15,
segment09**2 = 
L0x2001b208*x** 0+L0x2001b20c*x** 1+L0x2001b210*x** 2+L0x2001b214*x** 3+
L0x2001b218*x** 4+L0x2001b21c*x** 5+L0x2001b220*x** 6+L0x2001b224*x** 7+
L0x2001b228*x** 8+L0x2001b22c*x** 9+L0x2001b230*x**10+L0x2001b234*x**11+
L0x2001b238*x**12+L0x2001b23c*x**13+L0x2001b240*x**14+L0x2001b244*x**15,
segment0a**2 = 
L0x2001b248*x** 0+L0x2001b24c*x** 1+L0x2001b250*x** 2+L0x2001b254*x** 3+
L0x2001b258*x** 4+L0x2001b25c*x** 5+L0x2001b260*x** 6+L0x2001b264*x** 7+
L0x2001b268*x** 8+L0x2001b26c*x** 9+L0x2001b270*x**10+L0x2001b274*x**11+
L0x2001b278*x**12+L0x2001b27c*x**13+L0x2001b280*x**14+L0x2001b284*x**15,
segment0b**2 = 
L0x2001b288*x** 0+L0x2001b28c*x** 1+L0x2001b290*x** 2+L0x2001b294*x** 3+
L0x2001b298*x** 4+L0x2001b29c*x** 5+L0x2001b2a0*x** 6+L0x2001b2a4*x** 7+
L0x2001b2a8*x** 8+L0x2001b2ac*x** 9+L0x2001b2b0*x**10+L0x2001b2b4*x**11+
L0x2001b2b8*x**12+L0x2001b2bc*x**13+L0x2001b2c0*x**14+L0x2001b2c4*x**15,
segment0c**2 = 
L0x2001b2c8*x** 0+L0x2001b2cc*x** 1+L0x2001b2d0*x** 2+L0x2001b2d4*x** 3+
L0x2001b2d8*x** 4+L0x2001b2dc*x** 5+L0x2001b2e0*x** 6+L0x2001b2e4*x** 7+
L0x2001b2e8*x** 8+L0x2001b2ec*x** 9+L0x2001b2f0*x**10+L0x2001b2f4*x**11+
L0x2001b2f8*x**12+L0x2001b2fc*x**13+L0x2001b300*x**14+L0x2001b304*x**15,
segment0d**2 = 
L0x2001b308*x** 0+L0x2001b30c*x** 1+L0x2001b310*x** 2+L0x2001b314*x** 3+
L0x2001b318*x** 4+L0x2001b31c*x** 5+L0x2001b320*x** 6+L0x2001b324*x** 7+
L0x2001b328*x** 8+L0x2001b32c*x** 9+L0x2001b330*x**10+L0x2001b334*x**11+
L0x2001b338*x**12+L0x2001b33c*x**13+L0x2001b340*x**14+L0x2001b344*x**15,
segment0e**2 = 
L0x2001b348*x** 0+L0x2001b34c*x** 1+L0x2001b350*x** 2+L0x2001b354*x** 3+
L0x2001b358*x** 4+L0x2001b35c*x** 5+L0x2001b360*x** 6+L0x2001b364*x** 7+
L0x2001b368*x** 8+L0x2001b36c*x** 9+L0x2001b370*x**10+L0x2001b374*x**11+
L0x2001b378*x**12+L0x2001b37c*x**13+L0x2001b380*x**14+L0x2001b384*x**15,
segment0f**2 = 
L0x2001b388*x** 0+L0x2001b38c*x** 1+L0x2001b390*x** 2+L0x2001b394*x** 3+
L0x2001b398*x** 4+L0x2001b39c*x** 5+L0x2001b3a0*x** 6+L0x2001b3a4*x** 7+
L0x2001b3a8*x** 8+L0x2001b3ac*x** 9+L0x2001b3b0*x**10+L0x2001b3b4*x**11+
L0x2001b3b8*x**12+L0x2001b3bc*x**13+L0x2001b3c0*x**14+L0x2001b3c4*x**15,
segment0 = 
(segment00**2)*x** 0+(segment01**2)*x**16+(segment02**2)*x**32+
(segment03**2)*x**48+(segment04**2)*x**64+(segment05**2)*x**80+
(segment06**2)*x**96+(segment07**2)*x**112+(segment08**2)*x**128+
(segment09**2)*x**144+(segment0a**2)*x**160+(segment0b**2)*x**176+
(segment0c**2)*x**192+(segment0d**2)*x**208+(segment0e**2)*x**224+
(segment0f**2)*x**240
] prove with [ precondition ];



(**************** CUT    1 ****************)

ecut and [
eqmod (64*inp_poly00**2) (segment0) [1043969, x**4 - 1],
eqmod (64*inp_poly01**2) (segment0) [1043969, x**4 - 1043968],
eqmod (64*inp_poly02**2) (segment0) [1043969, x**4 - 554923],
eqmod (64*inp_poly03**2) (segment0) [1043969, x**4 - 489046],
eqmod (64*inp_poly04**2) (segment0) [1043969, x**4 - 287998],
eqmod (64*inp_poly05**2) (segment0) [1043969, x**4 - 755971],
eqmod (64*inp_poly06**2) (segment0) [1043969, x**4 - 719789],
eqmod (64*inp_poly07**2) (segment0) [1043969, x**4 - 324180],
eqmod (64*inp_poly08**2) (segment0) [1043969, x**4 - 29512],
eqmod (64*inp_poly09**2) (segment0) [1043969, x**4 - 1014457],
eqmod (64*inp_poly0a**2) (segment0) [1043969, x**4 - 145873],
eqmod (64*inp_poly0b**2) (segment0) [1043969, x**4 - 898096],
eqmod (64*inp_poly0c**2) (segment0) [1043969, x**4 - 445347],
eqmod (64*inp_poly0d**2) (segment0) [1043969, x**4 - 598622],
eqmod (64*inp_poly0e**2) (segment0) [1043969, x**4 - 775725],
eqmod (64*inp_poly0f**2) (segment0) [1043969, x**4 - 268244],
eqmod (64*inp_poly10**2) (segment0) [1043969, x**4 - 754540],
eqmod (64*inp_poly11**2) (segment0) [1043969, x**4 - 289429],
eqmod (64*inp_poly12**2) (segment0) [1043969, x**4 - 689776],
eqmod (64*inp_poly13**2) (segment0) [1043969, x**4 - 354193],
eqmod (64*inp_poly14**2) (segment0) [1043969, x**4 - 731663],
eqmod (64*inp_poly15**2) (segment0) [1043969, x**4 - 312306],
eqmod (64*inp_poly16**2) (segment0) [1043969, x**4 - 379345],
eqmod (64*inp_poly17**2) (segment0) [1043969, x**4 - 664624],
eqmod (64*inp_poly18**2) (segment0) [1043969, x**4 - 125710],
eqmod (64*inp_poly19**2) (segment0) [1043969, x**4 - 918259],
eqmod (64*inp_poly1a**2) (segment0) [1043969, x**4 - 317781],
eqmod (64*inp_poly1b**2) (segment0) [1043969, x**4 - 726188],
eqmod (64*inp_poly1c**2) (segment0) [1043969, x**4 - 427629],
eqmod (64*inp_poly1d**2) (segment0) [1043969, x**4 - 616340],
eqmod (64*inp_poly1e**2) (segment0) [1043969, x**4 - 750053],
eqmod (64*inp_poly1f**2) (segment0) [1043969, x**4 - 293916],
eqmod (64*inp_poly20**2) (segment0) [1043969, x**4 - 587782],
eqmod (64*inp_poly21**2) (segment0) [1043969, x**4 - 456187],
eqmod (64*inp_poly22**2) (segment0) [1043969, x**4 - 252302],
eqmod (64*inp_poly23**2) (segment0) [1043969, x**4 - 791667],
eqmod (64*inp_poly24**2) (segment0) [1043969, x**4 - 467086],
eqmod (64*inp_poly25**2) (segment0) [1043969, x**4 - 576883],
eqmod (64*inp_poly26**2) (segment0) [1043969, x**4 - 141058],
eqmod (64*inp_poly27**2) (segment0) [1043969, x**4 - 902911],
eqmod (64*inp_poly28**2) (segment0) [1043969, x**4 - 33480],
eqmod (64*inp_poly29**2) (segment0) [1043969, x**4 - 1010489],
eqmod (64*inp_poly2a**2) (segment0) [1043969, x**4 - 349716],
eqmod (64*inp_poly2b**2) (segment0) [1043969, x**4 - 694253],
eqmod (64*inp_poly2c**2) (segment0) [1043969, x**4 - 75356],
eqmod (64*inp_poly2d**2) (segment0) [1043969, x**4 - 968613],
eqmod (64*inp_poly2e**2) (segment0) [1043969, x**4 - 599293],
eqmod (64*inp_poly2f**2) (segment0) [1043969, x**4 - 444676],
eqmod (64*inp_poly30**2) (segment0) [1043969, x**4 - 899855],
eqmod (64*inp_poly31**2) (segment0) [1043969, x**4 - 144114],
eqmod (64*inp_poly32**2) (segment0) [1043969, x**4 - 28054],
eqmod (64*inp_poly33**2) (segment0) [1043969, x**4 - 1015915],
eqmod (64*inp_poly34**2) (segment0) [1043969, x**4 - 531761],
eqmod (64*inp_poly35**2) (segment0) [1043969, x**4 - 512208],
eqmod (64*inp_poly36**2) (segment0) [1043969, x**4 - 219801],
eqmod (64*inp_poly37**2) (segment0) [1043969, x**4 - 824168],
eqmod (64*inp_poly38**2) (segment0) [1043969, x**4 - 37338],
eqmod (64*inp_poly39**2) (segment0) [1043969, x**4 - 1006631],
eqmod (64*inp_poly3a**2) (segment0) [1043969, x**4 - 62231],
eqmod (64*inp_poly3b**2) (segment0) [1043969, x**4 - 981738],
eqmod (64*inp_poly3c**2) (segment0) [1043969, x**4 - 388624],
eqmod (64*inp_poly3d**2) (segment0) [1043969, x**4 - 655345],
eqmod (64*inp_poly3e**2) (segment0) [1043969, x**4 - 587715],
eqmod (64*inp_poly3f**2) (segment0) [1043969, x**4 - 456254]
] prove with [ precondition ];



(**************** CUT    2 ****************)

ecut and [
segment10**2 = 
L0x2001b3c8*x** 0+L0x2001b3cc*x** 1+L0x2001b3d0*x** 2+L0x2001b3d4*x** 3+
L0x2001b3d8*x** 4+L0x2001b3dc*x** 5+L0x2001b3e0*x** 6+L0x2001b3e4*x** 7+
L0x2001b3e8*x** 8+L0x2001b3ec*x** 9+L0x2001b3f0*x**10+L0x2001b3f4*x**11+
L0x2001b3f8*x**12+L0x2001b3fc*x**13+L0x2001b400*x**14+L0x2001b404*x**15,
segment11**2 = 
L0x2001b408*x** 0+L0x2001b40c*x** 1+L0x2001b410*x** 2+L0x2001b414*x** 3+
L0x2001b418*x** 4+L0x2001b41c*x** 5+L0x2001b420*x** 6+L0x2001b424*x** 7+
L0x2001b428*x** 8+L0x2001b42c*x** 9+L0x2001b430*x**10+L0x2001b434*x**11+
L0x2001b438*x**12+L0x2001b43c*x**13+L0x2001b440*x**14+L0x2001b444*x**15,
segment12**2 = 
L0x2001b448*x** 0+L0x2001b44c*x** 1+L0x2001b450*x** 2+L0x2001b454*x** 3+
L0x2001b458*x** 4+L0x2001b45c*x** 5+L0x2001b460*x** 6+L0x2001b464*x** 7+
L0x2001b468*x** 8+L0x2001b46c*x** 9+L0x2001b470*x**10+L0x2001b474*x**11+
L0x2001b478*x**12+L0x2001b47c*x**13+L0x2001b480*x**14+L0x2001b484*x**15,
segment13**2 = 
L0x2001b488*x** 0+L0x2001b48c*x** 1+L0x2001b490*x** 2+L0x2001b494*x** 3+
L0x2001b498*x** 4+L0x2001b49c*x** 5+L0x2001b4a0*x** 6+L0x2001b4a4*x** 7+
L0x2001b4a8*x** 8+L0x2001b4ac*x** 9+L0x2001b4b0*x**10+L0x2001b4b4*x**11+
L0x2001b4b8*x**12+L0x2001b4bc*x**13+L0x2001b4c0*x**14+L0x2001b4c4*x**15,
segment14**2 = 
L0x2001b4c8*x** 0+L0x2001b4cc*x** 1+L0x2001b4d0*x** 2+L0x2001b4d4*x** 3+
L0x2001b4d8*x** 4+L0x2001b4dc*x** 5+L0x2001b4e0*x** 6+L0x2001b4e4*x** 7+
L0x2001b4e8*x** 8+L0x2001b4ec*x** 9+L0x2001b4f0*x**10+L0x2001b4f4*x**11+
L0x2001b4f8*x**12+L0x2001b4fc*x**13+L0x2001b500*x**14+L0x2001b504*x**15,
segment15**2 = 
L0x2001b508*x** 0+L0x2001b50c*x** 1+L0x2001b510*x** 2+L0x2001b514*x** 3+
L0x2001b518*x** 4+L0x2001b51c*x** 5+L0x2001b520*x** 6+L0x2001b524*x** 7+
L0x2001b528*x** 8+L0x2001b52c*x** 9+L0x2001b530*x**10+L0x2001b534*x**11+
L0x2001b538*x**12+L0x2001b53c*x**13+L0x2001b540*x**14+L0x2001b544*x**15,
segment16**2 = 
L0x2001b548*x** 0+L0x2001b54c*x** 1+L0x2001b550*x** 2+L0x2001b554*x** 3+
L0x2001b558*x** 4+L0x2001b55c*x** 5+L0x2001b560*x** 6+L0x2001b564*x** 7+
L0x2001b568*x** 8+L0x2001b56c*x** 9+L0x2001b570*x**10+L0x2001b574*x**11+
L0x2001b578*x**12+L0x2001b57c*x**13+L0x2001b580*x**14+L0x2001b584*x**15,
segment17**2 = 
L0x2001b588*x** 0+L0x2001b58c*x** 1+L0x2001b590*x** 2+L0x2001b594*x** 3+
L0x2001b598*x** 4+L0x2001b59c*x** 5+L0x2001b5a0*x** 6+L0x2001b5a4*x** 7+
L0x2001b5a8*x** 8+L0x2001b5ac*x** 9+L0x2001b5b0*x**10+L0x2001b5b4*x**11+
L0x2001b5b8*x**12+L0x2001b5bc*x**13+L0x2001b5c0*x**14+L0x2001b5c4*x**15,
segment18**2 = 
L0x2001b5c8*x** 0+L0x2001b5cc*x** 1+L0x2001b5d0*x** 2+L0x2001b5d4*x** 3+
L0x2001b5d8*x** 4+L0x2001b5dc*x** 5+L0x2001b5e0*x** 6+L0x2001b5e4*x** 7+
L0x2001b5e8*x** 8+L0x2001b5ec*x** 9+L0x2001b5f0*x**10+L0x2001b5f4*x**11+
L0x2001b5f8*x**12+L0x2001b5fc*x**13+L0x2001b600*x**14+L0x2001b604*x**15,
segment19**2 = 
L0x2001b608*x** 0+L0x2001b60c*x** 1+L0x2001b610*x** 2+L0x2001b614*x** 3+
L0x2001b618*x** 4+L0x2001b61c*x** 5+L0x2001b620*x** 6+L0x2001b624*x** 7+
L0x2001b628*x** 8+L0x2001b62c*x** 9+L0x2001b630*x**10+L0x2001b634*x**11+
L0x2001b638*x**12+L0x2001b63c*x**13+L0x2001b640*x**14+L0x2001b644*x**15,
segment1a**2 = 
L0x2001b648*x** 0+L0x2001b64c*x** 1+L0x2001b650*x** 2+L0x2001b654*x** 3+
L0x2001b658*x** 4+L0x2001b65c*x** 5+L0x2001b660*x** 6+L0x2001b664*x** 7+
L0x2001b668*x** 8+L0x2001b66c*x** 9+L0x2001b670*x**10+L0x2001b674*x**11+
L0x2001b678*x**12+L0x2001b67c*x**13+L0x2001b680*x**14+L0x2001b684*x**15,
segment1b**2 = 
L0x2001b688*x** 0+L0x2001b68c*x** 1+L0x2001b690*x** 2+L0x2001b694*x** 3+
L0x2001b698*x** 4+L0x2001b69c*x** 5+L0x2001b6a0*x** 6+L0x2001b6a4*x** 7+
L0x2001b6a8*x** 8+L0x2001b6ac*x** 9+L0x2001b6b0*x**10+L0x2001b6b4*x**11+
L0x2001b6b8*x**12+L0x2001b6bc*x**13+L0x2001b6c0*x**14+L0x2001b6c4*x**15,
segment1c**2 = 
L0x2001b6c8*x** 0+L0x2001b6cc*x** 1+L0x2001b6d0*x** 2+L0x2001b6d4*x** 3+
L0x2001b6d8*x** 4+L0x2001b6dc*x** 5+L0x2001b6e0*x** 6+L0x2001b6e4*x** 7+
L0x2001b6e8*x** 8+L0x2001b6ec*x** 9+L0x2001b6f0*x**10+L0x2001b6f4*x**11+
L0x2001b6f8*x**12+L0x2001b6fc*x**13+L0x2001b700*x**14+L0x2001b704*x**15,
segment1d**2 = 
L0x2001b708*x** 0+L0x2001b70c*x** 1+L0x2001b710*x** 2+L0x2001b714*x** 3+
L0x2001b718*x** 4+L0x2001b71c*x** 5+L0x2001b720*x** 6+L0x2001b724*x** 7+
L0x2001b728*x** 8+L0x2001b72c*x** 9+L0x2001b730*x**10+L0x2001b734*x**11+
L0x2001b738*x**12+L0x2001b73c*x**13+L0x2001b740*x**14+L0x2001b744*x**15,
segment1e**2 = 
L0x2001b748*x** 0+L0x2001b74c*x** 1+L0x2001b750*x** 2+L0x2001b754*x** 3+
L0x2001b758*x** 4+L0x2001b75c*x** 5+L0x2001b760*x** 6+L0x2001b764*x** 7+
L0x2001b768*x** 8+L0x2001b76c*x** 9+L0x2001b770*x**10+L0x2001b774*x**11+
L0x2001b778*x**12+L0x2001b77c*x**13+L0x2001b780*x**14+L0x2001b784*x**15,
segment1f**2 = 
L0x2001b788*x** 0+L0x2001b78c*x** 1+L0x2001b790*x** 2+L0x2001b794*x** 3+
L0x2001b798*x** 4+L0x2001b79c*x** 5+L0x2001b7a0*x** 6+L0x2001b7a4*x** 7+
L0x2001b7a8*x** 8+L0x2001b7ac*x** 9+L0x2001b7b0*x**10+L0x2001b7b4*x**11+
L0x2001b7b8*x**12+L0x2001b7bc*x**13+L0x2001b7c0*x**14+L0x2001b7c4*x**15,
segment1 = 
(segment10**2)*x** 0+(segment11**2)*x**16+(segment12**2)*x**32+
(segment13**2)*x**48+(segment14**2)*x**64+(segment15**2)*x**80+
(segment16**2)*x**96+(segment17**2)*x**112+(segment18**2)*x**128+
(segment19**2)*x**144+(segment1a**2)*x**160+(segment1b**2)*x**176+
(segment1c**2)*x**192+(segment1d**2)*x**208+(segment1e**2)*x**224+
(segment1f**2)*x**240
] prove with [ precondition ];



(**************** CUT    3 ****************)

ecut and [
eqmod (64*inp_poly40**2) (segment1) [1043969, x**4 - 1013205],
eqmod (64*inp_poly41**2) (segment1) [1043969, x**4 - 30764],
eqmod (64*inp_poly42**2) (segment1) [1043969, x**4 - 373885],
eqmod (64*inp_poly43**2) (segment1) [1043969, x**4 - 670084],
eqmod (64*inp_poly44**2) (segment1) [1043969, x**4 - 194431],
eqmod (64*inp_poly45**2) (segment1) [1043969, x**4 - 849538],
eqmod (64*inp_poly46**2) (segment1) [1043969, x**4 - 37663],
eqmod (64*inp_poly47**2) (segment1) [1043969, x**4 - 1006306],
eqmod (64*inp_poly48**2) (segment1) [1043969, x**4 - 345862],
eqmod (64*inp_poly49**2) (segment1) [1043969, x**4 - 698107],
eqmod (64*inp_poly4a**2) (segment1) [1043969, x**4 - 385759],
eqmod (64*inp_poly4b**2) (segment1) [1043969, x**4 - 658210],
eqmod (64*inp_poly4c**2) (segment1) [1043969, x**4 - 394048],
eqmod (64*inp_poly4d**2) (segment1) [1043969, x**4 - 649921],
eqmod (64*inp_poly4e**2) (segment1) [1043969, x**4 - 727440],
eqmod (64*inp_poly4f**2) (segment1) [1043969, x**4 - 316529],
eqmod (64*inp_poly50**2) (segment1) [1043969, x**4 - 1026124],
eqmod (64*inp_poly51**2) (segment1) [1043969, x**4 - 17845],
eqmod (64*inp_poly52**2) (segment1) [1043969, x**4 - 488999],
eqmod (64*inp_poly53**2) (segment1) [1043969, x**4 - 554970],
eqmod (64*inp_poly54**2) (segment1) [1043969, x**4 - 135077],
eqmod (64*inp_poly55**2) (segment1) [1043969, x**4 - 908892],
eqmod (64*inp_poly56**2) (segment1) [1043969, x**4 - 359871],
eqmod (64*inp_poly57**2) (segment1) [1043969, x**4 - 684098],
eqmod (64*inp_poly58**2) (segment1) [1043969, x**4 - 562705],
eqmod (64*inp_poly59**2) (segment1) [1043969, x**4 - 481264],
eqmod (64*inp_poly5a**2) (segment1) [1043969, x**4 - 555001],
eqmod (64*inp_poly5b**2) (segment1) [1043969, x**4 - 488968],
eqmod (64*inp_poly5c**2) (segment1) [1043969, x**4 - 518782],
eqmod (64*inp_poly5d**2) (segment1) [1043969, x**4 - 525187],
eqmod (64*inp_poly5e**2) (segment1) [1043969, x**4 - 216315],
eqmod (64*inp_poly5f**2) (segment1) [1043969, x**4 - 827654],
eqmod (64*inp_poly60**2) (segment1) [1043969, x**4 - 61601],
eqmod (64*inp_poly61**2) (segment1) [1043969, x**4 - 982368],
eqmod (64*inp_poly62**2) (segment1) [1043969, x**4 - 90787],
eqmod (64*inp_poly63**2) (segment1) [1043969, x**4 - 953182],
eqmod (64*inp_poly64**2) (segment1) [1043969, x**4 - 799581],
eqmod (64*inp_poly65**2) (segment1) [1043969, x**4 - 244388],
eqmod (64*inp_poly66**2) (segment1) [1043969, x**4 - 270821],
eqmod (64*inp_poly67**2) (segment1) [1043969, x**4 - 773148],
eqmod (64*inp_poly68**2) (segment1) [1043969, x**4 - 418683],
eqmod (64*inp_poly69**2) (segment1) [1043969, x**4 - 625286],
eqmod (64*inp_poly6a**2) (segment1) [1043969, x**4 - 481490],
eqmod (64*inp_poly6b**2) (segment1) [1043969, x**4 - 562479],
eqmod (64*inp_poly6c**2) (segment1) [1043969, x**4 - 403165],
eqmod (64*inp_poly6d**2) (segment1) [1043969, x**4 - 640804],
eqmod (64*inp_poly6e**2) (segment1) [1043969, x**4 - 886657],
eqmod (64*inp_poly6f**2) (segment1) [1043969, x**4 - 157312],
eqmod (64*inp_poly70**2) (segment1) [1043969, x**4 - 830722],
eqmod (64*inp_poly71**2) (segment1) [1043969, x**4 - 213247],
eqmod (64*inp_poly72**2) (segment1) [1043969, x**4 - 309107],
eqmod (64*inp_poly73**2) (segment1) [1043969, x**4 - 734862],
eqmod (64*inp_poly74**2) (segment1) [1043969, x**4 - 942795],
eqmod (64*inp_poly75**2) (segment1) [1043969, x**4 - 101174],
eqmod (64*inp_poly76**2) (segment1) [1043969, x**4 - 873218],
eqmod (64*inp_poly77**2) (segment1) [1043969, x**4 - 170751],
eqmod (64*inp_poly78**2) (segment1) [1043969, x**4 - 743637],
eqmod (64*inp_poly79**2) (segment1) [1043969, x**4 - 300332],
eqmod (64*inp_poly7a**2) (segment1) [1043969, x**4 - 164662],
eqmod (64*inp_poly7b**2) (segment1) [1043969, x**4 - 879307],
eqmod (64*inp_poly7c**2) (segment1) [1043969, x**4 - 948221],
eqmod (64*inp_poly7d**2) (segment1) [1043969, x**4 - 95748],
eqmod (64*inp_poly7e**2) (segment1) [1043969, x**4 - 34851],
eqmod (64*inp_poly7f**2) (segment1) [1043969, x**4 - 1009118]
] prove with [ precondition ];



(**************** CUT    4 ****************)

ecut and [
segment20**2 = 
L0x2001b7c8*x** 0+L0x2001b7cc*x** 1+L0x2001b7d0*x** 2+L0x2001b7d4*x** 3+
L0x2001b7d8*x** 4+L0x2001b7dc*x** 5+L0x2001b7e0*x** 6+L0x2001b7e4*x** 7+
L0x2001b7e8*x** 8+L0x2001b7ec*x** 9+L0x2001b7f0*x**10+L0x2001b7f4*x**11+
L0x2001b7f8*x**12+L0x2001b7fc*x**13+L0x2001b800*x**14+L0x2001b804*x**15,
segment21**2 = 
L0x2001b808*x** 0+L0x2001b80c*x** 1+L0x2001b810*x** 2+L0x2001b814*x** 3+
L0x2001b818*x** 4+L0x2001b81c*x** 5+L0x2001b820*x** 6+L0x2001b824*x** 7+
L0x2001b828*x** 8+L0x2001b82c*x** 9+L0x2001b830*x**10+L0x2001b834*x**11+
L0x2001b838*x**12+L0x2001b83c*x**13+L0x2001b840*x**14+L0x2001b844*x**15,
segment22**2 = 
L0x2001b848*x** 0+L0x2001b84c*x** 1+L0x2001b850*x** 2+L0x2001b854*x** 3+
L0x2001b858*x** 4+L0x2001b85c*x** 5+L0x2001b860*x** 6+L0x2001b864*x** 7+
L0x2001b868*x** 8+L0x2001b86c*x** 9+L0x2001b870*x**10+L0x2001b874*x**11+
L0x2001b878*x**12+L0x2001b87c*x**13+L0x2001b880*x**14+L0x2001b884*x**15,
segment23**2 = 
L0x2001b888*x** 0+L0x2001b88c*x** 1+L0x2001b890*x** 2+L0x2001b894*x** 3+
L0x2001b898*x** 4+L0x2001b89c*x** 5+L0x2001b8a0*x** 6+L0x2001b8a4*x** 7+
L0x2001b8a8*x** 8+L0x2001b8ac*x** 9+L0x2001b8b0*x**10+L0x2001b8b4*x**11+
L0x2001b8b8*x**12+L0x2001b8bc*x**13+L0x2001b8c0*x**14+L0x2001b8c4*x**15,
segment24**2 = 
L0x2001b8c8*x** 0+L0x2001b8cc*x** 1+L0x2001b8d0*x** 2+L0x2001b8d4*x** 3+
L0x2001b8d8*x** 4+L0x2001b8dc*x** 5+L0x2001b8e0*x** 6+L0x2001b8e4*x** 7+
L0x2001b8e8*x** 8+L0x2001b8ec*x** 9+L0x2001b8f0*x**10+L0x2001b8f4*x**11+
L0x2001b8f8*x**12+L0x2001b8fc*x**13+L0x2001b900*x**14+L0x2001b904*x**15,
segment25**2 = 
L0x2001b908*x** 0+L0x2001b90c*x** 1+L0x2001b910*x** 2+L0x2001b914*x** 3+
L0x2001b918*x** 4+L0x2001b91c*x** 5+L0x2001b920*x** 6+L0x2001b924*x** 7+
L0x2001b928*x** 8+L0x2001b92c*x** 9+L0x2001b930*x**10+L0x2001b934*x**11+
L0x2001b938*x**12+L0x2001b93c*x**13+L0x2001b940*x**14+L0x2001b944*x**15,
segment26**2 = 
L0x2001b948*x** 0+L0x2001b94c*x** 1+L0x2001b950*x** 2+L0x2001b954*x** 3+
L0x2001b958*x** 4+L0x2001b95c*x** 5+L0x2001b960*x** 6+L0x2001b964*x** 7+
L0x2001b968*x** 8+L0x2001b96c*x** 9+L0x2001b970*x**10+L0x2001b974*x**11+
L0x2001b978*x**12+L0x2001b97c*x**13+L0x2001b980*x**14+L0x2001b984*x**15,
segment27**2 = 
L0x2001b988*x** 0+L0x2001b98c*x** 1+L0x2001b990*x** 2+L0x2001b994*x** 3+
L0x2001b998*x** 4+L0x2001b99c*x** 5+L0x2001b9a0*x** 6+L0x2001b9a4*x** 7+
L0x2001b9a8*x** 8+L0x2001b9ac*x** 9+L0x2001b9b0*x**10+L0x2001b9b4*x**11+
L0x2001b9b8*x**12+L0x2001b9bc*x**13+L0x2001b9c0*x**14+L0x2001b9c4*x**15,
segment28**2 = 
L0x2001b9c8*x** 0+L0x2001b9cc*x** 1+L0x2001b9d0*x** 2+L0x2001b9d4*x** 3+
L0x2001b9d8*x** 4+L0x2001b9dc*x** 5+L0x2001b9e0*x** 6+L0x2001b9e4*x** 7+
L0x2001b9e8*x** 8+L0x2001b9ec*x** 9+L0x2001b9f0*x**10+L0x2001b9f4*x**11+
L0x2001b9f8*x**12+L0x2001b9fc*x**13+L0x2001ba00*x**14+L0x2001ba04*x**15,
segment29**2 = 
L0x2001ba08*x** 0+L0x2001ba0c*x** 1+L0x2001ba10*x** 2+L0x2001ba14*x** 3+
L0x2001ba18*x** 4+L0x2001ba1c*x** 5+L0x2001ba20*x** 6+L0x2001ba24*x** 7+
L0x2001ba28*x** 8+L0x2001ba2c*x** 9+L0x2001ba30*x**10+L0x2001ba34*x**11+
L0x2001ba38*x**12+L0x2001ba3c*x**13+L0x2001ba40*x**14+L0x2001ba44*x**15,
segment2a**2 = 
L0x2001ba48*x** 0+L0x2001ba4c*x** 1+L0x2001ba50*x** 2+L0x2001ba54*x** 3+
L0x2001ba58*x** 4+L0x2001ba5c*x** 5+L0x2001ba60*x** 6+L0x2001ba64*x** 7+
L0x2001ba68*x** 8+L0x2001ba6c*x** 9+L0x2001ba70*x**10+L0x2001ba74*x**11+
L0x2001ba78*x**12+L0x2001ba7c*x**13+L0x2001ba80*x**14+L0x2001ba84*x**15,
segment2b**2 = 
L0x2001ba88*x** 0+L0x2001ba8c*x** 1+L0x2001ba90*x** 2+L0x2001ba94*x** 3+
L0x2001ba98*x** 4+L0x2001ba9c*x** 5+L0x2001baa0*x** 6+L0x2001baa4*x** 7+
L0x2001baa8*x** 8+L0x2001baac*x** 9+L0x2001bab0*x**10+L0x2001bab4*x**11+
L0x2001bab8*x**12+L0x2001babc*x**13+L0x2001bac0*x**14+L0x2001bac4*x**15,
segment2c**2 = 
L0x2001bac8*x** 0+L0x2001bacc*x** 1+L0x2001bad0*x** 2+L0x2001bad4*x** 3+
L0x2001bad8*x** 4+L0x2001badc*x** 5+L0x2001bae0*x** 6+L0x2001bae4*x** 7+
L0x2001bae8*x** 8+L0x2001baec*x** 9+L0x2001baf0*x**10+L0x2001baf4*x**11+
L0x2001baf8*x**12+L0x2001bafc*x**13+L0x2001bb00*x**14+L0x2001bb04*x**15,
segment2d**2 = 
L0x2001bb08*x** 0+L0x2001bb0c*x** 1+L0x2001bb10*x** 2+L0x2001bb14*x** 3+
L0x2001bb18*x** 4+L0x2001bb1c*x** 5+L0x2001bb20*x** 6+L0x2001bb24*x** 7+
L0x2001bb28*x** 8+L0x2001bb2c*x** 9+L0x2001bb30*x**10+L0x2001bb34*x**11+
L0x2001bb38*x**12+L0x2001bb3c*x**13+L0x2001bb40*x**14+L0x2001bb44*x**15,
segment2e**2 = 
L0x2001bb48*x** 0+L0x2001bb4c*x** 1+L0x2001bb50*x** 2+L0x2001bb54*x** 3+
L0x2001bb58*x** 4+L0x2001bb5c*x** 5+L0x2001bb60*x** 6+L0x2001bb64*x** 7+
L0x2001bb68*x** 8+L0x2001bb6c*x** 9+L0x2001bb70*x**10+L0x2001bb74*x**11+
L0x2001bb78*x**12+L0x2001bb7c*x**13+L0x2001bb80*x**14+L0x2001bb84*x**15,
segment2f**2 = 
L0x2001bb88*x** 0+L0x2001bb8c*x** 1+L0x2001bb90*x** 2+L0x2001bb94*x** 3+
L0x2001bb98*x** 4+L0x2001bb9c*x** 5+L0x2001bba0*x** 6+L0x2001bba4*x** 7+
L0x2001bba8*x** 8+L0x2001bbac*x** 9+L0x2001bbb0*x**10+L0x2001bbb4*x**11+
L0x2001bbb8*x**12+L0x2001bbbc*x**13+L0x2001bbc0*x**14+L0x2001bbc4*x**15,
segment2 = 
(segment20**2)*x** 0+(segment21**2)*x**16+(segment22**2)*x**32+
(segment23**2)*x**48+(segment24**2)*x**64+(segment25**2)*x**80+
(segment26**2)*x**96+(segment27**2)*x**112+(segment28**2)*x**128+
(segment29**2)*x**144+(segment2a**2)*x**160+(segment2b**2)*x**176+
(segment2c**2)*x**192+(segment2d**2)*x**208+(segment2e**2)*x**224+
(segment2f**2)*x**240
] prove with [ precondition ];



(**************** CUT    5 ****************)

ecut and [
eqmod (64*inp_poly80**2) (segment2) [1043969, x**4 - 941631],
eqmod (64*inp_poly81**2) (segment2) [1043969, x**4 - 102338],
eqmod (64*inp_poly82**2) (segment2) [1043969, x**4 - 115688],
eqmod (64*inp_poly83**2) (segment2) [1043969, x**4 - 928281],
eqmod (64*inp_poly84**2) (segment2) [1043969, x**4 - 193484],
eqmod (64*inp_poly85**2) (segment2) [1043969, x**4 - 850485],
eqmod (64*inp_poly86**2) (segment2) [1043969, x**4 - 685958],
eqmod (64*inp_poly87**2) (segment2) [1043969, x**4 - 358011],
eqmod (64*inp_poly88**2) (segment2) [1043969, x**4 - 3261],
eqmod (64*inp_poly89**2) (segment2) [1043969, x**4 - 1040708],
eqmod (64*inp_poly8a**2) (segment2) [1043969, x**4 - 405626],
eqmod (64*inp_poly8b**2) (segment2) [1043969, x**4 - 638343],
eqmod (64*inp_poly8c**2) (segment2) [1043969, x**4 - 633347],
eqmod (64*inp_poly8d**2) (segment2) [1043969, x**4 - 410622],
eqmod (64*inp_poly8e**2) (segment2) [1043969, x**4 - 389617],
eqmod (64*inp_poly8f**2) (segment2) [1043969, x**4 - 654352],
eqmod (64*inp_poly90**2) (segment2) [1043969, x**4 - 96534],
eqmod (64*inp_poly91**2) (segment2) [1043969, x**4 - 947435],
eqmod (64*inp_poly92**2) (segment2) [1043969, x**4 - 799554],
eqmod (64*inp_poly93**2) (segment2) [1043969, x**4 - 244415],
eqmod (64*inp_poly94**2) (segment2) [1043969, x**4 - 704462],
eqmod (64*inp_poly95**2) (segment2) [1043969, x**4 - 339507],
eqmod (64*inp_poly96**2) (segment2) [1043969, x**4 - 666593],
eqmod (64*inp_poly97**2) (segment2) [1043969, x**4 - 377376],
eqmod (64*inp_poly98**2) (segment2) [1043969, x**4 - 963976],
eqmod (64*inp_poly99**2) (segment2) [1043969, x**4 - 79993],
eqmod (64*inp_poly9a**2) (segment2) [1043969, x**4 - 650310],
eqmod (64*inp_poly9b**2) (segment2) [1043969, x**4 - 393659],
eqmod (64*inp_poly9c**2) (segment2) [1043969, x**4 - 483878],
eqmod (64*inp_poly9d**2) (segment2) [1043969, x**4 - 560091],
eqmod (64*inp_poly9e**2) (segment2) [1043969, x**4 - 984749],
eqmod (64*inp_poly9f**2) (segment2) [1043969, x**4 - 59220],
eqmod (64*inp_polya0**2) (segment2) [1043969, x**4 - 15495],
eqmod (64*inp_polya1**2) (segment2) [1043969, x**4 - 1028474],
eqmod (64*inp_polya2**2) (segment2) [1043969, x**4 - 403201],
eqmod (64*inp_polya3**2) (segment2) [1043969, x**4 - 640768],
eqmod (64*inp_polya4**2) (segment2) [1043969, x**4 - 605504],
eqmod (64*inp_polya5**2) (segment2) [1043969, x**4 - 438465],
eqmod (64*inp_polya6**2) (segment2) [1043969, x**4 - 409728],
eqmod (64*inp_polya7**2) (segment2) [1043969, x**4 - 634241],
eqmod (64*inp_polya8**2) (segment2) [1043969, x**4 - 30018],
eqmod (64*inp_polya9**2) (segment2) [1043969, x**4 - 1013951],
eqmod (64*inp_polyaa**2) (segment2) [1043969, x**4 - 109250],
eqmod (64*inp_polyab**2) (segment2) [1043969, x**4 - 934719],
eqmod (64*inp_polyac**2) (segment2) [1043969, x**4 - 16675],
eqmod (64*inp_polyad**2) (segment2) [1043969, x**4 - 1027294],
eqmod (64*inp_polyae**2) (segment2) [1043969, x**4 - 643778],
eqmod (64*inp_polyaf**2) (segment2) [1043969, x**4 - 400191],
eqmod (64*inp_polyb0**2) (segment2) [1043969, x**4 - 188469],
eqmod (64*inp_polyb1**2) (segment2) [1043969, x**4 - 855500],
eqmod (64*inp_polyb2**2) (segment2) [1043969, x**4 - 968467],
eqmod (64*inp_polyb3**2) (segment2) [1043969, x**4 - 75502],
eqmod (64*inp_polyb4**2) (segment2) [1043969, x**4 - 658814],
eqmod (64*inp_polyb5**2) (segment2) [1043969, x**4 - 385155],
eqmod (64*inp_polyb6**2) (segment2) [1043969, x**4 - 405305],
eqmod (64*inp_polyb7**2) (segment2) [1043969, x**4 - 638664],
eqmod (64*inp_polyb8**2) (segment2) [1043969, x**4 - 874265],
eqmod (64*inp_polyb9**2) (segment2) [1043969, x**4 - 169704],
eqmod (64*inp_polyba**2) (segment2) [1043969, x**4 - 658791],
eqmod (64*inp_polybb**2) (segment2) [1043969, x**4 - 385178],
eqmod (64*inp_polybc**2) (segment2) [1043969, x**4 - 40112],
eqmod (64*inp_polybd**2) (segment2) [1043969, x**4 - 1003857],
eqmod (64*inp_polybe**2) (segment2) [1043969, x**4 - 608327],
eqmod (64*inp_polybf**2) (segment2) [1043969, x**4 - 435642]
] prove with [ precondition ];



(**************** CUT    6 ****************)

ecut and [
segment30**2 = 
L0x2001bbc8*x** 0+L0x2001bbcc*x** 1+L0x2001bbd0*x** 2+L0x2001bbd4*x** 3+
L0x2001bbd8*x** 4+L0x2001bbdc*x** 5+L0x2001bbe0*x** 6+L0x2001bbe4*x** 7+
L0x2001bbe8*x** 8+L0x2001bbec*x** 9+L0x2001bbf0*x**10+L0x2001bbf4*x**11+
L0x2001bbf8*x**12+L0x2001bbfc*x**13+L0x2001bc00*x**14+L0x2001bc04*x**15,
segment31**2 = 
L0x2001bc08*x** 0+L0x2001bc0c*x** 1+L0x2001bc10*x** 2+L0x2001bc14*x** 3+
L0x2001bc18*x** 4+L0x2001bc1c*x** 5+L0x2001bc20*x** 6+L0x2001bc24*x** 7+
L0x2001bc28*x** 8+L0x2001bc2c*x** 9+L0x2001bc30*x**10+L0x2001bc34*x**11+
L0x2001bc38*x**12+L0x2001bc3c*x**13+L0x2001bc40*x**14+L0x2001bc44*x**15,
segment32**2 = 
L0x2001bc48*x** 0+L0x2001bc4c*x** 1+L0x2001bc50*x** 2+L0x2001bc54*x** 3+
L0x2001bc58*x** 4+L0x2001bc5c*x** 5+L0x2001bc60*x** 6+L0x2001bc64*x** 7+
L0x2001bc68*x** 8+L0x2001bc6c*x** 9+L0x2001bc70*x**10+L0x2001bc74*x**11+
L0x2001bc78*x**12+L0x2001bc7c*x**13+L0x2001bc80*x**14+L0x2001bc84*x**15,
segment33**2 = 
L0x2001bc88*x** 0+L0x2001bc8c*x** 1+L0x2001bc90*x** 2+L0x2001bc94*x** 3+
L0x2001bc98*x** 4+L0x2001bc9c*x** 5+L0x2001bca0*x** 6+L0x2001bca4*x** 7+
L0x2001bca8*x** 8+L0x2001bcac*x** 9+L0x2001bcb0*x**10+L0x2001bcb4*x**11+
L0x2001bcb8*x**12+L0x2001bcbc*x**13+L0x2001bcc0*x**14+L0x2001bcc4*x**15,
segment34**2 = 
L0x2001bcc8*x** 0+L0x2001bccc*x** 1+L0x2001bcd0*x** 2+L0x2001bcd4*x** 3+
L0x2001bcd8*x** 4+L0x2001bcdc*x** 5+L0x2001bce0*x** 6+L0x2001bce4*x** 7+
L0x2001bce8*x** 8+L0x2001bcec*x** 9+L0x2001bcf0*x**10+L0x2001bcf4*x**11+
L0x2001bcf8*x**12+L0x2001bcfc*x**13+L0x2001bd00*x**14+L0x2001bd04*x**15,
segment35**2 = 
L0x2001bd08*x** 0+L0x2001bd0c*x** 1+L0x2001bd10*x** 2+L0x2001bd14*x** 3+
L0x2001bd18*x** 4+L0x2001bd1c*x** 5+L0x2001bd20*x** 6+L0x2001bd24*x** 7+
L0x2001bd28*x** 8+L0x2001bd2c*x** 9+L0x2001bd30*x**10+L0x2001bd34*x**11+
L0x2001bd38*x**12+L0x2001bd3c*x**13+L0x2001bd40*x**14+L0x2001bd44*x**15,
segment36**2 = 
L0x2001bd48*x** 0+L0x2001bd4c*x** 1+L0x2001bd50*x** 2+L0x2001bd54*x** 3+
L0x2001bd58*x** 4+L0x2001bd5c*x** 5+L0x2001bd60*x** 6+L0x2001bd64*x** 7+
L0x2001bd68*x** 8+L0x2001bd6c*x** 9+L0x2001bd70*x**10+L0x2001bd74*x**11+
L0x2001bd78*x**12+L0x2001bd7c*x**13+L0x2001bd80*x**14+L0x2001bd84*x**15,
segment37**2 = 
L0x2001bd88*x** 0+L0x2001bd8c*x** 1+L0x2001bd90*x** 2+L0x2001bd94*x** 3+
L0x2001bd98*x** 4+L0x2001bd9c*x** 5+L0x2001bda0*x** 6+L0x2001bda4*x** 7+
L0x2001bda8*x** 8+L0x2001bdac*x** 9+L0x2001bdb0*x**10+L0x2001bdb4*x**11+
L0x2001bdb8*x**12+L0x2001bdbc*x**13+L0x2001bdc0*x**14+L0x2001bdc4*x**15,
segment38**2 = 
L0x2001bdc8*x** 0+L0x2001bdcc*x** 1+L0x2001bdd0*x** 2+L0x2001bdd4*x** 3+
L0x2001bdd8*x** 4+L0x2001bddc*x** 5+L0x2001bde0*x** 6+L0x2001bde4*x** 7+
L0x2001bde8*x** 8+L0x2001bdec*x** 9+L0x2001bdf0*x**10+L0x2001bdf4*x**11+
L0x2001bdf8*x**12+L0x2001bdfc*x**13+L0x2001be00*x**14+L0x2001be04*x**15,
segment39**2 = 
L0x2001be08*x** 0+L0x2001be0c*x** 1+L0x2001be10*x** 2+L0x2001be14*x** 3+
L0x2001be18*x** 4+L0x2001be1c*x** 5+L0x2001be20*x** 6+L0x2001be24*x** 7+
L0x2001be28*x** 8+L0x2001be2c*x** 9+L0x2001be30*x**10+L0x2001be34*x**11+
L0x2001be38*x**12+L0x2001be3c*x**13+L0x2001be40*x**14+L0x2001be44*x**15,
segment3a**2 = 
L0x2001be48*x** 0+L0x2001be4c*x** 1+L0x2001be50*x** 2+L0x2001be54*x** 3+
L0x2001be58*x** 4+L0x2001be5c*x** 5+L0x2001be60*x** 6+L0x2001be64*x** 7+
L0x2001be68*x** 8+L0x2001be6c*x** 9+L0x2001be70*x**10+L0x2001be74*x**11+
L0x2001be78*x**12+L0x2001be7c*x**13+L0x2001be80*x**14+L0x2001be84*x**15,
segment3b**2 = 
L0x2001be88*x** 0+L0x2001be8c*x** 1+L0x2001be90*x** 2+L0x2001be94*x** 3+
L0x2001be98*x** 4+L0x2001be9c*x** 5+L0x2001bea0*x** 6+L0x2001bea4*x** 7+
L0x2001bea8*x** 8+L0x2001beac*x** 9+L0x2001beb0*x**10+L0x2001beb4*x**11+
L0x2001beb8*x**12+L0x2001bebc*x**13+L0x2001bec0*x**14+L0x2001bec4*x**15,
segment3c**2 = 
L0x2001bec8*x** 0+L0x2001becc*x** 1+L0x2001bed0*x** 2+L0x2001bed4*x** 3+
L0x2001bed8*x** 4+L0x2001bedc*x** 5+L0x2001bee0*x** 6+L0x2001bee4*x** 7+
L0x2001bee8*x** 8+L0x2001beec*x** 9+L0x2001bef0*x**10+L0x2001bef4*x**11+
L0x2001bef8*x**12+L0x2001befc*x**13+L0x2001bf00*x**14+L0x2001bf04*x**15,
segment3d**2 = 
L0x2001bf08*x** 0+L0x2001bf0c*x** 1+L0x2001bf10*x** 2+L0x2001bf14*x** 3+
L0x2001bf18*x** 4+L0x2001bf1c*x** 5+L0x2001bf20*x** 6+L0x2001bf24*x** 7+
L0x2001bf28*x** 8+L0x2001bf2c*x** 9+L0x2001bf30*x**10+L0x2001bf34*x**11+
L0x2001bf38*x**12+L0x2001bf3c*x**13+L0x2001bf40*x**14+L0x2001bf44*x**15,
segment3e**2 = 
L0x2001bf48*x** 0+L0x2001bf4c*x** 1+L0x2001bf50*x** 2+L0x2001bf54*x** 3+
L0x2001bf58*x** 4+L0x2001bf5c*x** 5+L0x2001bf60*x** 6+L0x2001bf64*x** 7+
L0x2001bf68*x** 8+L0x2001bf6c*x** 9+L0x2001bf70*x**10+L0x2001bf74*x**11+
L0x2001bf78*x**12+L0x2001bf7c*x**13+L0x2001bf80*x**14+L0x2001bf84*x**15,
segment3f**2 = 
L0x2001bf88*x** 0+L0x2001bf8c*x** 1+L0x2001bf90*x** 2+L0x2001bf94*x** 3+
L0x2001bf98*x** 4+L0x2001bf9c*x** 5+L0x2001bfa0*x** 6+L0x2001bfa4*x** 7+
L0x2001bfa8*x** 8+L0x2001bfac*x** 9+L0x2001bfb0*x**10+L0x2001bfb4*x**11+
L0x2001bfb8*x**12+L0x2001bfbc*x**13+L0x2001bfc0*x**14+L0x2001bfc4*x**15,
segment3 = 
(segment30**2)*x** 0+(segment31**2)*x**16+(segment32**2)*x**32+
(segment33**2)*x**48+(segment34**2)*x**64+(segment35**2)*x**80+
(segment36**2)*x**96+(segment37**2)*x**112+(segment38**2)*x**128+
(segment39**2)*x**144+(segment3a**2)*x**160+(segment3b**2)*x**176+
(segment3c**2)*x**192+(segment3d**2)*x**208+(segment3e**2)*x**224+
(segment3f**2)*x**240
] prove with [ precondition ];



(**************** CUT    7 ****************)

ecut and [
eqmod (64*inp_polyc0**2) (segment3) [1043969, x**4 - 759697],
eqmod (64*inp_polyc1**2) (segment3) [1043969, x**4 - 284272],
eqmod (64*inp_polyc2**2) (segment3) [1043969, x**4 - 908658],
eqmod (64*inp_polyc3**2) (segment3) [1043969, x**4 - 135311],
eqmod (64*inp_polyc4**2) (segment3) [1043969, x**4 - 369462],
eqmod (64*inp_polyc5**2) (segment3) [1043969, x**4 - 674507],
eqmod (64*inp_polyc6**2) (segment3) [1043969, x**4 - 1021423],
eqmod (64*inp_polyc7**2) (segment3) [1043969, x**4 - 22546],
eqmod (64*inp_polyc8**2) (segment3) [1043969, x**4 - 943589],
eqmod (64*inp_polyc9**2) (segment3) [1043969, x**4 - 100380],
eqmod (64*inp_polyca**2) (segment3) [1043969, x**4 - 927162],
eqmod (64*inp_polycb**2) (segment3) [1043969, x**4 - 116807],
eqmod (64*inp_polycc**2) (segment3) [1043969, x**4 - 350308],
eqmod (64*inp_polycd**2) (segment3) [1043969, x**4 - 693661],
eqmod (64*inp_polyce**2) (segment3) [1043969, x**4 - 674670],
eqmod (64*inp_polycf**2) (segment3) [1043969, x**4 - 369299],
eqmod (64*inp_polyd0**2) (segment3) [1043969, x**4 - 319829],
eqmod (64*inp_polyd1**2) (segment3) [1043969, x**4 - 724140],
eqmod (64*inp_polyd2**2) (segment3) [1043969, x**4 - 518322],
eqmod (64*inp_polyd3**2) (segment3) [1043969, x**4 - 525647],
eqmod (64*inp_polyd4**2) (segment3) [1043969, x**4 - 727472],
eqmod (64*inp_polyd5**2) (segment3) [1043969, x**4 - 316497],
eqmod (64*inp_polyd6**2) (segment3) [1043969, x**4 - 659984],
eqmod (64*inp_polyd7**2) (segment3) [1043969, x**4 - 383985],
eqmod (64*inp_polyd8**2) (segment3) [1043969, x**4 - 269719],
eqmod (64*inp_polyd9**2) (segment3) [1043969, x**4 - 774250],
eqmod (64*inp_polyda**2) (segment3) [1043969, x**4 - 485076],
eqmod (64*inp_polydb**2) (segment3) [1043969, x**4 - 558893],
eqmod (64*inp_polydc**2) (segment3) [1043969, x**4 - 975148],
eqmod (64*inp_polydd**2) (segment3) [1043969, x**4 - 68821],
eqmod (64*inp_polyde**2) (segment3) [1043969, x**4 - 118175],
eqmod (64*inp_polydf**2) (segment3) [1043969, x**4 - 925794],
eqmod (64*inp_polye0**2) (segment3) [1043969, x**4 - 405653],
eqmod (64*inp_polye1**2) (segment3) [1043969, x**4 - 638316],
eqmod (64*inp_polye2**2) (segment3) [1043969, x**4 - 364094],
eqmod (64*inp_polye3**2) (segment3) [1043969, x**4 - 679875],
eqmod (64*inp_polye4**2) (segment3) [1043969, x**4 - 857780],
eqmod (64*inp_polye5**2) (segment3) [1043969, x**4 - 186189],
eqmod (64*inp_polye6**2) (segment3) [1043969, x**4 - 9514],
eqmod (64*inp_polye7**2) (segment3) [1043969, x**4 - 1034455],
eqmod (64*inp_polye8**2) (segment3) [1043969, x**4 - 438813],
eqmod (64*inp_polye9**2) (segment3) [1043969, x**4 - 605156],
eqmod (64*inp_polyea**2) (segment3) [1043969, x**4 - 613180],
eqmod (64*inp_polyeb**2) (segment3) [1043969, x**4 - 430789],
eqmod (64*inp_polyec**2) (segment3) [1043969, x**4 - 643048],
eqmod (64*inp_polyed**2) (segment3) [1043969, x**4 - 400921],
eqmod (64*inp_polyee**2) (segment3) [1043969, x**4 - 993476],
eqmod (64*inp_polyef**2) (segment3) [1043969, x**4 - 50493],
eqmod (64*inp_polyf0**2) (segment3) [1043969, x**4 - 143510],
eqmod (64*inp_polyf1**2) (segment3) [1043969, x**4 - 900459],
eqmod (64*inp_polyf2**2) (segment3) [1043969, x**4 - 956472],
eqmod (64*inp_polyf3**2) (segment3) [1043969, x**4 - 87497],
eqmod (64*inp_polyf4**2) (segment3) [1043969, x**4 - 904239],
eqmod (64*inp_polyf5**2) (segment3) [1043969, x**4 - 139730],
eqmod (64*inp_polyf6**2) (segment3) [1043969, x**4 - 362716],
eqmod (64*inp_polyf7**2) (segment3) [1043969, x**4 - 681253],
eqmod (64*inp_polyf8**2) (segment3) [1043969, x**4 - 928856],
eqmod (64*inp_polyf9**2) (segment3) [1043969, x**4 - 115113],
eqmod (64*inp_polyfa**2) (segment3) [1043969, x**4 - 567842],
eqmod (64*inp_polyfb**2) (segment3) [1043969, x**4 - 476127],
eqmod (64*inp_polyfc**2) (segment3) [1043969, x**4 - 1009759],
eqmod (64*inp_polyfd**2) (segment3) [1043969, x**4 - 34210],
eqmod (64*inp_polyfe**2) (segment3) [1043969, x**4 - 660435],
eqmod (64*inp_polyff**2) (segment3) [1043969, x**4 - 383534]
] prove with [ precondition ];



(**************** CUT    8 ****************)

ecut true;



(**************** initialization ****************)

mov r2 1993076223@uint32; mov r3 1043969@sint32;



(**************** pointers ****************)

nondet r0@uint32; nondet r1@uint32; nondet r11@uint32;



(**************** constants ****************)

mov L0x800be00 ( -14494)@sint32; mov L0x800be04 (-191052)@sint32;



(* #! -> SP = 0x200177c0 *)
#! 0x200177c0 = 0x200177c0;
(* #stmdb	sp!, {r4, r5, r6, r7, r8, r9, r10, r11, r12, lr}#! EA = L0x200177c0; PC = 0x8002538 *)
#stmdb	sp!, {%%r4, %%r5, %%r6, %%r7, %%r8, %%r9, %%r10, %%r11, %%r12, %%lr}#! L0x200177c0 = L0x200177c0; 0x8002538 = 0x8002538;
(* #ldr.w	r11, [sp, #40]	; 0x28                     #! EA = L0x200177c0; Value = 0x2001afc8; PC = 0x800253c *)
#ldr.w	%%r11, [sp, #40]	; 0x28                     #! L0x200177c0 = L0x200177c0; 0x2001afc8 = 0x2001afc8; 0x800253c = 0x800253c;
(* vmov	s0, r11                                    #! PC = 0x8002540 *)
mov s0 r11;
(* ldr.w	r8, [r1, #4]                              #! EA = L0x800be04; Value = 0xfffd15b4; PC = 0x8002544 *)
mov r8 L0x800be04;
(* ldr.w	r9, [r1]                                  #! EA = L0x800be00; Value = 0xffffc762; PC = 0x8002548 *)
mov r9 L0x800be00;
(* vmov	r11, s0                                    #! PC = 0x800254c *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001afc8; Value = 0x000ab617; PC = 0x8002550 *)
mov r4 L0x2001afc8;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b3c8; Value = 0x00fe2e21; PC = 0x8002554 *)
mov r5 L0x2001b3c8;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b7c8; Value = 0xffe30f0c; PC = 0x8002558 *)
mov r6 L0x2001b7c8;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bbc8; Value = 0xffcd1486; PC = 0x800255c *)
mov r7 L0x2001bbc8;
(* add	r4, r5                                      #! PC = 0x8002560 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8002562 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8002564 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002568 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x800256c *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002570 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8002574 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002578 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x800257a *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800257c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002580 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* vmov	s4, s5, r4, r5                             #! PC = 0x8002584 *)
mov s4 r4;
mov s5 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x8002588 *)
mov s6 r6;
mov s7 r7;


(******************** CUT   9 ********************)

ghost c0000@sint32,c0256@sint32,c0512@sint32,c0768@sint32 :
and [c0000=s4, c0256=s5, c0512=s6, c0768=s7] && true;

ecut and [
eqmod (c0000*x**  0+c0256*x**256+c0512*x**512+c0768*x**768)
      (4*L0x2001afc8*x**  0) [1043969, x**256 - 1],
eqmod (c0000*x**  0+c0256*x**256+c0512*x**512+c0768*x**768)
      (4*L0x2001b3c8*x**  0) [1043969, x**256 + 1],
eqmod (c0000*x**  0+c0256*x**256+c0512*x**512+c0768*x**768)
      (4*L0x2001b7c8*x**  0) [1043969, x**256 - 554923],
eqmod (c0000*x**  0+c0256*x**256+c0512*x**512+c0768*x**768)
      (4*L0x2001bbc8*x**  0) [1043969, x**256 + 554923]
];


(* vmov	r11, s0                                    #! PC = 0x800258c *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001afcc; Value = 0x0037eda0; PC = 0x8002590 *)
mov r4 L0x2001afcc;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b3cc; Value = 0x00d5d043; PC = 0x8002594 *)
mov r5 L0x2001b3cc;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b7cc; Value = 0xff267620; PC = 0x8002598 *)
mov r6 L0x2001b7cc;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bbcc; Value = 0xff870dc7; PC = 0x800259c *)
mov r7 L0x2001bbcc;
(* add	r4, r5                                      #! PC = 0x80025a0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80025a2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80025a4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80025a8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80025ac *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80025b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80025b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80025b8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80025ba *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80025bc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80025c0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* vmov	s8, s9, r4, r5                             #! PC = 0x80025c4 *)
mov s8 r4;
mov s9 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x80025c8 *)
mov s10 r6;
mov s11 r7;


(******************** CUT  10 ********************)

ghost c0001@sint32,c0257@sint32,c0513@sint32,c0769@sint32 :
and [c0001=s8, c0257=s9, c0513=s10, c0769=s11] && true;

ecut and [
eqmod (c0001*x**  1+c0257*x**257+c0513*x**513+c0769*x**769)
      (4*L0x2001afcc*x**  1) [1043969, x**256 - 1],
eqmod (c0001*x**  1+c0257*x**257+c0513*x**513+c0769*x**769)
      (4*L0x2001b3cc*x**  1) [1043969, x**256 + 1],
eqmod (c0001*x**  1+c0257*x**257+c0513*x**513+c0769*x**769)
      (4*L0x2001b7cc*x**  1) [1043969, x**256 - 554923],
eqmod (c0001*x**  1+c0257*x**257+c0513*x**513+c0769*x**769)
      (4*L0x2001bbcc*x**  1) [1043969, x**256 + 554923]
];


(* vmov	r11, s0                                    #! PC = 0x80025cc *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001afd0; Value = 0xfff8b755; PC = 0x80025d0 *)
mov r4 L0x2001afd0;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b3d0; Value = 0xffd1b9a6; PC = 0x80025d4 *)
mov r5 L0x2001b3d0;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b7d0; Value = 0x005d1d7e; PC = 0x80025d8 *)
mov r6 L0x2001b7d0;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bbd0; Value = 0xff9bec4d; PC = 0x80025dc *)
mov r7 L0x2001bbd0;
(* add	r4, r5                                      #! PC = 0x80025e0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80025e2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80025e4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80025e8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80025ec *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80025f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80025f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80025f8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80025fa *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80025fc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002600 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* vmov	s12, s13, r4, r5                           #! PC = 0x8002604 *)
mov s12 r4;
mov s13 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x8002608 *)
mov s14 r6;
mov s15 r7;


(******************** CUT  11 ********************)

ghost c0002@sint32,c0258@sint32,c0514@sint32,c0770@sint32 :
and [c0002=s12, c0258=s13, c0514=s14, c0770=s15] && true;

ecut and [
eqmod (c0002*x**  2+c0258*x**258+c0514*x**514+c0770*x**770)
      (4*L0x2001afd0*x**  2) [1043969, x**256 - 1],
eqmod (c0002*x**  2+c0258*x**258+c0514*x**514+c0770*x**770)
      (4*L0x2001b3d0*x**  2) [1043969, x**256 + 1],
eqmod (c0002*x**  2+c0258*x**258+c0514*x**514+c0770*x**770)
      (4*L0x2001b7d0*x**  2) [1043969, x**256 - 554923],
eqmod (c0002*x**  2+c0258*x**258+c0514*x**514+c0770*x**770)
      (4*L0x2001bbd0*x**  2) [1043969, x**256 + 554923]
];


(* vmov	r11, s0                                    #! PC = 0x800260c *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x8002610 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x8002614 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x8002618 *)
adds dontcare r0 r0 6@uint32;
(* add.w	r12, r0, #498	; 0x1f2                     #! PC = 0x800261c *)
adds dontcare r12 r0 498@uint32;
(* vmov	s2, r12                                    #! PC = 0x8002620 *)
mov s2 r12;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001afd4; Value = 0x00375d67; PC = 0x8002628 *)
mov r4 L0x2001afd4;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b3d4; Value = 0xfe6d434d; PC = 0x800262c *)
mov r5 L0x2001b3d4;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b7d4; Value = 0x0035bb0f; PC = 0x8002630 *)
mov r6 L0x2001b7d4;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bbd4; Value = 0xffe27d46; PC = 0x8002634 *)
mov r7 L0x2001bbd4;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  12 ********************)

ghost c0003@sint32,c0259@sint32,c0515@sint32,c0771@sint32 :
and [c0003=r4, c0259=r5, c0515=r6, c0771=r7] && true;

ecut and [
eqmod (c0003*x**  3+c0259*x**259+c0515*x**515+c0771*x**771)
      (4*L0x2001afd4*x**  3) [1043969, x**256 - 1],
eqmod (c0003*x**  3+c0259*x**259+c0515*x**515+c0771*x**771)
      (4*L0x2001b3d4*x**  3) [1043969, x**256 + 1],
eqmod (c0003*x**  3+c0259*x**259+c0515*x**515+c0771*x**771)
      (4*L0x2001b7d4*x**  3) [1043969, x**256 - 554923],
eqmod (c0003*x**  3+c0259*x**259+c0515*x**515+c0771*x**771)
      (4*L0x2001bbd4*x**  3) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf003@sint32 : cf003 = r4 && cf003 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf259@sint32 : cf259 = r5 && cf259 = r5;

(******************** CUT  13 ********************)
ecut and [
eqmod 256*cf003 2**32*(c0003+c0512) 1043969,
eqmod 256*cf259 2**32*(c0259+c0768) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200187ce; PC = 0x80026a0 *)
mov L0x200187ce r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x200189ce; PC = 0x80026a4 *)
mov L0x200189ce r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001afd8; Value = 0x001377c0; PC = 0x80026b0 *)
mov r4 L0x2001afd8;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b3d8; Value = 0xffeb94e0; PC = 0x80026b4 *)
mov r5 L0x2001b3d8;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b7d8; Value = 0xfff95f6b; PC = 0x80026b8 *)
mov r6 L0x2001b7d8;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bbd8; Value = 0xfff9c5c7; PC = 0x80026bc *)
mov r7 L0x2001bbd8;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  14 ********************)

ghost c0004@sint32,c0260@sint32,c0516@sint32,c0772@sint32 :
and [c0004=r4, c0260=r5, c0516=r6, c0772=r7] && true;

ecut and [
eqmod (c0004*x**  4+c0260*x**260+c0516*x**516+c0772*x**772)
      (4*L0x2001afd8*x**  4) [1043969, x**256 - 1],
eqmod (c0004*x**  4+c0260*x**260+c0516*x**516+c0772*x**772)
      (4*L0x2001b3d8*x**  4) [1043969, x**256 + 1],
eqmod (c0004*x**  4+c0260*x**260+c0516*x**516+c0772*x**772)
      (4*L0x2001b7d8*x**  4) [1043969, x**256 - 554923],
eqmod (c0004*x**  4+c0260*x**260+c0516*x**516+c0772*x**772)
      (4*L0x2001bbd8*x**  4) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf004@sint32 : cf004 = r4 && cf004 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf260@sint32 : cf260 = r5 && cf260 = r5;

(******************** CUT  15 ********************)
ecut and [
eqmod 256*cf004 2**32*(c0004+c0513) 1043969,
eqmod 256*cf260 2**32*(c0260+c0769) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200187d0; PC = 0x8002728 *)
mov L0x200187d0 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x200189d0; PC = 0x800272c *)
mov L0x200189d0 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001afdc; Value = 0x0012e63e; PC = 0x8002738 *)
mov r4 L0x2001afdc;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b3dc; Value = 0x002fdaed; PC = 0x800273c *)
mov r5 L0x2001b3dc;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b7dc; Value = 0x000175dd; PC = 0x8002740 *)
mov r6 L0x2001b7dc;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bbdc; Value = 0x0041190e; PC = 0x8002744 *)
mov r7 L0x2001bbdc;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  16 ********************)

ghost c0005@sint32,c0261@sint32,c0517@sint32,c0773@sint32 :
and [c0005=r4, c0261=r5, c0517=r6, c0773=r7] && true;

ecut and [
eqmod (c0005*x**  5+c0261*x**261+c0517*x**517+c0773*x**773)
      (4*L0x2001afdc*x**  5) [1043969, x**256 - 1],
eqmod (c0005*x**  5+c0261*x**261+c0517*x**517+c0773*x**773)
      (4*L0x2001b3dc*x**  5) [1043969, x**256 + 1],
eqmod (c0005*x**  5+c0261*x**261+c0517*x**517+c0773*x**773)
      (4*L0x2001b7dc*x**  5) [1043969, x**256 - 554923],
eqmod (c0005*x**  5+c0261*x**261+c0517*x**517+c0773*x**773)
      (4*L0x2001bbdc*x**  5) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf005@sint32 : cf005 = r4 && cf005 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf261@sint32 : cf261 = r5 && cf261 = r5;

(******************** CUT  17 ********************)
ecut and [
eqmod 256*cf005 2**32*(c0005+c0514) 1043969,
eqmod 256*cf261 2**32*(c0261+c0770) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200187d2; PC = 0x80027b0 *)
mov L0x200187d2 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x200189d2; PC = 0x80027b4 *)
mov L0x200189d2 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001afe0; Value = 0xffee908d; PC = 0x8002628 *)
mov r4 L0x2001afe0;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b3e0; Value = 0xffddac1a; PC = 0x800262c *)
mov r5 L0x2001b3e0;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b7e0; Value = 0xffe5ef5b; PC = 0x8002630 *)
mov r6 L0x2001b7e0;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bbe0; Value = 0x00166735; PC = 0x8002634 *)
mov r7 L0x2001bbe0;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  18 ********************)

ghost c0006@sint32,c0262@sint32,c0518@sint32,c0774@sint32 :
and [c0006=r4, c0262=r5, c0518=r6, c0774=r7] && true;

ecut and [
eqmod (c0006*x**  6+c0262*x**262+c0518*x**518+c0774*x**774)
      (4*L0x2001afe0*x**  6) [1043969, x**256 - 1],
eqmod (c0006*x**  6+c0262*x**262+c0518*x**518+c0774*x**774)
      (4*L0x2001b3e0*x**  6) [1043969, x**256 + 1],
eqmod (c0006*x**  6+c0262*x**262+c0518*x**518+c0774*x**774)
      (4*L0x2001b7e0*x**  6) [1043969, x**256 - 554923],
eqmod (c0006*x**  6+c0262*x**262+c0518*x**518+c0774*x**774)
      (4*L0x2001bbe0*x**  6) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf006@sint32 : cf006 = r4 && cf006 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf262@sint32 : cf262 = r5 && cf262 = r5;

(******************** CUT  19 ********************)
ecut and [
eqmod 256*cf006 2**32*(c0006+c0515) 1043969,
eqmod 256*cf262 2**32*(c0262+c0771) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200187d4; PC = 0x80026a0 *)
mov L0x200187d4 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x200189d4; PC = 0x80026a4 *)
mov L0x200189d4 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001afe4; Value = 0xfff14ef6; PC = 0x80026b0 *)
mov r4 L0x2001afe4;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b3e4; Value = 0xffe593d3; PC = 0x80026b4 *)
mov r5 L0x2001b3e4;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b7e4; Value = 0xfff49098; PC = 0x80026b8 *)
mov r6 L0x2001b7e4;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bbe4; Value = 0xffd92977; PC = 0x80026bc *)
mov r7 L0x2001bbe4;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  20 ********************)

ghost c0007@sint32,c0263@sint32,c0519@sint32,c0775@sint32 :
and [c0007=r4, c0263=r5, c0519=r6, c0775=r7] && true;

ecut and [
eqmod (c0007*x**  7+c0263*x**263+c0519*x**519+c0775*x**775)
      (4*L0x2001afe4*x**  7) [1043969, x**256 - 1],
eqmod (c0007*x**  7+c0263*x**263+c0519*x**519+c0775*x**775)
      (4*L0x2001b3e4*x**  7) [1043969, x**256 + 1],
eqmod (c0007*x**  7+c0263*x**263+c0519*x**519+c0775*x**775)
      (4*L0x2001b7e4*x**  7) [1043969, x**256 - 554923],
eqmod (c0007*x**  7+c0263*x**263+c0519*x**519+c0775*x**775)
      (4*L0x2001bbe4*x**  7) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf007@sint32 : cf007 = r4 && cf007 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf263@sint32 : cf263 = r5 && cf263 = r5;

(******************** CUT  21 ********************)
ecut and [
eqmod 256*cf007 2**32*(c0007+c0516) 1043969,
eqmod 256*cf263 2**32*(c0263+c0772) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200187d6; PC = 0x8002728 *)
mov L0x200187d6 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x200189d6; PC = 0x800272c *)
mov L0x200189d6 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001afe8; Value = 0x0010b98d; PC = 0x8002738 *)
mov r4 L0x2001afe8;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b3e8; Value = 0x00097b13; PC = 0x800273c *)
mov r5 L0x2001b3e8;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b7e8; Value = 0xffea2529; PC = 0x8002740 *)
mov r6 L0x2001b7e8;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bbe8; Value = 0x000426c1; PC = 0x8002744 *)
mov r7 L0x2001bbe8;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  22 ********************)

ghost c0008@sint32,c0264@sint32,c0520@sint32,c0776@sint32 :
and [c0008=r4, c0264=r5, c0520=r6, c0776=r7] && true;

ecut and [
eqmod (c0008*x**  8+c0264*x**264+c0520*x**520+c0776*x**776)
      (4*L0x2001afe8*x**  8) [1043969, x**256 - 1],
eqmod (c0008*x**  8+c0264*x**264+c0520*x**520+c0776*x**776)
      (4*L0x2001b3e8*x**  8) [1043969, x**256 + 1],
eqmod (c0008*x**  8+c0264*x**264+c0520*x**520+c0776*x**776)
      (4*L0x2001b7e8*x**  8) [1043969, x**256 - 554923],
eqmod (c0008*x**  8+c0264*x**264+c0520*x**520+c0776*x**776)
      (4*L0x2001bbe8*x**  8) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf008@sint32 : cf008 = r4 && cf008 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf264@sint32 : cf264 = r5 && cf264 = r5;

(******************** CUT  23 ********************)
ecut and [
eqmod 256*cf008 2**32*(c0008+c0517) 1043969,
eqmod 256*cf264 2**32*(c0264+c0773) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200187d8; PC = 0x80027b0 *)
mov L0x200187d8 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x200189d8; PC = 0x80027b4 *)
mov L0x200189d8 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001afec; Value = 0x000caa4b; PC = 0x8002628 *)
mov r4 L0x2001afec;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b3ec; Value = 0x00004dd6; PC = 0x800262c *)
mov r5 L0x2001b3ec;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b7ec; Value = 0x00156c49; PC = 0x8002630 *)
mov r6 L0x2001b7ec;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bbec; Value = 0x00046413; PC = 0x8002634 *)
mov r7 L0x2001bbec;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  24 ********************)

ghost c0009@sint32,c0265@sint32,c0521@sint32,c0777@sint32 :
and [c0009=r4, c0265=r5, c0521=r6, c0777=r7] && true;

ecut and [
eqmod (c0009*x**  9+c0265*x**265+c0521*x**521+c0777*x**777)
      (4*L0x2001afec*x**  9) [1043969, x**256 - 1],
eqmod (c0009*x**  9+c0265*x**265+c0521*x**521+c0777*x**777)
      (4*L0x2001b3ec*x**  9) [1043969, x**256 + 1],
eqmod (c0009*x**  9+c0265*x**265+c0521*x**521+c0777*x**777)
      (4*L0x2001b7ec*x**  9) [1043969, x**256 - 554923],
eqmod (c0009*x**  9+c0265*x**265+c0521*x**521+c0777*x**777)
      (4*L0x2001bbec*x**  9) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf009@sint32 : cf009 = r4 && cf009 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf265@sint32 : cf265 = r5 && cf265 = r5;

(******************** CUT  25 ********************)
ecut and [
eqmod 256*cf009 2**32*(c0009+c0518) 1043969,
eqmod 256*cf265 2**32*(c0265+c0774) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200187da; PC = 0x80026a0 *)
mov L0x200187da r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x200189da; PC = 0x80026a4 *)
mov L0x200189da r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001aff0; Value = 0x0002697c; PC = 0x80026b0 *)
mov r4 L0x2001aff0;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b3f0; Value = 0x0027d3fc; PC = 0x80026b4 *)
mov r5 L0x2001b3f0;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b7f0; Value = 0x001ae98d; PC = 0x80026b8 *)
mov r6 L0x2001b7f0;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bbf0; Value = 0xfff8742a; PC = 0x80026bc *)
mov r7 L0x2001bbf0;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  26 ********************)

ghost c0010@sint32,c0266@sint32,c0522@sint32,c0778@sint32 :
and [c0010=r4, c0266=r5, c0522=r6, c0778=r7] && true;

ecut and [
eqmod (c0010*x** 10+c0266*x**266+c0522*x**522+c0778*x**778)
      (4*L0x2001aff0*x** 10) [1043969, x**256 - 1],
eqmod (c0010*x** 10+c0266*x**266+c0522*x**522+c0778*x**778)
      (4*L0x2001b3f0*x** 10) [1043969, x**256 + 1],
eqmod (c0010*x** 10+c0266*x**266+c0522*x**522+c0778*x**778)
      (4*L0x2001b7f0*x** 10) [1043969, x**256 - 554923],
eqmod (c0010*x** 10+c0266*x**266+c0522*x**522+c0778*x**778)
      (4*L0x2001bbf0*x** 10) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf010@sint32 : cf010 = r4 && cf010 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf266@sint32 : cf266 = r5 && cf266 = r5;

(******************** CUT  27 ********************)
ecut and [
eqmod 256*cf010 2**32*(c0010+c0519) 1043969,
eqmod 256*cf266 2**32*(c0266+c0775) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200187dc; PC = 0x8002728 *)
mov L0x200187dc r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x200189dc; PC = 0x800272c *)
mov L0x200189dc r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001aff4; Value = 0x001a50e6; PC = 0x8002738 *)
mov r4 L0x2001aff4;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b3f4; Value = 0xffee0519; PC = 0x800273c *)
mov r5 L0x2001b3f4;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b7f4; Value = 0x0014f1eb; PC = 0x8002740 *)
mov r6 L0x2001b7f4;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bbf4; Value = 0x00106226; PC = 0x8002744 *)
mov r7 L0x2001bbf4;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  28 ********************)

ghost c0011@sint32,c0267@sint32,c0523@sint32,c0779@sint32 :
and [c0011=r4, c0267=r5, c0523=r6, c0779=r7] && true;

ecut and [
eqmod (c0011*x** 11+c0267*x**267+c0523*x**523+c0779*x**779)
      (4*L0x2001aff4*x** 11) [1043969, x**256 - 1],
eqmod (c0011*x** 11+c0267*x**267+c0523*x**523+c0779*x**779)
      (4*L0x2001b3f4*x** 11) [1043969, x**256 + 1],
eqmod (c0011*x** 11+c0267*x**267+c0523*x**523+c0779*x**779)
      (4*L0x2001b7f4*x** 11) [1043969, x**256 - 554923],
eqmod (c0011*x** 11+c0267*x**267+c0523*x**523+c0779*x**779)
      (4*L0x2001bbf4*x** 11) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf011@sint32 : cf011 = r4 && cf011 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf267@sint32 : cf267 = r5 && cf267 = r5;

(******************** CUT  29 ********************)
ecut and [
eqmod 256*cf011 2**32*(c0011+c0520) 1043969,
eqmod 256*cf267 2**32*(c0267+c0776) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200187de; PC = 0x80027b0 *)
mov L0x200187de r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x200189de; PC = 0x80027b4 *)
mov L0x200189de r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001aff8; Value = 0x0004bdc0; PC = 0x8002628 *)
mov r4 L0x2001aff8;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b3f8; Value = 0x000a2e95; PC = 0x800262c *)
mov r5 L0x2001b3f8;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b7f8; Value = 0x00031db1; PC = 0x8002630 *)
mov r6 L0x2001b7f8;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bbf8; Value = 0xffecfafe; PC = 0x8002634 *)
mov r7 L0x2001bbf8;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  30 ********************)

ghost c0012@sint32,c0268@sint32,c0524@sint32,c0780@sint32 :
and [c0012=r4, c0268=r5, c0524=r6, c0780=r7] && true;

ecut and [
eqmod (c0012*x** 12+c0268*x**268+c0524*x**524+c0780*x**780)
      (4*L0x2001aff8*x** 12) [1043969, x**256 - 1],
eqmod (c0012*x** 12+c0268*x**268+c0524*x**524+c0780*x**780)
      (4*L0x2001b3f8*x** 12) [1043969, x**256 + 1],
eqmod (c0012*x** 12+c0268*x**268+c0524*x**524+c0780*x**780)
      (4*L0x2001b7f8*x** 12) [1043969, x**256 - 554923],
eqmod (c0012*x** 12+c0268*x**268+c0524*x**524+c0780*x**780)
      (4*L0x2001bbf8*x** 12) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf012@sint32 : cf012 = r4 && cf012 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf268@sint32 : cf268 = r5 && cf268 = r5;

(******************** CUT  31 ********************)
ecut and [
eqmod 256*cf012 2**32*(c0012+c0521) 1043969,
eqmod 256*cf268 2**32*(c0268+c0777) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200187e0; PC = 0x80026a0 *)
mov L0x200187e0 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x200189e0; PC = 0x80026a4 *)
mov L0x200189e0 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001affc; Value = 0x000bb5c3; PC = 0x80026b0 *)
mov r4 L0x2001affc;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b3fc; Value = 0xffe8aa78; PC = 0x80026b4 *)
mov r5 L0x2001b3fc;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b7fc; Value = 0x0011ebdb; PC = 0x80026b8 *)
mov r6 L0x2001b7fc;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bbfc; Value = 0xffea1d9d; PC = 0x80026bc *)
mov r7 L0x2001bbfc;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  32 ********************)

ghost c0013@sint32,c0269@sint32,c0525@sint32,c0781@sint32 :
and [c0013=r4, c0269=r5, c0525=r6, c0781=r7] && true;

ecut and [
eqmod (c0013*x** 13+c0269*x**269+c0525*x**525+c0781*x**781)
      (4*L0x2001affc*x** 13) [1043969, x**256 - 1],
eqmod (c0013*x** 13+c0269*x**269+c0525*x**525+c0781*x**781)
      (4*L0x2001b3fc*x** 13) [1043969, x**256 + 1],
eqmod (c0013*x** 13+c0269*x**269+c0525*x**525+c0781*x**781)
      (4*L0x2001b7fc*x** 13) [1043969, x**256 - 554923],
eqmod (c0013*x** 13+c0269*x**269+c0525*x**525+c0781*x**781)
      (4*L0x2001bbfc*x** 13) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf013@sint32 : cf013 = r4 && cf013 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf269@sint32 : cf269 = r5 && cf269 = r5;

(******************** CUT  33 ********************)
ecut and [
eqmod 256*cf013 2**32*(c0013+c0522) 1043969,
eqmod 256*cf269 2**32*(c0269+c0778) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200187e2; PC = 0x8002728 *)
mov L0x200187e2 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x200189e2; PC = 0x800272c *)
mov L0x200189e2 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b000; Value = 0x0002ad70; PC = 0x8002738 *)
mov r4 L0x2001b000;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b400; Value = 0x000227a2; PC = 0x800273c *)
mov r5 L0x2001b400;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b800; Value = 0x00080428; PC = 0x8002740 *)
mov r6 L0x2001b800;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bc00; Value = 0xffebab77; PC = 0x8002744 *)
mov r7 L0x2001bc00;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  34 ********************)

ghost c0014@sint32,c0270@sint32,c0526@sint32,c0782@sint32 :
and [c0014=r4, c0270=r5, c0526=r6, c0782=r7] && true;

ecut and [
eqmod (c0014*x** 14+c0270*x**270+c0526*x**526+c0782*x**782)
      (4*L0x2001b000*x** 14) [1043969, x**256 - 1],
eqmod (c0014*x** 14+c0270*x**270+c0526*x**526+c0782*x**782)
      (4*L0x2001b400*x** 14) [1043969, x**256 + 1],
eqmod (c0014*x** 14+c0270*x**270+c0526*x**526+c0782*x**782)
      (4*L0x2001b800*x** 14) [1043969, x**256 - 554923],
eqmod (c0014*x** 14+c0270*x**270+c0526*x**526+c0782*x**782)
      (4*L0x2001bc00*x** 14) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf014@sint32 : cf014 = r4 && cf014 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf270@sint32 : cf270 = r5 && cf270 = r5;

(******************** CUT  35 ********************)
ecut and [
eqmod 256*cf014 2**32*(c0014+c0523) 1043969,
eqmod 256*cf270 2**32*(c0270+c0779) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200187e4; PC = 0x80027b0 *)
mov L0x200187e4 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x200189e4; PC = 0x80027b4 *)
mov L0x200189e4 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b004; Value = 0xfff92d9b; PC = 0x8002628 *)
mov r4 L0x2001b004;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b404; Value = 0xffe820a2; PC = 0x800262c *)
mov r5 L0x2001b404;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b804; Value = 0xfff6919b; PC = 0x8002630 *)
mov r6 L0x2001b804;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bc04; Value = 0xffe757df; PC = 0x8002634 *)
mov r7 L0x2001bc04;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  36 ********************)

ghost c0015@sint32,c0271@sint32,c0527@sint32,c0783@sint32 :
and [c0015=r4, c0271=r5, c0527=r6, c0783=r7] && true;

ecut and [
eqmod (c0015*x** 15+c0271*x**271+c0527*x**527+c0783*x**783)
      (4*L0x2001b004*x** 15) [1043969, x**256 - 1],
eqmod (c0015*x** 15+c0271*x**271+c0527*x**527+c0783*x**783)
      (4*L0x2001b404*x** 15) [1043969, x**256 + 1],
eqmod (c0015*x** 15+c0271*x**271+c0527*x**527+c0783*x**783)
      (4*L0x2001b804*x** 15) [1043969, x**256 - 554923],
eqmod (c0015*x** 15+c0271*x**271+c0527*x**527+c0783*x**783)
      (4*L0x2001bc04*x** 15) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf015@sint32 : cf015 = r4 && cf015 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf271@sint32 : cf271 = r5 && cf271 = r5;

(******************** CUT  37 ********************)
ecut and [
eqmod 256*cf015 2**32*(c0015+c0524) 1043969,
eqmod 256*cf271 2**32*(c0271+c0780) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200187e6; PC = 0x80026a0 *)
mov L0x200187e6 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x200189e6; PC = 0x80026a4 *)
mov L0x200189e6 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b008; Value = 0x000d3259; PC = 0x80026b0 *)
mov r4 L0x2001b008;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b408; Value = 0xffe0ed75; PC = 0x80026b4 *)
mov r5 L0x2001b408;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b808; Value = 0x000536a0; PC = 0x80026b8 *)
mov r6 L0x2001b808;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bc08; Value = 0xfff1ed46; PC = 0x80026bc *)
mov r7 L0x2001bc08;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  38 ********************)

ghost c0016@sint32,c0272@sint32,c0528@sint32,c0784@sint32 :
and [c0016=r4, c0272=r5, c0528=r6, c0784=r7] && true;

ecut and [
eqmod (c0016*x** 16+c0272*x**272+c0528*x**528+c0784*x**784)
      (4*L0x2001b008*x** 16) [1043969, x**256 - 1],
eqmod (c0016*x** 16+c0272*x**272+c0528*x**528+c0784*x**784)
      (4*L0x2001b408*x** 16) [1043969, x**256 + 1],
eqmod (c0016*x** 16+c0272*x**272+c0528*x**528+c0784*x**784)
      (4*L0x2001b808*x** 16) [1043969, x**256 - 554923],
eqmod (c0016*x** 16+c0272*x**272+c0528*x**528+c0784*x**784)
      (4*L0x2001bc08*x** 16) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf016@sint32 : cf016 = r4 && cf016 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf272@sint32 : cf272 = r5 && cf272 = r5;

(******************** CUT  39 ********************)
ecut and [
eqmod 256*cf016 2**32*(c0016+c0525) 1043969,
eqmod 256*cf272 2**32*(c0272+c0781) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200187e8; PC = 0x8002728 *)
mov L0x200187e8 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x200189e8; PC = 0x800272c *)
mov L0x200189e8 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b00c; Value = 0x00133e5f; PC = 0x8002738 *)
mov r4 L0x2001b00c;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b40c; Value = 0x0014c9cd; PC = 0x800273c *)
mov r5 L0x2001b40c;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b80c; Value = 0xfff42fce; PC = 0x8002740 *)
mov r6 L0x2001b80c;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bc0c; Value = 0xffeb9d98; PC = 0x8002744 *)
mov r7 L0x2001bc0c;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  40 ********************)

ghost c0017@sint32,c0273@sint32,c0529@sint32,c0785@sint32 :
and [c0017=r4, c0273=r5, c0529=r6, c0785=r7] && true;

ecut and [
eqmod (c0017*x** 17+c0273*x**273+c0529*x**529+c0785*x**785)
      (4*L0x2001b00c*x** 17) [1043969, x**256 - 1],
eqmod (c0017*x** 17+c0273*x**273+c0529*x**529+c0785*x**785)
      (4*L0x2001b40c*x** 17) [1043969, x**256 + 1],
eqmod (c0017*x** 17+c0273*x**273+c0529*x**529+c0785*x**785)
      (4*L0x2001b80c*x** 17) [1043969, x**256 - 554923],
eqmod (c0017*x** 17+c0273*x**273+c0529*x**529+c0785*x**785)
      (4*L0x2001bc0c*x** 17) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf017@sint32 : cf017 = r4 && cf017 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf273@sint32 : cf273 = r5 && cf273 = r5;

(******************** CUT  41 ********************)
ecut and [
eqmod 256*cf017 2**32*(c0017+c0526) 1043969,
eqmod 256*cf273 2**32*(c0273+c0782) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200187ea; PC = 0x80027b0 *)
mov L0x200187ea r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x200189ea; PC = 0x80027b4 *)
mov L0x200189ea r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b010; Value = 0xfff5711a; PC = 0x8002628 *)
mov r4 L0x2001b010;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b410; Value = 0x000ddfdd; PC = 0x800262c *)
mov r5 L0x2001b410;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b810; Value = 0x0000f603; PC = 0x8002630 *)
mov r6 L0x2001b810;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bc10; Value = 0xfffe7f02; PC = 0x8002634 *)
mov r7 L0x2001bc10;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  42 ********************)

ghost c0018@sint32,c0274@sint32,c0530@sint32,c0786@sint32 :
and [c0018=r4, c0274=r5, c0530=r6, c0786=r7] && true;

ecut and [
eqmod (c0018*x** 18+c0274*x**274+c0530*x**530+c0786*x**786)
      (4*L0x2001b010*x** 18) [1043969, x**256 - 1],
eqmod (c0018*x** 18+c0274*x**274+c0530*x**530+c0786*x**786)
      (4*L0x2001b410*x** 18) [1043969, x**256 + 1],
eqmod (c0018*x** 18+c0274*x**274+c0530*x**530+c0786*x**786)
      (4*L0x2001b810*x** 18) [1043969, x**256 - 554923],
eqmod (c0018*x** 18+c0274*x**274+c0530*x**530+c0786*x**786)
      (4*L0x2001bc10*x** 18) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf018@sint32 : cf018 = r4 && cf018 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf274@sint32 : cf274 = r5 && cf274 = r5;

(******************** CUT  43 ********************)
ecut and [
eqmod 256*cf018 2**32*(c0018+c0527) 1043969,
eqmod 256*cf274 2**32*(c0274+c0783) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200187ec; PC = 0x80026a0 *)
mov L0x200187ec r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x200189ec; PC = 0x80026a4 *)
mov L0x200189ec r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b014; Value = 0x0000a38b; PC = 0x80026b0 *)
mov r4 L0x2001b014;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b414; Value = 0xfffca1a8; PC = 0x80026b4 *)
mov r5 L0x2001b414;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b814; Value = 0x00176467; PC = 0x80026b8 *)
mov r6 L0x2001b814;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bc14; Value = 0x000a6017; PC = 0x80026bc *)
mov r7 L0x2001bc14;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  44 ********************)

ghost c0019@sint32,c0275@sint32,c0531@sint32,c0787@sint32 :
and [c0019=r4, c0275=r5, c0531=r6, c0787=r7] && true;

ecut and [
eqmod (c0019*x** 19+c0275*x**275+c0531*x**531+c0787*x**787)
      (4*L0x2001b014*x** 19) [1043969, x**256 - 1],
eqmod (c0019*x** 19+c0275*x**275+c0531*x**531+c0787*x**787)
      (4*L0x2001b414*x** 19) [1043969, x**256 + 1],
eqmod (c0019*x** 19+c0275*x**275+c0531*x**531+c0787*x**787)
      (4*L0x2001b814*x** 19) [1043969, x**256 - 554923],
eqmod (c0019*x** 19+c0275*x**275+c0531*x**531+c0787*x**787)
      (4*L0x2001bc14*x** 19) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf019@sint32 : cf019 = r4 && cf019 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf275@sint32 : cf275 = r5 && cf275 = r5;

(******************** CUT  45 ********************)
ecut and [
eqmod 256*cf019 2**32*(c0019+c0528) 1043969,
eqmod 256*cf275 2**32*(c0275+c0784) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200187ee; PC = 0x8002728 *)
mov L0x200187ee r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x200189ee; PC = 0x800272c *)
mov L0x200189ee r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b018; Value = 0xfff548c8; PC = 0x8002738 *)
mov r4 L0x2001b018;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b418; Value = 0xfff72d12; PC = 0x800273c *)
mov r5 L0x2001b418;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b818; Value = 0xffedaf0e; PC = 0x8002740 *)
mov r6 L0x2001b818;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bc18; Value = 0x00143c48; PC = 0x8002744 *)
mov r7 L0x2001bc18;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  46 ********************)

ghost c0020@sint32,c0276@sint32,c0532@sint32,c0788@sint32 :
and [c0020=r4, c0276=r5, c0532=r6, c0788=r7] && true;

ecut and [
eqmod (c0020*x** 20+c0276*x**276+c0532*x**532+c0788*x**788)
      (4*L0x2001b018*x** 20) [1043969, x**256 - 1],
eqmod (c0020*x** 20+c0276*x**276+c0532*x**532+c0788*x**788)
      (4*L0x2001b418*x** 20) [1043969, x**256 + 1],
eqmod (c0020*x** 20+c0276*x**276+c0532*x**532+c0788*x**788)
      (4*L0x2001b818*x** 20) [1043969, x**256 - 554923],
eqmod (c0020*x** 20+c0276*x**276+c0532*x**532+c0788*x**788)
      (4*L0x2001bc18*x** 20) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf020@sint32 : cf020 = r4 && cf020 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf276@sint32 : cf276 = r5 && cf276 = r5;

(******************** CUT  47 ********************)
ecut and [
eqmod 256*cf020 2**32*(c0020+c0529) 1043969,
eqmod 256*cf276 2**32*(c0276+c0785) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200187f0; PC = 0x80027b0 *)
mov L0x200187f0 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x200189f0; PC = 0x80027b4 *)
mov L0x200189f0 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b01c; Value = 0x000182ec; PC = 0x8002628 *)
mov r4 L0x2001b01c;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b41c; Value = 0xfffbdfbe; PC = 0x800262c *)
mov r5 L0x2001b41c;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b81c; Value = 0xfff9d1fa; PC = 0x8002630 *)
mov r6 L0x2001b81c;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bc1c; Value = 0x00022bdf; PC = 0x8002634 *)
mov r7 L0x2001bc1c;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  48 ********************)

ghost c0021@sint32,c0277@sint32,c0533@sint32,c0789@sint32 :
and [c0021=r4, c0277=r5, c0533=r6, c0789=r7] && true;

ecut and [
eqmod (c0021*x** 21+c0277*x**277+c0533*x**533+c0789*x**789)
      (4*L0x2001b01c*x** 21) [1043969, x**256 - 1],
eqmod (c0021*x** 21+c0277*x**277+c0533*x**533+c0789*x**789)
      (4*L0x2001b41c*x** 21) [1043969, x**256 + 1],
eqmod (c0021*x** 21+c0277*x**277+c0533*x**533+c0789*x**789)
      (4*L0x2001b81c*x** 21) [1043969, x**256 - 554923],
eqmod (c0021*x** 21+c0277*x**277+c0533*x**533+c0789*x**789)
      (4*L0x2001bc1c*x** 21) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf021@sint32 : cf021 = r4 && cf021 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf277@sint32 : cf277 = r5 && cf277 = r5;

(******************** CUT  49 ********************)
ecut and [
eqmod 256*cf021 2**32*(c0021+c0530) 1043969,
eqmod 256*cf277 2**32*(c0277+c0786) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200187f2; PC = 0x80026a0 *)
mov L0x200187f2 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x200189f2; PC = 0x80026a4 *)
mov L0x200189f2 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b020; Value = 0xfff1f64d; PC = 0x80026b0 *)
mov r4 L0x2001b020;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b420; Value = 0x000d9364; PC = 0x80026b4 *)
mov r5 L0x2001b420;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b820; Value = 0xffece677; PC = 0x80026b8 *)
mov r6 L0x2001b820;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bc20; Value = 0xfff4c09b; PC = 0x80026bc *)
mov r7 L0x2001bc20;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  50 ********************)

ghost c0022@sint32,c0278@sint32,c0534@sint32,c0790@sint32 :
and [c0022=r4, c0278=r5, c0534=r6, c0790=r7] && true;

ecut and [
eqmod (c0022*x** 22+c0278*x**278+c0534*x**534+c0790*x**790)
      (4*L0x2001b020*x** 22) [1043969, x**256 - 1],
eqmod (c0022*x** 22+c0278*x**278+c0534*x**534+c0790*x**790)
      (4*L0x2001b420*x** 22) [1043969, x**256 + 1],
eqmod (c0022*x** 22+c0278*x**278+c0534*x**534+c0790*x**790)
      (4*L0x2001b820*x** 22) [1043969, x**256 - 554923],
eqmod (c0022*x** 22+c0278*x**278+c0534*x**534+c0790*x**790)
      (4*L0x2001bc20*x** 22) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf022@sint32 : cf022 = r4 && cf022 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf278@sint32 : cf278 = r5 && cf278 = r5;

(******************** CUT  51 ********************)
ecut and [
eqmod 256*cf022 2**32*(c0022+c0531) 1043969,
eqmod 256*cf278 2**32*(c0278+c0787) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200187f4; PC = 0x8002728 *)
mov L0x200187f4 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x200189f4; PC = 0x800272c *)
mov L0x200189f4 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b024; Value = 0x000b33c7; PC = 0x8002738 *)
mov r4 L0x2001b024;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b424; Value = 0x00130130; PC = 0x800273c *)
mov r5 L0x2001b424;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b824; Value = 0xffed508c; PC = 0x8002740 *)
mov r6 L0x2001b824;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bc24; Value = 0x0002c0b7; PC = 0x8002744 *)
mov r7 L0x2001bc24;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  52 ********************)

ghost c0023@sint32,c0279@sint32,c0535@sint32,c0791@sint32 :
and [c0023=r4, c0279=r5, c0535=r6, c0791=r7] && true;

ecut and [
eqmod (c0023*x** 23+c0279*x**279+c0535*x**535+c0791*x**791)
      (4*L0x2001b024*x** 23) [1043969, x**256 - 1],
eqmod (c0023*x** 23+c0279*x**279+c0535*x**535+c0791*x**791)
      (4*L0x2001b424*x** 23) [1043969, x**256 + 1],
eqmod (c0023*x** 23+c0279*x**279+c0535*x**535+c0791*x**791)
      (4*L0x2001b824*x** 23) [1043969, x**256 - 554923],
eqmod (c0023*x** 23+c0279*x**279+c0535*x**535+c0791*x**791)
      (4*L0x2001bc24*x** 23) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf023@sint32 : cf023 = r4 && cf023 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf279@sint32 : cf279 = r5 && cf279 = r5;

(******************** CUT  53 ********************)
ecut and [
eqmod 256*cf023 2**32*(c0023+c0532) 1043969,
eqmod 256*cf279 2**32*(c0279+c0788) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200187f6; PC = 0x80027b0 *)
mov L0x200187f6 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x200189f6; PC = 0x80027b4 *)
mov L0x200189f6 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b028; Value = 0xffff2299; PC = 0x8002628 *)
mov r4 L0x2001b028;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b428; Value = 0xfff631ab; PC = 0x800262c *)
mov r5 L0x2001b428;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b828; Value = 0xfff45a98; PC = 0x8002630 *)
mov r6 L0x2001b828;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bc28; Value = 0x00122d10; PC = 0x8002634 *)
mov r7 L0x2001bc28;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  54 ********************)

ghost c0024@sint32,c0280@sint32,c0536@sint32,c0792@sint32 :
and [c0024=r4, c0280=r5, c0536=r6, c0792=r7] && true;

ecut and [
eqmod (c0024*x** 24+c0280*x**280+c0536*x**536+c0792*x**792)
      (4*L0x2001b028*x** 24) [1043969, x**256 - 1],
eqmod (c0024*x** 24+c0280*x**280+c0536*x**536+c0792*x**792)
      (4*L0x2001b428*x** 24) [1043969, x**256 + 1],
eqmod (c0024*x** 24+c0280*x**280+c0536*x**536+c0792*x**792)
      (4*L0x2001b828*x** 24) [1043969, x**256 - 554923],
eqmod (c0024*x** 24+c0280*x**280+c0536*x**536+c0792*x**792)
      (4*L0x2001bc28*x** 24) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf024@sint32 : cf024 = r4 && cf024 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf280@sint32 : cf280 = r5 && cf280 = r5;

(******************** CUT  55 ********************)
ecut and [
eqmod 256*cf024 2**32*(c0024+c0533) 1043969,
eqmod 256*cf280 2**32*(c0280+c0789) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200187f8; PC = 0x80026a0 *)
mov L0x200187f8 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x200189f8; PC = 0x80026a4 *)
mov L0x200189f8 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b02c; Value = 0x0000d0ce; PC = 0x80026b0 *)
mov r4 L0x2001b02c;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b42c; Value = 0x0006ef84; PC = 0x80026b4 *)
mov r5 L0x2001b42c;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b82c; Value = 0xffe821da; PC = 0x80026b8 *)
mov r6 L0x2001b82c;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bc2c; Value = 0xfff2afba; PC = 0x80026bc *)
mov r7 L0x2001bc2c;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  56 ********************)

ghost c0025@sint32,c0281@sint32,c0537@sint32,c0793@sint32 :
and [c0025=r4, c0281=r5, c0537=r6, c0793=r7] && true;

ecut and [
eqmod (c0025*x** 25+c0281*x**281+c0537*x**537+c0793*x**793)
      (4*L0x2001b02c*x** 25) [1043969, x**256 - 1],
eqmod (c0025*x** 25+c0281*x**281+c0537*x**537+c0793*x**793)
      (4*L0x2001b42c*x** 25) [1043969, x**256 + 1],
eqmod (c0025*x** 25+c0281*x**281+c0537*x**537+c0793*x**793)
      (4*L0x2001b82c*x** 25) [1043969, x**256 - 554923],
eqmod (c0025*x** 25+c0281*x**281+c0537*x**537+c0793*x**793)
      (4*L0x2001bc2c*x** 25) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf025@sint32 : cf025 = r4 && cf025 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf281@sint32 : cf281 = r5 && cf281 = r5;

(******************** CUT  57 ********************)
ecut and [
eqmod 256*cf025 2**32*(c0025+c0534) 1043969,
eqmod 256*cf281 2**32*(c0281+c0790) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200187fa; PC = 0x8002728 *)
mov L0x200187fa r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x200189fa; PC = 0x800272c *)
mov L0x200189fa r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b030; Value = 0x00025cb7; PC = 0x8002738 *)
mov r4 L0x2001b030;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b430; Value = 0x000953d4; PC = 0x800273c *)
mov r5 L0x2001b430;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b830; Value = 0xfff4ce1f; PC = 0x8002740 *)
mov r6 L0x2001b830;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bc30; Value = 0x0009465b; PC = 0x8002744 *)
mov r7 L0x2001bc30;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  58 ********************)

ghost c0026@sint32,c0282@sint32,c0538@sint32,c0794@sint32 :
and [c0026=r4, c0282=r5, c0538=r6, c0794=r7] && true;

ecut and [
eqmod (c0026*x** 26+c0282*x**282+c0538*x**538+c0794*x**794)
      (4*L0x2001b030*x** 26) [1043969, x**256 - 1],
eqmod (c0026*x** 26+c0282*x**282+c0538*x**538+c0794*x**794)
      (4*L0x2001b430*x** 26) [1043969, x**256 + 1],
eqmod (c0026*x** 26+c0282*x**282+c0538*x**538+c0794*x**794)
      (4*L0x2001b830*x** 26) [1043969, x**256 - 554923],
eqmod (c0026*x** 26+c0282*x**282+c0538*x**538+c0794*x**794)
      (4*L0x2001bc30*x** 26) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf026@sint32 : cf026 = r4 && cf026 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf282@sint32 : cf282 = r5 && cf282 = r5;

(******************** CUT  59 ********************)
ecut and [
eqmod 256*cf026 2**32*(c0026+c0535) 1043969,
eqmod 256*cf282 2**32*(c0282+c0791) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200187fc; PC = 0x80027b0 *)
mov L0x200187fc r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x200189fc; PC = 0x80027b4 *)
mov L0x200189fc r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b034; Value = 0x00061840; PC = 0x8002628 *)
mov r4 L0x2001b034;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b434; Value = 0x0006697e; PC = 0x800262c *)
mov r5 L0x2001b434;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b834; Value = 0xffee1ddb; PC = 0x8002630 *)
mov r6 L0x2001b834;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bc34; Value = 0x000525e7; PC = 0x8002634 *)
mov r7 L0x2001bc34;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  60 ********************)

ghost c0027@sint32,c0283@sint32,c0539@sint32,c0795@sint32 :
and [c0027=r4, c0283=r5, c0539=r6, c0795=r7] && true;

ecut and [
eqmod (c0027*x** 27+c0283*x**283+c0539*x**539+c0795*x**795)
      (4*L0x2001b034*x** 27) [1043969, x**256 - 1],
eqmod (c0027*x** 27+c0283*x**283+c0539*x**539+c0795*x**795)
      (4*L0x2001b434*x** 27) [1043969, x**256 + 1],
eqmod (c0027*x** 27+c0283*x**283+c0539*x**539+c0795*x**795)
      (4*L0x2001b834*x** 27) [1043969, x**256 - 554923],
eqmod (c0027*x** 27+c0283*x**283+c0539*x**539+c0795*x**795)
      (4*L0x2001bc34*x** 27) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf027@sint32 : cf027 = r4 && cf027 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf283@sint32 : cf283 = r5 && cf283 = r5;

(******************** CUT  61 ********************)
ecut and [
eqmod 256*cf027 2**32*(c0027+c0536) 1043969,
eqmod 256*cf283 2**32*(c0283+c0792) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200187fe; PC = 0x80026a0 *)
mov L0x200187fe r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x200189fe; PC = 0x80026a4 *)
mov L0x200189fe r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b038; Value = 0x001c97b8; PC = 0x80026b0 *)
mov r4 L0x2001b038;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b438; Value = 0x0005584e; PC = 0x80026b4 *)
mov r5 L0x2001b438;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b838; Value = 0x00067944; PC = 0x80026b8 *)
mov r6 L0x2001b838;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bc38; Value = 0xffde6b4b; PC = 0x80026bc *)
mov r7 L0x2001bc38;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  62 ********************)

ghost c0028@sint32,c0284@sint32,c0540@sint32,c0796@sint32 :
and [c0028=r4, c0284=r5, c0540=r6, c0796=r7] && true;

ecut and [
eqmod (c0028*x** 28+c0284*x**284+c0540*x**540+c0796*x**796)
      (4*L0x2001b038*x** 28) [1043969, x**256 - 1],
eqmod (c0028*x** 28+c0284*x**284+c0540*x**540+c0796*x**796)
      (4*L0x2001b438*x** 28) [1043969, x**256 + 1],
eqmod (c0028*x** 28+c0284*x**284+c0540*x**540+c0796*x**796)
      (4*L0x2001b838*x** 28) [1043969, x**256 - 554923],
eqmod (c0028*x** 28+c0284*x**284+c0540*x**540+c0796*x**796)
      (4*L0x2001bc38*x** 28) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf028@sint32 : cf028 = r4 && cf028 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf284@sint32 : cf284 = r5 && cf284 = r5;

(******************** CUT  63 ********************)
ecut and [
eqmod 256*cf028 2**32*(c0028+c0537) 1043969,
eqmod 256*cf284 2**32*(c0284+c0793) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018800; PC = 0x8002728 *)
mov L0x20018800 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a00; PC = 0x800272c *)
mov L0x20018a00 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b03c; Value = 0xfff11b2e; PC = 0x8002738 *)
mov r4 L0x2001b03c;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b43c; Value = 0xffeb6d4d; PC = 0x800273c *)
mov r5 L0x2001b43c;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b83c; Value = 0x00197c17; PC = 0x8002740 *)
mov r6 L0x2001b83c;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bc3c; Value = 0x00009a67; PC = 0x8002744 *)
mov r7 L0x2001bc3c;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  64 ********************)

ghost c0029@sint32,c0285@sint32,c0541@sint32,c0797@sint32 :
and [c0029=r4, c0285=r5, c0541=r6, c0797=r7] && true;

ecut and [
eqmod (c0029*x** 29+c0285*x**285+c0541*x**541+c0797*x**797)
      (4*L0x2001b03c*x** 29) [1043969, x**256 - 1],
eqmod (c0029*x** 29+c0285*x**285+c0541*x**541+c0797*x**797)
      (4*L0x2001b43c*x** 29) [1043969, x**256 + 1],
eqmod (c0029*x** 29+c0285*x**285+c0541*x**541+c0797*x**797)
      (4*L0x2001b83c*x** 29) [1043969, x**256 - 554923],
eqmod (c0029*x** 29+c0285*x**285+c0541*x**541+c0797*x**797)
      (4*L0x2001bc3c*x** 29) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf029@sint32 : cf029 = r4 && cf029 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf285@sint32 : cf285 = r5 && cf285 = r5;

(******************** CUT  65 ********************)
ecut and [
eqmod 256*cf029 2**32*(c0029+c0538) 1043969,
eqmod 256*cf285 2**32*(c0285+c0794) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018802; PC = 0x80027b0 *)
mov L0x20018802 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a02; PC = 0x80027b4 *)
mov L0x20018a02 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b040; Value = 0xfff2ce11; PC = 0x8002628 *)
mov r4 L0x2001b040;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b440; Value = 0xffeeacb3; PC = 0x800262c *)
mov r5 L0x2001b440;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b840; Value = 0xffec0e76; PC = 0x8002630 *)
mov r6 L0x2001b840;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bc40; Value = 0xfff91d98; PC = 0x8002634 *)
mov r7 L0x2001bc40;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  66 ********************)

ghost c0030@sint32,c0286@sint32,c0542@sint32,c0798@sint32 :
and [c0030=r4, c0286=r5, c0542=r6, c0798=r7] && true;

ecut and [
eqmod (c0030*x** 30+c0286*x**286+c0542*x**542+c0798*x**798)
      (4*L0x2001b040*x** 30) [1043969, x**256 - 1],
eqmod (c0030*x** 30+c0286*x**286+c0542*x**542+c0798*x**798)
      (4*L0x2001b440*x** 30) [1043969, x**256 + 1],
eqmod (c0030*x** 30+c0286*x**286+c0542*x**542+c0798*x**798)
      (4*L0x2001b840*x** 30) [1043969, x**256 - 554923],
eqmod (c0030*x** 30+c0286*x**286+c0542*x**542+c0798*x**798)
      (4*L0x2001bc40*x** 30) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf030@sint32 : cf030 = r4 && cf030 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf286@sint32 : cf286 = r5 && cf286 = r5;

(******************** CUT  67 ********************)
ecut and [
eqmod 256*cf030 2**32*(c0030+c0539) 1043969,
eqmod 256*cf286 2**32*(c0286+c0795) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018804; PC = 0x80026a0 *)
mov L0x20018804 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018a04; PC = 0x80026a4 *)
mov L0x20018a04 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b044; Value = 0x000b4781; PC = 0x80026b0 *)
mov r4 L0x2001b044;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b444; Value = 0xffef3aa8; PC = 0x80026b4 *)
mov r5 L0x2001b444;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b844; Value = 0x00087ba9; PC = 0x80026b8 *)
mov r6 L0x2001b844;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bc44; Value = 0xfff212ed; PC = 0x80026bc *)
mov r7 L0x2001bc44;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  68 ********************)

ghost c0031@sint32,c0287@sint32,c0543@sint32,c0799@sint32 :
and [c0031=r4, c0287=r5, c0543=r6, c0799=r7] && true;

ecut and [
eqmod (c0031*x** 31+c0287*x**287+c0543*x**543+c0799*x**799)
      (4*L0x2001b044*x** 31) [1043969, x**256 - 1],
eqmod (c0031*x** 31+c0287*x**287+c0543*x**543+c0799*x**799)
      (4*L0x2001b444*x** 31) [1043969, x**256 + 1],
eqmod (c0031*x** 31+c0287*x**287+c0543*x**543+c0799*x**799)
      (4*L0x2001b844*x** 31) [1043969, x**256 - 554923],
eqmod (c0031*x** 31+c0287*x**287+c0543*x**543+c0799*x**799)
      (4*L0x2001bc44*x** 31) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf031@sint32 : cf031 = r4 && cf031 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf287@sint32 : cf287 = r5 && cf287 = r5;

(******************** CUT  69 ********************)
ecut and [
eqmod 256*cf031 2**32*(c0031+c0540) 1043969,
eqmod 256*cf287 2**32*(c0287+c0796) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018806; PC = 0x8002728 *)
mov L0x20018806 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a06; PC = 0x800272c *)
mov L0x20018a06 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b048; Value = 0xfffb5f98; PC = 0x8002738 *)
mov r4 L0x2001b048;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b448; Value = 0x000009ca; PC = 0x800273c *)
mov r5 L0x2001b448;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b848; Value = 0x00063fd0; PC = 0x8002740 *)
mov r6 L0x2001b848;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bc48; Value = 0x0005b124; PC = 0x8002744 *)
mov r7 L0x2001bc48;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  70 ********************)

ghost c0032@sint32,c0288@sint32,c0544@sint32,c0800@sint32 :
and [c0032=r4, c0288=r5, c0544=r6, c0800=r7] && true;

ecut and [
eqmod (c0032*x** 32+c0288*x**288+c0544*x**544+c0800*x**800)
      (4*L0x2001b048*x** 32) [1043969, x**256 - 1],
eqmod (c0032*x** 32+c0288*x**288+c0544*x**544+c0800*x**800)
      (4*L0x2001b448*x** 32) [1043969, x**256 + 1],
eqmod (c0032*x** 32+c0288*x**288+c0544*x**544+c0800*x**800)
      (4*L0x2001b848*x** 32) [1043969, x**256 - 554923],
eqmod (c0032*x** 32+c0288*x**288+c0544*x**544+c0800*x**800)
      (4*L0x2001bc48*x** 32) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf032@sint32 : cf032 = r4 && cf032 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf288@sint32 : cf288 = r5 && cf288 = r5;

(******************** CUT  71 ********************)
ecut and [
eqmod 256*cf032 2**32*(c0032+c0541) 1043969,
eqmod 256*cf288 2**32*(c0288+c0797) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018808; PC = 0x80027b0 *)
mov L0x20018808 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a08; PC = 0x80027b4 *)
mov L0x20018a08 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b04c; Value = 0x00057143; PC = 0x8002628 *)
mov r4 L0x2001b04c;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b44c; Value = 0x0002b462; PC = 0x800262c *)
mov r5 L0x2001b44c;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b84c; Value = 0xfff72514; PC = 0x8002630 *)
mov r6 L0x2001b84c;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bc4c; Value = 0xfffc4ec8; PC = 0x8002634 *)
mov r7 L0x2001bc4c;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  72 ********************)

ghost c0033@sint32,c0289@sint32,c0545@sint32,c0801@sint32 :
and [c0033=r4, c0289=r5, c0545=r6, c0801=r7] && true;

ecut and [
eqmod (c0033*x** 33+c0289*x**289+c0545*x**545+c0801*x**801)
      (4*L0x2001b04c*x** 33) [1043969, x**256 - 1],
eqmod (c0033*x** 33+c0289*x**289+c0545*x**545+c0801*x**801)
      (4*L0x2001b44c*x** 33) [1043969, x**256 + 1],
eqmod (c0033*x** 33+c0289*x**289+c0545*x**545+c0801*x**801)
      (4*L0x2001b84c*x** 33) [1043969, x**256 - 554923],
eqmod (c0033*x** 33+c0289*x**289+c0545*x**545+c0801*x**801)
      (4*L0x2001bc4c*x** 33) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf033@sint32 : cf033 = r4 && cf033 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf289@sint32 : cf289 = r5 && cf289 = r5;

(******************** CUT  73 ********************)
ecut and [
eqmod 256*cf033 2**32*(c0033+c0542) 1043969,
eqmod 256*cf289 2**32*(c0289+c0798) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x2001880a; PC = 0x80026a0 *)
mov L0x2001880a r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018a0a; PC = 0x80026a4 *)
mov L0x20018a0a r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b050; Value = 0xfff84159; PC = 0x80026b0 *)
mov r4 L0x2001b050;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b450; Value = 0xfff82c83; PC = 0x80026b4 *)
mov r5 L0x2001b450;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b850; Value = 0xffeb7c9e; PC = 0x80026b8 *)
mov r6 L0x2001b850;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bc50; Value = 0x00114ab4; PC = 0x80026bc *)
mov r7 L0x2001bc50;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  74 ********************)

ghost c0034@sint32,c0290@sint32,c0546@sint32,c0802@sint32 :
and [c0034=r4, c0290=r5, c0546=r6, c0802=r7] && true;

ecut and [
eqmod (c0034*x** 34+c0290*x**290+c0546*x**546+c0802*x**802)
      (4*L0x2001b050*x** 34) [1043969, x**256 - 1],
eqmod (c0034*x** 34+c0290*x**290+c0546*x**546+c0802*x**802)
      (4*L0x2001b450*x** 34) [1043969, x**256 + 1],
eqmod (c0034*x** 34+c0290*x**290+c0546*x**546+c0802*x**802)
      (4*L0x2001b850*x** 34) [1043969, x**256 - 554923],
eqmod (c0034*x** 34+c0290*x**290+c0546*x**546+c0802*x**802)
      (4*L0x2001bc50*x** 34) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf034@sint32 : cf034 = r4 && cf034 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf290@sint32 : cf290 = r5 && cf290 = r5;

(******************** CUT  75 ********************)
ecut and [
eqmod 256*cf034 2**32*(c0034+c0543) 1043969,
eqmod 256*cf290 2**32*(c0290+c0799) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x2001880c; PC = 0x8002728 *)
mov L0x2001880c r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a0c; PC = 0x800272c *)
mov L0x20018a0c r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b054; Value = 0x002675d4; PC = 0x8002738 *)
mov r4 L0x2001b054;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b454; Value = 0xffffa6c1; PC = 0x800273c *)
mov r5 L0x2001b454;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b854; Value = 0x0003fcff; PC = 0x8002740 *)
mov r6 L0x2001b854;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bc54; Value = 0x00153899; PC = 0x8002744 *)
mov r7 L0x2001bc54;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  76 ********************)

ghost c0035@sint32,c0291@sint32,c0547@sint32,c0803@sint32 :
and [c0035=r4, c0291=r5, c0547=r6, c0803=r7] && true;

ecut and [
eqmod (c0035*x** 35+c0291*x**291+c0547*x**547+c0803*x**803)
      (4*L0x2001b054*x** 35) [1043969, x**256 - 1],
eqmod (c0035*x** 35+c0291*x**291+c0547*x**547+c0803*x**803)
      (4*L0x2001b454*x** 35) [1043969, x**256 + 1],
eqmod (c0035*x** 35+c0291*x**291+c0547*x**547+c0803*x**803)
      (4*L0x2001b854*x** 35) [1043969, x**256 - 554923],
eqmod (c0035*x** 35+c0291*x**291+c0547*x**547+c0803*x**803)
      (4*L0x2001bc54*x** 35) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf035@sint32 : cf035 = r4 && cf035 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf291@sint32 : cf291 = r5 && cf291 = r5;

(******************** CUT  77 ********************)
ecut and [
eqmod 256*cf035 2**32*(c0035+c0544) 1043969,
eqmod 256*cf291 2**32*(c0291+c0800) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x2001880e; PC = 0x80027b0 *)
mov L0x2001880e r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a0e; PC = 0x80027b4 *)
mov L0x20018a0e r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b058; Value = 0x00156568; PC = 0x8002628 *)
mov r4 L0x2001b058;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b458; Value = 0x0001ebbb; PC = 0x800262c *)
mov r5 L0x2001b458;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b858; Value = 0xfff21d35; PC = 0x8002630 *)
mov r6 L0x2001b858;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bc58; Value = 0x000ce4aa; PC = 0x8002634 *)
mov r7 L0x2001bc58;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  78 ********************)

ghost c0036@sint32,c0292@sint32,c0548@sint32,c0804@sint32 :
and [c0036=r4, c0292=r5, c0548=r6, c0804=r7] && true;

ecut and [
eqmod (c0036*x** 36+c0292*x**292+c0548*x**548+c0804*x**804)
      (4*L0x2001b058*x** 36) [1043969, x**256 - 1],
eqmod (c0036*x** 36+c0292*x**292+c0548*x**548+c0804*x**804)
      (4*L0x2001b458*x** 36) [1043969, x**256 + 1],
eqmod (c0036*x** 36+c0292*x**292+c0548*x**548+c0804*x**804)
      (4*L0x2001b858*x** 36) [1043969, x**256 - 554923],
eqmod (c0036*x** 36+c0292*x**292+c0548*x**548+c0804*x**804)
      (4*L0x2001bc58*x** 36) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf036@sint32 : cf036 = r4 && cf036 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf292@sint32 : cf292 = r5 && cf292 = r5;

(******************** CUT  79 ********************)
ecut and [
eqmod 256*cf036 2**32*(c0036+c0545) 1043969,
eqmod 256*cf292 2**32*(c0292+c0801) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018810; PC = 0x80026a0 *)
mov L0x20018810 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018a10; PC = 0x80026a4 *)
mov L0x20018a10 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b05c; Value = 0xfffd531e; PC = 0x80026b0 *)
mov r4 L0x2001b05c;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b45c; Value = 0x00038320; PC = 0x80026b4 *)
mov r5 L0x2001b45c;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b85c; Value = 0xfff7c039; PC = 0x80026b8 *)
mov r6 L0x2001b85c;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bc5c; Value = 0x00044c23; PC = 0x80026bc *)
mov r7 L0x2001bc5c;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  80 ********************)

ghost c0037@sint32,c0293@sint32,c0549@sint32,c0805@sint32 :
and [c0037=r4, c0293=r5, c0549=r6, c0805=r7] && true;

ecut and [
eqmod (c0037*x** 37+c0293*x**293+c0549*x**549+c0805*x**805)
      (4*L0x2001b05c*x** 37) [1043969, x**256 - 1],
eqmod (c0037*x** 37+c0293*x**293+c0549*x**549+c0805*x**805)
      (4*L0x2001b45c*x** 37) [1043969, x**256 + 1],
eqmod (c0037*x** 37+c0293*x**293+c0549*x**549+c0805*x**805)
      (4*L0x2001b85c*x** 37) [1043969, x**256 - 554923],
eqmod (c0037*x** 37+c0293*x**293+c0549*x**549+c0805*x**805)
      (4*L0x2001bc5c*x** 37) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf037@sint32 : cf037 = r4 && cf037 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf293@sint32 : cf293 = r5 && cf293 = r5;

(******************** CUT  81 ********************)
ecut and [
eqmod 256*cf037 2**32*(c0037+c0546) 1043969,
eqmod 256*cf293 2**32*(c0293+c0802) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018812; PC = 0x8002728 *)
mov L0x20018812 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a12; PC = 0x800272c *)
mov L0x20018a12 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b060; Value = 0x0008dee5; PC = 0x8002738 *)
mov r4 L0x2001b060;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b460; Value = 0x00035ace; PC = 0x800273c *)
mov r5 L0x2001b460;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b860; Value = 0xfff38bfe; PC = 0x8002740 *)
mov r6 L0x2001b860;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bc60; Value = 0xfffebc38; PC = 0x8002744 *)
mov r7 L0x2001bc60;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  82 ********************)

ghost c0038@sint32,c0294@sint32,c0550@sint32,c0806@sint32 :
and [c0038=r4, c0294=r5, c0550=r6, c0806=r7] && true;

ecut and [
eqmod (c0038*x** 38+c0294*x**294+c0550*x**550+c0806*x**806)
      (4*L0x2001b060*x** 38) [1043969, x**256 - 1],
eqmod (c0038*x** 38+c0294*x**294+c0550*x**550+c0806*x**806)
      (4*L0x2001b460*x** 38) [1043969, x**256 + 1],
eqmod (c0038*x** 38+c0294*x**294+c0550*x**550+c0806*x**806)
      (4*L0x2001b860*x** 38) [1043969, x**256 - 554923],
eqmod (c0038*x** 38+c0294*x**294+c0550*x**550+c0806*x**806)
      (4*L0x2001bc60*x** 38) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf038@sint32 : cf038 = r4 && cf038 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf294@sint32 : cf294 = r5 && cf294 = r5;

(******************** CUT  83 ********************)
ecut and [
eqmod 256*cf038 2**32*(c0038+c0547) 1043969,
eqmod 256*cf294 2**32*(c0294+c0803) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018814; PC = 0x80027b0 *)
mov L0x20018814 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a14; PC = 0x80027b4 *)
mov L0x20018a14 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b064; Value = 0xffdaa9b1; PC = 0x8002628 *)
mov r4 L0x2001b064;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b464; Value = 0x00032a9f; PC = 0x800262c *)
mov r5 L0x2001b464;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b864; Value = 0xfffbf5e3; PC = 0x8002630 *)
mov r6 L0x2001b864;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bc64; Value = 0xffff5b99; PC = 0x8002634 *)
mov r7 L0x2001bc64;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  84 ********************)

ghost c0039@sint32,c0295@sint32,c0551@sint32,c0807@sint32 :
and [c0039=r4, c0295=r5, c0551=r6, c0807=r7] && true;

ecut and [
eqmod (c0039*x** 39+c0295*x**295+c0551*x**551+c0807*x**807)
      (4*L0x2001b064*x** 39) [1043969, x**256 - 1],
eqmod (c0039*x** 39+c0295*x**295+c0551*x**551+c0807*x**807)
      (4*L0x2001b464*x** 39) [1043969, x**256 + 1],
eqmod (c0039*x** 39+c0295*x**295+c0551*x**551+c0807*x**807)
      (4*L0x2001b864*x** 39) [1043969, x**256 - 554923],
eqmod (c0039*x** 39+c0295*x**295+c0551*x**551+c0807*x**807)
      (4*L0x2001bc64*x** 39) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf039@sint32 : cf039 = r4 && cf039 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf295@sint32 : cf295 = r5 && cf295 = r5;

(******************** CUT  85 ********************)
ecut and [
eqmod 256*cf039 2**32*(c0039+c0548) 1043969,
eqmod 256*cf295 2**32*(c0295+c0804) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018816; PC = 0x80026a0 *)
mov L0x20018816 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018a16; PC = 0x80026a4 *)
mov L0x20018a16 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b068; Value = 0x000587b6; PC = 0x80026b0 *)
mov r4 L0x2001b068;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b468; Value = 0xfff963c2; PC = 0x80026b4 *)
mov r5 L0x2001b468;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b868; Value = 0xfffec941; PC = 0x80026b8 *)
mov r6 L0x2001b868;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bc68; Value = 0x00069e8a; PC = 0x80026bc *)
mov r7 L0x2001bc68;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  86 ********************)

ghost c0040@sint32,c0296@sint32,c0552@sint32,c0808@sint32 :
and [c0040=r4, c0296=r5, c0552=r6, c0808=r7] && true;

ecut and [
eqmod (c0040*x** 40+c0296*x**296+c0552*x**552+c0808*x**808)
      (4*L0x2001b068*x** 40) [1043969, x**256 - 1],
eqmod (c0040*x** 40+c0296*x**296+c0552*x**552+c0808*x**808)
      (4*L0x2001b468*x** 40) [1043969, x**256 + 1],
eqmod (c0040*x** 40+c0296*x**296+c0552*x**552+c0808*x**808)
      (4*L0x2001b868*x** 40) [1043969, x**256 - 554923],
eqmod (c0040*x** 40+c0296*x**296+c0552*x**552+c0808*x**808)
      (4*L0x2001bc68*x** 40) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf040@sint32 : cf040 = r4 && cf040 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf296@sint32 : cf296 = r5 && cf296 = r5;

(******************** CUT  87 ********************)
ecut and [
eqmod 256*cf040 2**32*(c0040+c0549) 1043969,
eqmod 256*cf296 2**32*(c0296+c0805) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018818; PC = 0x8002728 *)
mov L0x20018818 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a18; PC = 0x800272c *)
mov L0x20018a18 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b06c; Value = 0xfffc1616; PC = 0x8002738 *)
mov r4 L0x2001b06c;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b46c; Value = 0x0001e639; PC = 0x800273c *)
mov r5 L0x2001b46c;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b86c; Value = 0xffff9bb1; PC = 0x8002740 *)
mov r6 L0x2001b86c;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bc6c; Value = 0x000bbde2; PC = 0x8002744 *)
mov r7 L0x2001bc6c;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  88 ********************)

ghost c0041@sint32,c0297@sint32,c0553@sint32,c0809@sint32 :
and [c0041=r4, c0297=r5, c0553=r6, c0809=r7] && true;

ecut and [
eqmod (c0041*x** 41+c0297*x**297+c0553*x**553+c0809*x**809)
      (4*L0x2001b06c*x** 41) [1043969, x**256 - 1],
eqmod (c0041*x** 41+c0297*x**297+c0553*x**553+c0809*x**809)
      (4*L0x2001b46c*x** 41) [1043969, x**256 + 1],
eqmod (c0041*x** 41+c0297*x**297+c0553*x**553+c0809*x**809)
      (4*L0x2001b86c*x** 41) [1043969, x**256 - 554923],
eqmod (c0041*x** 41+c0297*x**297+c0553*x**553+c0809*x**809)
      (4*L0x2001bc6c*x** 41) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf041@sint32 : cf041 = r4 && cf041 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf297@sint32 : cf297 = r5 && cf297 = r5;

(******************** CUT  89 ********************)
ecut and [
eqmod 256*cf041 2**32*(c0041+c0550) 1043969,
eqmod 256*cf297 2**32*(c0297+c0806) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x2001881a; PC = 0x80027b0 *)
mov L0x2001881a r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a1a; PC = 0x80027b4 *)
mov L0x20018a1a r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b070; Value = 0x0009ba8c; PC = 0x8002628 *)
mov r4 L0x2001b070;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b470; Value = 0x000ad6fe; PC = 0x800262c *)
mov r5 L0x2001b470;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b870; Value = 0xfffc2eea; PC = 0x8002630 *)
mov r6 L0x2001b870;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bc70; Value = 0x000aee4a; PC = 0x8002634 *)
mov r7 L0x2001bc70;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  90 ********************)

ghost c0042@sint32,c0298@sint32,c0554@sint32,c0810@sint32 :
and [c0042=r4, c0298=r5, c0554=r6, c0810=r7] && true;

ecut and [
eqmod (c0042*x** 42+c0298*x**298+c0554*x**554+c0810*x**810)
      (4*L0x2001b070*x** 42) [1043969, x**256 - 1],
eqmod (c0042*x** 42+c0298*x**298+c0554*x**554+c0810*x**810)
      (4*L0x2001b470*x** 42) [1043969, x**256 + 1],
eqmod (c0042*x** 42+c0298*x**298+c0554*x**554+c0810*x**810)
      (4*L0x2001b870*x** 42) [1043969, x**256 - 554923],
eqmod (c0042*x** 42+c0298*x**298+c0554*x**554+c0810*x**810)
      (4*L0x2001bc70*x** 42) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf042@sint32 : cf042 = r4 && cf042 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf298@sint32 : cf298 = r5 && cf298 = r5;

(******************** CUT  91 ********************)
ecut and [
eqmod 256*cf042 2**32*(c0042+c0551) 1043969,
eqmod 256*cf298 2**32*(c0298+c0807) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x2001881c; PC = 0x80026a0 *)
mov L0x2001881c r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018a1c; PC = 0x80026a4 *)
mov L0x20018a1c r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b074; Value = 0x0006cc27; PC = 0x80026b0 *)
mov r4 L0x2001b074;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b474; Value = 0xfff50f92; PC = 0x80026b4 *)
mov r5 L0x2001b474;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b874; Value = 0x0002b754; PC = 0x80026b8 *)
mov r6 L0x2001b874;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bc74; Value = 0xfff80b84; PC = 0x80026bc *)
mov r7 L0x2001bc74;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  92 ********************)

ghost c0043@sint32,c0299@sint32,c0555@sint32,c0811@sint32 :
and [c0043=r4, c0299=r5, c0555=r6, c0811=r7] && true;

ecut and [
eqmod (c0043*x** 43+c0299*x**299+c0555*x**555+c0811*x**811)
      (4*L0x2001b074*x** 43) [1043969, x**256 - 1],
eqmod (c0043*x** 43+c0299*x**299+c0555*x**555+c0811*x**811)
      (4*L0x2001b474*x** 43) [1043969, x**256 + 1],
eqmod (c0043*x** 43+c0299*x**299+c0555*x**555+c0811*x**811)
      (4*L0x2001b874*x** 43) [1043969, x**256 - 554923],
eqmod (c0043*x** 43+c0299*x**299+c0555*x**555+c0811*x**811)
      (4*L0x2001bc74*x** 43) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf043@sint32 : cf043 = r4 && cf043 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf299@sint32 : cf299 = r5 && cf299 = r5;

(******************** CUT  93 ********************)
ecut and [
eqmod 256*cf043 2**32*(c0043+c0552) 1043969,
eqmod 256*cf299 2**32*(c0299+c0808) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x2001881e; PC = 0x8002728 *)
mov L0x2001881e r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a1e; PC = 0x800272c *)
mov L0x20018a1e r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b078; Value = 0xfffbd0c5; PC = 0x8002738 *)
mov r4 L0x2001b078;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b478; Value = 0x00068f3e; PC = 0x800273c *)
mov r5 L0x2001b478;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b878; Value = 0xfff0fca7; PC = 0x8002740 *)
mov r6 L0x2001b878;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bc78; Value = 0x0001a076; PC = 0x8002744 *)
mov r7 L0x2001bc78;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  94 ********************)

ghost c0044@sint32,c0300@sint32,c0556@sint32,c0812@sint32 :
and [c0044=r4, c0300=r5, c0556=r6, c0812=r7] && true;

ecut and [
eqmod (c0044*x** 44+c0300*x**300+c0556*x**556+c0812*x**812)
      (4*L0x2001b078*x** 44) [1043969, x**256 - 1],
eqmod (c0044*x** 44+c0300*x**300+c0556*x**556+c0812*x**812)
      (4*L0x2001b478*x** 44) [1043969, x**256 + 1],
eqmod (c0044*x** 44+c0300*x**300+c0556*x**556+c0812*x**812)
      (4*L0x2001b878*x** 44) [1043969, x**256 - 554923],
eqmod (c0044*x** 44+c0300*x**300+c0556*x**556+c0812*x**812)
      (4*L0x2001bc78*x** 44) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf044@sint32 : cf044 = r4 && cf044 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf300@sint32 : cf300 = r5 && cf300 = r5;

(******************** CUT  95 ********************)
ecut and [
eqmod 256*cf044 2**32*(c0044+c0553) 1043969,
eqmod 256*cf300 2**32*(c0300+c0809) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018820; PC = 0x80027b0 *)
mov L0x20018820 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a20; PC = 0x80027b4 *)
mov L0x20018a20 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b07c; Value = 0x0007e2aa; PC = 0x8002628 *)
mov r4 L0x2001b07c;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b47c; Value = 0xffeed9ec; PC = 0x800262c *)
mov r5 L0x2001b47c;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b87c; Value = 0xfff047b7; PC = 0x8002630 *)
mov r6 L0x2001b87c;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bc7c; Value = 0xfff83c8b; PC = 0x8002634 *)
mov r7 L0x2001bc7c;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  96 ********************)

ghost c0045@sint32,c0301@sint32,c0557@sint32,c0813@sint32 :
and [c0045=r4, c0301=r5, c0557=r6, c0813=r7] && true;

ecut and [
eqmod (c0045*x** 45+c0301*x**301+c0557*x**557+c0813*x**813)
      (4*L0x2001b07c*x** 45) [1043969, x**256 - 1],
eqmod (c0045*x** 45+c0301*x**301+c0557*x**557+c0813*x**813)
      (4*L0x2001b47c*x** 45) [1043969, x**256 + 1],
eqmod (c0045*x** 45+c0301*x**301+c0557*x**557+c0813*x**813)
      (4*L0x2001b87c*x** 45) [1043969, x**256 - 554923],
eqmod (c0045*x** 45+c0301*x**301+c0557*x**557+c0813*x**813)
      (4*L0x2001bc7c*x** 45) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf045@sint32 : cf045 = r4 && cf045 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf301@sint32 : cf301 = r5 && cf301 = r5;

(******************** CUT  97 ********************)
ecut and [
eqmod 256*cf045 2**32*(c0045+c0554) 1043969,
eqmod 256*cf301 2**32*(c0301+c0810) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018822; PC = 0x80026a0 *)
mov L0x20018822 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018a22; PC = 0x80026a4 *)
mov L0x20018a22 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b080; Value = 0x00000185; PC = 0x80026b0 *)
mov r4 L0x2001b080;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b480; Value = 0x0000fb23; PC = 0x80026b4 *)
mov r5 L0x2001b480;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b880; Value = 0x0006b6c7; PC = 0x80026b8 *)
mov r6 L0x2001b880;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bc80; Value = 0x00043737; PC = 0x80026bc *)
mov r7 L0x2001bc80;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT  98 ********************)

ghost c0046@sint32,c0302@sint32,c0558@sint32,c0814@sint32 :
and [c0046=r4, c0302=r5, c0558=r6, c0814=r7] && true;

ecut and [
eqmod (c0046*x** 46+c0302*x**302+c0558*x**558+c0814*x**814)
      (4*L0x2001b080*x** 46) [1043969, x**256 - 1],
eqmod (c0046*x** 46+c0302*x**302+c0558*x**558+c0814*x**814)
      (4*L0x2001b480*x** 46) [1043969, x**256 + 1],
eqmod (c0046*x** 46+c0302*x**302+c0558*x**558+c0814*x**814)
      (4*L0x2001b880*x** 46) [1043969, x**256 - 554923],
eqmod (c0046*x** 46+c0302*x**302+c0558*x**558+c0814*x**814)
      (4*L0x2001bc80*x** 46) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf046@sint32 : cf046 = r4 && cf046 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf302@sint32 : cf302 = r5 && cf302 = r5;

(******************** CUT  99 ********************)
ecut and [
eqmod 256*cf046 2**32*(c0046+c0555) 1043969,
eqmod 256*cf302 2**32*(c0302+c0811) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018824; PC = 0x8002728 *)
mov L0x20018824 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a24; PC = 0x800272c *)
mov L0x20018a24 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b084; Value = 0x0011a9ac; PC = 0x8002738 *)
mov r4 L0x2001b084;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b484; Value = 0xfff5909b; PC = 0x800273c *)
mov r5 L0x2001b484;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b884; Value = 0x0004d63f; PC = 0x8002740 *)
mov r6 L0x2001b884;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bc84; Value = 0x0000442b; PC = 0x8002744 *)
mov r7 L0x2001bc84;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 100 ********************)

ghost c0047@sint32,c0303@sint32,c0559@sint32,c0815@sint32 :
and [c0047=r4, c0303=r5, c0559=r6, c0815=r7] && true;

ecut and [
eqmod (c0047*x** 47+c0303*x**303+c0559*x**559+c0815*x**815)
      (4*L0x2001b084*x** 47) [1043969, x**256 - 1],
eqmod (c0047*x** 47+c0303*x**303+c0559*x**559+c0815*x**815)
      (4*L0x2001b484*x** 47) [1043969, x**256 + 1],
eqmod (c0047*x** 47+c0303*x**303+c0559*x**559+c0815*x**815)
      (4*L0x2001b884*x** 47) [1043969, x**256 - 554923],
eqmod (c0047*x** 47+c0303*x**303+c0559*x**559+c0815*x**815)
      (4*L0x2001bc84*x** 47) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf047@sint32 : cf047 = r4 && cf047 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf303@sint32 : cf303 = r5 && cf303 = r5;

(******************** CUT 101 ********************)
ecut and [
eqmod 256*cf047 2**32*(c0047+c0556) 1043969,
eqmod 256*cf303 2**32*(c0303+c0812) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018826; PC = 0x80027b0 *)
mov L0x20018826 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a26; PC = 0x80027b4 *)
mov L0x20018a26 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b088; Value = 0x001226e2; PC = 0x8002628 *)
mov r4 L0x2001b088;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b488; Value = 0xffef1e34; PC = 0x800262c *)
mov r5 L0x2001b488;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b888; Value = 0xfff3af7b; PC = 0x8002630 *)
mov r6 L0x2001b888;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bc88; Value = 0x00039047; PC = 0x8002634 *)
mov r7 L0x2001bc88;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 102 ********************)

ghost c0048@sint32,c0304@sint32,c0560@sint32,c0816@sint32 :
and [c0048=r4, c0304=r5, c0560=r6, c0816=r7] && true;

ecut and [
eqmod (c0048*x** 48+c0304*x**304+c0560*x**560+c0816*x**816)
      (4*L0x2001b088*x** 48) [1043969, x**256 - 1],
eqmod (c0048*x** 48+c0304*x**304+c0560*x**560+c0816*x**816)
      (4*L0x2001b488*x** 48) [1043969, x**256 + 1],
eqmod (c0048*x** 48+c0304*x**304+c0560*x**560+c0816*x**816)
      (4*L0x2001b888*x** 48) [1043969, x**256 - 554923],
eqmod (c0048*x** 48+c0304*x**304+c0560*x**560+c0816*x**816)
      (4*L0x2001bc88*x** 48) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf048@sint32 : cf048 = r4 && cf048 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf304@sint32 : cf304 = r5 && cf304 = r5;

(******************** CUT 103 ********************)
ecut and [
eqmod 256*cf048 2**32*(c0048+c0557) 1043969,
eqmod 256*cf304 2**32*(c0304+c0813) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018828; PC = 0x80026a0 *)
mov L0x20018828 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018a28; PC = 0x80026a4 *)
mov L0x20018a28 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b08c; Value = 0x000a113e; PC = 0x80026b0 *)
mov r4 L0x2001b08c;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b48c; Value = 0xfff8f354; PC = 0x80026b4 *)
mov r5 L0x2001b48c;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b88c; Value = 0xffff9ff5; PC = 0x80026b8 *)
mov r6 L0x2001b88c;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bc8c; Value = 0x00127dbf; PC = 0x80026bc *)
mov r7 L0x2001bc8c;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 104 ********************)

ghost c0049@sint32,c0305@sint32,c0561@sint32,c0817@sint32 :
and [c0049=r4, c0305=r5, c0561=r6, c0817=r7] && true;

ecut and [
eqmod (c0049*x** 49+c0305*x**305+c0561*x**561+c0817*x**817)
      (4*L0x2001b08c*x** 49) [1043969, x**256 - 1],
eqmod (c0049*x** 49+c0305*x**305+c0561*x**561+c0817*x**817)
      (4*L0x2001b48c*x** 49) [1043969, x**256 + 1],
eqmod (c0049*x** 49+c0305*x**305+c0561*x**561+c0817*x**817)
      (4*L0x2001b88c*x** 49) [1043969, x**256 - 554923],
eqmod (c0049*x** 49+c0305*x**305+c0561*x**561+c0817*x**817)
      (4*L0x2001bc8c*x** 49) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf049@sint32 : cf049 = r4 && cf049 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf305@sint32 : cf305 = r5 && cf305 = r5;

(******************** CUT 105 ********************)
ecut and [
eqmod 256*cf049 2**32*(c0049+c0558) 1043969,
eqmod 256*cf305 2**32*(c0305+c0814) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x2001882a; PC = 0x8002728 *)
mov L0x2001882a r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a2a; PC = 0x800272c *)
mov L0x20018a2a r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b090; Value = 0xfffa9062; PC = 0x8002738 *)
mov r4 L0x2001b090;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b490; Value = 0xfff8d4ee; PC = 0x800273c *)
mov r5 L0x2001b490;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b890; Value = 0x00057b4c; PC = 0x8002740 *)
mov r6 L0x2001b890;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bc90; Value = 0x000d99da; PC = 0x8002744 *)
mov r7 L0x2001bc90;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 106 ********************)

ghost c0050@sint32,c0306@sint32,c0562@sint32,c0818@sint32 :
and [c0050=r4, c0306=r5, c0562=r6, c0818=r7] && true;

ecut and [
eqmod (c0050*x** 50+c0306*x**306+c0562*x**562+c0818*x**818)
      (4*L0x2001b090*x** 50) [1043969, x**256 - 1],
eqmod (c0050*x** 50+c0306*x**306+c0562*x**562+c0818*x**818)
      (4*L0x2001b490*x** 50) [1043969, x**256 + 1],
eqmod (c0050*x** 50+c0306*x**306+c0562*x**562+c0818*x**818)
      (4*L0x2001b890*x** 50) [1043969, x**256 - 554923],
eqmod (c0050*x** 50+c0306*x**306+c0562*x**562+c0818*x**818)
      (4*L0x2001bc90*x** 50) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf050@sint32 : cf050 = r4 && cf050 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf306@sint32 : cf306 = r5 && cf306 = r5;

(******************** CUT 107 ********************)
ecut and [
eqmod 256*cf050 2**32*(c0050+c0559) 1043969,
eqmod 256*cf306 2**32*(c0306+c0815) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x2001882c; PC = 0x80027b0 *)
mov L0x2001882c r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a2c; PC = 0x80027b4 *)
mov L0x20018a2c r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b094; Value = 0xfff41e9d; PC = 0x8002628 *)
mov r4 L0x2001b094;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b494; Value = 0x00093daf; PC = 0x800262c *)
mov r5 L0x2001b494;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b894; Value = 0x0007595c; PC = 0x8002630 *)
mov r6 L0x2001b894;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bc94; Value = 0x0001601f; PC = 0x8002634 *)
mov r7 L0x2001bc94;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 108 ********************)

ghost c0051@sint32,c0307@sint32,c0563@sint32,c0819@sint32 :
and [c0051=r4, c0307=r5, c0563=r6, c0819=r7] && true;

ecut and [
eqmod (c0051*x** 51+c0307*x**307+c0563*x**563+c0819*x**819)
      (4*L0x2001b094*x** 51) [1043969, x**256 - 1],
eqmod (c0051*x** 51+c0307*x**307+c0563*x**563+c0819*x**819)
      (4*L0x2001b494*x** 51) [1043969, x**256 + 1],
eqmod (c0051*x** 51+c0307*x**307+c0563*x**563+c0819*x**819)
      (4*L0x2001b894*x** 51) [1043969, x**256 - 554923],
eqmod (c0051*x** 51+c0307*x**307+c0563*x**563+c0819*x**819)
      (4*L0x2001bc94*x** 51) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf051@sint32 : cf051 = r4 && cf051 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf307@sint32 : cf307 = r5 && cf307 = r5;

(******************** CUT 109 ********************)
ecut and [
eqmod 256*cf051 2**32*(c0051+c0560) 1043969,
eqmod 256*cf307 2**32*(c0307+c0816) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x2001882e; PC = 0x80026a0 *)
mov L0x2001882e r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018a2e; PC = 0x80026a4 *)
mov L0x20018a2e r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b098; Value = 0xffeb8376; PC = 0x80026b0 *)
mov r4 L0x2001b098;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b498; Value = 0x0006dd4a; PC = 0x80026b4 *)
mov r5 L0x2001b498;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b898; Value = 0xfffd40ad; PC = 0x80026b8 *)
mov r6 L0x2001b898;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bc98; Value = 0x000932e1; PC = 0x80026bc *)
mov r7 L0x2001bc98;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 110 ********************)

ghost c0052@sint32,c0308@sint32,c0564@sint32,c0820@sint32 :
and [c0052=r4, c0308=r5, c0564=r6, c0820=r7] && true;

ecut and [
eqmod (c0052*x** 52+c0308*x**308+c0564*x**564+c0820*x**820)
      (4*L0x2001b098*x** 52) [1043969, x**256 - 1],
eqmod (c0052*x** 52+c0308*x**308+c0564*x**564+c0820*x**820)
      (4*L0x2001b498*x** 52) [1043969, x**256 + 1],
eqmod (c0052*x** 52+c0308*x**308+c0564*x**564+c0820*x**820)
      (4*L0x2001b898*x** 52) [1043969, x**256 - 554923],
eqmod (c0052*x** 52+c0308*x**308+c0564*x**564+c0820*x**820)
      (4*L0x2001bc98*x** 52) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf052@sint32 : cf052 = r4 && cf052 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf308@sint32 : cf308 = r5 && cf308 = r5;

(******************** CUT 111 ********************)
ecut and [
eqmod 256*cf052 2**32*(c0052+c0561) 1043969,
eqmod 256*cf308 2**32*(c0308+c0817) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018830; PC = 0x8002728 *)
mov L0x20018830 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a30; PC = 0x800272c *)
mov L0x20018a30 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b09c; Value = 0xfff19e73; PC = 0x8002738 *)
mov r4 L0x2001b09c;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b49c; Value = 0x000077ef; PC = 0x800273c *)
mov r5 L0x2001b49c;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b89c; Value = 0x00061dfe; PC = 0x8002740 *)
mov r6 L0x2001b89c;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bc9c; Value = 0x0008cbc6; PC = 0x8002744 *)
mov r7 L0x2001bc9c;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 112 ********************)

ghost c0053@sint32,c0309@sint32,c0565@sint32,c0821@sint32 :
and [c0053=r4, c0309=r5, c0565=r6, c0821=r7] && true;

ecut and [
eqmod (c0053*x** 53+c0309*x**309+c0565*x**565+c0821*x**821)
      (4*L0x2001b09c*x** 53) [1043969, x**256 - 1],
eqmod (c0053*x** 53+c0309*x**309+c0565*x**565+c0821*x**821)
      (4*L0x2001b49c*x** 53) [1043969, x**256 + 1],
eqmod (c0053*x** 53+c0309*x**309+c0565*x**565+c0821*x**821)
      (4*L0x2001b89c*x** 53) [1043969, x**256 - 554923],
eqmod (c0053*x** 53+c0309*x**309+c0565*x**565+c0821*x**821)
      (4*L0x2001bc9c*x** 53) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf053@sint32 : cf053 = r4 && cf053 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf309@sint32 : cf309 = r5 && cf309 = r5;

(******************** CUT 113 ********************)
ecut and [
eqmod 256*cf053 2**32*(c0053+c0562) 1043969,
eqmod 256*cf309 2**32*(c0309+c0818) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018832; PC = 0x80027b0 *)
mov L0x20018832 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a32; PC = 0x80027b4 *)
mov L0x20018a32 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b0a0; Value = 0xfff95c37; PC = 0x8002628 *)
mov r4 L0x2001b0a0;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b4a0; Value = 0xfff9a6d5; PC = 0x800262c *)
mov r5 L0x2001b4a0;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b8a0; Value = 0x0003f5a9; PC = 0x8002630 *)
mov r6 L0x2001b8a0;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bca0; Value = 0x0002ef6d; PC = 0x8002634 *)
mov r7 L0x2001bca0;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 114 ********************)

ghost c0054@sint32,c0310@sint32,c0566@sint32,c0822@sint32 :
and [c0054=r4, c0310=r5, c0566=r6, c0822=r7] && true;

ecut and [
eqmod (c0054*x** 54+c0310*x**310+c0566*x**566+c0822*x**822)
      (4*L0x2001b0a0*x** 54) [1043969, x**256 - 1],
eqmod (c0054*x** 54+c0310*x**310+c0566*x**566+c0822*x**822)
      (4*L0x2001b4a0*x** 54) [1043969, x**256 + 1],
eqmod (c0054*x** 54+c0310*x**310+c0566*x**566+c0822*x**822)
      (4*L0x2001b8a0*x** 54) [1043969, x**256 - 554923],
eqmod (c0054*x** 54+c0310*x**310+c0566*x**566+c0822*x**822)
      (4*L0x2001bca0*x** 54) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf054@sint32 : cf054 = r4 && cf054 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf310@sint32 : cf310 = r5 && cf310 = r5;

(******************** CUT 115 ********************)
ecut and [
eqmod 256*cf054 2**32*(c0054+c0563) 1043969,
eqmod 256*cf310 2**32*(c0310+c0819) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018834; PC = 0x80026a0 *)
mov L0x20018834 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018a34; PC = 0x80026a4 *)
mov L0x20018a34 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b0a4; Value = 0xfff4c7fa; PC = 0x80026b0 *)
mov r4 L0x2001b0a4;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b4a4; Value = 0xffffbf78; PC = 0x80026b4 *)
mov r5 L0x2001b4a4;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b8a4; Value = 0xfff9b0f0; PC = 0x80026b8 *)
mov r6 L0x2001b8a4;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bca4; Value = 0xffffdb7d; PC = 0x80026bc *)
mov r7 L0x2001bca4;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 116 ********************)

ghost c0055@sint32,c0311@sint32,c0567@sint32,c0823@sint32 :
and [c0055=r4, c0311=r5, c0567=r6, c0823=r7] && true;

ecut and [
eqmod (c0055*x** 55+c0311*x**311+c0567*x**567+c0823*x**823)
      (4*L0x2001b0a4*x** 55) [1043969, x**256 - 1],
eqmod (c0055*x** 55+c0311*x**311+c0567*x**567+c0823*x**823)
      (4*L0x2001b4a4*x** 55) [1043969, x**256 + 1],
eqmod (c0055*x** 55+c0311*x**311+c0567*x**567+c0823*x**823)
      (4*L0x2001b8a4*x** 55) [1043969, x**256 - 554923],
eqmod (c0055*x** 55+c0311*x**311+c0567*x**567+c0823*x**823)
      (4*L0x2001bca4*x** 55) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf055@sint32 : cf055 = r4 && cf055 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf311@sint32 : cf311 = r5 && cf311 = r5;

(******************** CUT 117 ********************)
ecut and [
eqmod 256*cf055 2**32*(c0055+c0564) 1043969,
eqmod 256*cf311 2**32*(c0311+c0820) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018836; PC = 0x8002728 *)
mov L0x20018836 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a36; PC = 0x800272c *)
mov L0x20018a36 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b0a8; Value = 0xffffa646; PC = 0x8002738 *)
mov r4 L0x2001b0a8;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b4a8; Value = 0xfffc69f8; PC = 0x800273c *)
mov r5 L0x2001b4a8;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b8a8; Value = 0x00076e36; PC = 0x8002740 *)
mov r6 L0x2001b8a8;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bca8; Value = 0xfff1aa10; PC = 0x8002744 *)
mov r7 L0x2001bca8;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 118 ********************)

ghost c0056@sint32,c0312@sint32,c0568@sint32,c0824@sint32 :
and [c0056=r4, c0312=r5, c0568=r6, c0824=r7] && true;

ecut and [
eqmod (c0056*x** 56+c0312*x**312+c0568*x**568+c0824*x**824)
      (4*L0x2001b0a8*x** 56) [1043969, x**256 - 1],
eqmod (c0056*x** 56+c0312*x**312+c0568*x**568+c0824*x**824)
      (4*L0x2001b4a8*x** 56) [1043969, x**256 + 1],
eqmod (c0056*x** 56+c0312*x**312+c0568*x**568+c0824*x**824)
      (4*L0x2001b8a8*x** 56) [1043969, x**256 - 554923],
eqmod (c0056*x** 56+c0312*x**312+c0568*x**568+c0824*x**824)
      (4*L0x2001bca8*x** 56) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf056@sint32 : cf056 = r4 && cf056 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf312@sint32 : cf312 = r5 && cf312 = r5;

(******************** CUT 119 ********************)
ecut and [
eqmod 256*cf056 2**32*(c0056+c0565) 1043969,
eqmod 256*cf312 2**32*(c0312+c0821) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018838; PC = 0x80027b0 *)
mov L0x20018838 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a38; PC = 0x80027b4 *)
mov L0x20018a38 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b0ac; Value = 0x0002ee91; PC = 0x8002628 *)
mov r4 L0x2001b0ac;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b4ac; Value = 0x00164699; PC = 0x800262c *)
mov r5 L0x2001b4ac;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b8ac; Value = 0x00007bd4; PC = 0x8002630 *)
mov r6 L0x2001b8ac;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bcac; Value = 0x000266d3; PC = 0x8002634 *)
mov r7 L0x2001bcac;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 120 ********************)

ghost c0057@sint32,c0313@sint32,c0569@sint32,c0825@sint32 :
and [c0057=r4, c0313=r5, c0569=r6, c0825=r7] && true;

ecut and [
eqmod (c0057*x** 57+c0313*x**313+c0569*x**569+c0825*x**825)
      (4*L0x2001b0ac*x** 57) [1043969, x**256 - 1],
eqmod (c0057*x** 57+c0313*x**313+c0569*x**569+c0825*x**825)
      (4*L0x2001b4ac*x** 57) [1043969, x**256 + 1],
eqmod (c0057*x** 57+c0313*x**313+c0569*x**569+c0825*x**825)
      (4*L0x2001b8ac*x** 57) [1043969, x**256 - 554923],
eqmod (c0057*x** 57+c0313*x**313+c0569*x**569+c0825*x**825)
      (4*L0x2001bcac*x** 57) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf057@sint32 : cf057 = r4 && cf057 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf313@sint32 : cf313 = r5 && cf313 = r5;

(******************** CUT 121 ********************)
ecut and [
eqmod 256*cf057 2**32*(c0057+c0566) 1043969,
eqmod 256*cf313 2**32*(c0313+c0822) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x2001883a; PC = 0x80026a0 *)
mov L0x2001883a r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018a3a; PC = 0x80026a4 *)
mov L0x20018a3a r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b0b0; Value = 0x0014b7e6; PC = 0x80026b0 *)
mov r4 L0x2001b0b0;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b4b0; Value = 0x000639ea; PC = 0x80026b4 *)
mov r5 L0x2001b4b0;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b8b0; Value = 0x0006e47e; PC = 0x80026b8 *)
mov r6 L0x2001b8b0;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bcb0; Value = 0xfff6b28b; PC = 0x80026bc *)
mov r7 L0x2001bcb0;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 122 ********************)

ghost c0058@sint32,c0314@sint32,c0570@sint32,c0826@sint32 :
and [c0058=r4, c0314=r5, c0570=r6, c0826=r7] && true;

ecut and [
eqmod (c0058*x** 58+c0314*x**314+c0570*x**570+c0826*x**826)
      (4*L0x2001b0b0*x** 58) [1043969, x**256 - 1],
eqmod (c0058*x** 58+c0314*x**314+c0570*x**570+c0826*x**826)
      (4*L0x2001b4b0*x** 58) [1043969, x**256 + 1],
eqmod (c0058*x** 58+c0314*x**314+c0570*x**570+c0826*x**826)
      (4*L0x2001b8b0*x** 58) [1043969, x**256 - 554923],
eqmod (c0058*x** 58+c0314*x**314+c0570*x**570+c0826*x**826)
      (4*L0x2001bcb0*x** 58) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf058@sint32 : cf058 = r4 && cf058 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf314@sint32 : cf314 = r5 && cf314 = r5;

(******************** CUT 123 ********************)
ecut and [
eqmod 256*cf058 2**32*(c0058+c0567) 1043969,
eqmod 256*cf314 2**32*(c0314+c0823) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x2001883c; PC = 0x8002728 *)
mov L0x2001883c r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a3c; PC = 0x800272c *)
mov L0x20018a3c r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b0b4; Value = 0xfffd0a11; PC = 0x8002738 *)
mov r4 L0x2001b0b4;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b4b4; Value = 0x00035ff5; PC = 0x800273c *)
mov r5 L0x2001b4b4;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b8b4; Value = 0x0004864a; PC = 0x8002740 *)
mov r6 L0x2001b8b4;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bcb4; Value = 0x00067626; PC = 0x8002744 *)
mov r7 L0x2001bcb4;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 124 ********************)

ghost c0059@sint32,c0315@sint32,c0571@sint32,c0827@sint32 :
and [c0059=r4, c0315=r5, c0571=r6, c0827=r7] && true;

ecut and [
eqmod (c0059*x** 59+c0315*x**315+c0571*x**571+c0827*x**827)
      (4*L0x2001b0b4*x** 59) [1043969, x**256 - 1],
eqmod (c0059*x** 59+c0315*x**315+c0571*x**571+c0827*x**827)
      (4*L0x2001b4b4*x** 59) [1043969, x**256 + 1],
eqmod (c0059*x** 59+c0315*x**315+c0571*x**571+c0827*x**827)
      (4*L0x2001b8b4*x** 59) [1043969, x**256 - 554923],
eqmod (c0059*x** 59+c0315*x**315+c0571*x**571+c0827*x**827)
      (4*L0x2001bcb4*x** 59) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf059@sint32 : cf059 = r4 && cf059 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf315@sint32 : cf315 = r5 && cf315 = r5;

(******************** CUT 125 ********************)
ecut and [
eqmod 256*cf059 2**32*(c0059+c0568) 1043969,
eqmod 256*cf315 2**32*(c0315+c0824) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x2001883e; PC = 0x80027b0 *)
mov L0x2001883e r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a3e; PC = 0x80027b4 *)
mov L0x20018a3e r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b0b8; Value = 0x0002bfc0; PC = 0x8002628 *)
mov r4 L0x2001b0b8;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b4b8; Value = 0xfffcfe31; PC = 0x800262c *)
mov r5 L0x2001b4b8;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b8b8; Value = 0xfff5ecaf; PC = 0x8002630 *)
mov r6 L0x2001b8b8;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bcb8; Value = 0xfff708c2; PC = 0x8002634 *)
mov r7 L0x2001bcb8;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 126 ********************)

ghost c0060@sint32,c0316@sint32,c0572@sint32,c0828@sint32 :
and [c0060=r4, c0316=r5, c0572=r6, c0828=r7] && true;

ecut and [
eqmod (c0060*x** 60+c0316*x**316+c0572*x**572+c0828*x**828)
      (4*L0x2001b0b8*x** 60) [1043969, x**256 - 1],
eqmod (c0060*x** 60+c0316*x**316+c0572*x**572+c0828*x**828)
      (4*L0x2001b4b8*x** 60) [1043969, x**256 + 1],
eqmod (c0060*x** 60+c0316*x**316+c0572*x**572+c0828*x**828)
      (4*L0x2001b8b8*x** 60) [1043969, x**256 - 554923],
eqmod (c0060*x** 60+c0316*x**316+c0572*x**572+c0828*x**828)
      (4*L0x2001bcb8*x** 60) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf060@sint32 : cf060 = r4 && cf060 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf316@sint32 : cf316 = r5 && cf316 = r5;

(******************** CUT 127 ********************)
ecut and [
eqmod 256*cf060 2**32*(c0060+c0569) 1043969,
eqmod 256*cf316 2**32*(c0316+c0825) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018840; PC = 0x80026a0 *)
mov L0x20018840 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018a40; PC = 0x80026a4 *)
mov L0x20018a40 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b0bc; Value = 0xfff8dd4f; PC = 0x80026b0 *)
mov r4 L0x2001b0bc;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b4bc; Value = 0x000b1f5d; PC = 0x80026b4 *)
mov r5 L0x2001b4bc;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b8bc; Value = 0x000138c8; PC = 0x80026b8 *)
mov r6 L0x2001b8bc;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bcbc; Value = 0x0004a6a5; PC = 0x80026bc *)
mov r7 L0x2001bcbc;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 128 ********************)

ghost c0061@sint32,c0317@sint32,c0573@sint32,c0829@sint32 :
and [c0061=r4, c0317=r5, c0573=r6, c0829=r7] && true;

ecut and [
eqmod (c0061*x** 61+c0317*x**317+c0573*x**573+c0829*x**829)
      (4*L0x2001b0bc*x** 61) [1043969, x**256 - 1],
eqmod (c0061*x** 61+c0317*x**317+c0573*x**573+c0829*x**829)
      (4*L0x2001b4bc*x** 61) [1043969, x**256 + 1],
eqmod (c0061*x** 61+c0317*x**317+c0573*x**573+c0829*x**829)
      (4*L0x2001b8bc*x** 61) [1043969, x**256 - 554923],
eqmod (c0061*x** 61+c0317*x**317+c0573*x**573+c0829*x**829)
      (4*L0x2001bcbc*x** 61) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf061@sint32 : cf061 = r4 && cf061 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf317@sint32 : cf317 = r5 && cf317 = r5;

(******************** CUT 129 ********************)
ecut and [
eqmod 256*cf061 2**32*(c0061+c0570) 1043969,
eqmod 256*cf317 2**32*(c0317+c0826) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018842; PC = 0x8002728 *)
mov L0x20018842 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a42; PC = 0x800272c *)
mov L0x20018a42 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b0c0; Value = 0x00027106; PC = 0x8002738 *)
mov r4 L0x2001b0c0;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b4c0; Value = 0xfffef8e4; PC = 0x800273c *)
mov r5 L0x2001b4c0;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b8c0; Value = 0x0004bb37; PC = 0x8002740 *)
mov r6 L0x2001b8c0;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bcc0; Value = 0x00030077; PC = 0x8002744 *)
mov r7 L0x2001bcc0;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 130 ********************)

ghost c0062@sint32,c0318@sint32,c0574@sint32,c0830@sint32 :
and [c0062=r4, c0318=r5, c0574=r6, c0830=r7] && true;

ecut and [
eqmod (c0062*x** 62+c0318*x**318+c0574*x**574+c0830*x**830)
      (4*L0x2001b0c0*x** 62) [1043969, x**256 - 1],
eqmod (c0062*x** 62+c0318*x**318+c0574*x**574+c0830*x**830)
      (4*L0x2001b4c0*x** 62) [1043969, x**256 + 1],
eqmod (c0062*x** 62+c0318*x**318+c0574*x**574+c0830*x**830)
      (4*L0x2001b8c0*x** 62) [1043969, x**256 - 554923],
eqmod (c0062*x** 62+c0318*x**318+c0574*x**574+c0830*x**830)
      (4*L0x2001bcc0*x** 62) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf062@sint32 : cf062 = r4 && cf062 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf318@sint32 : cf318 = r5 && cf318 = r5;

(******************** CUT 131 ********************)
ecut and [
eqmod 256*cf062 2**32*(c0062+c0571) 1043969,
eqmod 256*cf318 2**32*(c0318+c0827) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018844; PC = 0x80027b0 *)
mov L0x20018844 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a44; PC = 0x80027b4 *)
mov L0x20018a44 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b0c4; Value = 0xfff71a10; PC = 0x8002628 *)
mov r4 L0x2001b0c4;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b4c4; Value = 0xfff804a9; PC = 0x800262c *)
mov r5 L0x2001b4c4;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b8c4; Value = 0x000db2c0; PC = 0x8002630 *)
mov r6 L0x2001b8c4;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bcc4; Value = 0xfff3b238; PC = 0x8002634 *)
mov r7 L0x2001bcc4;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 132 ********************)

ghost c0063@sint32,c0319@sint32,c0575@sint32,c0831@sint32 :
and [c0063=r4, c0319=r5, c0575=r6, c0831=r7] && true;

ecut and [
eqmod (c0063*x** 63+c0319*x**319+c0575*x**575+c0831*x**831)
      (4*L0x2001b0c4*x** 63) [1043969, x**256 - 1],
eqmod (c0063*x** 63+c0319*x**319+c0575*x**575+c0831*x**831)
      (4*L0x2001b4c4*x** 63) [1043969, x**256 + 1],
eqmod (c0063*x** 63+c0319*x**319+c0575*x**575+c0831*x**831)
      (4*L0x2001b8c4*x** 63) [1043969, x**256 - 554923],
eqmod (c0063*x** 63+c0319*x**319+c0575*x**575+c0831*x**831)
      (4*L0x2001bcc4*x** 63) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf063@sint32 : cf063 = r4 && cf063 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf319@sint32 : cf319 = r5 && cf319 = r5;

(******************** CUT 133 ********************)
ecut and [
eqmod 256*cf063 2**32*(c0063+c0572) 1043969,
eqmod 256*cf319 2**32*(c0319+c0828) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018846; PC = 0x80026a0 *)
mov L0x20018846 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018a46; PC = 0x80026a4 *)
mov L0x20018a46 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b0c8; Value = 0xfff49d00; PC = 0x80026b0 *)
mov r4 L0x2001b0c8;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b4c8; Value = 0x0008de1a; PC = 0x80026b4 *)
mov r5 L0x2001b4c8;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b8c8; Value = 0x00099ee3; PC = 0x80026b8 *)
mov r6 L0x2001b8c8;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bcc8; Value = 0xfffdc397; PC = 0x80026bc *)
mov r7 L0x2001bcc8;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 134 ********************)

ghost c0064@sint32,c0320@sint32,c0576@sint32,c0832@sint32 :
and [c0064=r4, c0320=r5, c0576=r6, c0832=r7] && true;

ecut and [
eqmod (c0064*x** 64+c0320*x**320+c0576*x**576+c0832*x**832)
      (4*L0x2001b0c8*x** 64) [1043969, x**256 - 1],
eqmod (c0064*x** 64+c0320*x**320+c0576*x**576+c0832*x**832)
      (4*L0x2001b4c8*x** 64) [1043969, x**256 + 1],
eqmod (c0064*x** 64+c0320*x**320+c0576*x**576+c0832*x**832)
      (4*L0x2001b8c8*x** 64) [1043969, x**256 - 554923],
eqmod (c0064*x** 64+c0320*x**320+c0576*x**576+c0832*x**832)
      (4*L0x2001bcc8*x** 64) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf064@sint32 : cf064 = r4 && cf064 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf320@sint32 : cf320 = r5 && cf320 = r5;

(******************** CUT 135 ********************)
ecut and [
eqmod 256*cf064 2**32*(c0064+c0573) 1043969,
eqmod 256*cf320 2**32*(c0320+c0829) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018848; PC = 0x8002728 *)
mov L0x20018848 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a48; PC = 0x800272c *)
mov L0x20018a48 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b0cc; Value = 0xffe48cba; PC = 0x8002738 *)
mov r4 L0x2001b0cc;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b4cc; Value = 0xfffbb8c1; PC = 0x800273c *)
mov r5 L0x2001b4cc;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b8cc; Value = 0xfffc03f0; PC = 0x8002740 *)
mov r6 L0x2001b8cc;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bccc; Value = 0xfff91244; PC = 0x8002744 *)
mov r7 L0x2001bccc;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 136 ********************)

ghost c0065@sint32,c0321@sint32,c0577@sint32,c0833@sint32 :
and [c0065=r4, c0321=r5, c0577=r6, c0833=r7] && true;

ecut and [
eqmod (c0065*x** 65+c0321*x**321+c0577*x**577+c0833*x**833)
      (4*L0x2001b0cc*x** 65) [1043969, x**256 - 1],
eqmod (c0065*x** 65+c0321*x**321+c0577*x**577+c0833*x**833)
      (4*L0x2001b4cc*x** 65) [1043969, x**256 + 1],
eqmod (c0065*x** 65+c0321*x**321+c0577*x**577+c0833*x**833)
      (4*L0x2001b8cc*x** 65) [1043969, x**256 - 554923],
eqmod (c0065*x** 65+c0321*x**321+c0577*x**577+c0833*x**833)
      (4*L0x2001bccc*x** 65) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf065@sint32 : cf065 = r4 && cf065 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf321@sint32 : cf321 = r5 && cf321 = r5;

(******************** CUT 137 ********************)
ecut and [
eqmod 256*cf065 2**32*(c0065+c0574) 1043969,
eqmod 256*cf321 2**32*(c0321+c0830) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x2001884a; PC = 0x80027b0 *)
mov L0x2001884a r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a4a; PC = 0x80027b4 *)
mov L0x20018a4a r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b0d0; Value = 0xfff5ad7a; PC = 0x8002628 *)
mov r4 L0x2001b0d0;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b4d0; Value = 0x00006459; PC = 0x800262c *)
mov r5 L0x2001b4d0;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b8d0; Value = 0x000de0b3; PC = 0x8002630 *)
mov r6 L0x2001b8d0;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bcd0; Value = 0xffff6667; PC = 0x8002634 *)
mov r7 L0x2001bcd0;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 138 ********************)

ghost c0066@sint32,c0322@sint32,c0578@sint32,c0834@sint32 :
and [c0066=r4, c0322=r5, c0578=r6, c0834=r7] && true;

ecut and [
eqmod (c0066*x** 66+c0322*x**322+c0578*x**578+c0834*x**834)
      (4*L0x2001b0d0*x** 66) [1043969, x**256 - 1],
eqmod (c0066*x** 66+c0322*x**322+c0578*x**578+c0834*x**834)
      (4*L0x2001b4d0*x** 66) [1043969, x**256 + 1],
eqmod (c0066*x** 66+c0322*x**322+c0578*x**578+c0834*x**834)
      (4*L0x2001b8d0*x** 66) [1043969, x**256 - 554923],
eqmod (c0066*x** 66+c0322*x**322+c0578*x**578+c0834*x**834)
      (4*L0x2001bcd0*x** 66) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf066@sint32 : cf066 = r4 && cf066 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf322@sint32 : cf322 = r5 && cf322 = r5;

(******************** CUT 139 ********************)
ecut and [
eqmod 256*cf066 2**32*(c0066+c0575) 1043969,
eqmod 256*cf322 2**32*(c0322+c0831) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x2001884c; PC = 0x80026a0 *)
mov L0x2001884c r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018a4c; PC = 0x80026a4 *)
mov L0x20018a4c r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b0d4; Value = 0xffe46669; PC = 0x80026b0 *)
mov r4 L0x2001b0d4;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b4d4; Value = 0xfff18778; PC = 0x80026b4 *)
mov r5 L0x2001b4d4;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b8d4; Value = 0xfff703db; PC = 0x80026b8 *)
mov r6 L0x2001b8d4;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bcd4; Value = 0xfffe5d21; PC = 0x80026bc *)
mov r7 L0x2001bcd4;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 140 ********************)

ghost c0067@sint32,c0323@sint32,c0579@sint32,c0835@sint32 :
and [c0067=r4, c0323=r5, c0579=r6, c0835=r7] && true;

ecut and [
eqmod (c0067*x** 67+c0323*x**323+c0579*x**579+c0835*x**835)
      (4*L0x2001b0d4*x** 67) [1043969, x**256 - 1],
eqmod (c0067*x** 67+c0323*x**323+c0579*x**579+c0835*x**835)
      (4*L0x2001b4d4*x** 67) [1043969, x**256 + 1],
eqmod (c0067*x** 67+c0323*x**323+c0579*x**579+c0835*x**835)
      (4*L0x2001b8d4*x** 67) [1043969, x**256 - 554923],
eqmod (c0067*x** 67+c0323*x**323+c0579*x**579+c0835*x**835)
      (4*L0x2001bcd4*x** 67) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf067@sint32 : cf067 = r4 && cf067 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf323@sint32 : cf323 = r5 && cf323 = r5;

(******************** CUT 141 ********************)
ecut and [
eqmod 256*cf067 2**32*(c0067+c0576) 1043969,
eqmod 256*cf323 2**32*(c0323+c0832) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x2001884e; PC = 0x8002728 *)
mov L0x2001884e r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a4e; PC = 0x800272c *)
mov L0x20018a4e r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b0d8; Value = 0xffe9720e; PC = 0x8002738 *)
mov r4 L0x2001b0d8;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b4d8; Value = 0x000012cf; PC = 0x800273c *)
mov r5 L0x2001b4d8;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b8d8; Value = 0xfffa9c57; PC = 0x8002740 *)
mov r6 L0x2001b8d8;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bcd8; Value = 0xfffd47d2; PC = 0x8002744 *)
mov r7 L0x2001bcd8;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 142 ********************)

ghost c0068@sint32,c0324@sint32,c0580@sint32,c0836@sint32 :
and [c0068=r4, c0324=r5, c0580=r6, c0836=r7] && true;

ecut and [
eqmod (c0068*x** 68+c0324*x**324+c0580*x**580+c0836*x**836)
      (4*L0x2001b0d8*x** 68) [1043969, x**256 - 1],
eqmod (c0068*x** 68+c0324*x**324+c0580*x**580+c0836*x**836)
      (4*L0x2001b4d8*x** 68) [1043969, x**256 + 1],
eqmod (c0068*x** 68+c0324*x**324+c0580*x**580+c0836*x**836)
      (4*L0x2001b8d8*x** 68) [1043969, x**256 - 554923],
eqmod (c0068*x** 68+c0324*x**324+c0580*x**580+c0836*x**836)
      (4*L0x2001bcd8*x** 68) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf068@sint32 : cf068 = r4 && cf068 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf324@sint32 : cf324 = r5 && cf324 = r5;

(******************** CUT 143 ********************)
ecut and [
eqmod 256*cf068 2**32*(c0068+c0577) 1043969,
eqmod 256*cf324 2**32*(c0324+c0833) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018850; PC = 0x80027b0 *)
mov L0x20018850 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a50; PC = 0x80027b4 *)
mov L0x20018a50 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b0dc; Value = 0xfff8ef6e; PC = 0x8002628 *)
mov r4 L0x2001b0dc;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b4dc; Value = 0xfffb39b1; PC = 0x800262c *)
mov r5 L0x2001b4dc;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b8dc; Value = 0x0002c4e5; PC = 0x8002630 *)
mov r6 L0x2001b8dc;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bcdc; Value = 0xfff54436; PC = 0x8002634 *)
mov r7 L0x2001bcdc;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 144 ********************)

ghost c0069@sint32,c0325@sint32,c0581@sint32,c0837@sint32 :
and [c0069=r4, c0325=r5, c0581=r6, c0837=r7] && true;

ecut and [
eqmod (c0069*x** 69+c0325*x**325+c0581*x**581+c0837*x**837)
      (4*L0x2001b0dc*x** 69) [1043969, x**256 - 1],
eqmod (c0069*x** 69+c0325*x**325+c0581*x**581+c0837*x**837)
      (4*L0x2001b4dc*x** 69) [1043969, x**256 + 1],
eqmod (c0069*x** 69+c0325*x**325+c0581*x**581+c0837*x**837)
      (4*L0x2001b8dc*x** 69) [1043969, x**256 - 554923],
eqmod (c0069*x** 69+c0325*x**325+c0581*x**581+c0837*x**837)
      (4*L0x2001bcdc*x** 69) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf069@sint32 : cf069 = r4 && cf069 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf325@sint32 : cf325 = r5 && cf325 = r5;

(******************** CUT 145 ********************)
ecut and [
eqmod 256*cf069 2**32*(c0069+c0578) 1043969,
eqmod 256*cf325 2**32*(c0325+c0834) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018852; PC = 0x80026a0 *)
mov L0x20018852 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018a52; PC = 0x80026a4 *)
mov L0x20018a52 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b0e0; Value = 0xfff2129b; PC = 0x80026b0 *)
mov r4 L0x2001b0e0;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b4e0; Value = 0x0008b8e1; PC = 0x80026b4 *)
mov r5 L0x2001b4e0;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b8e0; Value = 0xfffc2130; PC = 0x80026b8 *)
mov r6 L0x2001b8e0;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bce0; Value = 0x00074dde; PC = 0x80026bc *)
mov r7 L0x2001bce0;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 146 ********************)

ghost c0070@sint32,c0326@sint32,c0582@sint32,c0838@sint32 :
and [c0070=r4, c0326=r5, c0582=r6, c0838=r7] && true;

ecut and [
eqmod (c0070*x** 70+c0326*x**326+c0582*x**582+c0838*x**838)
      (4*L0x2001b0e0*x** 70) [1043969, x**256 - 1],
eqmod (c0070*x** 70+c0326*x**326+c0582*x**582+c0838*x**838)
      (4*L0x2001b4e0*x** 70) [1043969, x**256 + 1],
eqmod (c0070*x** 70+c0326*x**326+c0582*x**582+c0838*x**838)
      (4*L0x2001b8e0*x** 70) [1043969, x**256 - 554923],
eqmod (c0070*x** 70+c0326*x**326+c0582*x**582+c0838*x**838)
      (4*L0x2001bce0*x** 70) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf070@sint32 : cf070 = r4 && cf070 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf326@sint32 : cf326 = r5 && cf326 = r5;

(******************** CUT 147 ********************)
ecut and [
eqmod 256*cf070 2**32*(c0070+c0579) 1043969,
eqmod 256*cf326 2**32*(c0326+c0835) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018854; PC = 0x8002728 *)
mov L0x20018854 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a54; PC = 0x800272c *)
mov L0x20018a54 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b0e4; Value = 0xfffb1324; PC = 0x8002738 *)
mov r4 L0x2001b0e4;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b4e4; Value = 0xfffef8c8; PC = 0x800273c *)
mov r5 L0x2001b4e4;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b8e4; Value = 0xfffbcde9; PC = 0x8002740 *)
mov r6 L0x2001b8e4;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bce4; Value = 0x00033db9; PC = 0x8002744 *)
mov r7 L0x2001bce4;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 148 ********************)

ghost c0071@sint32,c0327@sint32,c0583@sint32,c0839@sint32 :
and [c0071=r4, c0327=r5, c0583=r6, c0839=r7] && true;

ecut and [
eqmod (c0071*x** 71+c0327*x**327+c0583*x**583+c0839*x**839)
      (4*L0x2001b0e4*x** 71) [1043969, x**256 - 1],
eqmod (c0071*x** 71+c0327*x**327+c0583*x**583+c0839*x**839)
      (4*L0x2001b4e4*x** 71) [1043969, x**256 + 1],
eqmod (c0071*x** 71+c0327*x**327+c0583*x**583+c0839*x**839)
      (4*L0x2001b8e4*x** 71) [1043969, x**256 - 554923],
eqmod (c0071*x** 71+c0327*x**327+c0583*x**583+c0839*x**839)
      (4*L0x2001bce4*x** 71) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf071@sint32 : cf071 = r4 && cf071 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf327@sint32 : cf327 = r5 && cf327 = r5;

(******************** CUT 149 ********************)
ecut and [
eqmod 256*cf071 2**32*(c0071+c0580) 1043969,
eqmod 256*cf327 2**32*(c0327+c0836) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018856; PC = 0x80027b0 *)
mov L0x20018856 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a56; PC = 0x80027b4 *)
mov L0x20018a56 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b0e8; Value = 0x00079cd9; PC = 0x8002628 *)
mov r4 L0x2001b0e8;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b4e8; Value = 0xfffbd7e2; PC = 0x800262c *)
mov r5 L0x2001b4e8;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b8e8; Value = 0x000e31bf; PC = 0x8002630 *)
mov r6 L0x2001b8e8;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bce8; Value = 0x00027ff6; PC = 0x8002634 *)
mov r7 L0x2001bce8;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 150 ********************)

ghost c0072@sint32,c0328@sint32,c0584@sint32,c0840@sint32 :
and [c0072=r4, c0328=r5, c0584=r6, c0840=r7] && true;

ecut and [
eqmod (c0072*x** 72+c0328*x**328+c0584*x**584+c0840*x**840)
      (4*L0x2001b0e8*x** 72) [1043969, x**256 - 1],
eqmod (c0072*x** 72+c0328*x**328+c0584*x**584+c0840*x**840)
      (4*L0x2001b4e8*x** 72) [1043969, x**256 + 1],
eqmod (c0072*x** 72+c0328*x**328+c0584*x**584+c0840*x**840)
      (4*L0x2001b8e8*x** 72) [1043969, x**256 - 554923],
eqmod (c0072*x** 72+c0328*x**328+c0584*x**584+c0840*x**840)
      (4*L0x2001bce8*x** 72) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf072@sint32 : cf072 = r4 && cf072 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf328@sint32 : cf328 = r5 && cf328 = r5;

(******************** CUT 151 ********************)
ecut and [
eqmod 256*cf072 2**32*(c0072+c0581) 1043969,
eqmod 256*cf328 2**32*(c0328+c0837) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018858; PC = 0x80026a0 *)
mov L0x20018858 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018a58; PC = 0x80026a4 *)
mov L0x20018a58 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b0ec; Value = 0x0003d009; PC = 0x80026b0 *)
mov r4 L0x2001b0ec;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b4ec; Value = 0xfff57d48; PC = 0x80026b4 *)
mov r5 L0x2001b4ec;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b8ec; Value = 0x000cf391; PC = 0x80026b8 *)
mov r6 L0x2001b8ec;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bcec; Value = 0xfffbd10e; PC = 0x80026bc *)
mov r7 L0x2001bcec;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 152 ********************)

ghost c0073@sint32,c0329@sint32,c0585@sint32,c0841@sint32 :
and [c0073=r4, c0329=r5, c0585=r6, c0841=r7] && true;

ecut and [
eqmod (c0073*x** 73+c0329*x**329+c0585*x**585+c0841*x**841)
      (4*L0x2001b0ec*x** 73) [1043969, x**256 - 1],
eqmod (c0073*x** 73+c0329*x**329+c0585*x**585+c0841*x**841)
      (4*L0x2001b4ec*x** 73) [1043969, x**256 + 1],
eqmod (c0073*x** 73+c0329*x**329+c0585*x**585+c0841*x**841)
      (4*L0x2001b8ec*x** 73) [1043969, x**256 - 554923],
eqmod (c0073*x** 73+c0329*x**329+c0585*x**585+c0841*x**841)
      (4*L0x2001bcec*x** 73) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf073@sint32 : cf073 = r4 && cf073 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf329@sint32 : cf329 = r5 && cf329 = r5;

(******************** CUT 153 ********************)
ecut and [
eqmod 256*cf073 2**32*(c0073+c0582) 1043969,
eqmod 256*cf329 2**32*(c0329+c0838) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x2001885a; PC = 0x8002728 *)
mov L0x2001885a r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a5a; PC = 0x800272c *)
mov L0x20018a5a r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b0f0; Value = 0xfffd9ccb; PC = 0x8002738 *)
mov r4 L0x2001b0f0;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b4f0; Value = 0x000af305; PC = 0x800273c *)
mov r5 L0x2001b4f0;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b8f0; Value = 0xfffe13de; PC = 0x8002740 *)
mov r6 L0x2001b8f0;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bcf0; Value = 0x000b733e; PC = 0x8002744 *)
mov r7 L0x2001bcf0;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 154 ********************)

ghost c0074@sint32,c0330@sint32,c0586@sint32,c0842@sint32 :
and [c0074=r4, c0330=r5, c0586=r6, c0842=r7] && true;

ecut and [
eqmod (c0074*x** 74+c0330*x**330+c0586*x**586+c0842*x**842)
      (4*L0x2001b0f0*x** 74) [1043969, x**256 - 1],
eqmod (c0074*x** 74+c0330*x**330+c0586*x**586+c0842*x**842)
      (4*L0x2001b4f0*x** 74) [1043969, x**256 + 1],
eqmod (c0074*x** 74+c0330*x**330+c0586*x**586+c0842*x**842)
      (4*L0x2001b8f0*x** 74) [1043969, x**256 - 554923],
eqmod (c0074*x** 74+c0330*x**330+c0586*x**586+c0842*x**842)
      (4*L0x2001bcf0*x** 74) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf074@sint32 : cf074 = r4 && cf074 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf330@sint32 : cf330 = r5 && cf330 = r5;

(******************** CUT 155 ********************)
ecut and [
eqmod 256*cf074 2**32*(c0074+c0583) 1043969,
eqmod 256*cf330 2**32*(c0330+c0839) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x2001885c; PC = 0x80027b0 *)
mov L0x2001885c r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a5c; PC = 0x80027b4 *)
mov L0x20018a5c r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b0f4; Value = 0xffdee71f; PC = 0x8002628 *)
mov r4 L0x2001b0f4;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b4f4; Value = 0x0002d421; PC = 0x800262c *)
mov r5 L0x2001b4f4;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b8f4; Value = 0x0003b69b; PC = 0x8002630 *)
mov r6 L0x2001b8f4;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bcf4; Value = 0x00042afe; PC = 0x8002634 *)
mov r7 L0x2001bcf4;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 156 ********************)

ghost c0075@sint32,c0331@sint32,c0587@sint32,c0843@sint32 :
and [c0075=r4, c0331=r5, c0587=r6, c0843=r7] && true;

ecut and [
eqmod (c0075*x** 75+c0331*x**331+c0587*x**587+c0843*x**843)
      (4*L0x2001b0f4*x** 75) [1043969, x**256 - 1],
eqmod (c0075*x** 75+c0331*x**331+c0587*x**587+c0843*x**843)
      (4*L0x2001b4f4*x** 75) [1043969, x**256 + 1],
eqmod (c0075*x** 75+c0331*x**331+c0587*x**587+c0843*x**843)
      (4*L0x2001b8f4*x** 75) [1043969, x**256 - 554923],
eqmod (c0075*x** 75+c0331*x**331+c0587*x**587+c0843*x**843)
      (4*L0x2001bcf4*x** 75) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf075@sint32 : cf075 = r4 && cf075 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf331@sint32 : cf331 = r5 && cf331 = r5;

(******************** CUT 157 ********************)
ecut and [
eqmod 256*cf075 2**32*(c0075+c0584) 1043969,
eqmod 256*cf331 2**32*(c0331+c0840) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x2001885e; PC = 0x80026a0 *)
mov L0x2001885e r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018a5e; PC = 0x80026a4 *)
mov L0x20018a5e r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b0f8; Value = 0xfff85ef4; PC = 0x80026b0 *)
mov r4 L0x2001b0f8;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b4f8; Value = 0x00068d74; PC = 0x80026b4 *)
mov r5 L0x2001b4f8;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b8f8; Value = 0xfff337fd; PC = 0x80026b8 *)
mov r6 L0x2001b8f8;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bcf8; Value = 0xfffd22d5; PC = 0x80026bc *)
mov r7 L0x2001bcf8;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 158 ********************)

ghost c0076@sint32,c0332@sint32,c0588@sint32,c0844@sint32 :
and [c0076=r4, c0332=r5, c0588=r6, c0844=r7] && true;

ecut and [
eqmod (c0076*x** 76+c0332*x**332+c0588*x**588+c0844*x**844)
      (4*L0x2001b0f8*x** 76) [1043969, x**256 - 1],
eqmod (c0076*x** 76+c0332*x**332+c0588*x**588+c0844*x**844)
      (4*L0x2001b4f8*x** 76) [1043969, x**256 + 1],
eqmod (c0076*x** 76+c0332*x**332+c0588*x**588+c0844*x**844)
      (4*L0x2001b8f8*x** 76) [1043969, x**256 - 554923],
eqmod (c0076*x** 76+c0332*x**332+c0588*x**588+c0844*x**844)
      (4*L0x2001bcf8*x** 76) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf076@sint32 : cf076 = r4 && cf076 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf332@sint32 : cf332 = r5 && cf332 = r5;

(******************** CUT 159 ********************)
ecut and [
eqmod 256*cf076 2**32*(c0076+c0585) 1043969,
eqmod 256*cf332 2**32*(c0332+c0841) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018860; PC = 0x8002728 *)
mov L0x20018860 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a60; PC = 0x800272c *)
mov L0x20018a60 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b0fc; Value = 0x000814af; PC = 0x8002738 *)
mov r4 L0x2001b0fc;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b4fc; Value = 0x0006560e; PC = 0x800273c *)
mov r5 L0x2001b4fc;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b8fc; Value = 0xfffbe891; PC = 0x8002740 *)
mov r6 L0x2001b8fc;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bcfc; Value = 0xfffc9233; PC = 0x8002744 *)
mov r7 L0x2001bcfc;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 160 ********************)

ghost c0077@sint32,c0333@sint32,c0589@sint32,c0845@sint32 :
and [c0077=r4, c0333=r5, c0589=r6, c0845=r7] && true;

ecut and [
eqmod (c0077*x** 77+c0333*x**333+c0589*x**589+c0845*x**845)
      (4*L0x2001b0fc*x** 77) [1043969, x**256 - 1],
eqmod (c0077*x** 77+c0333*x**333+c0589*x**589+c0845*x**845)
      (4*L0x2001b4fc*x** 77) [1043969, x**256 + 1],
eqmod (c0077*x** 77+c0333*x**333+c0589*x**589+c0845*x**845)
      (4*L0x2001b8fc*x** 77) [1043969, x**256 - 554923],
eqmod (c0077*x** 77+c0333*x**333+c0589*x**589+c0845*x**845)
      (4*L0x2001bcfc*x** 77) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf077@sint32 : cf077 = r4 && cf077 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf333@sint32 : cf333 = r5 && cf333 = r5;

(******************** CUT 161 ********************)
ecut and [
eqmod 256*cf077 2**32*(c0077+c0586) 1043969,
eqmod 256*cf333 2**32*(c0333+c0842) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018862; PC = 0x80027b0 *)
mov L0x20018862 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a62; PC = 0x80027b4 *)
mov L0x20018a62 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b100; Value = 0x000b5d06; PC = 0x8002628 *)
mov r4 L0x2001b100;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b500; Value = 0x0004395d; PC = 0x800262c *)
mov r5 L0x2001b500;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b900; Value = 0xffff0d6b; PC = 0x8002630 *)
mov r6 L0x2001b900;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bd00; Value = 0xfffa59ec; PC = 0x8002634 *)
mov r7 L0x2001bd00;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 162 ********************)

ghost c0078@sint32,c0334@sint32,c0590@sint32,c0846@sint32 :
and [c0078=r4, c0334=r5, c0590=r6, c0846=r7] && true;

ecut and [
eqmod (c0078*x** 78+c0334*x**334+c0590*x**590+c0846*x**846)
      (4*L0x2001b100*x** 78) [1043969, x**256 - 1],
eqmod (c0078*x** 78+c0334*x**334+c0590*x**590+c0846*x**846)
      (4*L0x2001b500*x** 78) [1043969, x**256 + 1],
eqmod (c0078*x** 78+c0334*x**334+c0590*x**590+c0846*x**846)
      (4*L0x2001b900*x** 78) [1043969, x**256 - 554923],
eqmod (c0078*x** 78+c0334*x**334+c0590*x**590+c0846*x**846)
      (4*L0x2001bd00*x** 78) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf078@sint32 : cf078 = r4 && cf078 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf334@sint32 : cf334 = r5 && cf334 = r5;

(******************** CUT 163 ********************)
ecut and [
eqmod 256*cf078 2**32*(c0078+c0587) 1043969,
eqmod 256*cf334 2**32*(c0334+c0843) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018864; PC = 0x80026a0 *)
mov L0x20018864 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018a64; PC = 0x80026a4 *)
mov L0x20018a64 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b104; Value = 0x001c7606; PC = 0x80026b0 *)
mov r4 L0x2001b104;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b504; Value = 0x00030146; PC = 0x80026b4 *)
mov r5 L0x2001b504;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b904; Value = 0x00050fac; PC = 0x80026b8 *)
mov r6 L0x2001b904;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bd04; Value = 0x0003b0d1; PC = 0x80026bc *)
mov r7 L0x2001bd04;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 164 ********************)

ghost c0079@sint32,c0335@sint32,c0591@sint32,c0847@sint32 :
and [c0079=r4, c0335=r5, c0591=r6, c0847=r7] && true;

ecut and [
eqmod (c0079*x** 79+c0335*x**335+c0591*x**591+c0847*x**847)
      (4*L0x2001b104*x** 79) [1043969, x**256 - 1],
eqmod (c0079*x** 79+c0335*x**335+c0591*x**591+c0847*x**847)
      (4*L0x2001b504*x** 79) [1043969, x**256 + 1],
eqmod (c0079*x** 79+c0335*x**335+c0591*x**591+c0847*x**847)
      (4*L0x2001b904*x** 79) [1043969, x**256 - 554923],
eqmod (c0079*x** 79+c0335*x**335+c0591*x**591+c0847*x**847)
      (4*L0x2001bd04*x** 79) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf079@sint32 : cf079 = r4 && cf079 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf335@sint32 : cf335 = r5 && cf335 = r5;

(******************** CUT 165 ********************)
ecut and [
eqmod 256*cf079 2**32*(c0079+c0588) 1043969,
eqmod 256*cf335 2**32*(c0335+c0844) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018866; PC = 0x8002728 *)
mov L0x20018866 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a66; PC = 0x800272c *)
mov L0x20018a66 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b108; Value = 0x00079737; PC = 0x8002738 *)
mov r4 L0x2001b108;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b508; Value = 0x000188fb; PC = 0x800273c *)
mov r5 L0x2001b508;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b908; Value = 0xfffdcb06; PC = 0x8002740 *)
mov r6 L0x2001b908;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bd08; Value = 0x000299d7; PC = 0x8002744 *)
mov r7 L0x2001bd08;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 166 ********************)

ghost c0080@sint32,c0336@sint32,c0592@sint32,c0848@sint32 :
and [c0080=r4, c0336=r5, c0592=r6, c0848=r7] && true;

ecut and [
eqmod (c0080*x** 80+c0336*x**336+c0592*x**592+c0848*x**848)
      (4*L0x2001b108*x** 80) [1043969, x**256 - 1],
eqmod (c0080*x** 80+c0336*x**336+c0592*x**592+c0848*x**848)
      (4*L0x2001b508*x** 80) [1043969, x**256 + 1],
eqmod (c0080*x** 80+c0336*x**336+c0592*x**592+c0848*x**848)
      (4*L0x2001b908*x** 80) [1043969, x**256 - 554923],
eqmod (c0080*x** 80+c0336*x**336+c0592*x**592+c0848*x**848)
      (4*L0x2001bd08*x** 80) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf080@sint32 : cf080 = r4 && cf080 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf336@sint32 : cf336 = r5 && cf336 = r5;

(******************** CUT 167 ********************)
ecut and [
eqmod 256*cf080 2**32*(c0080+c0589) 1043969,
eqmod 256*cf336 2**32*(c0336+c0845) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018868; PC = 0x80027b0 *)
mov L0x20018868 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a68; PC = 0x80027b4 *)
mov L0x20018a68 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b10c; Value = 0xffe8ccad; PC = 0x8002628 *)
mov r4 L0x2001b10c;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b50c; Value = 0x0002e590; PC = 0x800262c *)
mov r5 L0x2001b50c;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b90c; Value = 0xfffebef7; PC = 0x8002630 *)
mov r6 L0x2001b90c;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bd0c; Value = 0xfff9d54a; PC = 0x8002634 *)
mov r7 L0x2001bd0c;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 168 ********************)

ghost c0081@sint32,c0337@sint32,c0593@sint32,c0849@sint32 :
and [c0081=r4, c0337=r5, c0593=r6, c0849=r7] && true;

ecut and [
eqmod (c0081*x** 81+c0337*x**337+c0593*x**593+c0849*x**849)
      (4*L0x2001b10c*x** 81) [1043969, x**256 - 1],
eqmod (c0081*x** 81+c0337*x**337+c0593*x**593+c0849*x**849)
      (4*L0x2001b50c*x** 81) [1043969, x**256 + 1],
eqmod (c0081*x** 81+c0337*x**337+c0593*x**593+c0849*x**849)
      (4*L0x2001b90c*x** 81) [1043969, x**256 - 554923],
eqmod (c0081*x** 81+c0337*x**337+c0593*x**593+c0849*x**849)
      (4*L0x2001bd0c*x** 81) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf081@sint32 : cf081 = r4 && cf081 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf337@sint32 : cf337 = r5 && cf337 = r5;

(******************** CUT 169 ********************)
ecut and [
eqmod 256*cf081 2**32*(c0081+c0590) 1043969,
eqmod 256*cf337 2**32*(c0337+c0846) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x2001886a; PC = 0x80026a0 *)
mov L0x2001886a r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018a6a; PC = 0x80026a4 *)
mov L0x20018a6a r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b110; Value = 0xfff2f6ff; PC = 0x80026b0 *)
mov r4 L0x2001b110;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b510; Value = 0xfff6253b; PC = 0x80026b4 *)
mov r5 L0x2001b510;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b910; Value = 0x000e7679; PC = 0x80026b8 *)
mov r6 L0x2001b910;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bd10; Value = 0x000b4b4f; PC = 0x80026bc *)
mov r7 L0x2001bd10;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 170 ********************)

ghost c0082@sint32,c0338@sint32,c0594@sint32,c0850@sint32 :
and [c0082=r4, c0338=r5, c0594=r6, c0850=r7] && true;

ecut and [
eqmod (c0082*x** 82+c0338*x**338+c0594*x**594+c0850*x**850)
      (4*L0x2001b110*x** 82) [1043969, x**256 - 1],
eqmod (c0082*x** 82+c0338*x**338+c0594*x**594+c0850*x**850)
      (4*L0x2001b510*x** 82) [1043969, x**256 + 1],
eqmod (c0082*x** 82+c0338*x**338+c0594*x**594+c0850*x**850)
      (4*L0x2001b910*x** 82) [1043969, x**256 - 554923],
eqmod (c0082*x** 82+c0338*x**338+c0594*x**594+c0850*x**850)
      (4*L0x2001bd10*x** 82) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf082@sint32 : cf082 = r4 && cf082 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf338@sint32 : cf338 = r5 && cf338 = r5;

(******************** CUT 171 ********************)
ecut and [
eqmod 256*cf082 2**32*(c0082+c0591) 1043969,
eqmod 256*cf338 2**32*(c0338+c0847) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x2001886c; PC = 0x8002728 *)
mov L0x2001886c r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a6c; PC = 0x800272c *)
mov L0x20018a6c r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b114; Value = 0xfffa085f; PC = 0x8002738 *)
mov r4 L0x2001b114;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b514; Value = 0x000530f2; PC = 0x800273c *)
mov r5 L0x2001b514;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b914; Value = 0xfffad867; PC = 0x8002740 *)
mov r6 L0x2001b914;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bd14; Value = 0xfff2608d; PC = 0x8002744 *)
mov r7 L0x2001bd14;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 172 ********************)

ghost c0083@sint32,c0339@sint32,c0595@sint32,c0851@sint32 :
and [c0083=r4, c0339=r5, c0595=r6, c0851=r7] && true;

ecut and [
eqmod (c0083*x** 83+c0339*x**339+c0595*x**595+c0851*x**851)
      (4*L0x2001b114*x** 83) [1043969, x**256 - 1],
eqmod (c0083*x** 83+c0339*x**339+c0595*x**595+c0851*x**851)
      (4*L0x2001b514*x** 83) [1043969, x**256 + 1],
eqmod (c0083*x** 83+c0339*x**339+c0595*x**595+c0851*x**851)
      (4*L0x2001b914*x** 83) [1043969, x**256 - 554923],
eqmod (c0083*x** 83+c0339*x**339+c0595*x**595+c0851*x**851)
      (4*L0x2001bd14*x** 83) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf083@sint32 : cf083 = r4 && cf083 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf339@sint32 : cf339 = r5 && cf339 = r5;

(******************** CUT 173 ********************)
ecut and [
eqmod 256*cf083 2**32*(c0083+c0592) 1043969,
eqmod 256*cf339 2**32*(c0339+c0848) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x2001886e; PC = 0x80027b0 *)
mov L0x2001886e r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a6e; PC = 0x80027b4 *)
mov L0x20018a6e r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b118; Value = 0x0000ea50; PC = 0x8002628 *)
mov r4 L0x2001b118;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b518; Value = 0xffff9842; PC = 0x800262c *)
mov r5 L0x2001b518;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b918; Value = 0x00012529; PC = 0x8002630 *)
mov r6 L0x2001b918;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bd18; Value = 0xfffa1a03; PC = 0x8002634 *)
mov r7 L0x2001bd18;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 174 ********************)

ghost c0084@sint32,c0340@sint32,c0596@sint32,c0852@sint32 :
and [c0084=r4, c0340=r5, c0596=r6, c0852=r7] && true;

ecut and [
eqmod (c0084*x** 84+c0340*x**340+c0596*x**596+c0852*x**852)
      (4*L0x2001b118*x** 84) [1043969, x**256 - 1],
eqmod (c0084*x** 84+c0340*x**340+c0596*x**596+c0852*x**852)
      (4*L0x2001b518*x** 84) [1043969, x**256 + 1],
eqmod (c0084*x** 84+c0340*x**340+c0596*x**596+c0852*x**852)
      (4*L0x2001b918*x** 84) [1043969, x**256 - 554923],
eqmod (c0084*x** 84+c0340*x**340+c0596*x**596+c0852*x**852)
      (4*L0x2001bd18*x** 84) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf084@sint32 : cf084 = r4 && cf084 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf340@sint32 : cf340 = r5 && cf340 = r5;

(******************** CUT 175 ********************)
ecut and [
eqmod 256*cf084 2**32*(c0084+c0593) 1043969,
eqmod 256*cf340 2**32*(c0340+c0849) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018870; PC = 0x80026a0 *)
mov L0x20018870 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018a70; PC = 0x80026a4 *)
mov L0x20018a70 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b11c; Value = 0xfff2d706; PC = 0x80026b0 *)
mov r4 L0x2001b11c;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b51c; Value = 0x0000daab; PC = 0x80026b4 *)
mov r5 L0x2001b51c;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b91c; Value = 0xffff230b; PC = 0x80026b8 *)
mov r6 L0x2001b91c;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bd1c; Value = 0x00031605; PC = 0x80026bc *)
mov r7 L0x2001bd1c;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 176 ********************)

ghost c0085@sint32,c0341@sint32,c0597@sint32,c0853@sint32 :
and [c0085=r4, c0341=r5, c0597=r6, c0853=r7] && true;

ecut and [
eqmod (c0085*x** 85+c0341*x**341+c0597*x**597+c0853*x**853)
      (4*L0x2001b11c*x** 85) [1043969, x**256 - 1],
eqmod (c0085*x** 85+c0341*x**341+c0597*x**597+c0853*x**853)
      (4*L0x2001b51c*x** 85) [1043969, x**256 + 1],
eqmod (c0085*x** 85+c0341*x**341+c0597*x**597+c0853*x**853)
      (4*L0x2001b91c*x** 85) [1043969, x**256 - 554923],
eqmod (c0085*x** 85+c0341*x**341+c0597*x**597+c0853*x**853)
      (4*L0x2001bd1c*x** 85) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf085@sint32 : cf085 = r4 && cf085 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf341@sint32 : cf341 = r5 && cf341 = r5;

(******************** CUT 177 ********************)
ecut and [
eqmod 256*cf085 2**32*(c0085+c0594) 1043969,
eqmod 256*cf341 2**32*(c0341+c0850) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018872; PC = 0x8002728 *)
mov L0x20018872 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a72; PC = 0x800272c *)
mov L0x20018a72 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b120; Value = 0x00031318; PC = 0x8002738 *)
mov r4 L0x2001b120;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b520; Value = 0xfffc83df; PC = 0x800273c *)
mov r5 L0x2001b520;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b920; Value = 0x0004014f; PC = 0x8002740 *)
mov r6 L0x2001b920;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bd20; Value = 0x0000e31b; PC = 0x8002744 *)
mov r7 L0x2001bd20;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 178 ********************)

ghost c0086@sint32,c0342@sint32,c0598@sint32,c0854@sint32 :
and [c0086=r4, c0342=r5, c0598=r6, c0854=r7] && true;

ecut and [
eqmod (c0086*x** 86+c0342*x**342+c0598*x**598+c0854*x**854)
      (4*L0x2001b120*x** 86) [1043969, x**256 - 1],
eqmod (c0086*x** 86+c0342*x**342+c0598*x**598+c0854*x**854)
      (4*L0x2001b520*x** 86) [1043969, x**256 + 1],
eqmod (c0086*x** 86+c0342*x**342+c0598*x**598+c0854*x**854)
      (4*L0x2001b920*x** 86) [1043969, x**256 - 554923],
eqmod (c0086*x** 86+c0342*x**342+c0598*x**598+c0854*x**854)
      (4*L0x2001bd20*x** 86) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf086@sint32 : cf086 = r4 && cf086 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf342@sint32 : cf342 = r5 && cf342 = r5;

(******************** CUT 179 ********************)
ecut and [
eqmod 256*cf086 2**32*(c0086+c0595) 1043969,
eqmod 256*cf342 2**32*(c0342+c0851) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018874; PC = 0x80027b0 *)
mov L0x20018874 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a74; PC = 0x80027b4 *)
mov L0x20018a74 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b124; Value = 0xfff66c93; PC = 0x8002628 *)
mov r4 L0x2001b124;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b524; Value = 0xfffa5fc7; PC = 0x800262c *)
mov r5 L0x2001b524;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b924; Value = 0xfffb22a2; PC = 0x8002630 *)
mov r6 L0x2001b924;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bd24; Value = 0xffff716d; PC = 0x8002634 *)
mov r7 L0x2001bd24;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 180 ********************)

ghost c0087@sint32,c0343@sint32,c0599@sint32,c0855@sint32 :
and [c0087=r4, c0343=r5, c0599=r6, c0855=r7] && true;

ecut and [
eqmod (c0087*x** 87+c0343*x**343+c0599*x**599+c0855*x**855)
      (4*L0x2001b124*x** 87) [1043969, x**256 - 1],
eqmod (c0087*x** 87+c0343*x**343+c0599*x**599+c0855*x**855)
      (4*L0x2001b524*x** 87) [1043969, x**256 + 1],
eqmod (c0087*x** 87+c0343*x**343+c0599*x**599+c0855*x**855)
      (4*L0x2001b924*x** 87) [1043969, x**256 - 554923],
eqmod (c0087*x** 87+c0343*x**343+c0599*x**599+c0855*x**855)
      (4*L0x2001bd24*x** 87) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf087@sint32 : cf087 = r4 && cf087 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf343@sint32 : cf343 = r5 && cf343 = r5;

(******************** CUT 181 ********************)
ecut and [
eqmod 256*cf087 2**32*(c0087+c0596) 1043969,
eqmod 256*cf343 2**32*(c0343+c0852) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018876; PC = 0x80026a0 *)
mov L0x20018876 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018a76; PC = 0x80026a4 *)
mov L0x20018a76 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b128; Value = 0xfffb96b2; PC = 0x80026b0 *)
mov r4 L0x2001b128;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b528; Value = 0x000397d9; PC = 0x80026b4 *)
mov r5 L0x2001b528;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b928; Value = 0x000a8d56; PC = 0x80026b8 *)
mov r6 L0x2001b928;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bd28; Value = 0x00000b2d; PC = 0x80026bc *)
mov r7 L0x2001bd28;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 182 ********************)

ghost c0088@sint32,c0344@sint32,c0600@sint32,c0856@sint32 :
and [c0088=r4, c0344=r5, c0600=r6, c0856=r7] && true;

ecut and [
eqmod (c0088*x** 88+c0344*x**344+c0600*x**600+c0856*x**856)
      (4*L0x2001b128*x** 88) [1043969, x**256 - 1],
eqmod (c0088*x** 88+c0344*x**344+c0600*x**600+c0856*x**856)
      (4*L0x2001b528*x** 88) [1043969, x**256 + 1],
eqmod (c0088*x** 88+c0344*x**344+c0600*x**600+c0856*x**856)
      (4*L0x2001b928*x** 88) [1043969, x**256 - 554923],
eqmod (c0088*x** 88+c0344*x**344+c0600*x**600+c0856*x**856)
      (4*L0x2001bd28*x** 88) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf088@sint32 : cf088 = r4 && cf088 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf344@sint32 : cf344 = r5 && cf344 = r5;

(******************** CUT 183 ********************)
ecut and [
eqmod 256*cf088 2**32*(c0088+c0597) 1043969,
eqmod 256*cf344 2**32*(c0344+c0853) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018878; PC = 0x8002728 *)
mov L0x20018878 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a78; PC = 0x800272c *)
mov L0x20018a78 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b12c; Value = 0x0010991e; PC = 0x8002738 *)
mov r4 L0x2001b12c;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b52c; Value = 0xfff609a4; PC = 0x800273c *)
mov r5 L0x2001b52c;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b92c; Value = 0xfff62136; PC = 0x8002740 *)
mov r6 L0x2001b92c;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bd2c; Value = 0x00055113; PC = 0x8002744 *)
mov r7 L0x2001bd2c;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 184 ********************)

ghost c0089@sint32,c0345@sint32,c0601@sint32,c0857@sint32 :
and [c0089=r4, c0345=r5, c0601=r6, c0857=r7] && true;

ecut and [
eqmod (c0089*x** 89+c0345*x**345+c0601*x**601+c0857*x**857)
      (4*L0x2001b12c*x** 89) [1043969, x**256 - 1],
eqmod (c0089*x** 89+c0345*x**345+c0601*x**601+c0857*x**857)
      (4*L0x2001b52c*x** 89) [1043969, x**256 + 1],
eqmod (c0089*x** 89+c0345*x**345+c0601*x**601+c0857*x**857)
      (4*L0x2001b92c*x** 89) [1043969, x**256 - 554923],
eqmod (c0089*x** 89+c0345*x**345+c0601*x**601+c0857*x**857)
      (4*L0x2001bd2c*x** 89) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf089@sint32 : cf089 = r4 && cf089 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf345@sint32 : cf345 = r5 && cf345 = r5;

(******************** CUT 185 ********************)
ecut and [
eqmod 256*cf089 2**32*(c0089+c0598) 1043969,
eqmod 256*cf345 2**32*(c0345+c0854) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x2001887a; PC = 0x80027b0 *)
mov L0x2001887a r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a7a; PC = 0x80027b4 *)
mov L0x20018a7a r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b130; Value = 0xfff52607; PC = 0x8002628 *)
mov r4 L0x2001b130;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b530; Value = 0xfff7f094; PC = 0x800262c *)
mov r5 L0x2001b530;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b930; Value = 0xfffbbea2; PC = 0x8002630 *)
mov r6 L0x2001b930;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bd30; Value = 0xfffbc7bb; PC = 0x8002634 *)
mov r7 L0x2001bd30;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 186 ********************)

ghost c0090@sint32,c0346@sint32,c0602@sint32,c0858@sint32 :
and [c0090=r4, c0346=r5, c0602=r6, c0858=r7] && true;

ecut and [
eqmod (c0090*x** 90+c0346*x**346+c0602*x**602+c0858*x**858)
      (4*L0x2001b130*x** 90) [1043969, x**256 - 1],
eqmod (c0090*x** 90+c0346*x**346+c0602*x**602+c0858*x**858)
      (4*L0x2001b530*x** 90) [1043969, x**256 + 1],
eqmod (c0090*x** 90+c0346*x**346+c0602*x**602+c0858*x**858)
      (4*L0x2001b930*x** 90) [1043969, x**256 - 554923],
eqmod (c0090*x** 90+c0346*x**346+c0602*x**602+c0858*x**858)
      (4*L0x2001bd30*x** 90) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf090@sint32 : cf090 = r4 && cf090 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf346@sint32 : cf346 = r5 && cf346 = r5;

(******************** CUT 187 ********************)
ecut and [
eqmod 256*cf090 2**32*(c0090+c0599) 1043969,
eqmod 256*cf346 2**32*(c0346+c0855) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x2001887c; PC = 0x80026a0 *)
mov L0x2001887c r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018a7c; PC = 0x80026a4 *)
mov L0x20018a7c r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b134; Value = 0xffe76397; PC = 0x80026b0 *)
mov r4 L0x2001b134;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b534; Value = 0xfff90da5; PC = 0x80026b4 *)
mov r5 L0x2001b534;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b934; Value = 0xfffc96a7; PC = 0x80026b8 *)
mov r6 L0x2001b934;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bd34; Value = 0xfffd55f7; PC = 0x80026bc *)
mov r7 L0x2001bd34;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 188 ********************)

ghost c0091@sint32,c0347@sint32,c0603@sint32,c0859@sint32 :
and [c0091=r4, c0347=r5, c0603=r6, c0859=r7] && true;

ecut and [
eqmod (c0091*x** 91+c0347*x**347+c0603*x**603+c0859*x**859)
      (4*L0x2001b134*x** 91) [1043969, x**256 - 1],
eqmod (c0091*x** 91+c0347*x**347+c0603*x**603+c0859*x**859)
      (4*L0x2001b534*x** 91) [1043969, x**256 + 1],
eqmod (c0091*x** 91+c0347*x**347+c0603*x**603+c0859*x**859)
      (4*L0x2001b934*x** 91) [1043969, x**256 - 554923],
eqmod (c0091*x** 91+c0347*x**347+c0603*x**603+c0859*x**859)
      (4*L0x2001bd34*x** 91) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf091@sint32 : cf091 = r4 && cf091 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf347@sint32 : cf347 = r5 && cf347 = r5;

(******************** CUT 189 ********************)
ecut and [
eqmod 256*cf091 2**32*(c0091+c0600) 1043969,
eqmod 256*cf347 2**32*(c0347+c0856) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x2001887e; PC = 0x8002728 *)
mov L0x2001887e r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a7e; PC = 0x800272c *)
mov L0x20018a7e r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b138; Value = 0xfffe3943; PC = 0x8002738 *)
mov r4 L0x2001b138;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b538; Value = 0xfffe8dbe; PC = 0x800273c *)
mov r5 L0x2001b538;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b938; Value = 0x0005c6c8; PC = 0x8002740 *)
mov r6 L0x2001b938;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bd38; Value = 0xfffa58cd; PC = 0x8002744 *)
mov r7 L0x2001bd38;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 190 ********************)

ghost c0092@sint32,c0348@sint32,c0604@sint32,c0860@sint32 :
and [c0092=r4, c0348=r5, c0604=r6, c0860=r7] && true;

ecut and [
eqmod (c0092*x** 92+c0348*x**348+c0604*x**604+c0860*x**860)
      (4*L0x2001b138*x** 92) [1043969, x**256 - 1],
eqmod (c0092*x** 92+c0348*x**348+c0604*x**604+c0860*x**860)
      (4*L0x2001b538*x** 92) [1043969, x**256 + 1],
eqmod (c0092*x** 92+c0348*x**348+c0604*x**604+c0860*x**860)
      (4*L0x2001b938*x** 92) [1043969, x**256 - 554923],
eqmod (c0092*x** 92+c0348*x**348+c0604*x**604+c0860*x**860)
      (4*L0x2001bd38*x** 92) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf092@sint32 : cf092 = r4 && cf092 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf348@sint32 : cf348 = r5 && cf348 = r5;

(******************** CUT 191 ********************)
ecut and [
eqmod 256*cf092 2**32*(c0092+c0601) 1043969,
eqmod 256*cf348 2**32*(c0348+c0857) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018880; PC = 0x80027b0 *)
mov L0x20018880 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a80; PC = 0x80027b4 *)
mov L0x20018a80 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b13c; Value = 0x00002a20; PC = 0x8002628 *)
mov r4 L0x2001b13c;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b53c; Value = 0x0003f2b2; PC = 0x800262c *)
mov r5 L0x2001b53c;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b93c; Value = 0xfffc6b18; PC = 0x8002630 *)
mov r6 L0x2001b93c;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bd3c; Value = 0xfffb5e52; PC = 0x8002634 *)
mov r7 L0x2001bd3c;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 192 ********************)

ghost c0093@sint32,c0349@sint32,c0605@sint32,c0861@sint32 :
and [c0093=r4, c0349=r5, c0605=r6, c0861=r7] && true;

ecut and [
eqmod (c0093*x** 93+c0349*x**349+c0605*x**605+c0861*x**861)
      (4*L0x2001b13c*x** 93) [1043969, x**256 - 1],
eqmod (c0093*x** 93+c0349*x**349+c0605*x**605+c0861*x**861)
      (4*L0x2001b53c*x** 93) [1043969, x**256 + 1],
eqmod (c0093*x** 93+c0349*x**349+c0605*x**605+c0861*x**861)
      (4*L0x2001b93c*x** 93) [1043969, x**256 - 554923],
eqmod (c0093*x** 93+c0349*x**349+c0605*x**605+c0861*x**861)
      (4*L0x2001bd3c*x** 93) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf093@sint32 : cf093 = r4 && cf093 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf349@sint32 : cf349 = r5 && cf349 = r5;

(******************** CUT 193 ********************)
ecut and [
eqmod 256*cf093 2**32*(c0093+c0602) 1043969,
eqmod 256*cf349 2**32*(c0349+c0858) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018882; PC = 0x80026a0 *)
mov L0x20018882 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018a82; PC = 0x80026a4 *)
mov L0x20018a82 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b140; Value = 0xfff03116; PC = 0x80026b0 *)
mov r4 L0x2001b140;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b540; Value = 0x0002d07d; PC = 0x80026b4 *)
mov r5 L0x2001b540;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b940; Value = 0x0005fdd0; PC = 0x80026b8 *)
mov r6 L0x2001b940;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bd40; Value = 0x00028a7f; PC = 0x80026bc *)
mov r7 L0x2001bd40;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 194 ********************)

ghost c0094@sint32,c0350@sint32,c0606@sint32,c0862@sint32 :
and [c0094=r4, c0350=r5, c0606=r6, c0862=r7] && true;

ecut and [
eqmod (c0094*x** 94+c0350*x**350+c0606*x**606+c0862*x**862)
      (4*L0x2001b140*x** 94) [1043969, x**256 - 1],
eqmod (c0094*x** 94+c0350*x**350+c0606*x**606+c0862*x**862)
      (4*L0x2001b540*x** 94) [1043969, x**256 + 1],
eqmod (c0094*x** 94+c0350*x**350+c0606*x**606+c0862*x**862)
      (4*L0x2001b940*x** 94) [1043969, x**256 - 554923],
eqmod (c0094*x** 94+c0350*x**350+c0606*x**606+c0862*x**862)
      (4*L0x2001bd40*x** 94) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf094@sint32 : cf094 = r4 && cf094 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf350@sint32 : cf350 = r5 && cf350 = r5;

(******************** CUT 195 ********************)
ecut and [
eqmod 256*cf094 2**32*(c0094+c0603) 1043969,
eqmod 256*cf350 2**32*(c0350+c0859) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018884; PC = 0x8002728 *)
mov L0x20018884 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a84; PC = 0x800272c *)
mov L0x20018a84 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b144; Value = 0xfff23e1e; PC = 0x8002738 *)
mov r4 L0x2001b144;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b544; Value = 0x000151da; PC = 0x800273c *)
mov r5 L0x2001b544;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b944; Value = 0xffff6762; PC = 0x8002740 *)
mov r6 L0x2001b944;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bd44; Value = 0x000b33c9; PC = 0x8002744 *)
mov r7 L0x2001bd44;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 196 ********************)

ghost c0095@sint32,c0351@sint32,c0607@sint32,c0863@sint32 :
and [c0095=r4, c0351=r5, c0607=r6, c0863=r7] && true;

ecut and [
eqmod (c0095*x** 95+c0351*x**351+c0607*x**607+c0863*x**863)
      (4*L0x2001b144*x** 95) [1043969, x**256 - 1],
eqmod (c0095*x** 95+c0351*x**351+c0607*x**607+c0863*x**863)
      (4*L0x2001b544*x** 95) [1043969, x**256 + 1],
eqmod (c0095*x** 95+c0351*x**351+c0607*x**607+c0863*x**863)
      (4*L0x2001b944*x** 95) [1043969, x**256 - 554923],
eqmod (c0095*x** 95+c0351*x**351+c0607*x**607+c0863*x**863)
      (4*L0x2001bd44*x** 95) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf095@sint32 : cf095 = r4 && cf095 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf351@sint32 : cf351 = r5 && cf351 = r5;

(******************** CUT 197 ********************)
ecut and [
eqmod 256*cf095 2**32*(c0095+c0604) 1043969,
eqmod 256*cf351 2**32*(c0351+c0860) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018886; PC = 0x80027b0 *)
mov L0x20018886 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a86; PC = 0x80027b4 *)
mov L0x20018a86 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b148; Value = 0x000ebc91; PC = 0x8002628 *)
mov r4 L0x2001b148;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b548; Value = 0xfff8cf37; PC = 0x800262c *)
mov r5 L0x2001b548;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b948; Value = 0xfffa22c0; PC = 0x8002630 *)
mov r6 L0x2001b948;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bd48; Value = 0xfffe53f6; PC = 0x8002634 *)
mov r7 L0x2001bd48;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 198 ********************)

ghost c0096@sint32,c0352@sint32,c0608@sint32,c0864@sint32 :
and [c0096=r4, c0352=r5, c0608=r6, c0864=r7] && true;

ecut and [
eqmod (c0096*x** 96+c0352*x**352+c0608*x**608+c0864*x**864)
      (4*L0x2001b148*x** 96) [1043969, x**256 - 1],
eqmod (c0096*x** 96+c0352*x**352+c0608*x**608+c0864*x**864)
      (4*L0x2001b548*x** 96) [1043969, x**256 + 1],
eqmod (c0096*x** 96+c0352*x**352+c0608*x**608+c0864*x**864)
      (4*L0x2001b948*x** 96) [1043969, x**256 - 554923],
eqmod (c0096*x** 96+c0352*x**352+c0608*x**608+c0864*x**864)
      (4*L0x2001bd48*x** 96) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf096@sint32 : cf096 = r4 && cf096 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf352@sint32 : cf352 = r5 && cf352 = r5;

(******************** CUT 199 ********************)
ecut and [
eqmod 256*cf096 2**32*(c0096+c0605) 1043969,
eqmod 256*cf352 2**32*(c0352+c0861) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018888; PC = 0x80026a0 *)
mov L0x20018888 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018a88; PC = 0x80026a4 *)
mov L0x20018a88 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b14c; Value = 0xfffdb9c3; PC = 0x80026b0 *)
mov r4 L0x2001b14c;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b54c; Value = 0x0007fec5; PC = 0x80026b4 *)
mov r5 L0x2001b54c;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b94c; Value = 0xfffe5cf2; PC = 0x80026b8 *)
mov r6 L0x2001b94c;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bd4c; Value = 0x0001a2dd; PC = 0x80026bc *)
mov r7 L0x2001bd4c;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 200 ********************)

ghost c0097@sint32,c0353@sint32,c0609@sint32,c0865@sint32 :
and [c0097=r4, c0353=r5, c0609=r6, c0865=r7] && true;

ecut and [
eqmod (c0097*x** 97+c0353*x**353+c0609*x**609+c0865*x**865)
      (4*L0x2001b14c*x** 97) [1043969, x**256 - 1],
eqmod (c0097*x** 97+c0353*x**353+c0609*x**609+c0865*x**865)
      (4*L0x2001b54c*x** 97) [1043969, x**256 + 1],
eqmod (c0097*x** 97+c0353*x**353+c0609*x**609+c0865*x**865)
      (4*L0x2001b94c*x** 97) [1043969, x**256 - 554923],
eqmod (c0097*x** 97+c0353*x**353+c0609*x**609+c0865*x**865)
      (4*L0x2001bd4c*x** 97) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf097@sint32 : cf097 = r4 && cf097 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf353@sint32 : cf353 = r5 && cf353 = r5;

(******************** CUT 201 ********************)
ecut and [
eqmod 256*cf097 2**32*(c0097+c0606) 1043969,
eqmod 256*cf353 2**32*(c0353+c0862) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x2001888a; PC = 0x8002728 *)
mov L0x2001888a r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a8a; PC = 0x800272c *)
mov L0x20018a8a r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b150; Value = 0xfff511f0; PC = 0x8002738 *)
mov r4 L0x2001b150;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b550; Value = 0xffff4d78; PC = 0x800273c *)
mov r5 L0x2001b550;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b950; Value = 0x00017241; PC = 0x8002740 *)
mov r6 L0x2001b950;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bd50; Value = 0x0000a5a4; PC = 0x8002744 *)
mov r7 L0x2001bd50;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 202 ********************)

ghost c0098@sint32,c0354@sint32,c0610@sint32,c0866@sint32 :
and [c0098=r4, c0354=r5, c0610=r6, c0866=r7] && true;

ecut and [
eqmod (c0098*x** 98+c0354*x**354+c0610*x**610+c0866*x**866)
      (4*L0x2001b150*x** 98) [1043969, x**256 - 1],
eqmod (c0098*x** 98+c0354*x**354+c0610*x**610+c0866*x**866)
      (4*L0x2001b550*x** 98) [1043969, x**256 + 1],
eqmod (c0098*x** 98+c0354*x**354+c0610*x**610+c0866*x**866)
      (4*L0x2001b950*x** 98) [1043969, x**256 - 554923],
eqmod (c0098*x** 98+c0354*x**354+c0610*x**610+c0866*x**866)
      (4*L0x2001bd50*x** 98) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf098@sint32 : cf098 = r4 && cf098 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf354@sint32 : cf354 = r5 && cf354 = r5;

(******************** CUT 203 ********************)
ecut and [
eqmod 256*cf098 2**32*(c0098+c0607) 1043969,
eqmod 256*cf354 2**32*(c0354+c0863) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x2001888c; PC = 0x80027b0 *)
mov L0x2001888c r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a8c; PC = 0x80027b4 *)
mov L0x20018a8c r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b154; Value = 0x0016a72f; PC = 0x8002628 *)
mov r4 L0x2001b154;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b554; Value = 0xffff4bbd; PC = 0x800262c *)
mov r5 L0x2001b554;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b954; Value = 0x000d230c; PC = 0x8002630 *)
mov r6 L0x2001b954;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bd54; Value = 0xfffe7208; PC = 0x8002634 *)
mov r7 L0x2001bd54;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 204 ********************)

ghost c0099@sint32,c0355@sint32,c0611@sint32,c0867@sint32 :
and [c0099=r4, c0355=r5, c0611=r6, c0867=r7] && true;

ecut and [
eqmod (c0099*x** 99+c0355*x**355+c0611*x**611+c0867*x**867)
      (4*L0x2001b154*x** 99) [1043969, x**256 - 1],
eqmod (c0099*x** 99+c0355*x**355+c0611*x**611+c0867*x**867)
      (4*L0x2001b554*x** 99) [1043969, x**256 + 1],
eqmod (c0099*x** 99+c0355*x**355+c0611*x**611+c0867*x**867)
      (4*L0x2001b954*x** 99) [1043969, x**256 - 554923],
eqmod (c0099*x** 99+c0355*x**355+c0611*x**611+c0867*x**867)
      (4*L0x2001bd54*x** 99) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf099@sint32 : cf099 = r4 && cf099 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf355@sint32 : cf355 = r5 && cf355 = r5;

(******************** CUT 205 ********************)
ecut and [
eqmod 256*cf099 2**32*(c0099+c0608) 1043969,
eqmod 256*cf355 2**32*(c0355+c0864) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x2001888e; PC = 0x80026a0 *)
mov L0x2001888e r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018a8e; PC = 0x80026a4 *)
mov L0x20018a8e r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b158; Value = 0x0007f84e; PC = 0x80026b0 *)
mov r4 L0x2001b158;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b558; Value = 0x000a0af0; PC = 0x80026b4 *)
mov r5 L0x2001b558;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b958; Value = 0x0001f468; PC = 0x80026b8 *)
mov r6 L0x2001b958;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bd58; Value = 0xfffc8468; PC = 0x80026bc *)
mov r7 L0x2001bd58;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 206 ********************)

ghost c0100@sint32,c0356@sint32,c0612@sint32,c0868@sint32 :
and [c0100=r4, c0356=r5, c0612=r6, c0868=r7] && true;

ecut and [
eqmod (c0100*x**100+c0356*x**356+c0612*x**612+c0868*x**868)
      (4*L0x2001b158*x**100) [1043969, x**256 - 1],
eqmod (c0100*x**100+c0356*x**356+c0612*x**612+c0868*x**868)
      (4*L0x2001b558*x**100) [1043969, x**256 + 1],
eqmod (c0100*x**100+c0356*x**356+c0612*x**612+c0868*x**868)
      (4*L0x2001b958*x**100) [1043969, x**256 - 554923],
eqmod (c0100*x**100+c0356*x**356+c0612*x**612+c0868*x**868)
      (4*L0x2001bd58*x**100) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf100@sint32 : cf100 = r4 && cf100 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf356@sint32 : cf356 = r5 && cf356 = r5;

(******************** CUT 207 ********************)
ecut and [
eqmod 256*cf100 2**32*(c0100+c0609) 1043969,
eqmod 256*cf356 2**32*(c0356+c0865) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018890; PC = 0x8002728 *)
mov L0x20018890 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a90; PC = 0x800272c *)
mov L0x20018a90 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b15c; Value = 0x000409cb; PC = 0x8002738 *)
mov r4 L0x2001b15c;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b55c; Value = 0x00049417; PC = 0x800273c *)
mov r5 L0x2001b55c;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b95c; Value = 0xfffed673; PC = 0x8002740 *)
mov r6 L0x2001b95c;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bd5c; Value = 0x00034966; PC = 0x8002744 *)
mov r7 L0x2001bd5c;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 208 ********************)

ghost c0101@sint32,c0357@sint32,c0613@sint32,c0869@sint32 :
and [c0101=r4, c0357=r5, c0613=r6, c0869=r7] && true;

ecut and [
eqmod (c0101*x**101+c0357*x**357+c0613*x**613+c0869*x**869)
      (4*L0x2001b15c*x**101) [1043969, x**256 - 1],
eqmod (c0101*x**101+c0357*x**357+c0613*x**613+c0869*x**869)
      (4*L0x2001b55c*x**101) [1043969, x**256 + 1],
eqmod (c0101*x**101+c0357*x**357+c0613*x**613+c0869*x**869)
      (4*L0x2001b95c*x**101) [1043969, x**256 - 554923],
eqmod (c0101*x**101+c0357*x**357+c0613*x**613+c0869*x**869)
      (4*L0x2001bd5c*x**101) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf101@sint32 : cf101 = r4 && cf101 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf357@sint32 : cf357 = r5 && cf357 = r5;

(******************** CUT 209 ********************)
ecut and [
eqmod 256*cf101 2**32*(c0101+c0610) 1043969,
eqmod 256*cf357 2**32*(c0357+c0866) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018892; PC = 0x80027b0 *)
mov L0x20018892 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a92; PC = 0x80027b4 *)
mov L0x20018a92 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b160; Value = 0xffeac205; PC = 0x8002628 *)
mov r4 L0x2001b160;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b560; Value = 0xfffa3329; PC = 0x800262c *)
mov r5 L0x2001b560;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b960; Value = 0x000d55c6; PC = 0x8002630 *)
mov r6 L0x2001b960;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bd60; Value = 0x0005ebcd; PC = 0x8002634 *)
mov r7 L0x2001bd60;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 210 ********************)

ghost c0102@sint32,c0358@sint32,c0614@sint32,c0870@sint32 :
and [c0102=r4, c0358=r5, c0614=r6, c0870=r7] && true;

ecut and [
eqmod (c0102*x**102+c0358*x**358+c0614*x**614+c0870*x**870)
      (4*L0x2001b160*x**102) [1043969, x**256 - 1],
eqmod (c0102*x**102+c0358*x**358+c0614*x**614+c0870*x**870)
      (4*L0x2001b560*x**102) [1043969, x**256 + 1],
eqmod (c0102*x**102+c0358*x**358+c0614*x**614+c0870*x**870)
      (4*L0x2001b960*x**102) [1043969, x**256 - 554923],
eqmod (c0102*x**102+c0358*x**358+c0614*x**614+c0870*x**870)
      (4*L0x2001bd60*x**102) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf102@sint32 : cf102 = r4 && cf102 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf358@sint32 : cf358 = r5 && cf358 = r5;

(******************** CUT 211 ********************)
ecut and [
eqmod 256*cf102 2**32*(c0102+c0611) 1043969,
eqmod 256*cf358 2**32*(c0358+c0867) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018894; PC = 0x80026a0 *)
mov L0x20018894 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018a94; PC = 0x80026a4 *)
mov L0x20018a94 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b164; Value = 0xfff318fa; PC = 0x80026b0 *)
mov r4 L0x2001b164;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b564; Value = 0x0009e949; PC = 0x80026b4 *)
mov r5 L0x2001b564;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b964; Value = 0x00003328; PC = 0x80026b8 *)
mov r6 L0x2001b964;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bd64; Value = 0xfffe2968; PC = 0x80026bc *)
mov r7 L0x2001bd64;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 212 ********************)

ghost c0103@sint32,c0359@sint32,c0615@sint32,c0871@sint32 :
and [c0103=r4, c0359=r5, c0615=r6, c0871=r7] && true;

ecut and [
eqmod (c0103*x**103+c0359*x**359+c0615*x**615+c0871*x**871)
      (4*L0x2001b164*x**103) [1043969, x**256 - 1],
eqmod (c0103*x**103+c0359*x**359+c0615*x**615+c0871*x**871)
      (4*L0x2001b564*x**103) [1043969, x**256 + 1],
eqmod (c0103*x**103+c0359*x**359+c0615*x**615+c0871*x**871)
      (4*L0x2001b964*x**103) [1043969, x**256 - 554923],
eqmod (c0103*x**103+c0359*x**359+c0615*x**615+c0871*x**871)
      (4*L0x2001bd64*x**103) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf103@sint32 : cf103 = r4 && cf103 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf359@sint32 : cf359 = r5 && cf359 = r5;

(******************** CUT 213 ********************)
ecut and [
eqmod 256*cf103 2**32*(c0103+c0612) 1043969,
eqmod 256*cf359 2**32*(c0359+c0868) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018896; PC = 0x8002728 *)
mov L0x20018896 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a96; PC = 0x800272c *)
mov L0x20018a96 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b168; Value = 0xfffdeb65; PC = 0x8002738 *)
mov r4 L0x2001b168;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b568; Value = 0x000659e1; PC = 0x800273c *)
mov r5 L0x2001b568;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b968; Value = 0x0000f4c6; PC = 0x8002740 *)
mov r6 L0x2001b968;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bd68; Value = 0xfff94a66; PC = 0x8002744 *)
mov r7 L0x2001bd68;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 214 ********************)

ghost c0104@sint32,c0360@sint32,c0616@sint32,c0872@sint32 :
and [c0104=r4, c0360=r5, c0616=r6, c0872=r7] && true;

ecut and [
eqmod (c0104*x**104+c0360*x**360+c0616*x**616+c0872*x**872)
      (4*L0x2001b168*x**104) [1043969, x**256 - 1],
eqmod (c0104*x**104+c0360*x**360+c0616*x**616+c0872*x**872)
      (4*L0x2001b568*x**104) [1043969, x**256 + 1],
eqmod (c0104*x**104+c0360*x**360+c0616*x**616+c0872*x**872)
      (4*L0x2001b968*x**104) [1043969, x**256 - 554923],
eqmod (c0104*x**104+c0360*x**360+c0616*x**616+c0872*x**872)
      (4*L0x2001bd68*x**104) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf104@sint32 : cf104 = r4 && cf104 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf360@sint32 : cf360 = r5 && cf360 = r5;

(******************** CUT 215 ********************)
ecut and [
eqmod 256*cf104 2**32*(c0104+c0613) 1043969,
eqmod 256*cf360 2**32*(c0360+c0869) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018898; PC = 0x80027b0 *)
mov L0x20018898 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a98; PC = 0x80027b4 *)
mov L0x20018a98 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b16c; Value = 0x000cddda; PC = 0x8002628 *)
mov r4 L0x2001b16c;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b56c; Value = 0x00081251; PC = 0x800262c *)
mov r5 L0x2001b56c;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b96c; Value = 0x00053fce; PC = 0x8002630 *)
mov r6 L0x2001b96c;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bd6c; Value = 0xfffa367e; PC = 0x8002634 *)
mov r7 L0x2001bd6c;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 216 ********************)

ghost c0105@sint32,c0361@sint32,c0617@sint32,c0873@sint32 :
and [c0105=r4, c0361=r5, c0617=r6, c0873=r7] && true;

ecut and [
eqmod (c0105*x**105+c0361*x**361+c0617*x**617+c0873*x**873)
      (4*L0x2001b16c*x**105) [1043969, x**256 - 1],
eqmod (c0105*x**105+c0361*x**361+c0617*x**617+c0873*x**873)
      (4*L0x2001b56c*x**105) [1043969, x**256 + 1],
eqmod (c0105*x**105+c0361*x**361+c0617*x**617+c0873*x**873)
      (4*L0x2001b96c*x**105) [1043969, x**256 - 554923],
eqmod (c0105*x**105+c0361*x**361+c0617*x**617+c0873*x**873)
      (4*L0x2001bd6c*x**105) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf105@sint32 : cf105 = r4 && cf105 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf361@sint32 : cf361 = r5 && cf361 = r5;

(******************** CUT 217 ********************)
ecut and [
eqmod 256*cf105 2**32*(c0105+c0614) 1043969,
eqmod 256*cf361 2**32*(c0361+c0870) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x2001889a; PC = 0x80026a0 *)
mov L0x2001889a r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018a9a; PC = 0x80026a4 *)
mov L0x20018a9a r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b170; Value = 0x000af3f8; PC = 0x80026b0 *)
mov r4 L0x2001b170;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b570; Value = 0x000ca7ca; PC = 0x80026b4 *)
mov r5 L0x2001b570;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b970; Value = 0x00034ee1; PC = 0x80026b8 *)
mov r6 L0x2001b970;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bd70; Value = 0x0004b655; PC = 0x80026bc *)
mov r7 L0x2001bd70;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 218 ********************)

ghost c0106@sint32,c0362@sint32,c0618@sint32,c0874@sint32 :
and [c0106=r4, c0362=r5, c0618=r6, c0874=r7] && true;

ecut and [
eqmod (c0106*x**106+c0362*x**362+c0618*x**618+c0874*x**874)
      (4*L0x2001b170*x**106) [1043969, x**256 - 1],
eqmod (c0106*x**106+c0362*x**362+c0618*x**618+c0874*x**874)
      (4*L0x2001b570*x**106) [1043969, x**256 + 1],
eqmod (c0106*x**106+c0362*x**362+c0618*x**618+c0874*x**874)
      (4*L0x2001b970*x**106) [1043969, x**256 - 554923],
eqmod (c0106*x**106+c0362*x**362+c0618*x**618+c0874*x**874)
      (4*L0x2001bd70*x**106) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf106@sint32 : cf106 = r4 && cf106 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf362@sint32 : cf362 = r5 && cf362 = r5;

(******************** CUT 219 ********************)
ecut and [
eqmod 256*cf106 2**32*(c0106+c0615) 1043969,
eqmod 256*cf362 2**32*(c0362+c0871) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x2001889c; PC = 0x8002728 *)
mov L0x2001889c r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018a9c; PC = 0x800272c *)
mov L0x20018a9c r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b174; Value = 0x000978dd; PC = 0x8002738 *)
mov r4 L0x2001b174;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b574; Value = 0xfffc54dc; PC = 0x800273c *)
mov r5 L0x2001b574;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b974; Value = 0x0004acb2; PC = 0x8002740 *)
mov r6 L0x2001b974;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bd74; Value = 0x00016614; PC = 0x8002744 *)
mov r7 L0x2001bd74;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 220 ********************)

ghost c0107@sint32,c0363@sint32,c0619@sint32,c0875@sint32 :
and [c0107=r4, c0363=r5, c0619=r6, c0875=r7] && true;

ecut and [
eqmod (c0107*x**107+c0363*x**363+c0619*x**619+c0875*x**875)
      (4*L0x2001b174*x**107) [1043969, x**256 - 1],
eqmod (c0107*x**107+c0363*x**363+c0619*x**619+c0875*x**875)
      (4*L0x2001b574*x**107) [1043969, x**256 + 1],
eqmod (c0107*x**107+c0363*x**363+c0619*x**619+c0875*x**875)
      (4*L0x2001b974*x**107) [1043969, x**256 - 554923],
eqmod (c0107*x**107+c0363*x**363+c0619*x**619+c0875*x**875)
      (4*L0x2001bd74*x**107) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf107@sint32 : cf107 = r4 && cf107 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf363@sint32 : cf363 = r5 && cf363 = r5;

(******************** CUT 221 ********************)
ecut and [
eqmod 256*cf107 2**32*(c0107+c0616) 1043969,
eqmod 256*cf363 2**32*(c0363+c0872) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x2001889e; PC = 0x80027b0 *)
mov L0x2001889e r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018a9e; PC = 0x80027b4 *)
mov L0x20018a9e r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b178; Value = 0x0003a853; PC = 0x8002628 *)
mov r4 L0x2001b178;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b578; Value = 0xfffdeb88; PC = 0x800262c *)
mov r5 L0x2001b578;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b978; Value = 0x0002b39a; PC = 0x8002630 *)
mov r6 L0x2001b978;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bd78; Value = 0xfffd0b97; PC = 0x8002634 *)
mov r7 L0x2001bd78;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 222 ********************)

ghost c0108@sint32,c0364@sint32,c0620@sint32,c0876@sint32 :
and [c0108=r4, c0364=r5, c0620=r6, c0876=r7] && true;

ecut and [
eqmod (c0108*x**108+c0364*x**364+c0620*x**620+c0876*x**876)
      (4*L0x2001b178*x**108) [1043969, x**256 - 1],
eqmod (c0108*x**108+c0364*x**364+c0620*x**620+c0876*x**876)
      (4*L0x2001b578*x**108) [1043969, x**256 + 1],
eqmod (c0108*x**108+c0364*x**364+c0620*x**620+c0876*x**876)
      (4*L0x2001b978*x**108) [1043969, x**256 - 554923],
eqmod (c0108*x**108+c0364*x**364+c0620*x**620+c0876*x**876)
      (4*L0x2001bd78*x**108) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf108@sint32 : cf108 = r4 && cf108 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf364@sint32 : cf364 = r5 && cf364 = r5;

(******************** CUT 223 ********************)
ecut and [
eqmod 256*cf108 2**32*(c0108+c0617) 1043969,
eqmod 256*cf364 2**32*(c0364+c0873) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200188a0; PC = 0x80026a0 *)
mov L0x200188a0 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018aa0; PC = 0x80026a4 *)
mov L0x20018aa0 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b17c; Value = 0xfffc823c; PC = 0x80026b0 *)
mov r4 L0x2001b17c;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b57c; Value = 0x0004e618; PC = 0x80026b4 *)
mov r5 L0x2001b57c;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b97c; Value = 0xfffe0699; PC = 0x80026b8 *)
mov r6 L0x2001b97c;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bd7c; Value = 0xfffcdfe6; PC = 0x80026bc *)
mov r7 L0x2001bd7c;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 224 ********************)

ghost c0109@sint32,c0365@sint32,c0621@sint32,c0877@sint32 :
and [c0109=r4, c0365=r5, c0621=r6, c0877=r7] && true;

ecut and [
eqmod (c0109*x**109+c0365*x**365+c0621*x**621+c0877*x**877)
      (4*L0x2001b17c*x**109) [1043969, x**256 - 1],
eqmod (c0109*x**109+c0365*x**365+c0621*x**621+c0877*x**877)
      (4*L0x2001b57c*x**109) [1043969, x**256 + 1],
eqmod (c0109*x**109+c0365*x**365+c0621*x**621+c0877*x**877)
      (4*L0x2001b97c*x**109) [1043969, x**256 - 554923],
eqmod (c0109*x**109+c0365*x**365+c0621*x**621+c0877*x**877)
      (4*L0x2001bd7c*x**109) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf109@sint32 : cf109 = r4 && cf109 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf365@sint32 : cf365 = r5 && cf365 = r5;

(******************** CUT 225 ********************)
ecut and [
eqmod 256*cf109 2**32*(c0109+c0618) 1043969,
eqmod 256*cf365 2**32*(c0365+c0874) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200188a2; PC = 0x8002728 *)
mov L0x200188a2 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018aa2; PC = 0x800272c *)
mov L0x20018aa2 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b180; Value = 0xfffb62f5; PC = 0x8002738 *)
mov r4 L0x2001b180;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b580; Value = 0xfffd2672; PC = 0x800273c *)
mov r5 L0x2001b580;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b980; Value = 0xfff5f538; PC = 0x8002740 *)
mov r6 L0x2001b980;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bd80; Value = 0x0007fc98; PC = 0x8002744 *)
mov r7 L0x2001bd80;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 226 ********************)

ghost c0110@sint32,c0366@sint32,c0622@sint32,c0878@sint32 :
and [c0110=r4, c0366=r5, c0622=r6, c0878=r7] && true;

ecut and [
eqmod (c0110*x**110+c0366*x**366+c0622*x**622+c0878*x**878)
      (4*L0x2001b180*x**110) [1043969, x**256 - 1],
eqmod (c0110*x**110+c0366*x**366+c0622*x**622+c0878*x**878)
      (4*L0x2001b580*x**110) [1043969, x**256 + 1],
eqmod (c0110*x**110+c0366*x**366+c0622*x**622+c0878*x**878)
      (4*L0x2001b980*x**110) [1043969, x**256 - 554923],
eqmod (c0110*x**110+c0366*x**366+c0622*x**622+c0878*x**878)
      (4*L0x2001bd80*x**110) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf110@sint32 : cf110 = r4 && cf110 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf366@sint32 : cf366 = r5 && cf366 = r5;

(******************** CUT 227 ********************)
ecut and [
eqmod 256*cf110 2**32*(c0110+c0619) 1043969,
eqmod 256*cf366 2**32*(c0366+c0875) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200188a4; PC = 0x80027b0 *)
mov L0x200188a4 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018aa4; PC = 0x80027b4 *)
mov L0x20018aa4 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b184; Value = 0x000edc1a; PC = 0x8002628 *)
mov r4 L0x2001b184;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b584; Value = 0x000f8c62; PC = 0x800262c *)
mov r5 L0x2001b584;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b984; Value = 0xfffb9048; PC = 0x8002630 *)
mov r6 L0x2001b984;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bd84; Value = 0xfffe8bee; PC = 0x8002634 *)
mov r7 L0x2001bd84;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 228 ********************)

ghost c0111@sint32,c0367@sint32,c0623@sint32,c0879@sint32 :
and [c0111=r4, c0367=r5, c0623=r6, c0879=r7] && true;

ecut and [
eqmod (c0111*x**111+c0367*x**367+c0623*x**623+c0879*x**879)
      (4*L0x2001b184*x**111) [1043969, x**256 - 1],
eqmod (c0111*x**111+c0367*x**367+c0623*x**623+c0879*x**879)
      (4*L0x2001b584*x**111) [1043969, x**256 + 1],
eqmod (c0111*x**111+c0367*x**367+c0623*x**623+c0879*x**879)
      (4*L0x2001b984*x**111) [1043969, x**256 - 554923],
eqmod (c0111*x**111+c0367*x**367+c0623*x**623+c0879*x**879)
      (4*L0x2001bd84*x**111) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf111@sint32 : cf111 = r4 && cf111 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf367@sint32 : cf367 = r5 && cf367 = r5;

(******************** CUT 229 ********************)
ecut and [
eqmod 256*cf111 2**32*(c0111+c0620) 1043969,
eqmod 256*cf367 2**32*(c0367+c0876) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200188a6; PC = 0x80026a0 *)
mov L0x200188a6 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018aa6; PC = 0x80026a4 *)
mov L0x20018aa6 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b188; Value = 0xfffc173d; PC = 0x80026b0 *)
mov r4 L0x2001b188;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b588; Value = 0x0009b589; PC = 0x80026b4 *)
mov r5 L0x2001b588;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b988; Value = 0xfffcc97c; PC = 0x80026b8 *)
mov r6 L0x2001b988;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bd88; Value = 0xffff616b; PC = 0x80026bc *)
mov r7 L0x2001bd88;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 230 ********************)

ghost c0112@sint32,c0368@sint32,c0624@sint32,c0880@sint32 :
and [c0112=r4, c0368=r5, c0624=r6, c0880=r7] && true;

ecut and [
eqmod (c0112*x**112+c0368*x**368+c0624*x**624+c0880*x**880)
      (4*L0x2001b188*x**112) [1043969, x**256 - 1],
eqmod (c0112*x**112+c0368*x**368+c0624*x**624+c0880*x**880)
      (4*L0x2001b588*x**112) [1043969, x**256 + 1],
eqmod (c0112*x**112+c0368*x**368+c0624*x**624+c0880*x**880)
      (4*L0x2001b988*x**112) [1043969, x**256 - 554923],
eqmod (c0112*x**112+c0368*x**368+c0624*x**624+c0880*x**880)
      (4*L0x2001bd88*x**112) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf112@sint32 : cf112 = r4 && cf112 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf368@sint32 : cf368 = r5 && cf368 = r5;

(******************** CUT 231 ********************)
ecut and [
eqmod 256*cf112 2**32*(c0112+c0621) 1043969,
eqmod 256*cf368 2**32*(c0368+c0877) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200188a8; PC = 0x8002728 *)
mov L0x200188a8 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018aa8; PC = 0x800272c *)
mov L0x20018aa8 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b18c; Value = 0x0007d57e; PC = 0x8002738 *)
mov r4 L0x2001b18c;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b58c; Value = 0x00004f75; PC = 0x800273c *)
mov r5 L0x2001b58c;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b98c; Value = 0x00013b79; PC = 0x8002740 *)
mov r6 L0x2001b98c;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bd8c; Value = 0xfff8ffd8; PC = 0x8002744 *)
mov r7 L0x2001bd8c;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 232 ********************)

ghost c0113@sint32,c0369@sint32,c0625@sint32,c0881@sint32 :
and [c0113=r4, c0369=r5, c0625=r6, c0881=r7] && true;

ecut and [
eqmod (c0113*x**113+c0369*x**369+c0625*x**625+c0881*x**881)
      (4*L0x2001b18c*x**113) [1043969, x**256 - 1],
eqmod (c0113*x**113+c0369*x**369+c0625*x**625+c0881*x**881)
      (4*L0x2001b58c*x**113) [1043969, x**256 + 1],
eqmod (c0113*x**113+c0369*x**369+c0625*x**625+c0881*x**881)
      (4*L0x2001b98c*x**113) [1043969, x**256 - 554923],
eqmod (c0113*x**113+c0369*x**369+c0625*x**625+c0881*x**881)
      (4*L0x2001bd8c*x**113) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf113@sint32 : cf113 = r4 && cf113 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf369@sint32 : cf369 = r5 && cf369 = r5;

(******************** CUT 233 ********************)
ecut and [
eqmod 256*cf113 2**32*(c0113+c0622) 1043969,
eqmod 256*cf369 2**32*(c0369+c0878) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200188aa; PC = 0x80027b0 *)
mov L0x200188aa r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018aaa; PC = 0x80027b4 *)
mov L0x20018aaa r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b190; Value = 0x000b0840; PC = 0x8002628 *)
mov r4 L0x2001b190;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b590; Value = 0xffff75a7; PC = 0x800262c *)
mov r5 L0x2001b590;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b990; Value = 0x0002613f; PC = 0x8002630 *)
mov r6 L0x2001b990;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bd90; Value = 0xfff13d24; PC = 0x8002634 *)
mov r7 L0x2001bd90;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 234 ********************)

ghost c0114@sint32,c0370@sint32,c0626@sint32,c0882@sint32 :
and [c0114=r4, c0370=r5, c0626=r6, c0882=r7] && true;

ecut and [
eqmod (c0114*x**114+c0370*x**370+c0626*x**626+c0882*x**882)
      (4*L0x2001b190*x**114) [1043969, x**256 - 1],
eqmod (c0114*x**114+c0370*x**370+c0626*x**626+c0882*x**882)
      (4*L0x2001b590*x**114) [1043969, x**256 + 1],
eqmod (c0114*x**114+c0370*x**370+c0626*x**626+c0882*x**882)
      (4*L0x2001b990*x**114) [1043969, x**256 - 554923],
eqmod (c0114*x**114+c0370*x**370+c0626*x**626+c0882*x**882)
      (4*L0x2001bd90*x**114) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf114@sint32 : cf114 = r4 && cf114 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf370@sint32 : cf370 = r5 && cf370 = r5;

(******************** CUT 235 ********************)
ecut and [
eqmod 256*cf114 2**32*(c0114+c0623) 1043969,
eqmod 256*cf370 2**32*(c0370+c0879) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200188ac; PC = 0x80026a0 *)
mov L0x200188ac r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018aac; PC = 0x80026a4 *)
mov L0x20018aac r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b194; Value = 0x000095b4; PC = 0x80026b0 *)
mov r4 L0x2001b194;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b594; Value = 0x000ba801; PC = 0x80026b4 *)
mov r5 L0x2001b594;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b994; Value = 0x0002372b; PC = 0x80026b8 *)
mov r6 L0x2001b994;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bd94; Value = 0x000a2697; PC = 0x80026bc *)
mov r7 L0x2001bd94;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 236 ********************)

ghost c0115@sint32,c0371@sint32,c0627@sint32,c0883@sint32 :
and [c0115=r4, c0371=r5, c0627=r6, c0883=r7] && true;

ecut and [
eqmod (c0115*x**115+c0371*x**371+c0627*x**627+c0883*x**883)
      (4*L0x2001b194*x**115) [1043969, x**256 - 1],
eqmod (c0115*x**115+c0371*x**371+c0627*x**627+c0883*x**883)
      (4*L0x2001b594*x**115) [1043969, x**256 + 1],
eqmod (c0115*x**115+c0371*x**371+c0627*x**627+c0883*x**883)
      (4*L0x2001b994*x**115) [1043969, x**256 - 554923],
eqmod (c0115*x**115+c0371*x**371+c0627*x**627+c0883*x**883)
      (4*L0x2001bd94*x**115) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf115@sint32 : cf115 = r4 && cf115 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf371@sint32 : cf371 = r5 && cf371 = r5;

(******************** CUT 237 ********************)
ecut and [
eqmod 256*cf115 2**32*(c0115+c0624) 1043969,
eqmod 256*cf371 2**32*(c0371+c0880) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200188ae; PC = 0x8002728 *)
mov L0x200188ae r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018aae; PC = 0x800272c *)
mov L0x20018aae r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b198; Value = 0x000357d7; PC = 0x8002738 *)
mov r4 L0x2001b198;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b598; Value = 0xfff0b5a1; PC = 0x800273c *)
mov r5 L0x2001b598;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b998; Value = 0xfffb0879; PC = 0x8002740 *)
mov r6 L0x2001b998;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bd98; Value = 0x00089eaf; PC = 0x8002744 *)
mov r7 L0x2001bd98;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 238 ********************)

ghost c0116@sint32,c0372@sint32,c0628@sint32,c0884@sint32 :
and [c0116=r4, c0372=r5, c0628=r6, c0884=r7] && true;

ecut and [
eqmod (c0116*x**116+c0372*x**372+c0628*x**628+c0884*x**884)
      (4*L0x2001b198*x**116) [1043969, x**256 - 1],
eqmod (c0116*x**116+c0372*x**372+c0628*x**628+c0884*x**884)
      (4*L0x2001b598*x**116) [1043969, x**256 + 1],
eqmod (c0116*x**116+c0372*x**372+c0628*x**628+c0884*x**884)
      (4*L0x2001b998*x**116) [1043969, x**256 - 554923],
eqmod (c0116*x**116+c0372*x**372+c0628*x**628+c0884*x**884)
      (4*L0x2001bd98*x**116) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf116@sint32 : cf116 = r4 && cf116 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf372@sint32 : cf372 = r5 && cf372 = r5;

(******************** CUT 239 ********************)
ecut and [
eqmod 256*cf116 2**32*(c0116+c0625) 1043969,
eqmod 256*cf372 2**32*(c0372+c0881) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200188b0; PC = 0x80027b0 *)
mov L0x200188b0 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018ab0; PC = 0x80027b4 *)
mov L0x20018ab0 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b19c; Value = 0xfffcc59f; PC = 0x8002628 *)
mov r4 L0x2001b19c;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b59c; Value = 0x0001a78a; PC = 0x800262c *)
mov r5 L0x2001b59c;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b99c; Value = 0xfff28608; PC = 0x8002630 *)
mov r6 L0x2001b99c;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bd9c; Value = 0x0002a09e; PC = 0x8002634 *)
mov r7 L0x2001bd9c;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 240 ********************)

ghost c0117@sint32,c0373@sint32,c0629@sint32,c0885@sint32 :
and [c0117=r4, c0373=r5, c0629=r6, c0885=r7] && true;

ecut and [
eqmod (c0117*x**117+c0373*x**373+c0629*x**629+c0885*x**885)
      (4*L0x2001b19c*x**117) [1043969, x**256 - 1],
eqmod (c0117*x**117+c0373*x**373+c0629*x**629+c0885*x**885)
      (4*L0x2001b59c*x**117) [1043969, x**256 + 1],
eqmod (c0117*x**117+c0373*x**373+c0629*x**629+c0885*x**885)
      (4*L0x2001b99c*x**117) [1043969, x**256 - 554923],
eqmod (c0117*x**117+c0373*x**373+c0629*x**629+c0885*x**885)
      (4*L0x2001bd9c*x**117) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf117@sint32 : cf117 = r4 && cf117 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf373@sint32 : cf373 = r5 && cf373 = r5;

(******************** CUT 241 ********************)
ecut and [
eqmod 256*cf117 2**32*(c0117+c0626) 1043969,
eqmod 256*cf373 2**32*(c0373+c0882) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200188b2; PC = 0x80026a0 *)
mov L0x200188b2 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018ab2; PC = 0x80026a4 *)
mov L0x20018ab2 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b1a0; Value = 0xfff82e5f; PC = 0x80026b0 *)
mov r4 L0x2001b1a0;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b5a0; Value = 0xfff39c00; PC = 0x80026b4 *)
mov r5 L0x2001b5a0;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b9a0; Value = 0x000b0143; PC = 0x80026b8 *)
mov r6 L0x2001b9a0;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bda0; Value = 0x0004ab85; PC = 0x80026bc *)
mov r7 L0x2001bda0;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 242 ********************)

ghost c0118@sint32,c0374@sint32,c0630@sint32,c0886@sint32 :
and [c0118=r4, c0374=r5, c0630=r6, c0886=r7] && true;

ecut and [
eqmod (c0118*x**118+c0374*x**374+c0630*x**630+c0886*x**886)
      (4*L0x2001b1a0*x**118) [1043969, x**256 - 1],
eqmod (c0118*x**118+c0374*x**374+c0630*x**630+c0886*x**886)
      (4*L0x2001b5a0*x**118) [1043969, x**256 + 1],
eqmod (c0118*x**118+c0374*x**374+c0630*x**630+c0886*x**886)
      (4*L0x2001b9a0*x**118) [1043969, x**256 - 554923],
eqmod (c0118*x**118+c0374*x**374+c0630*x**630+c0886*x**886)
      (4*L0x2001bda0*x**118) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf118@sint32 : cf118 = r4 && cf118 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf374@sint32 : cf374 = r5 && cf374 = r5;

(******************** CUT 243 ********************)
ecut and [
eqmod 256*cf118 2**32*(c0118+c0627) 1043969,
eqmod 256*cf374 2**32*(c0374+c0883) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200188b4; PC = 0x8002728 *)
mov L0x200188b4 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018ab4; PC = 0x800272c *)
mov L0x20018ab4 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b1a4; Value = 0xfffde22d; PC = 0x8002738 *)
mov r4 L0x2001b1a4;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b5a4; Value = 0x00000184; PC = 0x800273c *)
mov r5 L0x2001b5a4;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b9a4; Value = 0xfffedd4b; PC = 0x8002740 *)
mov r6 L0x2001b9a4;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bda4; Value = 0x0008cfc6; PC = 0x8002744 *)
mov r7 L0x2001bda4;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 244 ********************)

ghost c0119@sint32,c0375@sint32,c0631@sint32,c0887@sint32 :
and [c0119=r4, c0375=r5, c0631=r6, c0887=r7] && true;

ecut and [
eqmod (c0119*x**119+c0375*x**375+c0631*x**631+c0887*x**887)
      (4*L0x2001b1a4*x**119) [1043969, x**256 - 1],
eqmod (c0119*x**119+c0375*x**375+c0631*x**631+c0887*x**887)
      (4*L0x2001b5a4*x**119) [1043969, x**256 + 1],
eqmod (c0119*x**119+c0375*x**375+c0631*x**631+c0887*x**887)
      (4*L0x2001b9a4*x**119) [1043969, x**256 - 554923],
eqmod (c0119*x**119+c0375*x**375+c0631*x**631+c0887*x**887)
      (4*L0x2001bda4*x**119) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf119@sint32 : cf119 = r4 && cf119 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf375@sint32 : cf375 = r5 && cf375 = r5;

(******************** CUT 245 ********************)
ecut and [
eqmod 256*cf119 2**32*(c0119+c0628) 1043969,
eqmod 256*cf375 2**32*(c0375+c0884) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200188b6; PC = 0x80027b0 *)
mov L0x200188b6 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018ab6; PC = 0x80027b4 *)
mov L0x20018ab6 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b1a8; Value = 0x0002ebb2; PC = 0x8002628 *)
mov r4 L0x2001b1a8;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b5a8; Value = 0xfff2d678; PC = 0x800262c *)
mov r5 L0x2001b5a8;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b9a8; Value = 0x000b8243; PC = 0x8002630 *)
mov r6 L0x2001b9a8;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bda8; Value = 0x0004d0a1; PC = 0x8002634 *)
mov r7 L0x2001bda8;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 246 ********************)

ghost c0120@sint32,c0376@sint32,c0632@sint32,c0888@sint32 :
and [c0120=r4, c0376=r5, c0632=r6, c0888=r7] && true;

ecut and [
eqmod (c0120*x**120+c0376*x**376+c0632*x**632+c0888*x**888)
      (4*L0x2001b1a8*x**120) [1043969, x**256 - 1],
eqmod (c0120*x**120+c0376*x**376+c0632*x**632+c0888*x**888)
      (4*L0x2001b5a8*x**120) [1043969, x**256 + 1],
eqmod (c0120*x**120+c0376*x**376+c0632*x**632+c0888*x**888)
      (4*L0x2001b9a8*x**120) [1043969, x**256 - 554923],
eqmod (c0120*x**120+c0376*x**376+c0632*x**632+c0888*x**888)
      (4*L0x2001bda8*x**120) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf120@sint32 : cf120 = r4 && cf120 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf376@sint32 : cf376 = r5 && cf376 = r5;

(******************** CUT 247 ********************)
ecut and [
eqmod 256*cf120 2**32*(c0120+c0629) 1043969,
eqmod 256*cf376 2**32*(c0376+c0885) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200188b8; PC = 0x80026a0 *)
mov L0x200188b8 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018ab8; PC = 0x80026a4 *)
mov L0x20018ab8 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b1ac; Value = 0xfff36f17; PC = 0x80026b0 *)
mov r4 L0x2001b1ac;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b5ac; Value = 0xfffb313b; PC = 0x80026b4 *)
mov r5 L0x2001b5ac;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b9ac; Value = 0x0004b831; PC = 0x80026b8 *)
mov r6 L0x2001b9ac;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bdac; Value = 0xffffcd39; PC = 0x80026bc *)
mov r7 L0x2001bdac;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 248 ********************)

ghost c0121@sint32,c0377@sint32,c0633@sint32,c0889@sint32 :
and [c0121=r4, c0377=r5, c0633=r6, c0889=r7] && true;

ecut and [
eqmod (c0121*x**121+c0377*x**377+c0633*x**633+c0889*x**889)
      (4*L0x2001b1ac*x**121) [1043969, x**256 - 1],
eqmod (c0121*x**121+c0377*x**377+c0633*x**633+c0889*x**889)
      (4*L0x2001b5ac*x**121) [1043969, x**256 + 1],
eqmod (c0121*x**121+c0377*x**377+c0633*x**633+c0889*x**889)
      (4*L0x2001b9ac*x**121) [1043969, x**256 - 554923],
eqmod (c0121*x**121+c0377*x**377+c0633*x**633+c0889*x**889)
      (4*L0x2001bdac*x**121) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf121@sint32 : cf121 = r4 && cf121 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf377@sint32 : cf377 = r5 && cf377 = r5;

(******************** CUT 249 ********************)
ecut and [
eqmod 256*cf121 2**32*(c0121+c0630) 1043969,
eqmod 256*cf377 2**32*(c0377+c0886) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200188ba; PC = 0x8002728 *)
mov L0x200188ba r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018aba; PC = 0x800272c *)
mov L0x20018aba r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b1b0; Value = 0xffffa98d; PC = 0x8002738 *)
mov r4 L0x2001b1b0;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b5b0; Value = 0x00019675; PC = 0x800273c *)
mov r5 L0x2001b5b0;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b9b0; Value = 0xfffff13c; PC = 0x8002740 *)
mov r6 L0x2001b9b0;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bdb0; Value = 0xfffd9738; PC = 0x8002744 *)
mov r7 L0x2001bdb0;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 250 ********************)

ghost c0122@sint32,c0378@sint32,c0634@sint32,c0890@sint32 :
and [c0122=r4, c0378=r5, c0634=r6, c0890=r7] && true;

ecut and [
eqmod (c0122*x**122+c0378*x**378+c0634*x**634+c0890*x**890)
      (4*L0x2001b1b0*x**122) [1043969, x**256 - 1],
eqmod (c0122*x**122+c0378*x**378+c0634*x**634+c0890*x**890)
      (4*L0x2001b5b0*x**122) [1043969, x**256 + 1],
eqmod (c0122*x**122+c0378*x**378+c0634*x**634+c0890*x**890)
      (4*L0x2001b9b0*x**122) [1043969, x**256 - 554923],
eqmod (c0122*x**122+c0378*x**378+c0634*x**634+c0890*x**890)
      (4*L0x2001bdb0*x**122) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf122@sint32 : cf122 = r4 && cf122 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf378@sint32 : cf378 = r5 && cf378 = r5;

(******************** CUT 251 ********************)
ecut and [
eqmod 256*cf122 2**32*(c0122+c0631) 1043969,
eqmod 256*cf378 2**32*(c0378+c0887) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200188bc; PC = 0x80027b0 *)
mov L0x200188bc r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018abc; PC = 0x80027b4 *)
mov L0x20018abc r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b1b4; Value = 0xfffaa7b8; PC = 0x8002628 *)
mov r4 L0x2001b1b4;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b5b4; Value = 0x000ca55b; PC = 0x800262c *)
mov r5 L0x2001b5b4;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b9b4; Value = 0xfffc0170; PC = 0x8002630 *)
mov r6 L0x2001b9b4;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bdb4; Value = 0x0000a967; PC = 0x8002634 *)
mov r7 L0x2001bdb4;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 252 ********************)

ghost c0123@sint32,c0379@sint32,c0635@sint32,c0891@sint32 :
and [c0123=r4, c0379=r5, c0635=r6, c0891=r7] && true;

ecut and [
eqmod (c0123*x**123+c0379*x**379+c0635*x**635+c0891*x**891)
      (4*L0x2001b1b4*x**123) [1043969, x**256 - 1],
eqmod (c0123*x**123+c0379*x**379+c0635*x**635+c0891*x**891)
      (4*L0x2001b5b4*x**123) [1043969, x**256 + 1],
eqmod (c0123*x**123+c0379*x**379+c0635*x**635+c0891*x**891)
      (4*L0x2001b9b4*x**123) [1043969, x**256 - 554923],
eqmod (c0123*x**123+c0379*x**379+c0635*x**635+c0891*x**891)
      (4*L0x2001bdb4*x**123) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf123@sint32 : cf123 = r4 && cf123 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf379@sint32 : cf379 = r5 && cf379 = r5;

(******************** CUT 253 ********************)
ecut and [
eqmod 256*cf123 2**32*(c0123+c0632) 1043969,
eqmod 256*cf379 2**32*(c0379+c0888) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200188be; PC = 0x80026a0 *)
mov L0x200188be r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018abe; PC = 0x80026a4 *)
mov L0x20018abe r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b1b8; Value = 0xfffac9d2; PC = 0x80026b0 *)
mov r4 L0x2001b1b8;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b5b8; Value = 0x000913c7; PC = 0x80026b4 *)
mov r5 L0x2001b5b8;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b9b8; Value = 0x000b88de; PC = 0x80026b8 *)
mov r6 L0x2001b9b8;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bdb8; Value = 0xfff66dc0; PC = 0x80026bc *)
mov r7 L0x2001bdb8;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 254 ********************)

ghost c0124@sint32,c0380@sint32,c0636@sint32,c0892@sint32 :
and [c0124=r4, c0380=r5, c0636=r6, c0892=r7] && true;

ecut and [
eqmod (c0124*x**124+c0380*x**380+c0636*x**636+c0892*x**892)
      (4*L0x2001b1b8*x**124) [1043969, x**256 - 1],
eqmod (c0124*x**124+c0380*x**380+c0636*x**636+c0892*x**892)
      (4*L0x2001b5b8*x**124) [1043969, x**256 + 1],
eqmod (c0124*x**124+c0380*x**380+c0636*x**636+c0892*x**892)
      (4*L0x2001b9b8*x**124) [1043969, x**256 - 554923],
eqmod (c0124*x**124+c0380*x**380+c0636*x**636+c0892*x**892)
      (4*L0x2001bdb8*x**124) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf124@sint32 : cf124 = r4 && cf124 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf380@sint32 : cf380 = r5 && cf380 = r5;

(******************** CUT 255 ********************)
ecut and [
eqmod 256*cf124 2**32*(c0124+c0633) 1043969,
eqmod 256*cf380 2**32*(c0380+c0889) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200188c0; PC = 0x8002728 *)
mov L0x200188c0 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018ac0; PC = 0x800272c *)
mov L0x20018ac0 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b1bc; Value = 0xfff6a65f; PC = 0x8002738 *)
mov r4 L0x2001b1bc;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b5bc; Value = 0x00021b47; PC = 0x800273c *)
mov r5 L0x2001b5bc;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b9bc; Value = 0x0001cea3; PC = 0x8002740 *)
mov r6 L0x2001b9bc;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bdbc; Value = 0x0005094d; PC = 0x8002744 *)
mov r7 L0x2001bdbc;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 256 ********************)

ghost c0125@sint32,c0381@sint32,c0637@sint32,c0893@sint32 :
and [c0125=r4, c0381=r5, c0637=r6, c0893=r7] && true;

ecut and [
eqmod (c0125*x**125+c0381*x**381+c0637*x**637+c0893*x**893)
      (4*L0x2001b1bc*x**125) [1043969, x**256 - 1],
eqmod (c0125*x**125+c0381*x**381+c0637*x**637+c0893*x**893)
      (4*L0x2001b5bc*x**125) [1043969, x**256 + 1],
eqmod (c0125*x**125+c0381*x**381+c0637*x**637+c0893*x**893)
      (4*L0x2001b9bc*x**125) [1043969, x**256 - 554923],
eqmod (c0125*x**125+c0381*x**381+c0637*x**637+c0893*x**893)
      (4*L0x2001bdbc*x**125) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf125@sint32 : cf125 = r4 && cf125 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf381@sint32 : cf381 = r5 && cf381 = r5;

(******************** CUT 257 ********************)
ecut and [
eqmod 256*cf125 2**32*(c0125+c0634) 1043969,
eqmod 256*cf381 2**32*(c0381+c0890) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200188c2; PC = 0x80027b0 *)
mov L0x200188c2 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018ac2; PC = 0x80027b4 *)
mov L0x20018ac2 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b1c0; Value = 0xfff44ead; PC = 0x8002628 *)
mov r4 L0x2001b1c0;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b5c0; Value = 0x00063883; PC = 0x800262c *)
mov r5 L0x2001b5c0;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b9c0; Value = 0xfffd897f; PC = 0x8002630 *)
mov r6 L0x2001b9c0;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bdc0; Value = 0x0000226c; PC = 0x8002634 *)
mov r7 L0x2001bdc0;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 258 ********************)

ghost c0126@sint32,c0382@sint32,c0638@sint32,c0894@sint32 :
and [c0126=r4, c0382=r5, c0638=r6, c0894=r7] && true;

ecut and [
eqmod (c0126*x**126+c0382*x**382+c0638*x**638+c0894*x**894)
      (4*L0x2001b1c0*x**126) [1043969, x**256 - 1],
eqmod (c0126*x**126+c0382*x**382+c0638*x**638+c0894*x**894)
      (4*L0x2001b5c0*x**126) [1043969, x**256 + 1],
eqmod (c0126*x**126+c0382*x**382+c0638*x**638+c0894*x**894)
      (4*L0x2001b9c0*x**126) [1043969, x**256 - 554923],
eqmod (c0126*x**126+c0382*x**382+c0638*x**638+c0894*x**894)
      (4*L0x2001bdc0*x**126) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf126@sint32 : cf126 = r4 && cf126 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf382@sint32 : cf382 = r5 && cf382 = r5;

(******************** CUT 259 ********************)
ecut and [
eqmod 256*cf126 2**32*(c0126+c0635) 1043969,
eqmod 256*cf382 2**32*(c0382+c0891) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200188c4; PC = 0x80026a0 *)
mov L0x200188c4 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018ac4; PC = 0x80026a4 *)
mov L0x20018ac4 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b1c4; Value = 0xfff3d110; PC = 0x80026b0 *)
mov r4 L0x2001b1c4;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b5c4; Value = 0xfffa0991; PC = 0x80026b4 *)
mov r5 L0x2001b5c4;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b9c4; Value = 0xfffea452; PC = 0x80026b8 *)
mov r6 L0x2001b9c4;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bdc4; Value = 0x0006c9ee; PC = 0x80026bc *)
mov r7 L0x2001bdc4;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 260 ********************)

ghost c0127@sint32,c0383@sint32,c0639@sint32,c0895@sint32 :
and [c0127=r4, c0383=r5, c0639=r6, c0895=r7] && true;

ecut and [
eqmod (c0127*x**127+c0383*x**383+c0639*x**639+c0895*x**895)
      (4*L0x2001b1c4*x**127) [1043969, x**256 - 1],
eqmod (c0127*x**127+c0383*x**383+c0639*x**639+c0895*x**895)
      (4*L0x2001b5c4*x**127) [1043969, x**256 + 1],
eqmod (c0127*x**127+c0383*x**383+c0639*x**639+c0895*x**895)
      (4*L0x2001b9c4*x**127) [1043969, x**256 - 554923],
eqmod (c0127*x**127+c0383*x**383+c0639*x**639+c0895*x**895)
      (4*L0x2001bdc4*x**127) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf127@sint32 : cf127 = r4 && cf127 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf383@sint32 : cf383 = r5 && cf383 = r5;

(******************** CUT 261 ********************)
ecut and [
eqmod 256*cf127 2**32*(c0127+c0636) 1043969,
eqmod 256*cf383 2**32*(c0383+c0892) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200188c6; PC = 0x8002728 *)
mov L0x200188c6 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018ac6; PC = 0x800272c *)
mov L0x20018ac6 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b1c8; Value = 0x0012155d; PC = 0x8002738 *)
mov r4 L0x2001b1c8;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b5c8; Value = 0xfffcd35c; PC = 0x800273c *)
mov r5 L0x2001b5c8;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b9c8; Value = 0x000115e6; PC = 0x8002740 *)
mov r6 L0x2001b9c8;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bdc8; Value = 0x00024b18; PC = 0x8002744 *)
mov r7 L0x2001bdc8;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 262 ********************)

ghost c0128@sint32,c0384@sint32,c0640@sint32,c0896@sint32 :
and [c0128=r4, c0384=r5, c0640=r6, c0896=r7] && true;

ecut and [
eqmod (c0128*x**128+c0384*x**384+c0640*x**640+c0896*x**896)
      (4*L0x2001b1c8*x**128) [1043969, x**256 - 1],
eqmod (c0128*x**128+c0384*x**384+c0640*x**640+c0896*x**896)
      (4*L0x2001b5c8*x**128) [1043969, x**256 + 1],
eqmod (c0128*x**128+c0384*x**384+c0640*x**640+c0896*x**896)
      (4*L0x2001b9c8*x**128) [1043969, x**256 - 554923],
eqmod (c0128*x**128+c0384*x**384+c0640*x**640+c0896*x**896)
      (4*L0x2001bdc8*x**128) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf128@sint32 : cf128 = r4 && cf128 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf384@sint32 : cf384 = r5 && cf384 = r5;

(******************** CUT 263 ********************)
ecut and [
eqmod 256*cf128 2**32*(c0128+c0637) 1043969,
eqmod 256*cf384 2**32*(c0384+c0893) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200188c8; PC = 0x80027b0 *)
mov L0x200188c8 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018ac8; PC = 0x80027b4 *)
mov L0x20018ac8 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b1cc; Value = 0xfffffeb2; PC = 0x8002628 *)
mov r4 L0x2001b1cc;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b5cc; Value = 0xfff84dca; PC = 0x800262c *)
mov r5 L0x2001b5cc;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b9cc; Value = 0x00054b03; PC = 0x8002630 *)
mov r6 L0x2001b9cc;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bdcc; Value = 0xfffff2a8; PC = 0x8002634 *)
mov r7 L0x2001bdcc;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 264 ********************)

ghost c0129@sint32,c0385@sint32,c0641@sint32,c0897@sint32 :
and [c0129=r4, c0385=r5, c0641=r6, c0897=r7] && true;

ecut and [
eqmod (c0129*x**129+c0385*x**385+c0641*x**641+c0897*x**897)
      (4*L0x2001b1cc*x**129) [1043969, x**256 - 1],
eqmod (c0129*x**129+c0385*x**385+c0641*x**641+c0897*x**897)
      (4*L0x2001b5cc*x**129) [1043969, x**256 + 1],
eqmod (c0129*x**129+c0385*x**385+c0641*x**641+c0897*x**897)
      (4*L0x2001b9cc*x**129) [1043969, x**256 - 554923],
eqmod (c0129*x**129+c0385*x**385+c0641*x**641+c0897*x**897)
      (4*L0x2001bdcc*x**129) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf129@sint32 : cf129 = r4 && cf129 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf385@sint32 : cf385 = r5 && cf385 = r5;

(******************** CUT 265 ********************)
ecut and [
eqmod 256*cf129 2**32*(c0129+c0638) 1043969,
eqmod 256*cf385 2**32*(c0385+c0894) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200188ca; PC = 0x80026a0 *)
mov L0x200188ca r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018aca; PC = 0x80026a4 *)
mov L0x20018aca r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b1d0; Value = 0x000db1a3; PC = 0x80026b0 *)
mov r4 L0x2001b1d0;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b5d0; Value = 0x00033b60; PC = 0x80026b4 *)
mov r5 L0x2001b5d0;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b9d0; Value = 0xffffbddc; PC = 0x80026b8 *)
mov r6 L0x2001b9d0;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bdd0; Value = 0x000189b7; PC = 0x80026bc *)
mov r7 L0x2001bdd0;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 266 ********************)

ghost c0130@sint32,c0386@sint32,c0642@sint32,c0898@sint32 :
and [c0130=r4, c0386=r5, c0642=r6, c0898=r7] && true;

ecut and [
eqmod (c0130*x**130+c0386*x**386+c0642*x**642+c0898*x**898)
      (4*L0x2001b1d0*x**130) [1043969, x**256 - 1],
eqmod (c0130*x**130+c0386*x**386+c0642*x**642+c0898*x**898)
      (4*L0x2001b5d0*x**130) [1043969, x**256 + 1],
eqmod (c0130*x**130+c0386*x**386+c0642*x**642+c0898*x**898)
      (4*L0x2001b9d0*x**130) [1043969, x**256 - 554923],
eqmod (c0130*x**130+c0386*x**386+c0642*x**642+c0898*x**898)
      (4*L0x2001bdd0*x**130) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf130@sint32 : cf130 = r4 && cf130 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf386@sint32 : cf386 = r5 && cf386 = r5;

(******************** CUT 267 ********************)
ecut and [
eqmod 256*cf130 2**32*(c0130+c0639) 1043969,
eqmod 256*cf386 2**32*(c0386+c0895) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200188cc; PC = 0x8002728 *)
mov L0x200188cc r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018acc; PC = 0x800272c *)
mov L0x20018acc r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b1d4; Value = 0xfffc6c63; PC = 0x8002738 *)
mov r4 L0x2001b1d4;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b5d4; Value = 0x00020e9f; PC = 0x800273c *)
mov r5 L0x2001b5d4;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b9d4; Value = 0x000287dd; PC = 0x8002740 *)
mov r6 L0x2001b9d4;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bdd4; Value = 0x00069699; PC = 0x8002744 *)
mov r7 L0x2001bdd4;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 268 ********************)

ghost c0131@sint32,c0387@sint32,c0643@sint32,c0899@sint32 :
and [c0131=r4, c0387=r5, c0643=r6, c0899=r7] && true;

ecut and [
eqmod (c0131*x**131+c0387*x**387+c0643*x**643+c0899*x**899)
      (4*L0x2001b1d4*x**131) [1043969, x**256 - 1],
eqmod (c0131*x**131+c0387*x**387+c0643*x**643+c0899*x**899)
      (4*L0x2001b5d4*x**131) [1043969, x**256 + 1],
eqmod (c0131*x**131+c0387*x**387+c0643*x**643+c0899*x**899)
      (4*L0x2001b9d4*x**131) [1043969, x**256 - 554923],
eqmod (c0131*x**131+c0387*x**387+c0643*x**643+c0899*x**899)
      (4*L0x2001bdd4*x**131) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf131@sint32 : cf131 = r4 && cf131 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf387@sint32 : cf387 = r5 && cf387 = r5;

(******************** CUT 269 ********************)
ecut and [
eqmod 256*cf131 2**32*(c0131+c0640) 1043969,
eqmod 256*cf387 2**32*(c0387+c0896) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200188ce; PC = 0x80027b0 *)
mov L0x200188ce r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018ace; PC = 0x80027b4 *)
mov L0x20018ace r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b1d8; Value = 0x0009fcb2; PC = 0x8002628 *)
mov r4 L0x2001b1d8;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b5d8; Value = 0x0004011a; PC = 0x800262c *)
mov r5 L0x2001b5d8;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b9d8; Value = 0xfff8425b; PC = 0x8002630 *)
mov r6 L0x2001b9d8;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bdd8; Value = 0x00068102; PC = 0x8002634 *)
mov r7 L0x2001bdd8;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 270 ********************)

ghost c0132@sint32,c0388@sint32,c0644@sint32,c0900@sint32 :
and [c0132=r4, c0388=r5, c0644=r6, c0900=r7] && true;

ecut and [
eqmod (c0132*x**132+c0388*x**388+c0644*x**644+c0900*x**900)
      (4*L0x2001b1d8*x**132) [1043969, x**256 - 1],
eqmod (c0132*x**132+c0388*x**388+c0644*x**644+c0900*x**900)
      (4*L0x2001b5d8*x**132) [1043969, x**256 + 1],
eqmod (c0132*x**132+c0388*x**388+c0644*x**644+c0900*x**900)
      (4*L0x2001b9d8*x**132) [1043969, x**256 - 554923],
eqmod (c0132*x**132+c0388*x**388+c0644*x**644+c0900*x**900)
      (4*L0x2001bdd8*x**132) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf132@sint32 : cf132 = r4 && cf132 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf388@sint32 : cf388 = r5 && cf388 = r5;

(******************** CUT 271 ********************)
ecut and [
eqmod 256*cf132 2**32*(c0132+c0641) 1043969,
eqmod 256*cf388 2**32*(c0388+c0897) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200188d0; PC = 0x80026a0 *)
mov L0x200188d0 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018ad0; PC = 0x80026a4 *)
mov L0x20018ad0 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b1dc; Value = 0xfff25cc0; PC = 0x80026b0 *)
mov r4 L0x2001b1dc;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b5dc; Value = 0x00040683; PC = 0x80026b4 *)
mov r5 L0x2001b5dc;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b9dc; Value = 0x00059750; PC = 0x80026b8 *)
mov r6 L0x2001b9dc;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bddc; Value = 0xfffac20e; PC = 0x80026bc *)
mov r7 L0x2001bddc;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 272 ********************)

ghost c0133@sint32,c0389@sint32,c0645@sint32,c0901@sint32 :
and [c0133=r4, c0389=r5, c0645=r6, c0901=r7] && true;

ecut and [
eqmod (c0133*x**133+c0389*x**389+c0645*x**645+c0901*x**901)
      (4*L0x2001b1dc*x**133) [1043969, x**256 - 1],
eqmod (c0133*x**133+c0389*x**389+c0645*x**645+c0901*x**901)
      (4*L0x2001b5dc*x**133) [1043969, x**256 + 1],
eqmod (c0133*x**133+c0389*x**389+c0645*x**645+c0901*x**901)
      (4*L0x2001b9dc*x**133) [1043969, x**256 - 554923],
eqmod (c0133*x**133+c0389*x**389+c0645*x**645+c0901*x**901)
      (4*L0x2001bddc*x**133) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf133@sint32 : cf133 = r4 && cf133 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf389@sint32 : cf389 = r5 && cf389 = r5;

(******************** CUT 273 ********************)
ecut and [
eqmod 256*cf133 2**32*(c0133+c0642) 1043969,
eqmod 256*cf389 2**32*(c0389+c0898) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200188d2; PC = 0x8002728 *)
mov L0x200188d2 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018ad2; PC = 0x800272c *)
mov L0x20018ad2 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b1e0; Value = 0xffea8319; PC = 0x8002738 *)
mov r4 L0x2001b1e0;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b5e0; Value = 0xfff8712d; PC = 0x800273c *)
mov r5 L0x2001b5e0;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b9e0; Value = 0x00000581; PC = 0x8002740 *)
mov r6 L0x2001b9e0;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bde0; Value = 0xfffe95c6; PC = 0x8002744 *)
mov r7 L0x2001bde0;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 274 ********************)

ghost c0134@sint32,c0390@sint32,c0646@sint32,c0902@sint32 :
and [c0134=r4, c0390=r5, c0646=r6, c0902=r7] && true;

ecut and [
eqmod (c0134*x**134+c0390*x**390+c0646*x**646+c0902*x**902)
      (4*L0x2001b1e0*x**134) [1043969, x**256 - 1],
eqmod (c0134*x**134+c0390*x**390+c0646*x**646+c0902*x**902)
      (4*L0x2001b5e0*x**134) [1043969, x**256 + 1],
eqmod (c0134*x**134+c0390*x**390+c0646*x**646+c0902*x**902)
      (4*L0x2001b9e0*x**134) [1043969, x**256 - 554923],
eqmod (c0134*x**134+c0390*x**390+c0646*x**646+c0902*x**902)
      (4*L0x2001bde0*x**134) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf134@sint32 : cf134 = r4 && cf134 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf390@sint32 : cf390 = r5 && cf390 = r5;

(******************** CUT 275 ********************)
ecut and [
eqmod 256*cf134 2**32*(c0134+c0643) 1043969,
eqmod 256*cf390 2**32*(c0390+c0899) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200188d4; PC = 0x80027b0 *)
mov L0x200188d4 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018ad4; PC = 0x80027b4 *)
mov L0x20018ad4 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b1e4; Value = 0x0022da5a; PC = 0x8002628 *)
mov r4 L0x2001b1e4;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b5e4; Value = 0xfffe6758; PC = 0x800262c *)
mov r5 L0x2001b5e4;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b9e4; Value = 0x00005baf; PC = 0x8002630 *)
mov r6 L0x2001b9e4;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bde4; Value = 0xfffe8318; PC = 0x8002634 *)
mov r7 L0x2001bde4;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 276 ********************)

ghost c0135@sint32,c0391@sint32,c0647@sint32,c0903@sint32 :
and [c0135=r4, c0391=r5, c0647=r6, c0903=r7] && true;

ecut and [
eqmod (c0135*x**135+c0391*x**391+c0647*x**647+c0903*x**903)
      (4*L0x2001b1e4*x**135) [1043969, x**256 - 1],
eqmod (c0135*x**135+c0391*x**391+c0647*x**647+c0903*x**903)
      (4*L0x2001b5e4*x**135) [1043969, x**256 + 1],
eqmod (c0135*x**135+c0391*x**391+c0647*x**647+c0903*x**903)
      (4*L0x2001b9e4*x**135) [1043969, x**256 - 554923],
eqmod (c0135*x**135+c0391*x**391+c0647*x**647+c0903*x**903)
      (4*L0x2001bde4*x**135) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf135@sint32 : cf135 = r4 && cf135 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf391@sint32 : cf391 = r5 && cf391 = r5;

(******************** CUT 277 ********************)
ecut and [
eqmod 256*cf135 2**32*(c0135+c0644) 1043969,
eqmod 256*cf391 2**32*(c0391+c0900) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200188d6; PC = 0x80026a0 *)
mov L0x200188d6 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018ad6; PC = 0x80026a4 *)
mov L0x20018ad6 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b1e8; Value = 0xfffb2b87; PC = 0x80026b0 *)
mov r4 L0x2001b1e8;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b5e8; Value = 0x00052e7e; PC = 0x80026b4 *)
mov r5 L0x2001b5e8;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b9e8; Value = 0x00057844; PC = 0x80026b8 *)
mov r6 L0x2001b9e8;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bde8; Value = 0x0003039f; PC = 0x80026bc *)
mov r7 L0x2001bde8;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 278 ********************)

ghost c0136@sint32,c0392@sint32,c0648@sint32,c0904@sint32 :
and [c0136=r4, c0392=r5, c0648=r6, c0904=r7] && true;

ecut and [
eqmod (c0136*x**136+c0392*x**392+c0648*x**648+c0904*x**904)
      (4*L0x2001b1e8*x**136) [1043969, x**256 - 1],
eqmod (c0136*x**136+c0392*x**392+c0648*x**648+c0904*x**904)
      (4*L0x2001b5e8*x**136) [1043969, x**256 + 1],
eqmod (c0136*x**136+c0392*x**392+c0648*x**648+c0904*x**904)
      (4*L0x2001b9e8*x**136) [1043969, x**256 - 554923],
eqmod (c0136*x**136+c0392*x**392+c0648*x**648+c0904*x**904)
      (4*L0x2001bde8*x**136) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf136@sint32 : cf136 = r4 && cf136 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf392@sint32 : cf392 = r5 && cf392 = r5;

(******************** CUT 279 ********************)
ecut and [
eqmod 256*cf136 2**32*(c0136+c0645) 1043969,
eqmod 256*cf392 2**32*(c0392+c0901) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200188d8; PC = 0x8002728 *)
mov L0x200188d8 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018ad8; PC = 0x800272c *)
mov L0x20018ad8 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b1ec; Value = 0xfff0878b; PC = 0x8002738 *)
mov r4 L0x2001b1ec;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b5ec; Value = 0x00072409; PC = 0x800273c *)
mov r5 L0x2001b5ec;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b9ec; Value = 0x0005e392; PC = 0x8002740 *)
mov r6 L0x2001b9ec;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bdec; Value = 0xfffcb514; PC = 0x8002744 *)
mov r7 L0x2001bdec;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 280 ********************)

ghost c0137@sint32,c0393@sint32,c0649@sint32,c0905@sint32 :
and [c0137=r4, c0393=r5, c0649=r6, c0905=r7] && true;

ecut and [
eqmod (c0137*x**137+c0393*x**393+c0649*x**649+c0905*x**905)
      (4*L0x2001b1ec*x**137) [1043969, x**256 - 1],
eqmod (c0137*x**137+c0393*x**393+c0649*x**649+c0905*x**905)
      (4*L0x2001b5ec*x**137) [1043969, x**256 + 1],
eqmod (c0137*x**137+c0393*x**393+c0649*x**649+c0905*x**905)
      (4*L0x2001b9ec*x**137) [1043969, x**256 - 554923],
eqmod (c0137*x**137+c0393*x**393+c0649*x**649+c0905*x**905)
      (4*L0x2001bdec*x**137) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf137@sint32 : cf137 = r4 && cf137 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf393@sint32 : cf393 = r5 && cf393 = r5;

(******************** CUT 281 ********************)
ecut and [
eqmod 256*cf137 2**32*(c0137+c0646) 1043969,
eqmod 256*cf393 2**32*(c0393+c0902) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200188da; PC = 0x80027b0 *)
mov L0x200188da r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018ada; PC = 0x80027b4 *)
mov L0x20018ada r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b1f0; Value = 0x000dd776; PC = 0x8002628 *)
mov r4 L0x2001b1f0;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b5f0; Value = 0xfffd9fa1; PC = 0x800262c *)
mov r5 L0x2001b5f0;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b9f0; Value = 0xfff89996; PC = 0x8002630 *)
mov r6 L0x2001b9f0;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bdf0; Value = 0xffffbb02; PC = 0x8002634 *)
mov r7 L0x2001bdf0;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 282 ********************)

ghost c0138@sint32,c0394@sint32,c0650@sint32,c0906@sint32 :
and [c0138=r4, c0394=r5, c0650=r6, c0906=r7] && true;

ecut and [
eqmod (c0138*x**138+c0394*x**394+c0650*x**650+c0906*x**906)
      (4*L0x2001b1f0*x**138) [1043969, x**256 - 1],
eqmod (c0138*x**138+c0394*x**394+c0650*x**650+c0906*x**906)
      (4*L0x2001b5f0*x**138) [1043969, x**256 + 1],
eqmod (c0138*x**138+c0394*x**394+c0650*x**650+c0906*x**906)
      (4*L0x2001b9f0*x**138) [1043969, x**256 - 554923],
eqmod (c0138*x**138+c0394*x**394+c0650*x**650+c0906*x**906)
      (4*L0x2001bdf0*x**138) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf138@sint32 : cf138 = r4 && cf138 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf394@sint32 : cf394 = r5 && cf394 = r5;

(******************** CUT 283 ********************)
ecut and [
eqmod 256*cf138 2**32*(c0138+c0647) 1043969,
eqmod 256*cf394 2**32*(c0394+c0903) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200188dc; PC = 0x80026a0 *)
mov L0x200188dc r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018adc; PC = 0x80026a4 *)
mov L0x20018adc r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b1f4; Value = 0x000809f2; PC = 0x80026b0 *)
mov r4 L0x2001b1f4;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b5f4; Value = 0x00031116; PC = 0x80026b4 *)
mov r5 L0x2001b5f4;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001b9f4; Value = 0x0006c2d6; PC = 0x80026b8 *)
mov r6 L0x2001b9f4;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bdf4; Value = 0xfffb724a; PC = 0x80026bc *)
mov r7 L0x2001bdf4;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 284 ********************)

ghost c0139@sint32,c0395@sint32,c0651@sint32,c0907@sint32 :
and [c0139=r4, c0395=r5, c0651=r6, c0907=r7] && true;

ecut and [
eqmod (c0139*x**139+c0395*x**395+c0651*x**651+c0907*x**907)
      (4*L0x2001b1f4*x**139) [1043969, x**256 - 1],
eqmod (c0139*x**139+c0395*x**395+c0651*x**651+c0907*x**907)
      (4*L0x2001b5f4*x**139) [1043969, x**256 + 1],
eqmod (c0139*x**139+c0395*x**395+c0651*x**651+c0907*x**907)
      (4*L0x2001b9f4*x**139) [1043969, x**256 - 554923],
eqmod (c0139*x**139+c0395*x**395+c0651*x**651+c0907*x**907)
      (4*L0x2001bdf4*x**139) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf139@sint32 : cf139 = r4 && cf139 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf395@sint32 : cf395 = r5 && cf395 = r5;

(******************** CUT 285 ********************)
ecut and [
eqmod 256*cf139 2**32*(c0139+c0648) 1043969,
eqmod 256*cf395 2**32*(c0395+c0904) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200188de; PC = 0x8002728 *)
mov L0x200188de r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018ade; PC = 0x800272c *)
mov L0x20018ade r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b1f8; Value = 0x00013b40; PC = 0x8002738 *)
mov r4 L0x2001b1f8;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b5f8; Value = 0xfffcccdb; PC = 0x800273c *)
mov r5 L0x2001b5f8;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001b9f8; Value = 0x00008a83; PC = 0x8002740 *)
mov r6 L0x2001b9f8;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bdf8; Value = 0xfffaace7; PC = 0x8002744 *)
mov r7 L0x2001bdf8;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 286 ********************)

ghost c0140@sint32,c0396@sint32,c0652@sint32,c0908@sint32 :
and [c0140=r4, c0396=r5, c0652=r6, c0908=r7] && true;

ecut and [
eqmod (c0140*x**140+c0396*x**396+c0652*x**652+c0908*x**908)
      (4*L0x2001b1f8*x**140) [1043969, x**256 - 1],
eqmod (c0140*x**140+c0396*x**396+c0652*x**652+c0908*x**908)
      (4*L0x2001b5f8*x**140) [1043969, x**256 + 1],
eqmod (c0140*x**140+c0396*x**396+c0652*x**652+c0908*x**908)
      (4*L0x2001b9f8*x**140) [1043969, x**256 - 554923],
eqmod (c0140*x**140+c0396*x**396+c0652*x**652+c0908*x**908)
      (4*L0x2001bdf8*x**140) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf140@sint32 : cf140 = r4 && cf140 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf396@sint32 : cf396 = r5 && cf396 = r5;

(******************** CUT 287 ********************)
ecut and [
eqmod 256*cf140 2**32*(c0140+c0649) 1043969,
eqmod 256*cf396 2**32*(c0396+c0905) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200188e0; PC = 0x80027b0 *)
mov L0x200188e0 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018ae0; PC = 0x80027b4 *)
mov L0x20018ae0 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b1fc; Value = 0xffff5c73; PC = 0x8002628 *)
mov r4 L0x2001b1fc;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b5fc; Value = 0x00028602; PC = 0x800262c *)
mov r5 L0x2001b5fc;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001b9fc; Value = 0xfff9e952; PC = 0x8002630 *)
mov r6 L0x2001b9fc;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bdfc; Value = 0x00043788; PC = 0x8002634 *)
mov r7 L0x2001bdfc;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 288 ********************)

ghost c0141@sint32,c0397@sint32,c0653@sint32,c0909@sint32 :
and [c0141=r4, c0397=r5, c0653=r6, c0909=r7] && true;

ecut and [
eqmod (c0141*x**141+c0397*x**397+c0653*x**653+c0909*x**909)
      (4*L0x2001b1fc*x**141) [1043969, x**256 - 1],
eqmod (c0141*x**141+c0397*x**397+c0653*x**653+c0909*x**909)
      (4*L0x2001b5fc*x**141) [1043969, x**256 + 1],
eqmod (c0141*x**141+c0397*x**397+c0653*x**653+c0909*x**909)
      (4*L0x2001b9fc*x**141) [1043969, x**256 - 554923],
eqmod (c0141*x**141+c0397*x**397+c0653*x**653+c0909*x**909)
      (4*L0x2001bdfc*x**141) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf141@sint32 : cf141 = r4 && cf141 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf397@sint32 : cf397 = r5 && cf397 = r5;

(******************** CUT 289 ********************)
ecut and [
eqmod 256*cf141 2**32*(c0141+c0650) 1043969,
eqmod 256*cf397 2**32*(c0397+c0906) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200188e2; PC = 0x80026a0 *)
mov L0x200188e2 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018ae2; PC = 0x80026a4 *)
mov L0x20018ae2 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b200; Value = 0x0003aa46; PC = 0x80026b0 *)
mov r4 L0x2001b200;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b600; Value = 0x00063df7; PC = 0x80026b4 *)
mov r5 L0x2001b600;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001ba00; Value = 0x00066422; PC = 0x80026b8 *)
mov r6 L0x2001ba00;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001be00; Value = 0xfffeab97; PC = 0x80026bc *)
mov r7 L0x2001be00;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 290 ********************)

ghost c0142@sint32,c0398@sint32,c0654@sint32,c0910@sint32 :
and [c0142=r4, c0398=r5, c0654=r6, c0910=r7] && true;

ecut and [
eqmod (c0142*x**142+c0398*x**398+c0654*x**654+c0910*x**910)
      (4*L0x2001b200*x**142) [1043969, x**256 - 1],
eqmod (c0142*x**142+c0398*x**398+c0654*x**654+c0910*x**910)
      (4*L0x2001b600*x**142) [1043969, x**256 + 1],
eqmod (c0142*x**142+c0398*x**398+c0654*x**654+c0910*x**910)
      (4*L0x2001ba00*x**142) [1043969, x**256 - 554923],
eqmod (c0142*x**142+c0398*x**398+c0654*x**654+c0910*x**910)
      (4*L0x2001be00*x**142) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf142@sint32 : cf142 = r4 && cf142 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf398@sint32 : cf398 = r5 && cf398 = r5;

(******************** CUT 291 ********************)
ecut and [
eqmod 256*cf142 2**32*(c0142+c0651) 1043969,
eqmod 256*cf398 2**32*(c0398+c0907) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200188e4; PC = 0x8002728 *)
mov L0x200188e4 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018ae4; PC = 0x800272c *)
mov L0x20018ae4 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b204; Value = 0xfff3f0f7; PC = 0x8002738 *)
mov r4 L0x2001b204;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b604; Value = 0x0003a32e; PC = 0x800273c *)
mov r5 L0x2001b604;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001ba04; Value = 0xfffb6658; PC = 0x8002740 *)
mov r6 L0x2001ba04;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001be04; Value = 0x000735b8; PC = 0x8002744 *)
mov r7 L0x2001be04;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 292 ********************)

ghost c0143@sint32,c0399@sint32,c0655@sint32,c0911@sint32 :
and [c0143=r4, c0399=r5, c0655=r6, c0911=r7] && true;

ecut and [
eqmod (c0143*x**143+c0399*x**399+c0655*x**655+c0911*x**911)
      (4*L0x2001b204*x**143) [1043969, x**256 - 1],
eqmod (c0143*x**143+c0399*x**399+c0655*x**655+c0911*x**911)
      (4*L0x2001b604*x**143) [1043969, x**256 + 1],
eqmod (c0143*x**143+c0399*x**399+c0655*x**655+c0911*x**911)
      (4*L0x2001ba04*x**143) [1043969, x**256 - 554923],
eqmod (c0143*x**143+c0399*x**399+c0655*x**655+c0911*x**911)
      (4*L0x2001be04*x**143) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf143@sint32 : cf143 = r4 && cf143 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf399@sint32 : cf399 = r5 && cf399 = r5;

(******************** CUT 293 ********************)
ecut and [
eqmod 256*cf143 2**32*(c0143+c0652) 1043969,
eqmod 256*cf399 2**32*(c0399+c0908) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200188e6; PC = 0x80027b0 *)
mov L0x200188e6 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018ae6; PC = 0x80027b4 *)
mov L0x20018ae6 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b208; Value = 0x000f89f1; PC = 0x8002628 *)
mov r4 L0x2001b208;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b608; Value = 0xfffe1161; PC = 0x800262c *)
mov r5 L0x2001b608;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001ba08; Value = 0x000644d0; PC = 0x8002630 *)
mov r6 L0x2001ba08;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001be08; Value = 0xfffbf930; PC = 0x8002634 *)
mov r7 L0x2001be08;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 294 ********************)

ghost c0144@sint32,c0400@sint32,c0656@sint32,c0912@sint32 :
and [c0144=r4, c0400=r5, c0656=r6, c0912=r7] && true;

ecut and [
eqmod (c0144*x**144+c0400*x**400+c0656*x**656+c0912*x**912)
      (4*L0x2001b208*x**144) [1043969, x**256 - 1],
eqmod (c0144*x**144+c0400*x**400+c0656*x**656+c0912*x**912)
      (4*L0x2001b608*x**144) [1043969, x**256 + 1],
eqmod (c0144*x**144+c0400*x**400+c0656*x**656+c0912*x**912)
      (4*L0x2001ba08*x**144) [1043969, x**256 - 554923],
eqmod (c0144*x**144+c0400*x**400+c0656*x**656+c0912*x**912)
      (4*L0x2001be08*x**144) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf144@sint32 : cf144 = r4 && cf144 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf400@sint32 : cf400 = r5 && cf400 = r5;

(******************** CUT 295 ********************)
ecut and [
eqmod 256*cf144 2**32*(c0144+c0653) 1043969,
eqmod 256*cf400 2**32*(c0400+c0909) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200188e8; PC = 0x80026a0 *)
mov L0x200188e8 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018ae8; PC = 0x80026a4 *)
mov L0x20018ae8 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b20c; Value = 0x00031b8d; PC = 0x80026b0 *)
mov r4 L0x2001b20c;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b60c; Value = 0xfff8caee; PC = 0x80026b4 *)
mov r5 L0x2001b60c;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001ba0c; Value = 0x0001d0b9; PC = 0x80026b8 *)
mov r6 L0x2001ba0c;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001be0c; Value = 0xfffb1d87; PC = 0x80026bc *)
mov r7 L0x2001be0c;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 296 ********************)

ghost c0145@sint32,c0401@sint32,c0657@sint32,c0913@sint32 :
and [c0145=r4, c0401=r5, c0657=r6, c0913=r7] && true;

ecut and [
eqmod (c0145*x**145+c0401*x**401+c0657*x**657+c0913*x**913)
      (4*L0x2001b20c*x**145) [1043969, x**256 - 1],
eqmod (c0145*x**145+c0401*x**401+c0657*x**657+c0913*x**913)
      (4*L0x2001b60c*x**145) [1043969, x**256 + 1],
eqmod (c0145*x**145+c0401*x**401+c0657*x**657+c0913*x**913)
      (4*L0x2001ba0c*x**145) [1043969, x**256 - 554923],
eqmod (c0145*x**145+c0401*x**401+c0657*x**657+c0913*x**913)
      (4*L0x2001be0c*x**145) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf145@sint32 : cf145 = r4 && cf145 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf401@sint32 : cf401 = r5 && cf401 = r5;

(******************** CUT 297 ********************)
ecut and [
eqmod 256*cf145 2**32*(c0145+c0654) 1043969,
eqmod 256*cf401 2**32*(c0401+c0910) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200188ea; PC = 0x8002728 *)
mov L0x200188ea r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018aea; PC = 0x800272c *)
mov L0x20018aea r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b210; Value = 0x0003d874; PC = 0x8002738 *)
mov r4 L0x2001b210;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b610; Value = 0xfffb668e; PC = 0x800273c *)
mov r5 L0x2001b610;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001ba10; Value = 0x0004033a; PC = 0x8002740 *)
mov r6 L0x2001ba10;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001be10; Value = 0xfffb28bf; PC = 0x8002744 *)
mov r7 L0x2001be10;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 298 ********************)

ghost c0146@sint32,c0402@sint32,c0658@sint32,c0914@sint32 :
and [c0146=r4, c0402=r5, c0658=r6, c0914=r7] && true;

ecut and [
eqmod (c0146*x**146+c0402*x**402+c0658*x**658+c0914*x**914)
      (4*L0x2001b210*x**146) [1043969, x**256 - 1],
eqmod (c0146*x**146+c0402*x**402+c0658*x**658+c0914*x**914)
      (4*L0x2001b610*x**146) [1043969, x**256 + 1],
eqmod (c0146*x**146+c0402*x**402+c0658*x**658+c0914*x**914)
      (4*L0x2001ba10*x**146) [1043969, x**256 - 554923],
eqmod (c0146*x**146+c0402*x**402+c0658*x**658+c0914*x**914)
      (4*L0x2001be10*x**146) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf146@sint32 : cf146 = r4 && cf146 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf402@sint32 : cf402 = r5 && cf402 = r5;

(******************** CUT 299 ********************)
ecut and [
eqmod 256*cf146 2**32*(c0146+c0655) 1043969,
eqmod 256*cf402 2**32*(c0402+c0911) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200188ec; PC = 0x80027b0 *)
mov L0x200188ec r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018aec; PC = 0x80027b4 *)
mov L0x20018aec r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b214; Value = 0x000048a9; PC = 0x8002628 *)
mov r4 L0x2001b214;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b614; Value = 0xfff91b1b; PC = 0x800262c *)
mov r5 L0x2001b614;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001ba14; Value = 0xfffe8917; PC = 0x8002630 *)
mov r6 L0x2001ba14;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001be14; Value = 0xfff9ee2a; PC = 0x8002634 *)
mov r7 L0x2001be14;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 300 ********************)

ghost c0147@sint32,c0403@sint32,c0659@sint32,c0915@sint32 :
and [c0147=r4, c0403=r5, c0659=r6, c0915=r7] && true;

ecut and [
eqmod (c0147*x**147+c0403*x**403+c0659*x**659+c0915*x**915)
      (4*L0x2001b214*x**147) [1043969, x**256 - 1],
eqmod (c0147*x**147+c0403*x**403+c0659*x**659+c0915*x**915)
      (4*L0x2001b614*x**147) [1043969, x**256 + 1],
eqmod (c0147*x**147+c0403*x**403+c0659*x**659+c0915*x**915)
      (4*L0x2001ba14*x**147) [1043969, x**256 - 554923],
eqmod (c0147*x**147+c0403*x**403+c0659*x**659+c0915*x**915)
      (4*L0x2001be14*x**147) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf147@sint32 : cf147 = r4 && cf147 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf403@sint32 : cf403 = r5 && cf403 = r5;

(******************** CUT 301 ********************)
ecut and [
eqmod 256*cf147 2**32*(c0147+c0656) 1043969,
eqmod 256*cf403 2**32*(c0403+c0912) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200188ee; PC = 0x80026a0 *)
mov L0x200188ee r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018aee; PC = 0x80026a4 *)
mov L0x20018aee r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b218; Value = 0xfff9e614; PC = 0x80026b0 *)
mov r4 L0x2001b218;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b618; Value = 0xfffadae4; PC = 0x80026b4 *)
mov r5 L0x2001b618;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001ba18; Value = 0x00049111; PC = 0x80026b8 *)
mov r6 L0x2001ba18;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001be18; Value = 0xfffaa971; PC = 0x80026bc *)
mov r7 L0x2001be18;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 302 ********************)

ghost c0148@sint32,c0404@sint32,c0660@sint32,c0916@sint32 :
and [c0148=r4, c0404=r5, c0660=r6, c0916=r7] && true;

ecut and [
eqmod (c0148*x**148+c0404*x**404+c0660*x**660+c0916*x**916)
      (4*L0x2001b218*x**148) [1043969, x**256 - 1],
eqmod (c0148*x**148+c0404*x**404+c0660*x**660+c0916*x**916)
      (4*L0x2001b618*x**148) [1043969, x**256 + 1],
eqmod (c0148*x**148+c0404*x**404+c0660*x**660+c0916*x**916)
      (4*L0x2001ba18*x**148) [1043969, x**256 - 554923],
eqmod (c0148*x**148+c0404*x**404+c0660*x**660+c0916*x**916)
      (4*L0x2001be18*x**148) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf148@sint32 : cf148 = r4 && cf148 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf404@sint32 : cf404 = r5 && cf404 = r5;

(******************** CUT 303 ********************)
ecut and [
eqmod 256*cf148 2**32*(c0148+c0657) 1043969,
eqmod 256*cf404 2**32*(c0404+c0913) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200188f0; PC = 0x8002728 *)
mov L0x200188f0 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018af0; PC = 0x800272c *)
mov L0x20018af0 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b21c; Value = 0x0014c8fa; PC = 0x8002738 *)
mov r4 L0x2001b21c;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b61c; Value = 0xfffed371; PC = 0x800273c *)
mov r5 L0x2001b61c;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001ba1c; Value = 0x00078cf7; PC = 0x8002740 *)
mov r6 L0x2001ba1c;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001be1c; Value = 0xfffd9e31; PC = 0x8002744 *)
mov r7 L0x2001be1c;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 304 ********************)

ghost c0149@sint32,c0405@sint32,c0661@sint32,c0917@sint32 :
and [c0149=r4, c0405=r5, c0661=r6, c0917=r7] && true;

ecut and [
eqmod (c0149*x**149+c0405*x**405+c0661*x**661+c0917*x**917)
      (4*L0x2001b21c*x**149) [1043969, x**256 - 1],
eqmod (c0149*x**149+c0405*x**405+c0661*x**661+c0917*x**917)
      (4*L0x2001b61c*x**149) [1043969, x**256 + 1],
eqmod (c0149*x**149+c0405*x**405+c0661*x**661+c0917*x**917)
      (4*L0x2001ba1c*x**149) [1043969, x**256 - 554923],
eqmod (c0149*x**149+c0405*x**405+c0661*x**661+c0917*x**917)
      (4*L0x2001be1c*x**149) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf149@sint32 : cf149 = r4 && cf149 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf405@sint32 : cf405 = r5 && cf405 = r5;

(******************** CUT 305 ********************)
ecut and [
eqmod 256*cf149 2**32*(c0149+c0658) 1043969,
eqmod 256*cf405 2**32*(c0405+c0914) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200188f2; PC = 0x80027b0 *)
mov L0x200188f2 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018af2; PC = 0x80027b4 *)
mov L0x20018af2 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b220; Value = 0xfff26dbf; PC = 0x8002628 *)
mov r4 L0x2001b220;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b620; Value = 0x00019c7a; PC = 0x800262c *)
mov r5 L0x2001b620;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001ba20; Value = 0x0004114e; PC = 0x8002630 *)
mov r6 L0x2001ba20;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001be20; Value = 0x0001a0ea; PC = 0x8002634 *)
mov r7 L0x2001be20;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 306 ********************)

ghost c0150@sint32,c0406@sint32,c0662@sint32,c0918@sint32 :
and [c0150=r4, c0406=r5, c0662=r6, c0918=r7] && true;

ecut and [
eqmod (c0150*x**150+c0406*x**406+c0662*x**662+c0918*x**918)
      (4*L0x2001b220*x**150) [1043969, x**256 - 1],
eqmod (c0150*x**150+c0406*x**406+c0662*x**662+c0918*x**918)
      (4*L0x2001b620*x**150) [1043969, x**256 + 1],
eqmod (c0150*x**150+c0406*x**406+c0662*x**662+c0918*x**918)
      (4*L0x2001ba20*x**150) [1043969, x**256 - 554923],
eqmod (c0150*x**150+c0406*x**406+c0662*x**662+c0918*x**918)
      (4*L0x2001be20*x**150) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf150@sint32 : cf150 = r4 && cf150 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf406@sint32 : cf406 = r5 && cf406 = r5;

(******************** CUT 307 ********************)
ecut and [
eqmod 256*cf150 2**32*(c0150+c0659) 1043969,
eqmod 256*cf406 2**32*(c0406+c0915) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200188f4; PC = 0x80026a0 *)
mov L0x200188f4 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018af4; PC = 0x80026a4 *)
mov L0x20018af4 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b224; Value = 0x000326ed; PC = 0x80026b0 *)
mov r4 L0x2001b224;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b624; Value = 0xfffebf5d; PC = 0x80026b4 *)
mov r5 L0x2001b624;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001ba24; Value = 0xfffe2cf0; PC = 0x80026b8 *)
mov r6 L0x2001ba24;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001be24; Value = 0xfff9972c; PC = 0x80026bc *)
mov r7 L0x2001be24;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 308 ********************)

ghost c0151@sint32,c0407@sint32,c0663@sint32,c0919@sint32 :
and [c0151=r4, c0407=r5, c0663=r6, c0919=r7] && true;

ecut and [
eqmod (c0151*x**151+c0407*x**407+c0663*x**663+c0919*x**919)
      (4*L0x2001b224*x**151) [1043969, x**256 - 1],
eqmod (c0151*x**151+c0407*x**407+c0663*x**663+c0919*x**919)
      (4*L0x2001b624*x**151) [1043969, x**256 + 1],
eqmod (c0151*x**151+c0407*x**407+c0663*x**663+c0919*x**919)
      (4*L0x2001ba24*x**151) [1043969, x**256 - 554923],
eqmod (c0151*x**151+c0407*x**407+c0663*x**663+c0919*x**919)
      (4*L0x2001be24*x**151) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf151@sint32 : cf151 = r4 && cf151 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf407@sint32 : cf407 = r5 && cf407 = r5;

(******************** CUT 309 ********************)
ecut and [
eqmod 256*cf151 2**32*(c0151+c0660) 1043969,
eqmod 256*cf407 2**32*(c0407+c0916) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200188f6; PC = 0x8002728 *)
mov L0x200188f6 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018af6; PC = 0x800272c *)
mov L0x20018af6 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b228; Value = 0xffefcf9f; PC = 0x8002738 *)
mov r4 L0x2001b228;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b628; Value = 0x0001d93a; PC = 0x800273c *)
mov r5 L0x2001b628;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001ba28; Value = 0xfffa79fa; PC = 0x8002740 *)
mov r6 L0x2001ba28;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001be28; Value = 0x0005a565; PC = 0x8002744 *)
mov r7 L0x2001be28;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 310 ********************)

ghost c0152@sint32,c0408@sint32,c0664@sint32,c0920@sint32 :
and [c0152=r4, c0408=r5, c0664=r6, c0920=r7] && true;

ecut and [
eqmod (c0152*x**152+c0408*x**408+c0664*x**664+c0920*x**920)
      (4*L0x2001b228*x**152) [1043969, x**256 - 1],
eqmod (c0152*x**152+c0408*x**408+c0664*x**664+c0920*x**920)
      (4*L0x2001b628*x**152) [1043969, x**256 + 1],
eqmod (c0152*x**152+c0408*x**408+c0664*x**664+c0920*x**920)
      (4*L0x2001ba28*x**152) [1043969, x**256 - 554923],
eqmod (c0152*x**152+c0408*x**408+c0664*x**664+c0920*x**920)
      (4*L0x2001be28*x**152) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf152@sint32 : cf152 = r4 && cf152 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf408@sint32 : cf408 = r5 && cf408 = r5;

(******************** CUT 311 ********************)
ecut and [
eqmod 256*cf152 2**32*(c0152+c0661) 1043969,
eqmod 256*cf408 2**32*(c0408+c0917) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200188f8; PC = 0x80027b0 *)
mov L0x200188f8 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018af8; PC = 0x80027b4 *)
mov L0x20018af8 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b22c; Value = 0x00080588; PC = 0x8002628 *)
mov r4 L0x2001b22c;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b62c; Value = 0xfffbca03; PC = 0x800262c *)
mov r5 L0x2001b62c;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001ba2c; Value = 0xfffe0114; PC = 0x8002630 *)
mov r6 L0x2001ba2c;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001be2c; Value = 0xfffeae6e; PC = 0x8002634 *)
mov r7 L0x2001be2c;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 312 ********************)

ghost c0153@sint32,c0409@sint32,c0665@sint32,c0921@sint32 :
and [c0153=r4, c0409=r5, c0665=r6, c0921=r7] && true;

ecut and [
eqmod (c0153*x**153+c0409*x**409+c0665*x**665+c0921*x**921)
      (4*L0x2001b22c*x**153) [1043969, x**256 - 1],
eqmod (c0153*x**153+c0409*x**409+c0665*x**665+c0921*x**921)
      (4*L0x2001b62c*x**153) [1043969, x**256 + 1],
eqmod (c0153*x**153+c0409*x**409+c0665*x**665+c0921*x**921)
      (4*L0x2001ba2c*x**153) [1043969, x**256 - 554923],
eqmod (c0153*x**153+c0409*x**409+c0665*x**665+c0921*x**921)
      (4*L0x2001be2c*x**153) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf153@sint32 : cf153 = r4 && cf153 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf409@sint32 : cf409 = r5 && cf409 = r5;

(******************** CUT 313 ********************)
ecut and [
eqmod 256*cf153 2**32*(c0153+c0662) 1043969,
eqmod 256*cf409 2**32*(c0409+c0918) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200188fa; PC = 0x80026a0 *)
mov L0x200188fa r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018afa; PC = 0x80026a4 *)
mov L0x20018afa r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b230; Value = 0x00124631; PC = 0x80026b0 *)
mov r4 L0x2001b230;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b630; Value = 0x0003935d; PC = 0x80026b4 *)
mov r5 L0x2001b630;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001ba30; Value = 0xfffd3077; PC = 0x80026b8 *)
mov r6 L0x2001ba30;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001be30; Value = 0xfff82b6f; PC = 0x80026bc *)
mov r7 L0x2001be30;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 314 ********************)

ghost c0154@sint32,c0410@sint32,c0666@sint32,c0922@sint32 :
and [c0154=r4, c0410=r5, c0666=r6, c0922=r7] && true;

ecut and [
eqmod (c0154*x**154+c0410*x**410+c0666*x**666+c0922*x**922)
      (4*L0x2001b230*x**154) [1043969, x**256 - 1],
eqmod (c0154*x**154+c0410*x**410+c0666*x**666+c0922*x**922)
      (4*L0x2001b630*x**154) [1043969, x**256 + 1],
eqmod (c0154*x**154+c0410*x**410+c0666*x**666+c0922*x**922)
      (4*L0x2001ba30*x**154) [1043969, x**256 - 554923],
eqmod (c0154*x**154+c0410*x**410+c0666*x**666+c0922*x**922)
      (4*L0x2001be30*x**154) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf154@sint32 : cf154 = r4 && cf154 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf410@sint32 : cf410 = r5 && cf410 = r5;

(******************** CUT 315 ********************)
ecut and [
eqmod 256*cf154 2**32*(c0154+c0663) 1043969,
eqmod 256*cf410 2**32*(c0410+c0919) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200188fc; PC = 0x8002728 *)
mov L0x200188fc r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018afc; PC = 0x800272c *)
mov L0x20018afc r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b234; Value = 0xfff480ae; PC = 0x8002738 *)
mov r4 L0x2001b234;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b634; Value = 0x00000ef6; PC = 0x800273c *)
mov r5 L0x2001b634;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001ba34; Value = 0x0007a01a; PC = 0x8002740 *)
mov r6 L0x2001ba34;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001be34; Value = 0x0004db20; PC = 0x8002744 *)
mov r7 L0x2001be34;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 316 ********************)

ghost c0155@sint32,c0411@sint32,c0667@sint32,c0923@sint32 :
and [c0155=r4, c0411=r5, c0667=r6, c0923=r7] && true;

ecut and [
eqmod (c0155*x**155+c0411*x**411+c0667*x**667+c0923*x**923)
      (4*L0x2001b234*x**155) [1043969, x**256 - 1],
eqmod (c0155*x**155+c0411*x**411+c0667*x**667+c0923*x**923)
      (4*L0x2001b634*x**155) [1043969, x**256 + 1],
eqmod (c0155*x**155+c0411*x**411+c0667*x**667+c0923*x**923)
      (4*L0x2001ba34*x**155) [1043969, x**256 - 554923],
eqmod (c0155*x**155+c0411*x**411+c0667*x**667+c0923*x**923)
      (4*L0x2001be34*x**155) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf155@sint32 : cf155 = r4 && cf155 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf411@sint32 : cf411 = r5 && cf411 = r5;

(******************** CUT 317 ********************)
ecut and [
eqmod 256*cf155 2**32*(c0155+c0664) 1043969,
eqmod 256*cf411 2**32*(c0411+c0920) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200188fe; PC = 0x80027b0 *)
mov L0x200188fe r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018afe; PC = 0x80027b4 *)
mov L0x20018afe r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b238; Value = 0x0006ae0a; PC = 0x8002628 *)
mov r4 L0x2001b238;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b638; Value = 0x0002e69c; PC = 0x800262c *)
mov r5 L0x2001b638;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001ba38; Value = 0xfff81de2; PC = 0x8002630 *)
mov r6 L0x2001ba38;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001be38; Value = 0x00011422; PC = 0x8002634 *)
mov r7 L0x2001be38;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 318 ********************)

ghost c0156@sint32,c0412@sint32,c0668@sint32,c0924@sint32 :
and [c0156=r4, c0412=r5, c0668=r6, c0924=r7] && true;

ecut and [
eqmod (c0156*x**156+c0412*x**412+c0668*x**668+c0924*x**924)
      (4*L0x2001b238*x**156) [1043969, x**256 - 1],
eqmod (c0156*x**156+c0412*x**412+c0668*x**668+c0924*x**924)
      (4*L0x2001b638*x**156) [1043969, x**256 + 1],
eqmod (c0156*x**156+c0412*x**412+c0668*x**668+c0924*x**924)
      (4*L0x2001ba38*x**156) [1043969, x**256 - 554923],
eqmod (c0156*x**156+c0412*x**412+c0668*x**668+c0924*x**924)
      (4*L0x2001be38*x**156) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf156@sint32 : cf156 = r4 && cf156 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf412@sint32 : cf412 = r5 && cf412 = r5;

(******************** CUT 319 ********************)
ecut and [
eqmod 256*cf156 2**32*(c0156+c0665) 1043969,
eqmod 256*cf412 2**32*(c0412+c0921) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018900; PC = 0x80026a0 *)
mov L0x20018900 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b00; PC = 0x80026a4 *)
mov L0x20018b00 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b23c; Value = 0xfffeebea; PC = 0x80026b0 *)
mov r4 L0x2001b23c;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b63c; Value = 0x0000aee5; PC = 0x80026b4 *)
mov r5 L0x2001b63c;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001ba3c; Value = 0xfffc8bb1; PC = 0x80026b8 *)
mov r6 L0x2001ba3c;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001be3c; Value = 0x0007ac4d; PC = 0x80026bc *)
mov r7 L0x2001be3c;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 320 ********************)

ghost c0157@sint32,c0413@sint32,c0669@sint32,c0925@sint32 :
and [c0157=r4, c0413=r5, c0669=r6, c0925=r7] && true;

ecut and [
eqmod (c0157*x**157+c0413*x**413+c0669*x**669+c0925*x**925)
      (4*L0x2001b23c*x**157) [1043969, x**256 - 1],
eqmod (c0157*x**157+c0413*x**413+c0669*x**669+c0925*x**925)
      (4*L0x2001b63c*x**157) [1043969, x**256 + 1],
eqmod (c0157*x**157+c0413*x**413+c0669*x**669+c0925*x**925)
      (4*L0x2001ba3c*x**157) [1043969, x**256 - 554923],
eqmod (c0157*x**157+c0413*x**413+c0669*x**669+c0925*x**925)
      (4*L0x2001be3c*x**157) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf157@sint32 : cf157 = r4 && cf157 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf413@sint32 : cf413 = r5 && cf413 = r5;

(******************** CUT 321 ********************)
ecut and [
eqmod 256*cf157 2**32*(c0157+c0666) 1043969,
eqmod 256*cf413 2**32*(c0413+c0922) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018902; PC = 0x8002728 *)
mov L0x20018902 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b02; PC = 0x800272c *)
mov L0x20018b02 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b240; Value = 0x000d72bf; PC = 0x8002738 *)
mov r4 L0x2001b240;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b640; Value = 0xfff8d27e; PC = 0x800273c *)
mov r5 L0x2001b640;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001ba40; Value = 0xfff827ae; PC = 0x8002740 *)
mov r6 L0x2001ba40;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001be40; Value = 0x0002a7bf; PC = 0x8002744 *)
mov r7 L0x2001be40;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 322 ********************)

ghost c0158@sint32,c0414@sint32,c0670@sint32,c0926@sint32 :
and [c0158=r4, c0414=r5, c0670=r6, c0926=r7] && true;

ecut and [
eqmod (c0158*x**158+c0414*x**414+c0670*x**670+c0926*x**926)
      (4*L0x2001b240*x**158) [1043969, x**256 - 1],
eqmod (c0158*x**158+c0414*x**414+c0670*x**670+c0926*x**926)
      (4*L0x2001b640*x**158) [1043969, x**256 + 1],
eqmod (c0158*x**158+c0414*x**414+c0670*x**670+c0926*x**926)
      (4*L0x2001ba40*x**158) [1043969, x**256 - 554923],
eqmod (c0158*x**158+c0414*x**414+c0670*x**670+c0926*x**926)
      (4*L0x2001be40*x**158) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf158@sint32 : cf158 = r4 && cf158 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf414@sint32 : cf414 = r5 && cf414 = r5;

(******************** CUT 323 ********************)
ecut and [
eqmod 256*cf158 2**32*(c0158+c0667) 1043969,
eqmod 256*cf414 2**32*(c0414+c0923) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018904; PC = 0x80027b0 *)
mov L0x20018904 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018b04; PC = 0x80027b4 *)
mov L0x20018b04 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b244; Value = 0x0002bf53; PC = 0x8002628 *)
mov r4 L0x2001b244;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b644; Value = 0xfff8c249; PC = 0x800262c *)
mov r5 L0x2001b644;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001ba44; Value = 0x00015d39; PC = 0x8002630 *)
mov r6 L0x2001ba44;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001be44; Value = 0x0004b1dd; PC = 0x8002634 *)
mov r7 L0x2001be44;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 324 ********************)

ghost c0159@sint32,c0415@sint32,c0671@sint32,c0927@sint32 :
and [c0159=r4, c0415=r5, c0671=r6, c0927=r7] && true;

ecut and [
eqmod (c0159*x**159+c0415*x**415+c0671*x**671+c0927*x**927)
      (4*L0x2001b244*x**159) [1043969, x**256 - 1],
eqmod (c0159*x**159+c0415*x**415+c0671*x**671+c0927*x**927)
      (4*L0x2001b644*x**159) [1043969, x**256 + 1],
eqmod (c0159*x**159+c0415*x**415+c0671*x**671+c0927*x**927)
      (4*L0x2001ba44*x**159) [1043969, x**256 - 554923],
eqmod (c0159*x**159+c0415*x**415+c0671*x**671+c0927*x**927)
      (4*L0x2001be44*x**159) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf159@sint32 : cf159 = r4 && cf159 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf415@sint32 : cf415 = r5 && cf415 = r5;

(******************** CUT 325 ********************)
ecut and [
eqmod 256*cf159 2**32*(c0159+c0668) 1043969,
eqmod 256*cf415 2**32*(c0415+c0924) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018906; PC = 0x80026a0 *)
mov L0x20018906 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b06; PC = 0x80026a4 *)
mov L0x20018b06 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b248; Value = 0x001091c6; PC = 0x80026b0 *)
mov r4 L0x2001b248;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b648; Value = 0xffff2342; PC = 0x80026b4 *)
mov r5 L0x2001b648;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001ba48; Value = 0x000219e0; PC = 0x80026b8 *)
mov r6 L0x2001ba48;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001be48; Value = 0xfff8bc43; PC = 0x80026bc *)
mov r7 L0x2001be48;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 326 ********************)

ghost c0160@sint32,c0416@sint32,c0672@sint32,c0928@sint32 :
and [c0160=r4, c0416=r5, c0672=r6, c0928=r7] && true;

ecut and [
eqmod (c0160*x**160+c0416*x**416+c0672*x**672+c0928*x**928)
      (4*L0x2001b248*x**160) [1043969, x**256 - 1],
eqmod (c0160*x**160+c0416*x**416+c0672*x**672+c0928*x**928)
      (4*L0x2001b648*x**160) [1043969, x**256 + 1],
eqmod (c0160*x**160+c0416*x**416+c0672*x**672+c0928*x**928)
      (4*L0x2001ba48*x**160) [1043969, x**256 - 554923],
eqmod (c0160*x**160+c0416*x**416+c0672*x**672+c0928*x**928)
      (4*L0x2001be48*x**160) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf160@sint32 : cf160 = r4 && cf160 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf416@sint32 : cf416 = r5 && cf416 = r5;

(******************** CUT 327 ********************)
ecut and [
eqmod 256*cf160 2**32*(c0160+c0669) 1043969,
eqmod 256*cf416 2**32*(c0416+c0925) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018908; PC = 0x8002728 *)
mov L0x20018908 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b08; PC = 0x800272c *)
mov L0x20018b08 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b24c; Value = 0x000d0571; PC = 0x8002738 *)
mov r4 L0x2001b24c;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b64c; Value = 0xfffa6536; PC = 0x800273c *)
mov r5 L0x2001b64c;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001ba4c; Value = 0xfffc6f67; PC = 0x8002740 *)
mov r6 L0x2001ba4c;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001be4c; Value = 0x000380f3; PC = 0x8002744 *)
mov r7 L0x2001be4c;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 328 ********************)

ghost c0161@sint32,c0417@sint32,c0673@sint32,c0929@sint32 :
and [c0161=r4, c0417=r5, c0673=r6, c0929=r7] && true;

ecut and [
eqmod (c0161*x**161+c0417*x**417+c0673*x**673+c0929*x**929)
      (4*L0x2001b24c*x**161) [1043969, x**256 - 1],
eqmod (c0161*x**161+c0417*x**417+c0673*x**673+c0929*x**929)
      (4*L0x2001b64c*x**161) [1043969, x**256 + 1],
eqmod (c0161*x**161+c0417*x**417+c0673*x**673+c0929*x**929)
      (4*L0x2001ba4c*x**161) [1043969, x**256 - 554923],
eqmod (c0161*x**161+c0417*x**417+c0673*x**673+c0929*x**929)
      (4*L0x2001be4c*x**161) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf161@sint32 : cf161 = r4 && cf161 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf417@sint32 : cf417 = r5 && cf417 = r5;

(******************** CUT 329 ********************)
ecut and [
eqmod 256*cf161 2**32*(c0161+c0670) 1043969,
eqmod 256*cf417 2**32*(c0417+c0926) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x2001890a; PC = 0x80027b0 *)
mov L0x2001890a r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018b0a; PC = 0x80027b4 *)
mov L0x20018b0a r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b250; Value = 0xfffecbc9; PC = 0x8002628 *)
mov r4 L0x2001b250;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b650; Value = 0xfffa30b0; PC = 0x800262c *)
mov r5 L0x2001b650;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001ba50; Value = 0xfffb196f; PC = 0x8002630 *)
mov r6 L0x2001ba50;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001be50; Value = 0x0002168d; PC = 0x8002634 *)
mov r7 L0x2001be50;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 330 ********************)

ghost c0162@sint32,c0418@sint32,c0674@sint32,c0930@sint32 :
and [c0162=r4, c0418=r5, c0674=r6, c0930=r7] && true;

ecut and [
eqmod (c0162*x**162+c0418*x**418+c0674*x**674+c0930*x**930)
      (4*L0x2001b250*x**162) [1043969, x**256 - 1],
eqmod (c0162*x**162+c0418*x**418+c0674*x**674+c0930*x**930)
      (4*L0x2001b650*x**162) [1043969, x**256 + 1],
eqmod (c0162*x**162+c0418*x**418+c0674*x**674+c0930*x**930)
      (4*L0x2001ba50*x**162) [1043969, x**256 - 554923],
eqmod (c0162*x**162+c0418*x**418+c0674*x**674+c0930*x**930)
      (4*L0x2001be50*x**162) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf162@sint32 : cf162 = r4 && cf162 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf418@sint32 : cf418 = r5 && cf418 = r5;

(******************** CUT 331 ********************)
ecut and [
eqmod 256*cf162 2**32*(c0162+c0671) 1043969,
eqmod 256*cf418 2**32*(c0418+c0927) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x2001890c; PC = 0x80026a0 *)
mov L0x2001890c r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b0c; PC = 0x80026a4 *)
mov L0x20018b0c r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b254; Value = 0x0018ac5a; PC = 0x80026b0 *)
mov r4 L0x2001b254;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b654; Value = 0xfff93ae9; PC = 0x80026b4 *)
mov r5 L0x2001b654;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001ba54; Value = 0xfffe3a5e; PC = 0x80026b8 *)
mov r6 L0x2001ba54;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001be54; Value = 0x0005d5b8; PC = 0x80026bc *)
mov r7 L0x2001be54;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 332 ********************)

ghost c0163@sint32,c0419@sint32,c0675@sint32,c0931@sint32 :
and [c0163=r4, c0419=r5, c0675=r6, c0931=r7] && true;

ecut and [
eqmod (c0163*x**163+c0419*x**419+c0675*x**675+c0931*x**931)
      (4*L0x2001b254*x**163) [1043969, x**256 - 1],
eqmod (c0163*x**163+c0419*x**419+c0675*x**675+c0931*x**931)
      (4*L0x2001b654*x**163) [1043969, x**256 + 1],
eqmod (c0163*x**163+c0419*x**419+c0675*x**675+c0931*x**931)
      (4*L0x2001ba54*x**163) [1043969, x**256 - 554923],
eqmod (c0163*x**163+c0419*x**419+c0675*x**675+c0931*x**931)
      (4*L0x2001be54*x**163) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf163@sint32 : cf163 = r4 && cf163 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf419@sint32 : cf419 = r5 && cf419 = r5;

(******************** CUT 333 ********************)
ecut and [
eqmod 256*cf163 2**32*(c0163+c0672) 1043969,
eqmod 256*cf419 2**32*(c0419+c0928) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x2001890e; PC = 0x8002728 *)
mov L0x2001890e r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b0e; PC = 0x800272c *)
mov L0x20018b0e r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b258; Value = 0x0008fcba; PC = 0x8002738 *)
mov r4 L0x2001b258;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b658; Value = 0xfff97777; PC = 0x800273c *)
mov r5 L0x2001b658;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001ba58; Value = 0xfffb98b4; PC = 0x8002740 *)
mov r6 L0x2001ba58;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001be58; Value = 0x00043a0b; PC = 0x8002744 *)
mov r7 L0x2001be58;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 334 ********************)

ghost c0164@sint32,c0420@sint32,c0676@sint32,c0932@sint32 :
and [c0164=r4, c0420=r5, c0676=r6, c0932=r7] && true;

ecut and [
eqmod (c0164*x**164+c0420*x**420+c0676*x**676+c0932*x**932)
      (4*L0x2001b258*x**164) [1043969, x**256 - 1],
eqmod (c0164*x**164+c0420*x**420+c0676*x**676+c0932*x**932)
      (4*L0x2001b658*x**164) [1043969, x**256 + 1],
eqmod (c0164*x**164+c0420*x**420+c0676*x**676+c0932*x**932)
      (4*L0x2001ba58*x**164) [1043969, x**256 - 554923],
eqmod (c0164*x**164+c0420*x**420+c0676*x**676+c0932*x**932)
      (4*L0x2001be58*x**164) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf164@sint32 : cf164 = r4 && cf164 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf420@sint32 : cf420 = r5 && cf420 = r5;

(******************** CUT 335 ********************)
ecut and [
eqmod 256*cf164 2**32*(c0164+c0673) 1043969,
eqmod 256*cf420 2**32*(c0420+c0929) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018910; PC = 0x80027b0 *)
mov L0x20018910 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018b10; PC = 0x80027b4 *)
mov L0x20018b10 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b25c; Value = 0x000aad0c; PC = 0x8002628 *)
mov r4 L0x2001b25c;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b65c; Value = 0xfffca40e; PC = 0x800262c *)
mov r5 L0x2001b65c;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001ba5c; Value = 0xfffde5ac; PC = 0x8002630 *)
mov r6 L0x2001ba5c;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001be5c; Value = 0x00060950; PC = 0x8002634 *)
mov r7 L0x2001be5c;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 336 ********************)

ghost c0165@sint32,c0421@sint32,c0677@sint32,c0933@sint32 :
and [c0165=r4, c0421=r5, c0677=r6, c0933=r7] && true;

ecut and [
eqmod (c0165*x**165+c0421*x**421+c0677*x**677+c0933*x**933)
      (4*L0x2001b25c*x**165) [1043969, x**256 - 1],
eqmod (c0165*x**165+c0421*x**421+c0677*x**677+c0933*x**933)
      (4*L0x2001b65c*x**165) [1043969, x**256 + 1],
eqmod (c0165*x**165+c0421*x**421+c0677*x**677+c0933*x**933)
      (4*L0x2001ba5c*x**165) [1043969, x**256 - 554923],
eqmod (c0165*x**165+c0421*x**421+c0677*x**677+c0933*x**933)
      (4*L0x2001be5c*x**165) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf165@sint32 : cf165 = r4 && cf165 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf421@sint32 : cf421 = r5 && cf421 = r5;

(******************** CUT 337 ********************)
ecut and [
eqmod 256*cf165 2**32*(c0165+c0674) 1043969,
eqmod 256*cf421 2**32*(c0421+c0930) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018912; PC = 0x80026a0 *)
mov L0x20018912 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b12; PC = 0x80026a4 *)
mov L0x20018b12 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b260; Value = 0xfffb45d5; PC = 0x80026b0 *)
mov r4 L0x2001b260;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b660; Value = 0x00058184; PC = 0x80026b4 *)
mov r5 L0x2001b660;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001ba60; Value = 0x0002ecd0; PC = 0x80026b8 *)
mov r6 L0x2001ba60;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001be60; Value = 0xfffb1064; PC = 0x80026bc *)
mov r7 L0x2001be60;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 338 ********************)

ghost c0166@sint32,c0422@sint32,c0678@sint32,c0934@sint32 :
and [c0166=r4, c0422=r5, c0678=r6, c0934=r7] && true;

ecut and [
eqmod (c0166*x**166+c0422*x**422+c0678*x**678+c0934*x**934)
      (4*L0x2001b260*x**166) [1043969, x**256 - 1],
eqmod (c0166*x**166+c0422*x**422+c0678*x**678+c0934*x**934)
      (4*L0x2001b660*x**166) [1043969, x**256 + 1],
eqmod (c0166*x**166+c0422*x**422+c0678*x**678+c0934*x**934)
      (4*L0x2001ba60*x**166) [1043969, x**256 - 554923],
eqmod (c0166*x**166+c0422*x**422+c0678*x**678+c0934*x**934)
      (4*L0x2001be60*x**166) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf166@sint32 : cf166 = r4 && cf166 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf422@sint32 : cf422 = r5 && cf422 = r5;

(******************** CUT 339 ********************)
ecut and [
eqmod 256*cf166 2**32*(c0166+c0675) 1043969,
eqmod 256*cf422 2**32*(c0422+c0931) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018914; PC = 0x8002728 *)
mov L0x20018914 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b14; PC = 0x800272c *)
mov L0x20018b14 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b264; Value = 0xffde28bd; PC = 0x8002738 *)
mov r4 L0x2001b264;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b664; Value = 0x00034f33; PC = 0x800273c *)
mov r5 L0x2001b664;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001ba64; Value = 0xffff2164; PC = 0x8002740 *)
mov r6 L0x2001ba64;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001be64; Value = 0x00064fe0; PC = 0x8002744 *)
mov r7 L0x2001be64;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 340 ********************)

ghost c0167@sint32,c0423@sint32,c0679@sint32,c0935@sint32 :
and [c0167=r4, c0423=r5, c0679=r6, c0935=r7] && true;

ecut and [
eqmod (c0167*x**167+c0423*x**423+c0679*x**679+c0935*x**935)
      (4*L0x2001b264*x**167) [1043969, x**256 - 1],
eqmod (c0167*x**167+c0423*x**423+c0679*x**679+c0935*x**935)
      (4*L0x2001b664*x**167) [1043969, x**256 + 1],
eqmod (c0167*x**167+c0423*x**423+c0679*x**679+c0935*x**935)
      (4*L0x2001ba64*x**167) [1043969, x**256 - 554923],
eqmod (c0167*x**167+c0423*x**423+c0679*x**679+c0935*x**935)
      (4*L0x2001be64*x**167) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf167@sint32 : cf167 = r4 && cf167 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf423@sint32 : cf423 = r5 && cf423 = r5;

(******************** CUT 341 ********************)
ecut and [
eqmod 256*cf167 2**32*(c0167+c0676) 1043969,
eqmod 256*cf423 2**32*(c0423+c0932) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018916; PC = 0x80027b0 *)
mov L0x20018916 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018b16; PC = 0x80027b4 *)
mov L0x20018b16 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b268; Value = 0xfff17a64; PC = 0x8002628 *)
mov r4 L0x2001b268;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b668; Value = 0x00045d60; PC = 0x800262c *)
mov r5 L0x2001b668;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001ba68; Value = 0xfffd6afc; PC = 0x8002630 *)
mov r6 L0x2001ba68;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001be68; Value = 0xfff815d8; PC = 0x8002634 *)
mov r7 L0x2001be68;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 342 ********************)

ghost c0168@sint32,c0424@sint32,c0680@sint32,c0936@sint32 :
and [c0168=r4, c0424=r5, c0680=r6, c0936=r7] && true;

ecut and [
eqmod (c0168*x**168+c0424*x**424+c0680*x**680+c0936*x**936)
      (4*L0x2001b268*x**168) [1043969, x**256 - 1],
eqmod (c0168*x**168+c0424*x**424+c0680*x**680+c0936*x**936)
      (4*L0x2001b668*x**168) [1043969, x**256 + 1],
eqmod (c0168*x**168+c0424*x**424+c0680*x**680+c0936*x**936)
      (4*L0x2001ba68*x**168) [1043969, x**256 - 554923],
eqmod (c0168*x**168+c0424*x**424+c0680*x**680+c0936*x**936)
      (4*L0x2001be68*x**168) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf168@sint32 : cf168 = r4 && cf168 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf424@sint32 : cf424 = r5 && cf424 = r5;

(******************** CUT 343 ********************)
ecut and [
eqmod 256*cf168 2**32*(c0168+c0677) 1043969,
eqmod 256*cf424 2**32*(c0424+c0933) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018918; PC = 0x80026a0 *)
mov L0x20018918 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b18; PC = 0x80026a4 *)
mov L0x20018b18 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b26c; Value = 0xfff44d4a; PC = 0x80026b0 *)
mov r4 L0x2001b26c;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b66c; Value = 0xfffa10a5; PC = 0x80026b4 *)
mov r5 L0x2001b66c;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001ba6c; Value = 0xfffd3971; PC = 0x80026b8 *)
mov r6 L0x2001ba6c;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001be6c; Value = 0x0003aa21; PC = 0x80026bc *)
mov r7 L0x2001be6c;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 344 ********************)

ghost c0169@sint32,c0425@sint32,c0681@sint32,c0937@sint32 :
and [c0169=r4, c0425=r5, c0681=r6, c0937=r7] && true;

ecut and [
eqmod (c0169*x**169+c0425*x**425+c0681*x**681+c0937*x**937)
      (4*L0x2001b26c*x**169) [1043969, x**256 - 1],
eqmod (c0169*x**169+c0425*x**425+c0681*x**681+c0937*x**937)
      (4*L0x2001b66c*x**169) [1043969, x**256 + 1],
eqmod (c0169*x**169+c0425*x**425+c0681*x**681+c0937*x**937)
      (4*L0x2001ba6c*x**169) [1043969, x**256 - 554923],
eqmod (c0169*x**169+c0425*x**425+c0681*x**681+c0937*x**937)
      (4*L0x2001be6c*x**169) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf169@sint32 : cf169 = r4 && cf169 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf425@sint32 : cf425 = r5 && cf425 = r5;

(******************** CUT 345 ********************)
ecut and [
eqmod 256*cf169 2**32*(c0169+c0678) 1043969,
eqmod 256*cf425 2**32*(c0425+c0934) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x2001891a; PC = 0x8002728 *)
mov L0x2001891a r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b1a; PC = 0x800272c *)
mov L0x20018b1a r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b270; Value = 0xffee02ae; PC = 0x8002738 *)
mov r4 L0x2001b270;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b670; Value = 0xfffddc4d; PC = 0x800273c *)
mov r5 L0x2001b670;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001ba70; Value = 0x00068a55; PC = 0x8002740 *)
mov r6 L0x2001ba70;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001be70; Value = 0x00030857; PC = 0x8002744 *)
mov r7 L0x2001be70;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 346 ********************)

ghost c0170@sint32,c0426@sint32,c0682@sint32,c0938@sint32 :
and [c0170=r4, c0426=r5, c0682=r6, c0938=r7] && true;

ecut and [
eqmod (c0170*x**170+c0426*x**426+c0682*x**682+c0938*x**938)
      (4*L0x2001b270*x**170) [1043969, x**256 - 1],
eqmod (c0170*x**170+c0426*x**426+c0682*x**682+c0938*x**938)
      (4*L0x2001b670*x**170) [1043969, x**256 + 1],
eqmod (c0170*x**170+c0426*x**426+c0682*x**682+c0938*x**938)
      (4*L0x2001ba70*x**170) [1043969, x**256 - 554923],
eqmod (c0170*x**170+c0426*x**426+c0682*x**682+c0938*x**938)
      (4*L0x2001be70*x**170) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf170@sint32 : cf170 = r4 && cf170 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf426@sint32 : cf426 = r5 && cf426 = r5;

(******************** CUT 347 ********************)
ecut and [
eqmod 256*cf170 2**32*(c0170+c0679) 1043969,
eqmod 256*cf426 2**32*(c0426+c0935) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x2001891c; PC = 0x80027b0 *)
mov L0x2001891c r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018b1c; PC = 0x80027b4 *)
mov L0x20018b1c r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b274; Value = 0x0008ce61; PC = 0x8002628 *)
mov r4 L0x2001b274;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b674; Value = 0x000210aa; PC = 0x800262c *)
mov r5 L0x2001b674;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001ba74; Value = 0xfffc8eab; PC = 0x8002630 *)
mov r6 L0x2001ba74;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001be74; Value = 0xffffadde; PC = 0x8002634 *)
mov r7 L0x2001be74;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 348 ********************)

ghost c0171@sint32,c0427@sint32,c0683@sint32,c0939@sint32 :
and [c0171=r4, c0427=r5, c0683=r6, c0939=r7] && true;

ecut and [
eqmod (c0171*x**171+c0427*x**427+c0683*x**683+c0939*x**939)
      (4*L0x2001b274*x**171) [1043969, x**256 - 1],
eqmod (c0171*x**171+c0427*x**427+c0683*x**683+c0939*x**939)
      (4*L0x2001b674*x**171) [1043969, x**256 + 1],
eqmod (c0171*x**171+c0427*x**427+c0683*x**683+c0939*x**939)
      (4*L0x2001ba74*x**171) [1043969, x**256 - 554923],
eqmod (c0171*x**171+c0427*x**427+c0683*x**683+c0939*x**939)
      (4*L0x2001be74*x**171) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf171@sint32 : cf171 = r4 && cf171 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf427@sint32 : cf427 = r5 && cf427 = r5;

(******************** CUT 349 ********************)
ecut and [
eqmod 256*cf171 2**32*(c0171+c0680) 1043969,
eqmod 256*cf427 2**32*(c0427+c0936) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x2001891e; PC = 0x80026a0 *)
mov L0x2001891e r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b1e; PC = 0x80026a4 *)
mov L0x20018b1e r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b278; Value = 0x000d2567; PC = 0x80026b0 *)
mov r4 L0x2001b278;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b678; Value = 0xfff93a04; PC = 0x80026b4 *)
mov r5 L0x2001b678;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001ba78; Value = 0x0005173b; PC = 0x80026b8 *)
mov r6 L0x2001ba78;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001be78; Value = 0xfffee730; PC = 0x80026bc *)
mov r7 L0x2001be78;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 350 ********************)

ghost c0172@sint32,c0428@sint32,c0684@sint32,c0940@sint32 :
and [c0172=r4, c0428=r5, c0684=r6, c0940=r7] && true;

ecut and [
eqmod (c0172*x**172+c0428*x**428+c0684*x**684+c0940*x**940)
      (4*L0x2001b278*x**172) [1043969, x**256 - 1],
eqmod (c0172*x**172+c0428*x**428+c0684*x**684+c0940*x**940)
      (4*L0x2001b678*x**172) [1043969, x**256 + 1],
eqmod (c0172*x**172+c0428*x**428+c0684*x**684+c0940*x**940)
      (4*L0x2001ba78*x**172) [1043969, x**256 - 554923],
eqmod (c0172*x**172+c0428*x**428+c0684*x**684+c0940*x**940)
      (4*L0x2001be78*x**172) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf172@sint32 : cf172 = r4 && cf172 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf428@sint32 : cf428 = r5 && cf428 = r5;

(******************** CUT 351 ********************)
ecut and [
eqmod 256*cf172 2**32*(c0172+c0681) 1043969,
eqmod 256*cf428 2**32*(c0428+c0937) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018920; PC = 0x8002728 *)
mov L0x20018920 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b20; PC = 0x800272c *)
mov L0x20018b20 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b27c; Value = 0xfffc0abc; PC = 0x8002738 *)
mov r4 L0x2001b27c;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b67c; Value = 0x0007f225; PC = 0x800273c *)
mov r5 L0x2001b67c;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001ba7c; Value = 0xfff9e33e; PC = 0x8002740 *)
mov r6 L0x2001ba7c;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001be7c; Value = 0x00071df7; PC = 0x8002744 *)
mov r7 L0x2001be7c;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 352 ********************)

ghost c0173@sint32,c0429@sint32,c0685@sint32,c0941@sint32 :
and [c0173=r4, c0429=r5, c0685=r6, c0941=r7] && true;

ecut and [
eqmod (c0173*x**173+c0429*x**429+c0685*x**685+c0941*x**941)
      (4*L0x2001b27c*x**173) [1043969, x**256 - 1],
eqmod (c0173*x**173+c0429*x**429+c0685*x**685+c0941*x**941)
      (4*L0x2001b67c*x**173) [1043969, x**256 + 1],
eqmod (c0173*x**173+c0429*x**429+c0685*x**685+c0941*x**941)
      (4*L0x2001ba7c*x**173) [1043969, x**256 - 554923],
eqmod (c0173*x**173+c0429*x**429+c0685*x**685+c0941*x**941)
      (4*L0x2001be7c*x**173) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf173@sint32 : cf173 = r4 && cf173 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf429@sint32 : cf429 = r5 && cf429 = r5;

(******************** CUT 353 ********************)
ecut and [
eqmod 256*cf173 2**32*(c0173+c0682) 1043969,
eqmod 256*cf429 2**32*(c0429+c0938) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018922; PC = 0x80027b0 *)
mov L0x20018922 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018b22; PC = 0x80027b4 *)
mov L0x20018b22 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b280; Value = 0xffed9d6f; PC = 0x8002628 *)
mov r4 L0x2001b280;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b680; Value = 0xffffbf22; PC = 0x800262c *)
mov r5 L0x2001b680;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001ba80; Value = 0xfffb5079; PC = 0x8002630 *)
mov r6 L0x2001ba80;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001be80; Value = 0xfff8594e; PC = 0x8002634 *)
mov r7 L0x2001be80;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 354 ********************)

ghost c0174@sint32,c0430@sint32,c0686@sint32,c0942@sint32 :
and [c0174=r4, c0430=r5, c0686=r6, c0942=r7] && true;

ecut and [
eqmod (c0174*x**174+c0430*x**430+c0686*x**686+c0942*x**942)
      (4*L0x2001b280*x**174) [1043969, x**256 - 1],
eqmod (c0174*x**174+c0430*x**430+c0686*x**686+c0942*x**942)
      (4*L0x2001b680*x**174) [1043969, x**256 + 1],
eqmod (c0174*x**174+c0430*x**430+c0686*x**686+c0942*x**942)
      (4*L0x2001ba80*x**174) [1043969, x**256 - 554923],
eqmod (c0174*x**174+c0430*x**430+c0686*x**686+c0942*x**942)
      (4*L0x2001be80*x**174) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf174@sint32 : cf174 = r4 && cf174 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf430@sint32 : cf430 = r5 && cf430 = r5;

(******************** CUT 355 ********************)
ecut and [
eqmod 256*cf174 2**32*(c0174+c0683) 1043969,
eqmod 256*cf430 2**32*(c0430+c0939) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018924; PC = 0x80026a0 *)
mov L0x20018924 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b24; PC = 0x80026a4 *)
mov L0x20018b24 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b284; Value = 0x000d2870; PC = 0x80026b0 *)
mov r4 L0x2001b284;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b684; Value = 0xfffb913b; PC = 0x80026b4 *)
mov r5 L0x2001b684;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001ba84; Value = 0x0002a2c8; PC = 0x80026b8 *)
mov r6 L0x2001ba84;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001be84; Value = 0xfffb12c7; PC = 0x80026bc *)
mov r7 L0x2001be84;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 356 ********************)

ghost c0175@sint32,c0431@sint32,c0687@sint32,c0943@sint32 :
and [c0175=r4, c0431=r5, c0687=r6, c0943=r7] && true;

ecut and [
eqmod (c0175*x**175+c0431*x**431+c0687*x**687+c0943*x**943)
      (4*L0x2001b284*x**175) [1043969, x**256 - 1],
eqmod (c0175*x**175+c0431*x**431+c0687*x**687+c0943*x**943)
      (4*L0x2001b684*x**175) [1043969, x**256 + 1],
eqmod (c0175*x**175+c0431*x**431+c0687*x**687+c0943*x**943)
      (4*L0x2001ba84*x**175) [1043969, x**256 - 554923],
eqmod (c0175*x**175+c0431*x**431+c0687*x**687+c0943*x**943)
      (4*L0x2001be84*x**175) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf175@sint32 : cf175 = r4 && cf175 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf431@sint32 : cf431 = r5 && cf431 = r5;

(******************** CUT 357 ********************)
ecut and [
eqmod 256*cf175 2**32*(c0175+c0684) 1043969,
eqmod 256*cf431 2**32*(c0431+c0940) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018926; PC = 0x8002728 *)
mov L0x20018926 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b26; PC = 0x800272c *)
mov L0x20018b26 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b288; Value = 0xfff930ce; PC = 0x8002738 *)
mov r4 L0x2001b288;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b688; Value = 0x0005e296; PC = 0x800273c *)
mov r5 L0x2001b688;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001ba88; Value = 0x00024481; PC = 0x8002740 *)
mov r6 L0x2001ba88;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001be88; Value = 0x0000b146; PC = 0x8002744 *)
mov r7 L0x2001be88;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 358 ********************)

ghost c0176@sint32,c0432@sint32,c0688@sint32,c0944@sint32 :
and [c0176=r4, c0432=r5, c0688=r6, c0944=r7] && true;

ecut and [
eqmod (c0176*x**176+c0432*x**432+c0688*x**688+c0944*x**944)
      (4*L0x2001b288*x**176) [1043969, x**256 - 1],
eqmod (c0176*x**176+c0432*x**432+c0688*x**688+c0944*x**944)
      (4*L0x2001b688*x**176) [1043969, x**256 + 1],
eqmod (c0176*x**176+c0432*x**432+c0688*x**688+c0944*x**944)
      (4*L0x2001ba88*x**176) [1043969, x**256 - 554923],
eqmod (c0176*x**176+c0432*x**432+c0688*x**688+c0944*x**944)
      (4*L0x2001be88*x**176) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf176@sint32 : cf176 = r4 && cf176 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf432@sint32 : cf432 = r5 && cf432 = r5;

(******************** CUT 359 ********************)
ecut and [
eqmod 256*cf176 2**32*(c0176+c0685) 1043969,
eqmod 256*cf432 2**32*(c0432+c0941) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018928; PC = 0x80027b0 *)
mov L0x20018928 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018b28; PC = 0x80027b4 *)
mov L0x20018b28 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b28c; Value = 0x0017b752; PC = 0x8002628 *)
mov r4 L0x2001b28c;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b68c; Value = 0x000007f5; PC = 0x800262c *)
mov r5 L0x2001b68c;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001ba8c; Value = 0xfff8a822; PC = 0x8002630 *)
mov r6 L0x2001ba8c;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001be8c; Value = 0xfffa6976; PC = 0x8002634 *)
mov r7 L0x2001be8c;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 360 ********************)

ghost c0177@sint32,c0433@sint32,c0689@sint32,c0945@sint32 :
and [c0177=r4, c0433=r5, c0689=r6, c0945=r7] && true;

ecut and [
eqmod (c0177*x**177+c0433*x**433+c0689*x**689+c0945*x**945)
      (4*L0x2001b28c*x**177) [1043969, x**256 - 1],
eqmod (c0177*x**177+c0433*x**433+c0689*x**689+c0945*x**945)
      (4*L0x2001b68c*x**177) [1043969, x**256 + 1],
eqmod (c0177*x**177+c0433*x**433+c0689*x**689+c0945*x**945)
      (4*L0x2001ba8c*x**177) [1043969, x**256 - 554923],
eqmod (c0177*x**177+c0433*x**433+c0689*x**689+c0945*x**945)
      (4*L0x2001be8c*x**177) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf177@sint32 : cf177 = r4 && cf177 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf433@sint32 : cf433 = r5 && cf433 = r5;

(******************** CUT 361 ********************)
ecut and [
eqmod 256*cf177 2**32*(c0177+c0686) 1043969,
eqmod 256*cf433 2**32*(c0433+c0942) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x2001892a; PC = 0x80026a0 *)
mov L0x2001892a r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b2a; PC = 0x80026a4 *)
mov L0x20018b2a r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b290; Value = 0xfff975a8; PC = 0x80026b0 *)
mov r4 L0x2001b290;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b690; Value = 0xfffd4849; PC = 0x80026b4 *)
mov r5 L0x2001b690;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001ba90; Value = 0x00034e3f; PC = 0x80026b8 *)
mov r6 L0x2001ba90;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001be90; Value = 0x00026dc8; PC = 0x80026bc *)
mov r7 L0x2001be90;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 362 ********************)

ghost c0178@sint32,c0434@sint32,c0690@sint32,c0946@sint32 :
and [c0178=r4, c0434=r5, c0690=r6, c0946=r7] && true;

ecut and [
eqmod (c0178*x**178+c0434*x**434+c0690*x**690+c0946*x**946)
      (4*L0x2001b290*x**178) [1043969, x**256 - 1],
eqmod (c0178*x**178+c0434*x**434+c0690*x**690+c0946*x**946)
      (4*L0x2001b690*x**178) [1043969, x**256 + 1],
eqmod (c0178*x**178+c0434*x**434+c0690*x**690+c0946*x**946)
      (4*L0x2001ba90*x**178) [1043969, x**256 - 554923],
eqmod (c0178*x**178+c0434*x**434+c0690*x**690+c0946*x**946)
      (4*L0x2001be90*x**178) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf178@sint32 : cf178 = r4 && cf178 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf434@sint32 : cf434 = r5 && cf434 = r5;

(******************** CUT 363 ********************)
ecut and [
eqmod 256*cf178 2**32*(c0178+c0687) 1043969,
eqmod 256*cf434 2**32*(c0434+c0943) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x2001892c; PC = 0x8002728 *)
mov L0x2001892c r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b2c; PC = 0x800272c *)
mov L0x20018b2c r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b294; Value = 0xfff6f2f5; PC = 0x8002738 *)
mov r4 L0x2001b294;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b694; Value = 0x0006d520; PC = 0x800273c *)
mov r5 L0x2001b694;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001ba94; Value = 0x0003bf86; PC = 0x8002740 *)
mov r6 L0x2001ba94;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001be94; Value = 0xfffabd19; PC = 0x8002744 *)
mov r7 L0x2001be94;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 364 ********************)

ghost c0179@sint32,c0435@sint32,c0691@sint32,c0947@sint32 :
and [c0179=r4, c0435=r5, c0691=r6, c0947=r7] && true;

ecut and [
eqmod (c0179*x**179+c0435*x**435+c0691*x**691+c0947*x**947)
      (4*L0x2001b294*x**179) [1043969, x**256 - 1],
eqmod (c0179*x**179+c0435*x**435+c0691*x**691+c0947*x**947)
      (4*L0x2001b694*x**179) [1043969, x**256 + 1],
eqmod (c0179*x**179+c0435*x**435+c0691*x**691+c0947*x**947)
      (4*L0x2001ba94*x**179) [1043969, x**256 - 554923],
eqmod (c0179*x**179+c0435*x**435+c0691*x**691+c0947*x**947)
      (4*L0x2001be94*x**179) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf179@sint32 : cf179 = r4 && cf179 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf435@sint32 : cf435 = r5 && cf435 = r5;

(******************** CUT 365 ********************)
ecut and [
eqmod 256*cf179 2**32*(c0179+c0688) 1043969,
eqmod 256*cf435 2**32*(c0435+c0944) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x2001892e; PC = 0x80027b0 *)
mov L0x2001892e r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018b2e; PC = 0x80027b4 *)
mov L0x20018b2e r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b298; Value = 0x0003f736; PC = 0x8002628 *)
mov r4 L0x2001b298;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b698; Value = 0xfffae1a2; PC = 0x800262c *)
mov r5 L0x2001b698;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001ba98; Value = 0xfffd1591; PC = 0x8002630 *)
mov r6 L0x2001ba98;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001be98; Value = 0x0003586b; PC = 0x8002634 *)
mov r7 L0x2001be98;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 366 ********************)

ghost c0180@sint32,c0436@sint32,c0692@sint32,c0948@sint32 :
and [c0180=r4, c0436=r5, c0692=r6, c0948=r7] && true;

ecut and [
eqmod (c0180*x**180+c0436*x**436+c0692*x**692+c0948*x**948)
      (4*L0x2001b298*x**180) [1043969, x**256 - 1],
eqmod (c0180*x**180+c0436*x**436+c0692*x**692+c0948*x**948)
      (4*L0x2001b698*x**180) [1043969, x**256 + 1],
eqmod (c0180*x**180+c0436*x**436+c0692*x**692+c0948*x**948)
      (4*L0x2001ba98*x**180) [1043969, x**256 - 554923],
eqmod (c0180*x**180+c0436*x**436+c0692*x**692+c0948*x**948)
      (4*L0x2001be98*x**180) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf180@sint32 : cf180 = r4 && cf180 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf436@sint32 : cf436 = r5 && cf436 = r5;

(******************** CUT 367 ********************)
ecut and [
eqmod 256*cf180 2**32*(c0180+c0689) 1043969,
eqmod 256*cf436 2**32*(c0436+c0945) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018930; PC = 0x80026a0 *)
mov L0x20018930 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b30; PC = 0x80026a4 *)
mov L0x20018b30 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b29c; Value = 0x0001875d; PC = 0x80026b0 *)
mov r4 L0x2001b29c;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b69c; Value = 0x000118dd; PC = 0x80026b4 *)
mov r5 L0x2001b69c;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001ba9c; Value = 0xfffdcd04; PC = 0x80026b8 *)
mov r6 L0x2001ba9c;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001be9c; Value = 0xffff3b60; PC = 0x80026bc *)
mov r7 L0x2001be9c;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 368 ********************)

ghost c0181@sint32,c0437@sint32,c0693@sint32,c0949@sint32 :
and [c0181=r4, c0437=r5, c0693=r6, c0949=r7] && true;

ecut and [
eqmod (c0181*x**181+c0437*x**437+c0693*x**693+c0949*x**949)
      (4*L0x2001b29c*x**181) [1043969, x**256 - 1],
eqmod (c0181*x**181+c0437*x**437+c0693*x**693+c0949*x**949)
      (4*L0x2001b69c*x**181) [1043969, x**256 + 1],
eqmod (c0181*x**181+c0437*x**437+c0693*x**693+c0949*x**949)
      (4*L0x2001ba9c*x**181) [1043969, x**256 - 554923],
eqmod (c0181*x**181+c0437*x**437+c0693*x**693+c0949*x**949)
      (4*L0x2001be9c*x**181) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf181@sint32 : cf181 = r4 && cf181 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf437@sint32 : cf437 = r5 && cf437 = r5;

(******************** CUT 369 ********************)
ecut and [
eqmod 256*cf181 2**32*(c0181+c0690) 1043969,
eqmod 256*cf437 2**32*(c0437+c0946) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018932; PC = 0x8002728 *)
mov L0x20018932 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b32; PC = 0x800272c *)
mov L0x20018b32 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b2a0; Value = 0xfff1d9ef; PC = 0x8002738 *)
mov r4 L0x2001b2a0;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b6a0; Value = 0xfffa132e; PC = 0x800273c *)
mov r5 L0x2001b6a0;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001baa0; Value = 0x0007c4fe; PC = 0x8002740 *)
mov r6 L0x2001baa0;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bea0; Value = 0x000414fa; PC = 0x8002744 *)
mov r7 L0x2001bea0;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 370 ********************)

ghost c0182@sint32,c0438@sint32,c0694@sint32,c0950@sint32 :
and [c0182=r4, c0438=r5, c0694=r6, c0950=r7] && true;

ecut and [
eqmod (c0182*x**182+c0438*x**438+c0694*x**694+c0950*x**950)
      (4*L0x2001b2a0*x**182) [1043969, x**256 - 1],
eqmod (c0182*x**182+c0438*x**438+c0694*x**694+c0950*x**950)
      (4*L0x2001b6a0*x**182) [1043969, x**256 + 1],
eqmod (c0182*x**182+c0438*x**438+c0694*x**694+c0950*x**950)
      (4*L0x2001baa0*x**182) [1043969, x**256 - 554923],
eqmod (c0182*x**182+c0438*x**438+c0694*x**694+c0950*x**950)
      (4*L0x2001bea0*x**182) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf182@sint32 : cf182 = r4 && cf182 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf438@sint32 : cf438 = r5 && cf438 = r5;

(******************** CUT 371 ********************)
ecut and [
eqmod 256*cf182 2**32*(c0182+c0691) 1043969,
eqmod 256*cf438 2**32*(c0438+c0947) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018934; PC = 0x80027b0 *)
mov L0x20018934 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018b34; PC = 0x80027b4 *)
mov L0x20018b34 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b2a4; Value = 0xfff0c3d4; PC = 0x8002628 *)
mov r4 L0x2001b2a4;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b6a4; Value = 0x0001fc76; PC = 0x800262c *)
mov r5 L0x2001b6a4;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001baa4; Value = 0x00034b48; PC = 0x8002630 *)
mov r6 L0x2001baa4;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bea4; Value = 0xfffdb503; PC = 0x8002634 *)
mov r7 L0x2001bea4;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 372 ********************)

ghost c0183@sint32,c0439@sint32,c0695@sint32,c0951@sint32 :
and [c0183=r4, c0439=r5, c0695=r6, c0951=r7] && true;

ecut and [
eqmod (c0183*x**183+c0439*x**439+c0695*x**695+c0951*x**951)
      (4*L0x2001b2a4*x**183) [1043969, x**256 - 1],
eqmod (c0183*x**183+c0439*x**439+c0695*x**695+c0951*x**951)
      (4*L0x2001b6a4*x**183) [1043969, x**256 + 1],
eqmod (c0183*x**183+c0439*x**439+c0695*x**695+c0951*x**951)
      (4*L0x2001baa4*x**183) [1043969, x**256 - 554923],
eqmod (c0183*x**183+c0439*x**439+c0695*x**695+c0951*x**951)
      (4*L0x2001bea4*x**183) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf183@sint32 : cf183 = r4 && cf183 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf439@sint32 : cf439 = r5 && cf439 = r5;

(******************** CUT 373 ********************)
ecut and [
eqmod 256*cf183 2**32*(c0183+c0692) 1043969,
eqmod 256*cf439 2**32*(c0439+c0948) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018936; PC = 0x80026a0 *)
mov L0x20018936 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b36; PC = 0x80026a4 *)
mov L0x20018b36 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b2a8; Value = 0x00091134; PC = 0x80026b0 *)
mov r4 L0x2001b2a8;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b6a8; Value = 0xfffe064b; PC = 0x80026b4 *)
mov r5 L0x2001b6a8;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001baa8; Value = 0xfffe6010; PC = 0x80026b8 *)
mov r6 L0x2001baa8;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bea8; Value = 0x0000c9c7; PC = 0x80026bc *)
mov r7 L0x2001bea8;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 374 ********************)

ghost c0184@sint32,c0440@sint32,c0696@sint32,c0952@sint32 :
and [c0184=r4, c0440=r5, c0696=r6, c0952=r7] && true;

ecut and [
eqmod (c0184*x**184+c0440*x**440+c0696*x**696+c0952*x**952)
      (4*L0x2001b2a8*x**184) [1043969, x**256 - 1],
eqmod (c0184*x**184+c0440*x**440+c0696*x**696+c0952*x**952)
      (4*L0x2001b6a8*x**184) [1043969, x**256 + 1],
eqmod (c0184*x**184+c0440*x**440+c0696*x**696+c0952*x**952)
      (4*L0x2001baa8*x**184) [1043969, x**256 - 554923],
eqmod (c0184*x**184+c0440*x**440+c0696*x**696+c0952*x**952)
      (4*L0x2001bea8*x**184) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf184@sint32 : cf184 = r4 && cf184 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf440@sint32 : cf440 = r5 && cf440 = r5;

(******************** CUT 375 ********************)
ecut and [
eqmod 256*cf184 2**32*(c0184+c0693) 1043969,
eqmod 256*cf440 2**32*(c0440+c0949) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018938; PC = 0x8002728 *)
mov L0x20018938 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b38; PC = 0x800272c *)
mov L0x20018b38 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b2ac; Value = 0xfff97bfb; PC = 0x8002738 *)
mov r4 L0x2001b2ac;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b6ac; Value = 0x0000d7cc; PC = 0x800273c *)
mov r5 L0x2001b6ac;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001baac; Value = 0x0006d7ac; PC = 0x8002740 *)
mov r6 L0x2001baac;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001beac; Value = 0xfff9ca2e; PC = 0x8002744 *)
mov r7 L0x2001beac;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 376 ********************)

ghost c0185@sint32,c0441@sint32,c0697@sint32,c0953@sint32 :
and [c0185=r4, c0441=r5, c0697=r6, c0953=r7] && true;

ecut and [
eqmod (c0185*x**185+c0441*x**441+c0697*x**697+c0953*x**953)
      (4*L0x2001b2ac*x**185) [1043969, x**256 - 1],
eqmod (c0185*x**185+c0441*x**441+c0697*x**697+c0953*x**953)
      (4*L0x2001b6ac*x**185) [1043969, x**256 + 1],
eqmod (c0185*x**185+c0441*x**441+c0697*x**697+c0953*x**953)
      (4*L0x2001baac*x**185) [1043969, x**256 - 554923],
eqmod (c0185*x**185+c0441*x**441+c0697*x**697+c0953*x**953)
      (4*L0x2001beac*x**185) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf185@sint32 : cf185 = r4 && cf185 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf441@sint32 : cf441 = r5 && cf441 = r5;

(******************** CUT 377 ********************)
ecut and [
eqmod 256*cf185 2**32*(c0185+c0694) 1043969,
eqmod 256*cf441 2**32*(c0441+c0950) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x2001893a; PC = 0x80027b0 *)
mov L0x2001893a r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018b3a; PC = 0x80027b4 *)
mov L0x20018b3a r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b2b0; Value = 0x00077224; PC = 0x8002628 *)
mov r4 L0x2001b2b0;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b6b0; Value = 0x00055f61; PC = 0x800262c *)
mov r5 L0x2001b6b0;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001bab0; Value = 0xfffa4bfa; PC = 0x8002630 *)
mov r6 L0x2001bab0;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001beb0; Value = 0x0007ea1f; PC = 0x8002634 *)
mov r7 L0x2001beb0;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 378 ********************)

ghost c0186@sint32,c0442@sint32,c0698@sint32,c0954@sint32 :
and [c0186=r4, c0442=r5, c0698=r6, c0954=r7] && true;

ecut and [
eqmod (c0186*x**186+c0442*x**442+c0698*x**698+c0954*x**954)
      (4*L0x2001b2b0*x**186) [1043969, x**256 - 1],
eqmod (c0186*x**186+c0442*x**442+c0698*x**698+c0954*x**954)
      (4*L0x2001b6b0*x**186) [1043969, x**256 + 1],
eqmod (c0186*x**186+c0442*x**442+c0698*x**698+c0954*x**954)
      (4*L0x2001bab0*x**186) [1043969, x**256 - 554923],
eqmod (c0186*x**186+c0442*x**442+c0698*x**698+c0954*x**954)
      (4*L0x2001beb0*x**186) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf186@sint32 : cf186 = r4 && cf186 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf442@sint32 : cf442 = r5 && cf442 = r5;

(******************** CUT 379 ********************)
ecut and [
eqmod 256*cf186 2**32*(c0186+c0695) 1043969,
eqmod 256*cf442 2**32*(c0442+c0951) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x2001893c; PC = 0x80026a0 *)
mov L0x2001893c r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b3c; PC = 0x80026a4 *)
mov L0x20018b3c r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b2b4; Value = 0x00041133; PC = 0x80026b0 *)
mov r4 L0x2001b2b4;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b6b4; Value = 0xfff9f807; PC = 0x80026b4 *)
mov r5 L0x2001b6b4;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001bab4; Value = 0xfff9ef4d; PC = 0x80026b8 *)
mov r6 L0x2001bab4;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001beb4; Value = 0xfff9da50; PC = 0x80026bc *)
mov r7 L0x2001beb4;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 380 ********************)

ghost c0187@sint32,c0443@sint32,c0699@sint32,c0955@sint32 :
and [c0187=r4, c0443=r5, c0699=r6, c0955=r7] && true;

ecut and [
eqmod (c0187*x**187+c0443*x**443+c0699*x**699+c0955*x**955)
      (4*L0x2001b2b4*x**187) [1043969, x**256 - 1],
eqmod (c0187*x**187+c0443*x**443+c0699*x**699+c0955*x**955)
      (4*L0x2001b6b4*x**187) [1043969, x**256 + 1],
eqmod (c0187*x**187+c0443*x**443+c0699*x**699+c0955*x**955)
      (4*L0x2001bab4*x**187) [1043969, x**256 - 554923],
eqmod (c0187*x**187+c0443*x**443+c0699*x**699+c0955*x**955)
      (4*L0x2001beb4*x**187) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf187@sint32 : cf187 = r4 && cf187 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf443@sint32 : cf443 = r5 && cf443 = r5;

(******************** CUT 381 ********************)
ecut and [
eqmod 256*cf187 2**32*(c0187+c0696) 1043969,
eqmod 256*cf443 2**32*(c0443+c0952) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x2001893e; PC = 0x8002728 *)
mov L0x2001893e r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b3e; PC = 0x800272c *)
mov L0x20018b3e r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b2b8; Value = 0x00108f9c; PC = 0x8002738 *)
mov r4 L0x2001b2b8;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b6b8; Value = 0xfffe4557; PC = 0x800273c *)
mov r5 L0x2001b6b8;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001bab8; Value = 0xffffa9df; PC = 0x8002740 *)
mov r6 L0x2001bab8;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001beb8; Value = 0xfffdc66e; PC = 0x8002744 *)
mov r7 L0x2001beb8;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 382 ********************)

ghost c0188@sint32,c0444@sint32,c0700@sint32,c0956@sint32 :
and [c0188=r4, c0444=r5, c0700=r6, c0956=r7] && true;

ecut and [
eqmod (c0188*x**188+c0444*x**444+c0700*x**700+c0956*x**956)
      (4*L0x2001b2b8*x**188) [1043969, x**256 - 1],
eqmod (c0188*x**188+c0444*x**444+c0700*x**700+c0956*x**956)
      (4*L0x2001b6b8*x**188) [1043969, x**256 + 1],
eqmod (c0188*x**188+c0444*x**444+c0700*x**700+c0956*x**956)
      (4*L0x2001bab8*x**188) [1043969, x**256 - 554923],
eqmod (c0188*x**188+c0444*x**444+c0700*x**700+c0956*x**956)
      (4*L0x2001beb8*x**188) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf188@sint32 : cf188 = r4 && cf188 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf444@sint32 : cf444 = r5 && cf444 = r5;

(******************** CUT 383 ********************)
ecut and [
eqmod 256*cf188 2**32*(c0188+c0697) 1043969,
eqmod 256*cf444 2**32*(c0444+c0953) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018940; PC = 0x80027b0 *)
mov L0x20018940 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018b40; PC = 0x80027b4 *)
mov L0x20018b40 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b2bc; Value = 0x000310bd; PC = 0x8002628 *)
mov r4 L0x2001b2bc;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b6bc; Value = 0x00034eb8; PC = 0x800262c *)
mov r5 L0x2001b6bc;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001babc; Value = 0x0002d754; PC = 0x8002630 *)
mov r6 L0x2001babc;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bebc; Value = 0xfffddcb5; PC = 0x8002634 *)
mov r7 L0x2001bebc;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 384 ********************)

ghost c0189@sint32,c0445@sint32,c0701@sint32,c0957@sint32 :
and [c0189=r4, c0445=r5, c0701=r6, c0957=r7] && true;

ecut and [
eqmod (c0189*x**189+c0445*x**445+c0701*x**701+c0957*x**957)
      (4*L0x2001b2bc*x**189) [1043969, x**256 - 1],
eqmod (c0189*x**189+c0445*x**445+c0701*x**701+c0957*x**957)
      (4*L0x2001b6bc*x**189) [1043969, x**256 + 1],
eqmod (c0189*x**189+c0445*x**445+c0701*x**701+c0957*x**957)
      (4*L0x2001babc*x**189) [1043969, x**256 - 554923],
eqmod (c0189*x**189+c0445*x**445+c0701*x**701+c0957*x**957)
      (4*L0x2001bebc*x**189) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf189@sint32 : cf189 = r4 && cf189 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf445@sint32 : cf445 = r5 && cf445 = r5;

(******************** CUT 385 ********************)
ecut and [
eqmod 256*cf189 2**32*(c0189+c0698) 1043969,
eqmod 256*cf445 2**32*(c0445+c0954) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018942; PC = 0x80026a0 *)
mov L0x20018942 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b42; PC = 0x80026a4 *)
mov L0x20018b42 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b2c0; Value = 0xfffb8680; PC = 0x80026b0 *)
mov r4 L0x2001b2c0;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b6c0; Value = 0xffffece4; PC = 0x80026b4 *)
mov r5 L0x2001b6c0;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001bac0; Value = 0x0003b632; PC = 0x80026b8 *)
mov r6 L0x2001bac0;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bec0; Value = 0xfffa5fa1; PC = 0x80026bc *)
mov r7 L0x2001bec0;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 386 ********************)

ghost c0190@sint32,c0446@sint32,c0702@sint32,c0958@sint32 :
and [c0190=r4, c0446=r5, c0702=r6, c0958=r7] && true;

ecut and [
eqmod (c0190*x**190+c0446*x**446+c0702*x**702+c0958*x**958)
      (4*L0x2001b2c0*x**190) [1043969, x**256 - 1],
eqmod (c0190*x**190+c0446*x**446+c0702*x**702+c0958*x**958)
      (4*L0x2001b6c0*x**190) [1043969, x**256 + 1],
eqmod (c0190*x**190+c0446*x**446+c0702*x**702+c0958*x**958)
      (4*L0x2001bac0*x**190) [1043969, x**256 - 554923],
eqmod (c0190*x**190+c0446*x**446+c0702*x**702+c0958*x**958)
      (4*L0x2001bec0*x**190) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf190@sint32 : cf190 = r4 && cf190 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf446@sint32 : cf446 = r5 && cf446 = r5;

(******************** CUT 387 ********************)
ecut and [
eqmod 256*cf190 2**32*(c0190+c0699) 1043969,
eqmod 256*cf446 2**32*(c0446+c0955) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018944; PC = 0x8002728 *)
mov L0x20018944 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b44; PC = 0x800272c *)
mov L0x20018b44 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b2c4; Value = 0x0005d236; PC = 0x8002738 *)
mov r4 L0x2001b2c4;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b6c4; Value = 0xfffec3f2; PC = 0x800273c *)
mov r5 L0x2001b6c4;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001bac4; Value = 0x00045579; PC = 0x8002740 *)
mov r6 L0x2001bac4;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bec4; Value = 0x000407d8; PC = 0x8002744 *)
mov r7 L0x2001bec4;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 388 ********************)

ghost c0191@sint32,c0447@sint32,c0703@sint32,c0959@sint32 :
and [c0191=r4, c0447=r5, c0703=r6, c0959=r7] && true;

ecut and [
eqmod (c0191*x**191+c0447*x**447+c0703*x**703+c0959*x**959)
      (4*L0x2001b2c4*x**191) [1043969, x**256 - 1],
eqmod (c0191*x**191+c0447*x**447+c0703*x**703+c0959*x**959)
      (4*L0x2001b6c4*x**191) [1043969, x**256 + 1],
eqmod (c0191*x**191+c0447*x**447+c0703*x**703+c0959*x**959)
      (4*L0x2001bac4*x**191) [1043969, x**256 - 554923],
eqmod (c0191*x**191+c0447*x**447+c0703*x**703+c0959*x**959)
      (4*L0x2001bec4*x**191) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf191@sint32 : cf191 = r4 && cf191 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf447@sint32 : cf447 = r5 && cf447 = r5;

(******************** CUT 389 ********************)
ecut and [
eqmod 256*cf191 2**32*(c0191+c0700) 1043969,
eqmod 256*cf447 2**32*(c0447+c0956) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018946; PC = 0x80027b0 *)
mov L0x20018946 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018b46; PC = 0x80027b4 *)
mov L0x20018b46 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b2c8; Value = 0x00025c64; PC = 0x8002628 *)
mov r4 L0x2001b2c8;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b6c8; Value = 0xfffdfaeb; PC = 0x800262c *)
mov r5 L0x2001b6c8;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001bac8; Value = 0xfffd4548; PC = 0x8002630 *)
mov r6 L0x2001bac8;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bec8; Value = 0xffff0757; PC = 0x8002634 *)
mov r7 L0x2001bec8;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 390 ********************)

ghost c0192@sint32,c0448@sint32,c0704@sint32,c0960@sint32 :
and [c0192=r4, c0448=r5, c0704=r6, c0960=r7] && true;

ecut and [
eqmod (c0192*x**192+c0448*x**448+c0704*x**704+c0960*x**960)
      (4*L0x2001b2c8*x**192) [1043969, x**256 - 1],
eqmod (c0192*x**192+c0448*x**448+c0704*x**704+c0960*x**960)
      (4*L0x2001b6c8*x**192) [1043969, x**256 + 1],
eqmod (c0192*x**192+c0448*x**448+c0704*x**704+c0960*x**960)
      (4*L0x2001bac8*x**192) [1043969, x**256 - 554923],
eqmod (c0192*x**192+c0448*x**448+c0704*x**704+c0960*x**960)
      (4*L0x2001bec8*x**192) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf192@sint32 : cf192 = r4 && cf192 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf448@sint32 : cf448 = r5 && cf448 = r5;

(******************** CUT 391 ********************)
ecut and [
eqmod 256*cf192 2**32*(c0192+c0701) 1043969,
eqmod 256*cf448 2**32*(c0448+c0957) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018948; PC = 0x80026a0 *)
mov L0x20018948 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b48; PC = 0x80026a4 *)
mov L0x20018b48 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b2cc; Value = 0xffd95218; PC = 0x80026b0 *)
mov r4 L0x2001b2cc;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b6cc; Value = 0xfffc140f; PC = 0x80026b4 *)
mov r5 L0x2001b6cc;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001bacc; Value = 0x00034bfe; PC = 0x80026b8 *)
mov r6 L0x2001bacc;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001becc; Value = 0xfffa41cb; PC = 0x80026bc *)
mov r7 L0x2001becc;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 392 ********************)

ghost c0193@sint32,c0449@sint32,c0705@sint32,c0961@sint32 :
and [c0193=r4, c0449=r5, c0705=r6, c0961=r7] && true;

ecut and [
eqmod (c0193*x**193+c0449*x**449+c0705*x**705+c0961*x**961)
      (4*L0x2001b2cc*x**193) [1043969, x**256 - 1],
eqmod (c0193*x**193+c0449*x**449+c0705*x**705+c0961*x**961)
      (4*L0x2001b6cc*x**193) [1043969, x**256 + 1],
eqmod (c0193*x**193+c0449*x**449+c0705*x**705+c0961*x**961)
      (4*L0x2001bacc*x**193) [1043969, x**256 - 554923],
eqmod (c0193*x**193+c0449*x**449+c0705*x**705+c0961*x**961)
      (4*L0x2001becc*x**193) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf193@sint32 : cf193 = r4 && cf193 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf449@sint32 : cf449 = r5 && cf449 = r5;

(******************** CUT 393 ********************)
ecut and [
eqmod 256*cf193 2**32*(c0193+c0702) 1043969,
eqmod 256*cf449 2**32*(c0449+c0958) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x2001894a; PC = 0x8002728 *)
mov L0x2001894a r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b4a; PC = 0x800272c *)
mov L0x20018b4a r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b2d0; Value = 0xffec2982; PC = 0x8002738 *)
mov r4 L0x2001b2d0;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b6d0; Value = 0x0004e072; PC = 0x800273c *)
mov r5 L0x2001b6d0;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001bad0; Value = 0x0005c66a; PC = 0x8002740 *)
mov r6 L0x2001bad0;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bed0; Value = 0x0003ada0; PC = 0x8002744 *)
mov r7 L0x2001bed0;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 394 ********************)

ghost c0194@sint32,c0450@sint32,c0706@sint32,c0962@sint32 :
and [c0194=r4, c0450=r5, c0706=r6, c0962=r7] && true;

ecut and [
eqmod (c0194*x**194+c0450*x**450+c0706*x**706+c0962*x**962)
      (4*L0x2001b2d0*x**194) [1043969, x**256 - 1],
eqmod (c0194*x**194+c0450*x**450+c0706*x**706+c0962*x**962)
      (4*L0x2001b6d0*x**194) [1043969, x**256 + 1],
eqmod (c0194*x**194+c0450*x**450+c0706*x**706+c0962*x**962)
      (4*L0x2001bad0*x**194) [1043969, x**256 - 554923],
eqmod (c0194*x**194+c0450*x**450+c0706*x**706+c0962*x**962)
      (4*L0x2001bed0*x**194) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf194@sint32 : cf194 = r4 && cf194 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf450@sint32 : cf450 = r5 && cf450 = r5;

(******************** CUT 395 ********************)
ecut and [
eqmod 256*cf194 2**32*(c0194+c0703) 1043969,
eqmod 256*cf450 2**32*(c0450+c0959) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x2001894c; PC = 0x80027b0 *)
mov L0x2001894c r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018b4c; PC = 0x80027b4 *)
mov L0x20018b4c r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b2d4; Value = 0xfff2599d; PC = 0x8002628 *)
mov r4 L0x2001b2d4;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b6d4; Value = 0xfffd2bfc; PC = 0x800262c *)
mov r5 L0x2001b6d4;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001bad4; Value = 0x00016bcf; PC = 0x8002630 *)
mov r6 L0x2001bad4;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bed4; Value = 0x0003cbec; PC = 0x8002634 *)
mov r7 L0x2001bed4;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 396 ********************)

ghost c0195@sint32,c0451@sint32,c0707@sint32,c0963@sint32 :
and [c0195=r4, c0451=r5, c0707=r6, c0963=r7] && true;

ecut and [
eqmod (c0195*x**195+c0451*x**451+c0707*x**707+c0963*x**963)
      (4*L0x2001b2d4*x**195) [1043969, x**256 - 1],
eqmod (c0195*x**195+c0451*x**451+c0707*x**707+c0963*x**963)
      (4*L0x2001b6d4*x**195) [1043969, x**256 + 1],
eqmod (c0195*x**195+c0451*x**451+c0707*x**707+c0963*x**963)
      (4*L0x2001bad4*x**195) [1043969, x**256 - 554923],
eqmod (c0195*x**195+c0451*x**451+c0707*x**707+c0963*x**963)
      (4*L0x2001bed4*x**195) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf195@sint32 : cf195 = r4 && cf195 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf451@sint32 : cf451 = r5 && cf451 = r5;

(******************** CUT 397 ********************)
ecut and [
eqmod 256*cf195 2**32*(c0195+c0704) 1043969,
eqmod 256*cf451 2**32*(c0451+c0960) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x2001894e; PC = 0x80026a0 *)
mov L0x2001894e r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b4e; PC = 0x80026a4 *)
mov L0x20018b4e r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b2d8; Value = 0xffedde68; PC = 0x80026b0 *)
mov r4 L0x2001b2d8;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b6d8; Value = 0xfffab366; PC = 0x80026b4 *)
mov r5 L0x2001b6d8;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001bad8; Value = 0x00034c01; PC = 0x80026b8 *)
mov r6 L0x2001bad8;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bed8; Value = 0xfffec714; PC = 0x80026bc *)
mov r7 L0x2001bed8;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 398 ********************)

ghost c0196@sint32,c0452@sint32,c0708@sint32,c0964@sint32 :
and [c0196=r4, c0452=r5, c0708=r6, c0964=r7] && true;

ecut and [
eqmod (c0196*x**196+c0452*x**452+c0708*x**708+c0964*x**964)
      (4*L0x2001b2d8*x**196) [1043969, x**256 - 1],
eqmod (c0196*x**196+c0452*x**452+c0708*x**708+c0964*x**964)
      (4*L0x2001b6d8*x**196) [1043969, x**256 + 1],
eqmod (c0196*x**196+c0452*x**452+c0708*x**708+c0964*x**964)
      (4*L0x2001bad8*x**196) [1043969, x**256 - 554923],
eqmod (c0196*x**196+c0452*x**452+c0708*x**708+c0964*x**964)
      (4*L0x2001bed8*x**196) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf196@sint32 : cf196 = r4 && cf196 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf452@sint32 : cf452 = r5 && cf452 = r5;

(******************** CUT 399 ********************)
ecut and [
eqmod 256*cf196 2**32*(c0196+c0705) 1043969,
eqmod 256*cf452 2**32*(c0452+c0961) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018950; PC = 0x8002728 *)
mov L0x20018950 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b50; PC = 0x800272c *)
mov L0x20018b50 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b2dc; Value = 0xfffb4964; PC = 0x8002738 *)
mov r4 L0x2001b2dc;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b6dc; Value = 0x000378ee; PC = 0x800273c *)
mov r5 L0x2001b6dc;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001badc; Value = 0xfff8f771; PC = 0x8002740 *)
mov r6 L0x2001badc;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bedc; Value = 0x00000d5b; PC = 0x8002744 *)
mov r7 L0x2001bedc;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 400 ********************)

ghost c0197@sint32,c0453@sint32,c0709@sint32,c0965@sint32 :
and [c0197=r4, c0453=r5, c0709=r6, c0965=r7] && true;

ecut and [
eqmod (c0197*x**197+c0453*x**453+c0709*x**709+c0965*x**965)
      (4*L0x2001b2dc*x**197) [1043969, x**256 - 1],
eqmod (c0197*x**197+c0453*x**453+c0709*x**709+c0965*x**965)
      (4*L0x2001b6dc*x**197) [1043969, x**256 + 1],
eqmod (c0197*x**197+c0453*x**453+c0709*x**709+c0965*x**965)
      (4*L0x2001badc*x**197) [1043969, x**256 - 554923],
eqmod (c0197*x**197+c0453*x**453+c0709*x**709+c0965*x**965)
      (4*L0x2001bedc*x**197) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf197@sint32 : cf197 = r4 && cf197 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf453@sint32 : cf453 = r5 && cf453 = r5;

(******************** CUT 401 ********************)
ecut and [
eqmod 256*cf197 2**32*(c0197+c0706) 1043969,
eqmod 256*cf453 2**32*(c0453+c0962) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018952; PC = 0x80027b0 *)
mov L0x20018952 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018b52; PC = 0x80027b4 *)
mov L0x20018b52 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b2e0; Value = 0xfffc054b; PC = 0x8002628 *)
mov r4 L0x2001b2e0;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b6e0; Value = 0xfff89b8f; PC = 0x800262c *)
mov r5 L0x2001b6e0;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001bae0; Value = 0x00078499; PC = 0x8002630 *)
mov r6 L0x2001bae0;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bee0; Value = 0x000007e1; PC = 0x8002634 *)
mov r7 L0x2001bee0;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 402 ********************)

ghost c0198@sint32,c0454@sint32,c0710@sint32,c0966@sint32 :
and [c0198=r4, c0454=r5, c0710=r6, c0966=r7] && true;

ecut and [
eqmod (c0198*x**198+c0454*x**454+c0710*x**710+c0966*x**966)
      (4*L0x2001b2e0*x**198) [1043969, x**256 - 1],
eqmod (c0198*x**198+c0454*x**454+c0710*x**710+c0966*x**966)
      (4*L0x2001b6e0*x**198) [1043969, x**256 + 1],
eqmod (c0198*x**198+c0454*x**454+c0710*x**710+c0966*x**966)
      (4*L0x2001bae0*x**198) [1043969, x**256 - 554923],
eqmod (c0198*x**198+c0454*x**454+c0710*x**710+c0966*x**966)
      (4*L0x2001bee0*x**198) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf198@sint32 : cf198 = r4 && cf198 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf454@sint32 : cf454 = r5 && cf454 = r5;

(******************** CUT 403 ********************)
ecut and [
eqmod 256*cf198 2**32*(c0198+c0707) 1043969,
eqmod 256*cf454 2**32*(c0454+c0963) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018954; PC = 0x80026a0 *)
mov L0x20018954 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b54; PC = 0x80026a4 *)
mov L0x20018b54 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b2e4; Value = 0x000451ec; PC = 0x80026b0 *)
mov r4 L0x2001b2e4;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b6e4; Value = 0xfffa7dcc; PC = 0x80026b4 *)
mov r5 L0x2001b6e4;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001bae4; Value = 0x0005d8d2; PC = 0x80026b8 *)
mov r6 L0x2001bae4;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bee4; Value = 0xfff87ef6; PC = 0x80026bc *)
mov r7 L0x2001bee4;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 404 ********************)

ghost c0199@sint32,c0455@sint32,c0711@sint32,c0967@sint32 :
and [c0199=r4, c0455=r5, c0711=r6, c0967=r7] && true;

ecut and [
eqmod (c0199*x**199+c0455*x**455+c0711*x**711+c0967*x**967)
      (4*L0x2001b2e4*x**199) [1043969, x**256 - 1],
eqmod (c0199*x**199+c0455*x**455+c0711*x**711+c0967*x**967)
      (4*L0x2001b6e4*x**199) [1043969, x**256 + 1],
eqmod (c0199*x**199+c0455*x**455+c0711*x**711+c0967*x**967)
      (4*L0x2001bae4*x**199) [1043969, x**256 - 554923],
eqmod (c0199*x**199+c0455*x**455+c0711*x**711+c0967*x**967)
      (4*L0x2001bee4*x**199) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf199@sint32 : cf199 = r4 && cf199 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf455@sint32 : cf455 = r5 && cf455 = r5;

(******************** CUT 405 ********************)
ecut and [
eqmod 256*cf199 2**32*(c0199+c0708) 1043969,
eqmod 256*cf455 2**32*(c0455+c0964) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018956; PC = 0x8002728 *)
mov L0x20018956 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b56; PC = 0x800272c *)
mov L0x20018b56 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b2e8; Value = 0x00014ae7; PC = 0x8002738 *)
mov r4 L0x2001b2e8;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b6e8; Value = 0x00070ef6; PC = 0x800273c *)
mov r5 L0x2001b6e8;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001bae8; Value = 0xffffd971; PC = 0x8002740 *)
mov r6 L0x2001bae8;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bee8; Value = 0xfffaa43f; PC = 0x8002744 *)
mov r7 L0x2001bee8;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 406 ********************)

ghost c0200@sint32,c0456@sint32,c0712@sint32,c0968@sint32 :
and [c0200=r4, c0456=r5, c0712=r6, c0968=r7] && true;

ecut and [
eqmod (c0200*x**200+c0456*x**456+c0712*x**712+c0968*x**968)
      (4*L0x2001b2e8*x**200) [1043969, x**256 - 1],
eqmod (c0200*x**200+c0456*x**456+c0712*x**712+c0968*x**968)
      (4*L0x2001b6e8*x**200) [1043969, x**256 + 1],
eqmod (c0200*x**200+c0456*x**456+c0712*x**712+c0968*x**968)
      (4*L0x2001bae8*x**200) [1043969, x**256 - 554923],
eqmod (c0200*x**200+c0456*x**456+c0712*x**712+c0968*x**968)
      (4*L0x2001bee8*x**200) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf200@sint32 : cf200 = r4 && cf200 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf456@sint32 : cf456 = r5 && cf456 = r5;

(******************** CUT 407 ********************)
ecut and [
eqmod 256*cf200 2**32*(c0200+c0709) 1043969,
eqmod 256*cf456 2**32*(c0456+c0965) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018958; PC = 0x80027b0 *)
mov L0x20018958 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018b58; PC = 0x80027b4 *)
mov L0x20018b58 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b2ec; Value = 0x000780c1; PC = 0x8002628 *)
mov r4 L0x2001b2ec;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b6ec; Value = 0x0000e908; PC = 0x800262c *)
mov r5 L0x2001b6ec;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001baec; Value = 0x000013b9; PC = 0x8002630 *)
mov r6 L0x2001baec;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001beec; Value = 0x00046b61; PC = 0x8002634 *)
mov r7 L0x2001beec;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 408 ********************)

ghost c0201@sint32,c0457@sint32,c0713@sint32,c0969@sint32 :
and [c0201=r4, c0457=r5, c0713=r6, c0969=r7] && true;

ecut and [
eqmod (c0201*x**201+c0457*x**457+c0713*x**713+c0969*x**969)
      (4*L0x2001b2ec*x**201) [1043969, x**256 - 1],
eqmod (c0201*x**201+c0457*x**457+c0713*x**713+c0969*x**969)
      (4*L0x2001b6ec*x**201) [1043969, x**256 + 1],
eqmod (c0201*x**201+c0457*x**457+c0713*x**713+c0969*x**969)
      (4*L0x2001baec*x**201) [1043969, x**256 - 554923],
eqmod (c0201*x**201+c0457*x**457+c0713*x**713+c0969*x**969)
      (4*L0x2001beec*x**201) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf201@sint32 : cf201 = r4 && cf201 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf457@sint32 : cf457 = r5 && cf457 = r5;

(******************** CUT 409 ********************)
ecut and [
eqmod 256*cf201 2**32*(c0201+c0710) 1043969,
eqmod 256*cf457 2**32*(c0457+c0966) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x2001895a; PC = 0x80026a0 *)
mov L0x2001895a r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b5a; PC = 0x80026a4 *)
mov L0x20018b5a r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b2f0; Value = 0x0006b11f; PC = 0x80026b0 *)
mov r4 L0x2001b2f0;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b6f0; Value = 0xfffca5cf; PC = 0x80026b4 *)
mov r5 L0x2001b6f0;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001baf0; Value = 0xfffeaf48; PC = 0x80026b8 *)
mov r6 L0x2001baf0;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bef0; Value = 0xfff8219b; PC = 0x80026bc *)
mov r7 L0x2001bef0;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 410 ********************)

ghost c0202@sint32,c0458@sint32,c0714@sint32,c0970@sint32 :
and [c0202=r4, c0458=r5, c0714=r6, c0970=r7] && true;

ecut and [
eqmod (c0202*x**202+c0458*x**458+c0714*x**714+c0970*x**970)
      (4*L0x2001b2f0*x**202) [1043969, x**256 - 1],
eqmod (c0202*x**202+c0458*x**458+c0714*x**714+c0970*x**970)
      (4*L0x2001b6f0*x**202) [1043969, x**256 + 1],
eqmod (c0202*x**202+c0458*x**458+c0714*x**714+c0970*x**970)
      (4*L0x2001baf0*x**202) [1043969, x**256 - 554923],
eqmod (c0202*x**202+c0458*x**458+c0714*x**714+c0970*x**970)
      (4*L0x2001bef0*x**202) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf202@sint32 : cf202 = r4 && cf202 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf458@sint32 : cf458 = r5 && cf458 = r5;

(******************** CUT 411 ********************)
ecut and [
eqmod 256*cf202 2**32*(c0202+c0711) 1043969,
eqmod 256*cf458 2**32*(c0458+c0967) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x2001895c; PC = 0x8002728 *)
mov L0x2001895c r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b5c; PC = 0x800272c *)
mov L0x20018b5c r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b2f4; Value = 0xffeaca09; PC = 0x8002738 *)
mov r4 L0x2001b2f4;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b6f4; Value = 0xfff84f9d; PC = 0x800273c *)
mov r5 L0x2001b6f4;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001baf4; Value = 0x0002cd3f; PC = 0x8002740 *)
mov r6 L0x2001baf4;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bef4; Value = 0x00055a3f; PC = 0x8002744 *)
mov r7 L0x2001bef4;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 412 ********************)

ghost c0203@sint32,c0459@sint32,c0715@sint32,c0971@sint32 :
and [c0203=r4, c0459=r5, c0715=r6, c0971=r7] && true;

ecut and [
eqmod (c0203*x**203+c0459*x**459+c0715*x**715+c0971*x**971)
      (4*L0x2001b2f4*x**203) [1043969, x**256 - 1],
eqmod (c0203*x**203+c0459*x**459+c0715*x**715+c0971*x**971)
      (4*L0x2001b6f4*x**203) [1043969, x**256 + 1],
eqmod (c0203*x**203+c0459*x**459+c0715*x**715+c0971*x**971)
      (4*L0x2001baf4*x**203) [1043969, x**256 - 554923],
eqmod (c0203*x**203+c0459*x**459+c0715*x**715+c0971*x**971)
      (4*L0x2001bef4*x**203) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf203@sint32 : cf203 = r4 && cf203 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf459@sint32 : cf459 = r5 && cf459 = r5;

(******************** CUT 413 ********************)
ecut and [
eqmod 256*cf203 2**32*(c0203+c0712) 1043969,
eqmod 256*cf459 2**32*(c0459+c0968) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x2001895e; PC = 0x80027b0 *)
mov L0x2001895e r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018b5e; PC = 0x80027b4 *)
mov L0x20018b5e r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b2f8; Value = 0xfffd8e90; PC = 0x8002628 *)
mov r4 L0x2001b2f8;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b6f8; Value = 0x0005ce3d; PC = 0x800262c *)
mov r5 L0x2001b6f8;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001baf8; Value = 0x0007c4e7; PC = 0x8002630 *)
mov r6 L0x2001baf8;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bef8; Value = 0xffff32c2; PC = 0x8002634 *)
mov r7 L0x2001bef8;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 414 ********************)

ghost c0204@sint32,c0460@sint32,c0716@sint32,c0972@sint32 :
and [c0204=r4, c0460=r5, c0716=r6, c0972=r7] && true;

ecut and [
eqmod (c0204*x**204+c0460*x**460+c0716*x**716+c0972*x**972)
      (4*L0x2001b2f8*x**204) [1043969, x**256 - 1],
eqmod (c0204*x**204+c0460*x**460+c0716*x**716+c0972*x**972)
      (4*L0x2001b6f8*x**204) [1043969, x**256 + 1],
eqmod (c0204*x**204+c0460*x**460+c0716*x**716+c0972*x**972)
      (4*L0x2001baf8*x**204) [1043969, x**256 - 554923],
eqmod (c0204*x**204+c0460*x**460+c0716*x**716+c0972*x**972)
      (4*L0x2001bef8*x**204) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf204@sint32 : cf204 = r4 && cf204 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf460@sint32 : cf460 = r5 && cf460 = r5;

(******************** CUT 415 ********************)
ecut and [
eqmod 256*cf204 2**32*(c0204+c0713) 1043969,
eqmod 256*cf460 2**32*(c0460+c0969) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018960; PC = 0x80026a0 *)
mov L0x20018960 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b60; PC = 0x80026a4 *)
mov L0x20018b60 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b2fc; Value = 0x00012a8b; PC = 0x80026b0 *)
mov r4 L0x2001b2fc;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b6fc; Value = 0xfffad7af; PC = 0x80026b4 *)
mov r5 L0x2001b6fc;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001bafc; Value = 0x0004f2c1; PC = 0x80026b8 *)
mov r6 L0x2001bafc;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001befc; Value = 0xfff9dcfe; PC = 0x80026bc *)
mov r7 L0x2001befc;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 416 ********************)

ghost c0205@sint32,c0461@sint32,c0717@sint32,c0973@sint32 :
and [c0205=r4, c0461=r5, c0717=r6, c0973=r7] && true;

ecut and [
eqmod (c0205*x**205+c0461*x**461+c0717*x**717+c0973*x**973)
      (4*L0x2001b2fc*x**205) [1043969, x**256 - 1],
eqmod (c0205*x**205+c0461*x**461+c0717*x**717+c0973*x**973)
      (4*L0x2001b6fc*x**205) [1043969, x**256 + 1],
eqmod (c0205*x**205+c0461*x**461+c0717*x**717+c0973*x**973)
      (4*L0x2001bafc*x**205) [1043969, x**256 - 554923],
eqmod (c0205*x**205+c0461*x**461+c0717*x**717+c0973*x**973)
      (4*L0x2001befc*x**205) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf205@sint32 : cf205 = r4 && cf205 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf461@sint32 : cf461 = r5 && cf461 = r5;

(******************** CUT 417 ********************)
ecut and [
eqmod 256*cf205 2**32*(c0205+c0714) 1043969,
eqmod 256*cf461 2**32*(c0461+c0970) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018962; PC = 0x8002728 *)
mov L0x20018962 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b62; PC = 0x800272c *)
mov L0x20018b62 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b300; Value = 0x0006d048; PC = 0x8002738 *)
mov r4 L0x2001b300;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b700; Value = 0x0000900e; PC = 0x800273c *)
mov r5 L0x2001b700;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001bb00; Value = 0xfffd62e2; PC = 0x8002740 *)
mov r6 L0x2001bb00;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bf00; Value = 0xfffa7a91; PC = 0x8002744 *)
mov r7 L0x2001bf00;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 418 ********************)

ghost c0206@sint32,c0462@sint32,c0718@sint32,c0974@sint32 :
and [c0206=r4, c0462=r5, c0718=r6, c0974=r7] && true;

ecut and [
eqmod (c0206*x**206+c0462*x**462+c0718*x**718+c0974*x**974)
      (4*L0x2001b300*x**206) [1043969, x**256 - 1],
eqmod (c0206*x**206+c0462*x**462+c0718*x**718+c0974*x**974)
      (4*L0x2001b700*x**206) [1043969, x**256 + 1],
eqmod (c0206*x**206+c0462*x**462+c0718*x**718+c0974*x**974)
      (4*L0x2001bb00*x**206) [1043969, x**256 - 554923],
eqmod (c0206*x**206+c0462*x**462+c0718*x**718+c0974*x**974)
      (4*L0x2001bf00*x**206) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf206@sint32 : cf206 = r4 && cf206 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf462@sint32 : cf462 = r5 && cf462 = r5;

(******************** CUT 419 ********************)
ecut and [
eqmod 256*cf206 2**32*(c0206+c0715) 1043969,
eqmod 256*cf462 2**32*(c0462+c0971) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018964; PC = 0x80027b0 *)
mov L0x20018964 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018b64; PC = 0x80027b4 *)
mov L0x20018b64 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b304; Value = 0x001d47dc; PC = 0x8002628 *)
mov r4 L0x2001b304;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b704; Value = 0xfff8b63c; PC = 0x800262c *)
mov r5 L0x2001b704;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001bb04; Value = 0x0007cc9a; PC = 0x8002630 *)
mov r6 L0x2001bb04;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bf04; Value = 0x0004b9da; PC = 0x8002634 *)
mov r7 L0x2001bf04;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 420 ********************)

ghost c0207@sint32,c0463@sint32,c0719@sint32,c0975@sint32 :
and [c0207=r4, c0463=r5, c0719=r6, c0975=r7] && true;

ecut and [
eqmod (c0207*x**207+c0463*x**463+c0719*x**719+c0975*x**975)
      (4*L0x2001b304*x**207) [1043969, x**256 - 1],
eqmod (c0207*x**207+c0463*x**463+c0719*x**719+c0975*x**975)
      (4*L0x2001b704*x**207) [1043969, x**256 + 1],
eqmod (c0207*x**207+c0463*x**463+c0719*x**719+c0975*x**975)
      (4*L0x2001bb04*x**207) [1043969, x**256 - 554923],
eqmod (c0207*x**207+c0463*x**463+c0719*x**719+c0975*x**975)
      (4*L0x2001bf04*x**207) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf207@sint32 : cf207 = r4 && cf207 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf463@sint32 : cf463 = r5 && cf463 = r5;

(******************** CUT 421 ********************)
ecut and [
eqmod 256*cf207 2**32*(c0207+c0716) 1043969,
eqmod 256*cf463 2**32*(c0463+c0972) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018966; PC = 0x80026a0 *)
mov L0x20018966 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b66; PC = 0x80026a4 *)
mov L0x20018b66 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b308; Value = 0x00076187; PC = 0x80026b0 *)
mov r4 L0x2001b308;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b708; Value = 0x0001e901; PC = 0x80026b4 *)
mov r5 L0x2001b708;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001bb08; Value = 0xfffcab18; PC = 0x80026b8 *)
mov r6 L0x2001bb08;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bf08; Value = 0xfffe88a3; PC = 0x80026bc *)
mov r7 L0x2001bf08;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 422 ********************)

ghost c0208@sint32,c0464@sint32,c0720@sint32,c0976@sint32 :
and [c0208=r4, c0464=r5, c0720=r6, c0976=r7] && true;

ecut and [
eqmod (c0208*x**208+c0464*x**464+c0720*x**720+c0976*x**976)
      (4*L0x2001b308*x**208) [1043969, x**256 - 1],
eqmod (c0208*x**208+c0464*x**464+c0720*x**720+c0976*x**976)
      (4*L0x2001b708*x**208) [1043969, x**256 + 1],
eqmod (c0208*x**208+c0464*x**464+c0720*x**720+c0976*x**976)
      (4*L0x2001bb08*x**208) [1043969, x**256 - 554923],
eqmod (c0208*x**208+c0464*x**464+c0720*x**720+c0976*x**976)
      (4*L0x2001bf08*x**208) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf208@sint32 : cf208 = r4 && cf208 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf464@sint32 : cf464 = r5 && cf464 = r5;

(******************** CUT 423 ********************)
ecut and [
eqmod 256*cf208 2**32*(c0208+c0717) 1043969,
eqmod 256*cf464 2**32*(c0464+c0973) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018968; PC = 0x8002728 *)
mov L0x20018968 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b68; PC = 0x800272c *)
mov L0x20018b68 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b30c; Value = 0xfff490c3; PC = 0x8002738 *)
mov r4 L0x2001b30c;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b70c; Value = 0x0000e8a3; PC = 0x800273c *)
mov r5 L0x2001b70c;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001bb0c; Value = 0x00053e28; PC = 0x8002740 *)
mov r6 L0x2001bb0c;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bf0c; Value = 0x0000a14b; PC = 0x8002744 *)
mov r7 L0x2001bf0c;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 424 ********************)

ghost c0209@sint32,c0465@sint32,c0721@sint32,c0977@sint32 :
and [c0209=r4, c0465=r5, c0721=r6, c0977=r7] && true;

ecut and [
eqmod (c0209*x**209+c0465*x**465+c0721*x**721+c0977*x**977)
      (4*L0x2001b30c*x**209) [1043969, x**256 - 1],
eqmod (c0209*x**209+c0465*x**465+c0721*x**721+c0977*x**977)
      (4*L0x2001b70c*x**209) [1043969, x**256 + 1],
eqmod (c0209*x**209+c0465*x**465+c0721*x**721+c0977*x**977)
      (4*L0x2001bb0c*x**209) [1043969, x**256 - 554923],
eqmod (c0209*x**209+c0465*x**465+c0721*x**721+c0977*x**977)
      (4*L0x2001bf0c*x**209) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf209@sint32 : cf209 = r4 && cf209 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf465@sint32 : cf465 = r5 && cf465 = r5;

(******************** CUT 425 ********************)
ecut and [
eqmod 256*cf209 2**32*(c0209+c0718) 1043969,
eqmod 256*cf465 2**32*(c0465+c0974) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x2001896a; PC = 0x80027b0 *)
mov L0x2001896a r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018b6a; PC = 0x80027b4 *)
mov L0x20018b6a r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b310; Value = 0xfffebe63; PC = 0x8002628 *)
mov r4 L0x2001b310;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b710; Value = 0x00078c6b; PC = 0x800262c *)
mov r5 L0x2001b710;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001bb10; Value = 0xfffdee9b; PC = 0x8002630 *)
mov r6 L0x2001bb10;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bf10; Value = 0x000356bd; PC = 0x8002634 *)
mov r7 L0x2001bf10;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 426 ********************)

ghost c0210@sint32,c0466@sint32,c0722@sint32,c0978@sint32 :
and [c0210=r4, c0466=r5, c0722=r6, c0978=r7] && true;

ecut and [
eqmod (c0210*x**210+c0466*x**466+c0722*x**722+c0978*x**978)
      (4*L0x2001b310*x**210) [1043969, x**256 - 1],
eqmod (c0210*x**210+c0466*x**466+c0722*x**722+c0978*x**978)
      (4*L0x2001b710*x**210) [1043969, x**256 + 1],
eqmod (c0210*x**210+c0466*x**466+c0722*x**722+c0978*x**978)
      (4*L0x2001bb10*x**210) [1043969, x**256 - 554923],
eqmod (c0210*x**210+c0466*x**466+c0722*x**722+c0978*x**978)
      (4*L0x2001bf10*x**210) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf210@sint32 : cf210 = r4 && cf210 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf466@sint32 : cf466 = r5 && cf466 = r5;

(******************** CUT 427 ********************)
ecut and [
eqmod 256*cf210 2**32*(c0210+c0719) 1043969,
eqmod 256*cf466 2**32*(c0466+c0975) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x2001896c; PC = 0x80026a0 *)
mov L0x2001896c r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b6c; PC = 0x80026a4 *)
mov L0x20018b6c r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b314; Value = 0xffedfec1; PC = 0x80026b0 *)
mov r4 L0x2001b314;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b714; Value = 0xfff9f11f; PC = 0x80026b4 *)
mov r5 L0x2001b714;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001bb14; Value = 0xfffc119e; PC = 0x80026b8 *)
mov r6 L0x2001bb14;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bf14; Value = 0xfffd3da7; PC = 0x80026bc *)
mov r7 L0x2001bf14;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 428 ********************)

ghost c0211@sint32,c0467@sint32,c0723@sint32,c0979@sint32 :
and [c0211=r4, c0467=r5, c0723=r6, c0979=r7] && true;

ecut and [
eqmod (c0211*x**211+c0467*x**467+c0723*x**723+c0979*x**979)
      (4*L0x2001b314*x**211) [1043969, x**256 - 1],
eqmod (c0211*x**211+c0467*x**467+c0723*x**723+c0979*x**979)
      (4*L0x2001b714*x**211) [1043969, x**256 + 1],
eqmod (c0211*x**211+c0467*x**467+c0723*x**723+c0979*x**979)
      (4*L0x2001bb14*x**211) [1043969, x**256 - 554923],
eqmod (c0211*x**211+c0467*x**467+c0723*x**723+c0979*x**979)
      (4*L0x2001bf14*x**211) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf211@sint32 : cf211 = r4 && cf211 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf467@sint32 : cf467 = r5 && cf467 = r5;

(******************** CUT 429 ********************)
ecut and [
eqmod 256*cf211 2**32*(c0211+c0720) 1043969,
eqmod 256*cf467 2**32*(c0467+c0976) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x2001896e; PC = 0x8002728 *)
mov L0x2001896e r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b6e; PC = 0x800272c *)
mov L0x20018b6e r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b318; Value = 0xfff999d8; PC = 0x8002738 *)
mov r4 L0x2001b318;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b718; Value = 0x00053a4e; PC = 0x800273c *)
mov r5 L0x2001b718;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001bb18; Value = 0x00062348; PC = 0x8002740 *)
mov r6 L0x2001bb18;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bf18; Value = 0xfffa5743; PC = 0x8002744 *)
mov r7 L0x2001bf18;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 430 ********************)

ghost c0212@sint32,c0468@sint32,c0724@sint32,c0980@sint32 :
and [c0212=r4, c0468=r5, c0724=r6, c0980=r7] && true;

ecut and [
eqmod (c0212*x**212+c0468*x**468+c0724*x**724+c0980*x**980)
      (4*L0x2001b318*x**212) [1043969, x**256 - 1],
eqmod (c0212*x**212+c0468*x**468+c0724*x**724+c0980*x**980)
      (4*L0x2001b718*x**212) [1043969, x**256 + 1],
eqmod (c0212*x**212+c0468*x**468+c0724*x**724+c0980*x**980)
      (4*L0x2001bb18*x**212) [1043969, x**256 - 554923],
eqmod (c0212*x**212+c0468*x**468+c0724*x**724+c0980*x**980)
      (4*L0x2001bf18*x**212) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf212@sint32 : cf212 = r4 && cf212 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf468@sint32 : cf468 = r5 && cf468 = r5;

(******************** CUT 431 ********************)
ecut and [
eqmod 256*cf212 2**32*(c0212+c0721) 1043969,
eqmod 256*cf468 2**32*(c0468+c0977) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018970; PC = 0x80027b0 *)
mov L0x20018970 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018b70; PC = 0x80027b4 *)
mov L0x20018b70 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b31c; Value = 0xfffa91c0; PC = 0x8002628 *)
mov r4 L0x2001b31c;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b71c; Value = 0xfffbe060; PC = 0x800262c *)
mov r5 L0x2001b71c;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001bb1c; Value = 0xfffd5032; PC = 0x8002630 *)
mov r6 L0x2001bb1c;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bf1c; Value = 0x00068838; PC = 0x8002634 *)
mov r7 L0x2001bf1c;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 432 ********************)

ghost c0213@sint32,c0469@sint32,c0725@sint32,c0981@sint32 :
and [c0213=r4, c0469=r5, c0725=r6, c0981=r7] && true;

ecut and [
eqmod (c0213*x**213+c0469*x**469+c0725*x**725+c0981*x**981)
      (4*L0x2001b31c*x**213) [1043969, x**256 - 1],
eqmod (c0213*x**213+c0469*x**469+c0725*x**725+c0981*x**981)
      (4*L0x2001b71c*x**213) [1043969, x**256 + 1],
eqmod (c0213*x**213+c0469*x**469+c0725*x**725+c0981*x**981)
      (4*L0x2001bb1c*x**213) [1043969, x**256 - 554923],
eqmod (c0213*x**213+c0469*x**469+c0725*x**725+c0981*x**981)
      (4*L0x2001bf1c*x**213) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf213@sint32 : cf213 = r4 && cf213 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf469@sint32 : cf469 = r5 && cf469 = r5;

(******************** CUT 433 ********************)
ecut and [
eqmod 256*cf213 2**32*(c0213+c0722) 1043969,
eqmod 256*cf469 2**32*(c0469+c0978) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018972; PC = 0x80026a0 *)
mov L0x20018972 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b72; PC = 0x80026a4 *)
mov L0x20018b72 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b320; Value = 0x000aff8c; PC = 0x80026b0 *)
mov r4 L0x2001b320;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b720; Value = 0xfffbd253; PC = 0x80026b4 *)
mov r5 L0x2001b720;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001bb20; Value = 0x0001f95c; PC = 0x80026b8 *)
mov r6 L0x2001bb20;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bf20; Value = 0x0000a848; PC = 0x80026bc *)
mov r7 L0x2001bf20;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 434 ********************)

ghost c0214@sint32,c0470@sint32,c0726@sint32,c0982@sint32 :
and [c0214=r4, c0470=r5, c0726=r6, c0982=r7] && true;

ecut and [
eqmod (c0214*x**214+c0470*x**470+c0726*x**726+c0982*x**982)
      (4*L0x2001b320*x**214) [1043969, x**256 - 1],
eqmod (c0214*x**214+c0470*x**470+c0726*x**726+c0982*x**982)
      (4*L0x2001b720*x**214) [1043969, x**256 + 1],
eqmod (c0214*x**214+c0470*x**470+c0726*x**726+c0982*x**982)
      (4*L0x2001bb20*x**214) [1043969, x**256 - 554923],
eqmod (c0214*x**214+c0470*x**470+c0726*x**726+c0982*x**982)
      (4*L0x2001bf20*x**214) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf214@sint32 : cf214 = r4 && cf214 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf470@sint32 : cf470 = r5 && cf470 = r5;

(******************** CUT 435 ********************)
ecut and [
eqmod 256*cf214 2**32*(c0214+c0723) 1043969,
eqmod 256*cf470 2**32*(c0470+c0979) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018974; PC = 0x8002728 *)
mov L0x20018974 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b74; PC = 0x800272c *)
mov L0x20018b74 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b324; Value = 0xffe99d5d; PC = 0x8002738 *)
mov r4 L0x2001b324;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b724; Value = 0xfff8260e; PC = 0x800273c *)
mov r5 L0x2001b724;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001bb24; Value = 0xfffcc1b3; PC = 0x8002740 *)
mov r6 L0x2001bb24;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bf24; Value = 0x00065e23; PC = 0x8002744 *)
mov r7 L0x2001bf24;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 436 ********************)

ghost c0215@sint32,c0471@sint32,c0727@sint32,c0983@sint32 :
and [c0215=r4, c0471=r5, c0727=r6, c0983=r7] && true;

ecut and [
eqmod (c0215*x**215+c0471*x**471+c0727*x**727+c0983*x**983)
      (4*L0x2001b324*x**215) [1043969, x**256 - 1],
eqmod (c0215*x**215+c0471*x**471+c0727*x**727+c0983*x**983)
      (4*L0x2001b724*x**215) [1043969, x**256 + 1],
eqmod (c0215*x**215+c0471*x**471+c0727*x**727+c0983*x**983)
      (4*L0x2001bb24*x**215) [1043969, x**256 - 554923],
eqmod (c0215*x**215+c0471*x**471+c0727*x**727+c0983*x**983)
      (4*L0x2001bf24*x**215) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf215@sint32 : cf215 = r4 && cf215 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf471@sint32 : cf471 = r5 && cf471 = r5;

(******************** CUT 437 ********************)
ecut and [
eqmod 256*cf215 2**32*(c0215+c0724) 1043969,
eqmod 256*cf471 2**32*(c0471+c0980) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018976; PC = 0x80027b0 *)
mov L0x20018976 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018b76; PC = 0x80027b4 *)
mov L0x20018b76 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b328; Value = 0x0005ceda; PC = 0x8002628 *)
mov r4 L0x2001b328;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b728; Value = 0x00023c5b; PC = 0x800262c *)
mov r5 L0x2001b728;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001bb28; Value = 0x000428f6; PC = 0x8002630 *)
mov r6 L0x2001bb28;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bf28; Value = 0xfffeb3b8; PC = 0x8002634 *)
mov r7 L0x2001bf28;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 438 ********************)

ghost c0216@sint32,c0472@sint32,c0728@sint32,c0984@sint32 :
and [c0216=r4, c0472=r5, c0728=r6, c0984=r7] && true;

ecut and [
eqmod (c0216*x**216+c0472*x**472+c0728*x**728+c0984*x**984)
      (4*L0x2001b328*x**216) [1043969, x**256 - 1],
eqmod (c0216*x**216+c0472*x**472+c0728*x**728+c0984*x**984)
      (4*L0x2001b728*x**216) [1043969, x**256 + 1],
eqmod (c0216*x**216+c0472*x**472+c0728*x**728+c0984*x**984)
      (4*L0x2001bb28*x**216) [1043969, x**256 - 554923],
eqmod (c0216*x**216+c0472*x**472+c0728*x**728+c0984*x**984)
      (4*L0x2001bf28*x**216) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf216@sint32 : cf216 = r4 && cf216 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf472@sint32 : cf472 = r5 && cf472 = r5;

(******************** CUT 439 ********************)
ecut and [
eqmod 256*cf216 2**32*(c0216+c0725) 1043969,
eqmod 256*cf472 2**32*(c0472+c0981) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018978; PC = 0x80026a0 *)
mov L0x20018978 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b78; PC = 0x80026a4 *)
mov L0x20018b78 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b32c; Value = 0x0006256c; PC = 0x80026b0 *)
mov r4 L0x2001b32c;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b72c; Value = 0xfffeb975; PC = 0x80026b4 *)
mov r5 L0x2001b72c;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001bb2c; Value = 0x00038d84; PC = 0x80026b8 *)
mov r6 L0x2001bb2c;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bf2c; Value = 0xfffe94fa; PC = 0x80026bc *)
mov r7 L0x2001bf2c;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 440 ********************)

ghost c0217@sint32,c0473@sint32,c0729@sint32,c0985@sint32 :
and [c0217=r4, c0473=r5, c0729=r6, c0985=r7] && true;

ecut and [
eqmod (c0217*x**217+c0473*x**473+c0729*x**729+c0985*x**985)
      (4*L0x2001b32c*x**217) [1043969, x**256 - 1],
eqmod (c0217*x**217+c0473*x**473+c0729*x**729+c0985*x**985)
      (4*L0x2001b72c*x**217) [1043969, x**256 + 1],
eqmod (c0217*x**217+c0473*x**473+c0729*x**729+c0985*x**985)
      (4*L0x2001bb2c*x**217) [1043969, x**256 - 554923],
eqmod (c0217*x**217+c0473*x**473+c0729*x**729+c0985*x**985)
      (4*L0x2001bf2c*x**217) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf217@sint32 : cf217 = r4 && cf217 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf473@sint32 : cf473 = r5 && cf473 = r5;

(******************** CUT 441 ********************)
ecut and [
eqmod 256*cf217 2**32*(c0217+c0726) 1043969,
eqmod 256*cf473 2**32*(c0473+c0982) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x2001897a; PC = 0x8002728 *)
mov L0x2001897a r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b7a; PC = 0x800272c *)
mov L0x20018b7a r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b330; Value = 0x0001c3cd; PC = 0x8002738 *)
mov r4 L0x2001b330;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b730; Value = 0xfff84b74; PC = 0x800273c *)
mov r5 L0x2001b730;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001bb30; Value = 0xfff882a7; PC = 0x8002740 *)
mov r6 L0x2001bb30;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bf30; Value = 0xfffe3bd7; PC = 0x8002744 *)
mov r7 L0x2001bf30;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 442 ********************)

ghost c0218@sint32,c0474@sint32,c0730@sint32,c0986@sint32 :
and [c0218=r4, c0474=r5, c0730=r6, c0986=r7] && true;

ecut and [
eqmod (c0218*x**218+c0474*x**474+c0730*x**730+c0986*x**986)
      (4*L0x2001b330*x**218) [1043969, x**256 - 1],
eqmod (c0218*x**218+c0474*x**474+c0730*x**730+c0986*x**986)
      (4*L0x2001b730*x**218) [1043969, x**256 + 1],
eqmod (c0218*x**218+c0474*x**474+c0730*x**730+c0986*x**986)
      (4*L0x2001bb30*x**218) [1043969, x**256 - 554923],
eqmod (c0218*x**218+c0474*x**474+c0730*x**730+c0986*x**986)
      (4*L0x2001bf30*x**218) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf218@sint32 : cf218 = r4 && cf218 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf474@sint32 : cf474 = r5 && cf474 = r5;

(******************** CUT 443 ********************)
ecut and [
eqmod 256*cf218 2**32*(c0218+c0727) 1043969,
eqmod 256*cf474 2**32*(c0474+c0983) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x2001897c; PC = 0x80027b0 *)
mov L0x2001897c r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018b7c; PC = 0x80027b4 *)
mov L0x20018b7c r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b334; Value = 0xffee99df; PC = 0x8002628 *)
mov r4 L0x2001b334;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b734; Value = 0x0000f490; PC = 0x800262c *)
mov r5 L0x2001b734;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001bb34; Value = 0xfffe3a4e; PC = 0x8002630 *)
mov r6 L0x2001bb34;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bf34; Value = 0x0002ba40; PC = 0x8002634 *)
mov r7 L0x2001bf34;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 444 ********************)

ghost c0219@sint32,c0475@sint32,c0731@sint32,c0987@sint32 :
and [c0219=r4, c0475=r5, c0731=r6, c0987=r7] && true;

ecut and [
eqmod (c0219*x**219+c0475*x**475+c0731*x**731+c0987*x**987)
      (4*L0x2001b334*x**219) [1043969, x**256 - 1],
eqmod (c0219*x**219+c0475*x**475+c0731*x**731+c0987*x**987)
      (4*L0x2001b734*x**219) [1043969, x**256 + 1],
eqmod (c0219*x**219+c0475*x**475+c0731*x**731+c0987*x**987)
      (4*L0x2001bb34*x**219) [1043969, x**256 - 554923],
eqmod (c0219*x**219+c0475*x**475+c0731*x**731+c0987*x**987)
      (4*L0x2001bf34*x**219) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf219@sint32 : cf219 = r4 && cf219 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf475@sint32 : cf475 = r5 && cf475 = r5;

(******************** CUT 445 ********************)
ecut and [
eqmod 256*cf219 2**32*(c0219+c0728) 1043969,
eqmod 256*cf475 2**32*(c0475+c0984) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x2001897e; PC = 0x80026a0 *)
mov L0x2001897e r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b7e; PC = 0x80026a4 *)
mov L0x20018b7e r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b338; Value = 0x0003cdcf; PC = 0x80026b0 *)
mov r4 L0x2001b338;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b738; Value = 0xfffe9848; PC = 0x80026b4 *)
mov r5 L0x2001b738;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001bb38; Value = 0xfffd0032; PC = 0x80026b8 *)
mov r6 L0x2001bb38;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bf38; Value = 0x00052187; PC = 0x80026bc *)
mov r7 L0x2001bf38;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 446 ********************)

ghost c0220@sint32,c0476@sint32,c0732@sint32,c0988@sint32 :
and [c0220=r4, c0476=r5, c0732=r6, c0988=r7] && true;

ecut and [
eqmod (c0220*x**220+c0476*x**476+c0732*x**732+c0988*x**988)
      (4*L0x2001b338*x**220) [1043969, x**256 - 1],
eqmod (c0220*x**220+c0476*x**476+c0732*x**732+c0988*x**988)
      (4*L0x2001b738*x**220) [1043969, x**256 + 1],
eqmod (c0220*x**220+c0476*x**476+c0732*x**732+c0988*x**988)
      (4*L0x2001bb38*x**220) [1043969, x**256 - 554923],
eqmod (c0220*x**220+c0476*x**476+c0732*x**732+c0988*x**988)
      (4*L0x2001bf38*x**220) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf220@sint32 : cf220 = r4 && cf220 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf476@sint32 : cf476 = r5 && cf476 = r5;

(******************** CUT 447 ********************)
ecut and [
eqmod 256*cf220 2**32*(c0220+c0729) 1043969,
eqmod 256*cf476 2**32*(c0476+c0985) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018980; PC = 0x8002728 *)
mov L0x20018980 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b80; PC = 0x800272c *)
mov L0x20018b80 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b33c; Value = 0x000edbfc; PC = 0x8002738 *)
mov r4 L0x2001b33c;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b73c; Value = 0xfff916d6; PC = 0x800273c *)
mov r5 L0x2001b73c;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001bb3c; Value = 0xfffe7046; PC = 0x8002740 *)
mov r6 L0x2001bb3c;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bf3c; Value = 0xfffa92a5; PC = 0x8002744 *)
mov r7 L0x2001bf3c;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 448 ********************)

ghost c0221@sint32,c0477@sint32,c0733@sint32,c0989@sint32 :
and [c0221=r4, c0477=r5, c0733=r6, c0989=r7] && true;

ecut and [
eqmod (c0221*x**221+c0477*x**477+c0733*x**733+c0989*x**989)
      (4*L0x2001b33c*x**221) [1043969, x**256 - 1],
eqmod (c0221*x**221+c0477*x**477+c0733*x**733+c0989*x**989)
      (4*L0x2001b73c*x**221) [1043969, x**256 + 1],
eqmod (c0221*x**221+c0477*x**477+c0733*x**733+c0989*x**989)
      (4*L0x2001bb3c*x**221) [1043969, x**256 - 554923],
eqmod (c0221*x**221+c0477*x**477+c0733*x**733+c0989*x**989)
      (4*L0x2001bf3c*x**221) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf221@sint32 : cf221 = r4 && cf221 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf477@sint32 : cf477 = r5 && cf477 = r5;

(******************** CUT 449 ********************)
ecut and [
eqmod 256*cf221 2**32*(c0221+c0730) 1043969,
eqmod 256*cf477 2**32*(c0477+c0986) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018982; PC = 0x80027b0 *)
mov L0x20018982 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018b82; PC = 0x80027b4 *)
mov L0x20018b82 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b340; Value = 0xfff2e68e; PC = 0x8002628 *)
mov r4 L0x2001b340;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b740; Value = 0xfff8b752; PC = 0x800262c *)
mov r5 L0x2001b740;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001bb40; Value = 0x0007e487; PC = 0x8002630 *)
mov r6 L0x2001bb40;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bf40; Value = 0xfffd08d2; PC = 0x8002634 *)
mov r7 L0x2001bf40;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 450 ********************)

ghost c0222@sint32,c0478@sint32,c0734@sint32,c0990@sint32 :
and [c0222=r4, c0478=r5, c0734=r6, c0990=r7] && true;

ecut and [
eqmod (c0222*x**222+c0478*x**478+c0734*x**734+c0990*x**990)
      (4*L0x2001b340*x**222) [1043969, x**256 - 1],
eqmod (c0222*x**222+c0478*x**478+c0734*x**734+c0990*x**990)
      (4*L0x2001b740*x**222) [1043969, x**256 + 1],
eqmod (c0222*x**222+c0478*x**478+c0734*x**734+c0990*x**990)
      (4*L0x2001bb40*x**222) [1043969, x**256 - 554923],
eqmod (c0222*x**222+c0478*x**478+c0734*x**734+c0990*x**990)
      (4*L0x2001bf40*x**222) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf222@sint32 : cf222 = r4 && cf222 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf478@sint32 : cf478 = r5 && cf478 = r5;

(******************** CUT 451 ********************)
ecut and [
eqmod 256*cf222 2**32*(c0222+c0731) 1043969,
eqmod 256*cf478 2**32*(c0478+c0987) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018984; PC = 0x80026a0 *)
mov L0x20018984 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b84; PC = 0x80026a4 *)
mov L0x20018b84 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b344; Value = 0xffe95086; PC = 0x80026b0 *)
mov r4 L0x2001b344;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b744; Value = 0xfffb0cbe; PC = 0x80026b4 *)
mov r5 L0x2001b744;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001bb44; Value = 0x00041a7f; PC = 0x80026b8 *)
mov r6 L0x2001bb44;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bf44; Value = 0x0003b249; PC = 0x80026bc *)
mov r7 L0x2001bf44;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 452 ********************)

ghost c0223@sint32,c0479@sint32,c0735@sint32,c0991@sint32 :
and [c0223=r4, c0479=r5, c0735=r6, c0991=r7] && true;

ecut and [
eqmod (c0223*x**223+c0479*x**479+c0735*x**735+c0991*x**991)
      (4*L0x2001b344*x**223) [1043969, x**256 - 1],
eqmod (c0223*x**223+c0479*x**479+c0735*x**735+c0991*x**991)
      (4*L0x2001b744*x**223) [1043969, x**256 + 1],
eqmod (c0223*x**223+c0479*x**479+c0735*x**735+c0991*x**991)
      (4*L0x2001bb44*x**223) [1043969, x**256 - 554923],
eqmod (c0223*x**223+c0479*x**479+c0735*x**735+c0991*x**991)
      (4*L0x2001bf44*x**223) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf223@sint32 : cf223 = r4 && cf223 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf479@sint32 : cf479 = r5 && cf479 = r5;

(******************** CUT 453 ********************)
ecut and [
eqmod 256*cf223 2**32*(c0223+c0732) 1043969,
eqmod 256*cf479 2**32*(c0479+c0988) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018986; PC = 0x8002728 *)
mov L0x20018986 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b86; PC = 0x800272c *)
mov L0x20018b86 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b348; Value = 0x000e9c19; PC = 0x8002738 *)
mov r4 L0x2001b348;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b748; Value = 0x00011c1b; PC = 0x800273c *)
mov r5 L0x2001b748;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001bb48; Value = 0xfff9acf7; PC = 0x8002740 *)
mov r6 L0x2001bb48;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bf48; Value = 0xfffe720d; PC = 0x8002744 *)
mov r7 L0x2001bf48;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 454 ********************)

ghost c0224@sint32,c0480@sint32,c0736@sint32,c0992@sint32 :
and [c0224=r4, c0480=r5, c0736=r6, c0992=r7] && true;

ecut and [
eqmod (c0224*x**224+c0480*x**480+c0736*x**736+c0992*x**992)
      (4*L0x2001b348*x**224) [1043969, x**256 - 1],
eqmod (c0224*x**224+c0480*x**480+c0736*x**736+c0992*x**992)
      (4*L0x2001b748*x**224) [1043969, x**256 + 1],
eqmod (c0224*x**224+c0480*x**480+c0736*x**736+c0992*x**992)
      (4*L0x2001bb48*x**224) [1043969, x**256 - 554923],
eqmod (c0224*x**224+c0480*x**480+c0736*x**736+c0992*x**992)
      (4*L0x2001bf48*x**224) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf224@sint32 : cf224 = r4 && cf224 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf480@sint32 : cf480 = r5 && cf480 = r5;

(******************** CUT 455 ********************)
ecut and [
eqmod 256*cf224 2**32*(c0224+c0733) 1043969,
eqmod 256*cf480 2**32*(c0480+c0989) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018988; PC = 0x80027b0 *)
mov L0x20018988 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018b88; PC = 0x80027b4 *)
mov L0x20018b88 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b34c; Value = 0x000604c5; PC = 0x8002628 *)
mov r4 L0x2001b34c;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b74c; Value = 0xfffd3e96; PC = 0x800262c *)
mov r5 L0x2001b74c;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001bb4c; Value = 0x00046ab1; PC = 0x8002630 *)
mov r6 L0x2001bb4c;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bf4c; Value = 0x00037b3d; PC = 0x8002634 *)
mov r7 L0x2001bf4c;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 456 ********************)

ghost c0225@sint32,c0481@sint32,c0737@sint32,c0993@sint32 :
and [c0225=r4, c0481=r5, c0737=r6, c0993=r7] && true;

ecut and [
eqmod (c0225*x**225+c0481*x**481+c0737*x**737+c0993*x**993)
      (4*L0x2001b34c*x**225) [1043969, x**256 - 1],
eqmod (c0225*x**225+c0481*x**481+c0737*x**737+c0993*x**993)
      (4*L0x2001b74c*x**225) [1043969, x**256 + 1],
eqmod (c0225*x**225+c0481*x**481+c0737*x**737+c0993*x**993)
      (4*L0x2001bb4c*x**225) [1043969, x**256 - 554923],
eqmod (c0225*x**225+c0481*x**481+c0737*x**737+c0993*x**993)
      (4*L0x2001bf4c*x**225) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf225@sint32 : cf225 = r4 && cf225 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf481@sint32 : cf481 = r5 && cf481 = r5;

(******************** CUT 457 ********************)
ecut and [
eqmod 256*cf225 2**32*(c0225+c0734) 1043969,
eqmod 256*cf481 2**32*(c0481+c0990) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x2001898a; PC = 0x80026a0 *)
mov L0x2001898a r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b8a; PC = 0x80026a4 *)
mov L0x20018b8a r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b350; Value = 0x0004691a; PC = 0x80026b0 *)
mov r4 L0x2001b350;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b750; Value = 0xfffbf160; PC = 0x80026b4 *)
mov r5 L0x2001b750;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001bb50; Value = 0x0003a61d; PC = 0x80026b8 *)
mov r6 L0x2001bb50;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bf50; Value = 0xfffdc0f0; PC = 0x80026bc *)
mov r7 L0x2001bf50;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 458 ********************)

ghost c0226@sint32,c0482@sint32,c0738@sint32,c0994@sint32 :
and [c0226=r4, c0482=r5, c0738=r6, c0994=r7] && true;

ecut and [
eqmod (c0226*x**226+c0482*x**482+c0738*x**738+c0994*x**994)
      (4*L0x2001b350*x**226) [1043969, x**256 - 1],
eqmod (c0226*x**226+c0482*x**482+c0738*x**738+c0994*x**994)
      (4*L0x2001b750*x**226) [1043969, x**256 + 1],
eqmod (c0226*x**226+c0482*x**482+c0738*x**738+c0994*x**994)
      (4*L0x2001bb50*x**226) [1043969, x**256 - 554923],
eqmod (c0226*x**226+c0482*x**482+c0738*x**738+c0994*x**994)
      (4*L0x2001bf50*x**226) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf226@sint32 : cf226 = r4 && cf226 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf482@sint32 : cf482 = r5 && cf482 = r5;

(******************** CUT 459 ********************)
ecut and [
eqmod 256*cf226 2**32*(c0226+c0735) 1043969,
eqmod 256*cf482 2**32*(c0482+c0991) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x2001898c; PC = 0x8002728 *)
mov L0x2001898c r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b8c; PC = 0x800272c *)
mov L0x20018b8c r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b354; Value = 0x0017f403; PC = 0x8002738 *)
mov r4 L0x2001b354;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b754; Value = 0xfff8e39b; PC = 0x800273c *)
mov r5 L0x2001b754;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001bb54; Value = 0x00018e34; PC = 0x8002740 *)
mov r6 L0x2001bb54;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bf54; Value = 0xffff3e6c; PC = 0x8002744 *)
mov r7 L0x2001bf54;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 460 ********************)

ghost c0227@sint32,c0483@sint32,c0739@sint32,c0995@sint32 :
and [c0227=r4, c0483=r5, c0739=r6, c0995=r7] && true;

ecut and [
eqmod (c0227*x**227+c0483*x**483+c0739*x**739+c0995*x**995)
      (4*L0x2001b354*x**227) [1043969, x**256 - 1],
eqmod (c0227*x**227+c0483*x**483+c0739*x**739+c0995*x**995)
      (4*L0x2001b754*x**227) [1043969, x**256 + 1],
eqmod (c0227*x**227+c0483*x**483+c0739*x**739+c0995*x**995)
      (4*L0x2001bb54*x**227) [1043969, x**256 - 554923],
eqmod (c0227*x**227+c0483*x**483+c0739*x**739+c0995*x**995)
      (4*L0x2001bf54*x**227) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf227@sint32 : cf227 = r4 && cf227 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf483@sint32 : cf483 = r5 && cf483 = r5;

(******************** CUT 461 ********************)
ecut and [
eqmod 256*cf227 2**32*(c0227+c0736) 1043969,
eqmod 256*cf483 2**32*(c0483+c0992) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x2001898e; PC = 0x80027b0 *)
mov L0x2001898e r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018b8e; PC = 0x80027b4 *)
mov L0x20018b8e r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b358; Value = 0x00026a78; PC = 0x8002628 *)
mov r4 L0x2001b358;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b758; Value = 0x0000ae7f; PC = 0x800262c *)
mov r5 L0x2001b758;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001bb58; Value = 0x000161dd; PC = 0x8002630 *)
mov r6 L0x2001bb58;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bf58; Value = 0xfffdb8ff; PC = 0x8002634 *)
mov r7 L0x2001bf58;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 462 ********************)

ghost c0228@sint32,c0484@sint32,c0740@sint32,c0996@sint32 :
and [c0228=r4, c0484=r5, c0740=r6, c0996=r7] && true;

ecut and [
eqmod (c0228*x**228+c0484*x**484+c0740*x**740+c0996*x**996)
      (4*L0x2001b358*x**228) [1043969, x**256 - 1],
eqmod (c0228*x**228+c0484*x**484+c0740*x**740+c0996*x**996)
      (4*L0x2001b758*x**228) [1043969, x**256 + 1],
eqmod (c0228*x**228+c0484*x**484+c0740*x**740+c0996*x**996)
      (4*L0x2001bb58*x**228) [1043969, x**256 - 554923],
eqmod (c0228*x**228+c0484*x**484+c0740*x**740+c0996*x**996)
      (4*L0x2001bf58*x**228) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf228@sint32 : cf228 = r4 && cf228 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf484@sint32 : cf484 = r5 && cf484 = r5;

(******************** CUT 463 ********************)
ecut and [
eqmod 256*cf228 2**32*(c0228+c0737) 1043969,
eqmod 256*cf484 2**32*(c0484+c0993) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018990; PC = 0x80026a0 *)
mov L0x20018990 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b90; PC = 0x80026a4 *)
mov L0x20018b90 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b35c; Value = 0x000c752b; PC = 0x80026b0 *)
mov r4 L0x2001b35c;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b75c; Value = 0x0004f285; PC = 0x80026b4 *)
mov r5 L0x2001b75c;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001bb5c; Value = 0x00029bea; PC = 0x80026b8 *)
mov r6 L0x2001bb5c;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bf5c; Value = 0xfff8e10a; PC = 0x80026bc *)
mov r7 L0x2001bf5c;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 464 ********************)

ghost c0229@sint32,c0485@sint32,c0741@sint32,c0997@sint32 :
and [c0229=r4, c0485=r5, c0741=r6, c0997=r7] && true;

ecut and [
eqmod (c0229*x**229+c0485*x**485+c0741*x**741+c0997*x**997)
      (4*L0x2001b35c*x**229) [1043969, x**256 - 1],
eqmod (c0229*x**229+c0485*x**485+c0741*x**741+c0997*x**997)
      (4*L0x2001b75c*x**229) [1043969, x**256 + 1],
eqmod (c0229*x**229+c0485*x**485+c0741*x**741+c0997*x**997)
      (4*L0x2001bb5c*x**229) [1043969, x**256 - 554923],
eqmod (c0229*x**229+c0485*x**485+c0741*x**741+c0997*x**997)
      (4*L0x2001bf5c*x**229) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf229@sint32 : cf229 = r4 && cf229 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf485@sint32 : cf485 = r5 && cf485 = r5;

(******************** CUT 465 ********************)
ecut and [
eqmod 256*cf229 2**32*(c0229+c0738) 1043969,
eqmod 256*cf485 2**32*(c0485+c0994) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018992; PC = 0x8002728 *)
mov L0x20018992 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b92; PC = 0x800272c *)
mov L0x20018b92 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b360; Value = 0xfff99ae5; PC = 0x8002738 *)
mov r4 L0x2001b360;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b760; Value = 0x0005fe17; PC = 0x800273c *)
mov r5 L0x2001b760;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001bb60; Value = 0xfffe003c; PC = 0x8002740 *)
mov r6 L0x2001bb60;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bf60; Value = 0x0005bc78; PC = 0x8002744 *)
mov r7 L0x2001bf60;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 466 ********************)

ghost c0230@sint32,c0486@sint32,c0742@sint32,c0998@sint32 :
and [c0230=r4, c0486=r5, c0742=r6, c0998=r7] && true;

ecut and [
eqmod (c0230*x**230+c0486*x**486+c0742*x**742+c0998*x**998)
      (4*L0x2001b360*x**230) [1043969, x**256 - 1],
eqmod (c0230*x**230+c0486*x**486+c0742*x**742+c0998*x**998)
      (4*L0x2001b760*x**230) [1043969, x**256 + 1],
eqmod (c0230*x**230+c0486*x**486+c0742*x**742+c0998*x**998)
      (4*L0x2001bb60*x**230) [1043969, x**256 - 554923],
eqmod (c0230*x**230+c0486*x**486+c0742*x**742+c0998*x**998)
      (4*L0x2001bf60*x**230) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf230@sint32 : cf230 = r4 && cf230 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf486@sint32 : cf486 = r5 && cf486 = r5;

(******************** CUT 467 ********************)
ecut and [
eqmod 256*cf230 2**32*(c0230+c0739) 1043969,
eqmod 256*cf486 2**32*(c0486+c0995) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x20018994; PC = 0x80027b0 *)
mov L0x20018994 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018b94; PC = 0x80027b4 *)
mov L0x20018b94 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b364; Value = 0xffe4d390; PC = 0x8002628 *)
mov r4 L0x2001b364;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b764; Value = 0x0001d193; PC = 0x800262c *)
mov r5 L0x2001b764;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001bb64; Value = 0xfffd2268; PC = 0x8002630 *)
mov r6 L0x2001bb64;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bf64; Value = 0x0006b23f; PC = 0x8002634 *)
mov r7 L0x2001bf64;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 468 ********************)

ghost c0231@sint32,c0487@sint32,c0743@sint32,c0999@sint32 :
and [c0231=r4, c0487=r5, c0743=r6, c0999=r7] && true;

ecut and [
eqmod (c0231*x**231+c0487*x**487+c0743*x**743+c0999*x**999)
      (4*L0x2001b364*x**231) [1043969, x**256 - 1],
eqmod (c0231*x**231+c0487*x**487+c0743*x**743+c0999*x**999)
      (4*L0x2001b764*x**231) [1043969, x**256 + 1],
eqmod (c0231*x**231+c0487*x**487+c0743*x**743+c0999*x**999)
      (4*L0x2001bb64*x**231) [1043969, x**256 - 554923],
eqmod (c0231*x**231+c0487*x**487+c0743*x**743+c0999*x**999)
      (4*L0x2001bf64*x**231) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf231@sint32 : cf231 = r4 && cf231 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf487@sint32 : cf487 = r5 && cf487 = r5;

(******************** CUT 469 ********************)
ecut and [
eqmod 256*cf231 2**32*(c0231+c0740) 1043969,
eqmod 256*cf487 2**32*(c0487+c0996) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x20018996; PC = 0x80026a0 *)
mov L0x20018996 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b96; PC = 0x80026a4 *)
mov L0x20018b96 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b368; Value = 0x000283d5; PC = 0x80026b0 *)
mov r4 L0x2001b368;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b768; Value = 0x00018c8b; PC = 0x80026b4 *)
mov r5 L0x2001b768;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001bb68; Value = 0x00011629; PC = 0x80026b8 *)
mov r6 L0x2001bb68;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bf68; Value = 0xfffe255c; PC = 0x80026bc *)
mov r7 L0x2001bf68;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 470 ********************)

ghost c0232@sint32,c0488@sint32,c0744@sint32,c1000@sint32 :
and [c0232=r4, c0488=r5, c0744=r6, c1000=r7] && true;

ecut and [
eqmod (c0232*x**232+c0488*x**488+c0744*x**744+c1000*x**1000)
      (4*L0x2001b368*x**232) [1043969, x**256 - 1],
eqmod (c0232*x**232+c0488*x**488+c0744*x**744+c1000*x**1000)
      (4*L0x2001b768*x**232) [1043969, x**256 + 1],
eqmod (c0232*x**232+c0488*x**488+c0744*x**744+c1000*x**1000)
      (4*L0x2001bb68*x**232) [1043969, x**256 - 554923],
eqmod (c0232*x**232+c0488*x**488+c0744*x**744+c1000*x**1000)
      (4*L0x2001bf68*x**232) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf232@sint32 : cf232 = r4 && cf232 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf488@sint32 : cf488 = r5 && cf488 = r5;

(******************** CUT 471 ********************)
ecut and [
eqmod 256*cf232 2**32*(c0232+c0741) 1043969,
eqmod 256*cf488 2**32*(c0488+c0997) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x20018998; PC = 0x8002728 *)
mov L0x20018998 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b98; PC = 0x800272c *)
mov L0x20018b98 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b36c; Value = 0x000350c6; PC = 0x8002738 *)
mov r4 L0x2001b36c;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b76c; Value = 0xfffd9146; PC = 0x800273c *)
mov r5 L0x2001b76c;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001bb6c; Value = 0xfffbf1f0; PC = 0x8002740 *)
mov r6 L0x2001bb6c;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bf6c; Value = 0x00050503; PC = 0x8002744 *)
mov r7 L0x2001bf6c;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 472 ********************)

ghost c0233@sint32,c0489@sint32,c0745@sint32,c1001@sint32 :
and [c0233=r4, c0489=r5, c0745=r6, c1001=r7] && true;

ecut and [
eqmod (c0233*x**233+c0489*x**489+c0745*x**745+c1001*x**1001)
      (4*L0x2001b36c*x**233) [1043969, x**256 - 1],
eqmod (c0233*x**233+c0489*x**489+c0745*x**745+c1001*x**1001)
      (4*L0x2001b76c*x**233) [1043969, x**256 + 1],
eqmod (c0233*x**233+c0489*x**489+c0745*x**745+c1001*x**1001)
      (4*L0x2001bb6c*x**233) [1043969, x**256 - 554923],
eqmod (c0233*x**233+c0489*x**489+c0745*x**745+c1001*x**1001)
      (4*L0x2001bf6c*x**233) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf233@sint32 : cf233 = r4 && cf233 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf489@sint32 : cf489 = r5 && cf489 = r5;

(******************** CUT 473 ********************)
ecut and [
eqmod 256*cf233 2**32*(c0233+c0742) 1043969,
eqmod 256*cf489 2**32*(c0489+c0998) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x2001899a; PC = 0x80027b0 *)
mov L0x2001899a r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018b9a; PC = 0x80027b4 *)
mov L0x20018b9a r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b370; Value = 0xfffef1f2; PC = 0x8002628 *)
mov r4 L0x2001b370;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b770; Value = 0xfffef28b; PC = 0x800262c *)
mov r5 L0x2001b770;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001bb70; Value = 0xfffcfc44; PC = 0x8002630 *)
mov r6 L0x2001bb70;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bf70; Value = 0x0001ee5d; PC = 0x8002634 *)
mov r7 L0x2001bf70;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 474 ********************)

ghost c0234@sint32,c0490@sint32,c0746@sint32,c1002@sint32 :
and [c0234=r4, c0490=r5, c0746=r6, c1002=r7] && true;

ecut and [
eqmod (c0234*x**234+c0490*x**490+c0746*x**746+c1002*x**1002)
      (4*L0x2001b370*x**234) [1043969, x**256 - 1],
eqmod (c0234*x**234+c0490*x**490+c0746*x**746+c1002*x**1002)
      (4*L0x2001b770*x**234) [1043969, x**256 + 1],
eqmod (c0234*x**234+c0490*x**490+c0746*x**746+c1002*x**1002)
      (4*L0x2001bb70*x**234) [1043969, x**256 - 554923],
eqmod (c0234*x**234+c0490*x**490+c0746*x**746+c1002*x**1002)
      (4*L0x2001bf70*x**234) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf234@sint32 : cf234 = r4 && cf234 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf490@sint32 : cf490 = r5 && cf490 = r5;

(******************** CUT 475 ********************)
ecut and [
eqmod 256*cf234 2**32*(c0234+c0743) 1043969,
eqmod 256*cf490 2**32*(c0490+c0999) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x2001899c; PC = 0x80026a0 *)
mov L0x2001899c r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018b9c; PC = 0x80026a4 *)
mov L0x20018b9c r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b374; Value = 0x0005077b; PC = 0x80026b0 *)
mov r4 L0x2001b374;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b774; Value = 0xfff9fa43; PC = 0x80026b4 *)
mov r5 L0x2001b774;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001bb74; Value = 0x0002a0f7; PC = 0x80026b8 *)
mov r6 L0x2001bb74;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bf74; Value = 0xffffa209; PC = 0x80026bc *)
mov r7 L0x2001bf74;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 476 ********************)

ghost c0235@sint32,c0491@sint32,c0747@sint32,c1003@sint32 :
and [c0235=r4, c0491=r5, c0747=r6, c1003=r7] && true;

ecut and [
eqmod (c0235*x**235+c0491*x**491+c0747*x**747+c1003*x**1003)
      (4*L0x2001b374*x**235) [1043969, x**256 - 1],
eqmod (c0235*x**235+c0491*x**491+c0747*x**747+c1003*x**1003)
      (4*L0x2001b774*x**235) [1043969, x**256 + 1],
eqmod (c0235*x**235+c0491*x**491+c0747*x**747+c1003*x**1003)
      (4*L0x2001bb74*x**235) [1043969, x**256 - 554923],
eqmod (c0235*x**235+c0491*x**491+c0747*x**747+c1003*x**1003)
      (4*L0x2001bf74*x**235) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf235@sint32 : cf235 = r4 && cf235 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf491@sint32 : cf491 = r5 && cf491 = r5;

(******************** CUT 477 ********************)
ecut and [
eqmod 256*cf235 2**32*(c0235+c0744) 1043969,
eqmod 256*cf491 2**32*(c0491+c1000) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x2001899e; PC = 0x8002728 *)
mov L0x2001899e r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018b9e; PC = 0x800272c *)
mov L0x20018b9e r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b378; Value = 0x00049fed; PC = 0x8002738 *)
mov r4 L0x2001b378;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b778; Value = 0x00054455; PC = 0x800273c *)
mov r5 L0x2001b778;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001bb78; Value = 0xfffea614; PC = 0x8002740 *)
mov r6 L0x2001bb78;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bf78; Value = 0x000061f4; PC = 0x8002744 *)
mov r7 L0x2001bf78;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 478 ********************)

ghost c0236@sint32,c0492@sint32,c0748@sint32,c1004@sint32 :
and [c0236=r4, c0492=r5, c0748=r6, c1004=r7] && true;

ecut and [
eqmod (c0236*x**236+c0492*x**492+c0748*x**748+c1004*x**1004)
      (4*L0x2001b378*x**236) [1043969, x**256 - 1],
eqmod (c0236*x**236+c0492*x**492+c0748*x**748+c1004*x**1004)
      (4*L0x2001b778*x**236) [1043969, x**256 + 1],
eqmod (c0236*x**236+c0492*x**492+c0748*x**748+c1004*x**1004)
      (4*L0x2001bb78*x**236) [1043969, x**256 - 554923],
eqmod (c0236*x**236+c0492*x**492+c0748*x**748+c1004*x**1004)
      (4*L0x2001bf78*x**236) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf236@sint32 : cf236 = r4 && cf236 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf492@sint32 : cf492 = r5 && cf492 = r5;

(******************** CUT 479 ********************)
ecut and [
eqmod 256*cf236 2**32*(c0236+c0745) 1043969,
eqmod 256*cf492 2**32*(c0492+c1001) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200189a0; PC = 0x80027b0 *)
mov L0x200189a0 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018ba0; PC = 0x80027b4 *)
mov L0x20018ba0 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b37c; Value = 0x00052ab6; PC = 0x8002628 *)
mov r4 L0x2001b37c;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b77c; Value = 0xfffb859e; PC = 0x800262c *)
mov r5 L0x2001b77c;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001bb7c; Value = 0x0002feb2; PC = 0x8002630 *)
mov r6 L0x2001bb7c;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bf7c; Value = 0xfffd2311; PC = 0x8002634 *)
mov r7 L0x2001bf7c;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 480 ********************)

ghost c0237@sint32,c0493@sint32,c0749@sint32,c1005@sint32 :
and [c0237=r4, c0493=r5, c0749=r6, c1005=r7] && true;

ecut and [
eqmod (c0237*x**237+c0493*x**493+c0749*x**749+c1005*x**1005)
      (4*L0x2001b37c*x**237) [1043969, x**256 - 1],
eqmod (c0237*x**237+c0493*x**493+c0749*x**749+c1005*x**1005)
      (4*L0x2001b77c*x**237) [1043969, x**256 + 1],
eqmod (c0237*x**237+c0493*x**493+c0749*x**749+c1005*x**1005)
      (4*L0x2001bb7c*x**237) [1043969, x**256 - 554923],
eqmod (c0237*x**237+c0493*x**493+c0749*x**749+c1005*x**1005)
      (4*L0x2001bf7c*x**237) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf237@sint32 : cf237 = r4 && cf237 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf493@sint32 : cf493 = r5 && cf493 = r5;

(******************** CUT 481 ********************)
ecut and [
eqmod 256*cf237 2**32*(c0237+c0746) 1043969,
eqmod 256*cf493 2**32*(c0493+c1002) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200189a2; PC = 0x80026a0 *)
mov L0x200189a2 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018ba2; PC = 0x80026a4 *)
mov L0x20018ba2 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b380; Value = 0xfffa2c23; PC = 0x80026b0 *)
mov r4 L0x2001b380;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b780; Value = 0xfff9300f; PC = 0x80026b4 *)
mov r5 L0x2001b780;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001bb80; Value = 0xfffbbbb6; PC = 0x80026b8 *)
mov r6 L0x2001bb80;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bf80; Value = 0xfff8ced9; PC = 0x80026bc *)
mov r7 L0x2001bf80;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 482 ********************)

ghost c0238@sint32,c0494@sint32,c0750@sint32,c1006@sint32 :
and [c0238=r4, c0494=r5, c0750=r6, c1006=r7] && true;

ecut and [
eqmod (c0238*x**238+c0494*x**494+c0750*x**750+c1006*x**1006)
      (4*L0x2001b380*x**238) [1043969, x**256 - 1],
eqmod (c0238*x**238+c0494*x**494+c0750*x**750+c1006*x**1006)
      (4*L0x2001b780*x**238) [1043969, x**256 + 1],
eqmod (c0238*x**238+c0494*x**494+c0750*x**750+c1006*x**1006)
      (4*L0x2001bb80*x**238) [1043969, x**256 - 554923],
eqmod (c0238*x**238+c0494*x**494+c0750*x**750+c1006*x**1006)
      (4*L0x2001bf80*x**238) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf238@sint32 : cf238 = r4 && cf238 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf494@sint32 : cf494 = r5 && cf494 = r5;

(******************** CUT 483 ********************)
ecut and [
eqmod 256*cf238 2**32*(c0238+c0747) 1043969,
eqmod 256*cf494 2**32*(c0494+c1003) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200189a4; PC = 0x8002728 *)
mov L0x200189a4 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018ba4; PC = 0x800272c *)
mov L0x20018ba4 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b384; Value = 0x0001932e; PC = 0x8002738 *)
mov r4 L0x2001b384;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b784; Value = 0xfffb63b1; PC = 0x800273c *)
mov r5 L0x2001b784;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001bb84; Value = 0x00037603; PC = 0x8002740 *)
mov r6 L0x2001bb84;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bf84; Value = 0x0002565b; PC = 0x8002744 *)
mov r7 L0x2001bf84;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 484 ********************)

ghost c0239@sint32,c0495@sint32,c0751@sint32,c1007@sint32 :
and [c0239=r4, c0495=r5, c0751=r6, c1007=r7] && true;

ecut and [
eqmod (c0239*x**239+c0495*x**495+c0751*x**751+c1007*x**1007)
      (4*L0x2001b384*x**239) [1043969, x**256 - 1],
eqmod (c0239*x**239+c0495*x**495+c0751*x**751+c1007*x**1007)
      (4*L0x2001b784*x**239) [1043969, x**256 + 1],
eqmod (c0239*x**239+c0495*x**495+c0751*x**751+c1007*x**1007)
      (4*L0x2001bb84*x**239) [1043969, x**256 - 554923],
eqmod (c0239*x**239+c0495*x**495+c0751*x**751+c1007*x**1007)
      (4*L0x2001bf84*x**239) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf239@sint32 : cf239 = r4 && cf239 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf495@sint32 : cf495 = r5 && cf495 = r5;

(******************** CUT 485 ********************)
ecut and [
eqmod 256*cf239 2**32*(c0239+c0748) 1043969,
eqmod 256*cf495 2**32*(c0495+c1004) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200189a6; PC = 0x80027b0 *)
mov L0x200189a6 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018ba6; PC = 0x80027b4 *)
mov L0x20018ba6 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b388; Value = 0x0008826b; PC = 0x8002628 *)
mov r4 L0x2001b388;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b788; Value = 0x000513ba; PC = 0x800262c *)
mov r5 L0x2001b788;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001bb88; Value = 0x00027b63; PC = 0x8002630 *)
mov r6 L0x2001bb88;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bf88; Value = 0x00001305; PC = 0x8002634 *)
mov r7 L0x2001bf88;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 486 ********************)

ghost c0240@sint32,c0496@sint32,c0752@sint32,c1008@sint32 :
and [c0240=r4, c0496=r5, c0752=r6, c1008=r7] && true;

ecut and [
eqmod (c0240*x**240+c0496*x**496+c0752*x**752+c1008*x**1008)
      (4*L0x2001b388*x**240) [1043969, x**256 - 1],
eqmod (c0240*x**240+c0496*x**496+c0752*x**752+c1008*x**1008)
      (4*L0x2001b788*x**240) [1043969, x**256 + 1],
eqmod (c0240*x**240+c0496*x**496+c0752*x**752+c1008*x**1008)
      (4*L0x2001bb88*x**240) [1043969, x**256 - 554923],
eqmod (c0240*x**240+c0496*x**496+c0752*x**752+c1008*x**1008)
      (4*L0x2001bf88*x**240) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf240@sint32 : cf240 = r4 && cf240 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf496@sint32 : cf496 = r5 && cf496 = r5;

(******************** CUT 487 ********************)
ecut and [
eqmod 256*cf240 2**32*(c0240+c0749) 1043969,
eqmod 256*cf496 2**32*(c0496+c1005) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200189a8; PC = 0x80026a0 *)
mov L0x200189a8 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018ba8; PC = 0x80026a4 *)
mov L0x20018ba8 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b38c; Value = 0xffff2f5e; PC = 0x80026b0 *)
mov r4 L0x2001b38c;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b78c; Value = 0x0007231f; PC = 0x80026b4 *)
mov r5 L0x2001b78c;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001bb8c; Value = 0x0002bc62; PC = 0x80026b8 *)
mov r6 L0x2001bb8c;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bf8c; Value = 0x00054b28; PC = 0x80026bc *)
mov r7 L0x2001bf8c;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 488 ********************)

ghost c0241@sint32,c0497@sint32,c0753@sint32,c1009@sint32 :
and [c0241=r4, c0497=r5, c0753=r6, c1009=r7] && true;

ecut and [
eqmod (c0241*x**241+c0497*x**497+c0753*x**753+c1009*x**1009)
      (4*L0x2001b38c*x**241) [1043969, x**256 - 1],
eqmod (c0241*x**241+c0497*x**497+c0753*x**753+c1009*x**1009)
      (4*L0x2001b78c*x**241) [1043969, x**256 + 1],
eqmod (c0241*x**241+c0497*x**497+c0753*x**753+c1009*x**1009)
      (4*L0x2001bb8c*x**241) [1043969, x**256 - 554923],
eqmod (c0241*x**241+c0497*x**497+c0753*x**753+c1009*x**1009)
      (4*L0x2001bf8c*x**241) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf241@sint32 : cf241 = r4 && cf241 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf497@sint32 : cf497 = r5 && cf497 = r5;

(******************** CUT 489 ********************)
ecut and [
eqmod 256*cf241 2**32*(c0241+c0750) 1043969,
eqmod 256*cf497 2**32*(c0497+c1006) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200189aa; PC = 0x8002728 *)
mov L0x200189aa r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018baa; PC = 0x800272c *)
mov L0x20018baa r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b390; Value = 0x0007aa96; PC = 0x8002738 *)
mov r4 L0x2001b390;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b790; Value = 0x0000dfb4; PC = 0x800273c *)
mov r5 L0x2001b790;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001bb90; Value = 0xfffb710a; PC = 0x8002740 *)
mov r6 L0x2001bb90;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bf90; Value = 0xfffd58c9; PC = 0x8002744 *)
mov r7 L0x2001bf90;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 490 ********************)

ghost c0242@sint32,c0498@sint32,c0754@sint32,c1010@sint32 :
and [c0242=r4, c0498=r5, c0754=r6, c1010=r7] && true;

ecut and [
eqmod (c0242*x**242+c0498*x**498+c0754*x**754+c1010*x**1010)
      (4*L0x2001b390*x**242) [1043969, x**256 - 1],
eqmod (c0242*x**242+c0498*x**498+c0754*x**754+c1010*x**1010)
      (4*L0x2001b790*x**242) [1043969, x**256 + 1],
eqmod (c0242*x**242+c0498*x**498+c0754*x**754+c1010*x**1010)
      (4*L0x2001bb90*x**242) [1043969, x**256 - 554923],
eqmod (c0242*x**242+c0498*x**498+c0754*x**754+c1010*x**1010)
      (4*L0x2001bf90*x**242) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf242@sint32 : cf242 = r4 && cf242 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf498@sint32 : cf498 = r5 && cf498 = r5;

(******************** CUT 491 ********************)
ecut and [
eqmod 256*cf242 2**32*(c0242+c0751) 1043969,
eqmod 256*cf498 2**32*(c0498+c1007) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200189ac; PC = 0x80027b0 *)
mov L0x200189ac r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018bac; PC = 0x80027b4 *)
mov L0x20018bac r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b394; Value = 0x000330b6; PC = 0x8002628 *)
mov r4 L0x2001b394;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b794; Value = 0xfffec64e; PC = 0x800262c *)
mov r5 L0x2001b794;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001bb94; Value = 0xfffd8981; PC = 0x8002630 *)
mov r6 L0x2001bb94;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bf94; Value = 0xfffc0b78; PC = 0x8002634 *)
mov r7 L0x2001bf94;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 492 ********************)

ghost c0243@sint32,c0499@sint32,c0755@sint32,c1011@sint32 :
and [c0243=r4, c0499=r5, c0755=r6, c1011=r7] && true;

ecut and [
eqmod (c0243*x**243+c0499*x**499+c0755*x**755+c1011*x**1011)
      (4*L0x2001b394*x**243) [1043969, x**256 - 1],
eqmod (c0243*x**243+c0499*x**499+c0755*x**755+c1011*x**1011)
      (4*L0x2001b794*x**243) [1043969, x**256 + 1],
eqmod (c0243*x**243+c0499*x**499+c0755*x**755+c1011*x**1011)
      (4*L0x2001bb94*x**243) [1043969, x**256 - 554923],
eqmod (c0243*x**243+c0499*x**499+c0755*x**755+c1011*x**1011)
      (4*L0x2001bf94*x**243) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf243@sint32 : cf243 = r4 && cf243 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf499@sint32 : cf499 = r5 && cf499 = r5;

(******************** CUT 493 ********************)
ecut and [
eqmod 256*cf243 2**32*(c0243+c0752) 1043969,
eqmod 256*cf499 2**32*(c0499+c1008) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200189ae; PC = 0x80026a0 *)
mov L0x200189ae r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018bae; PC = 0x80026a4 *)
mov L0x20018bae r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b398; Value = 0xfffe24d1; PC = 0x80026b0 *)
mov r4 L0x2001b398;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b798; Value = 0x00042546; PC = 0x80026b4 *)
mov r5 L0x2001b798;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001bb98; Value = 0x000654ce; PC = 0x80026b8 *)
mov r6 L0x2001bb98;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bf98; Value = 0x00072430; PC = 0x80026bc *)
mov r7 L0x2001bf98;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 494 ********************)

ghost c0244@sint32,c0500@sint32,c0756@sint32,c1012@sint32 :
and [c0244=r4, c0500=r5, c0756=r6, c1012=r7] && true;

ecut and [
eqmod (c0244*x**244+c0500*x**500+c0756*x**756+c1012*x**1012)
      (4*L0x2001b398*x**244) [1043969, x**256 - 1],
eqmod (c0244*x**244+c0500*x**500+c0756*x**756+c1012*x**1012)
      (4*L0x2001b798*x**244) [1043969, x**256 + 1],
eqmod (c0244*x**244+c0500*x**500+c0756*x**756+c1012*x**1012)
      (4*L0x2001bb98*x**244) [1043969, x**256 - 554923],
eqmod (c0244*x**244+c0500*x**500+c0756*x**756+c1012*x**1012)
      (4*L0x2001bf98*x**244) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf244@sint32 : cf244 = r4 && cf244 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf500@sint32 : cf500 = r5 && cf500 = r5;

(******************** CUT 495 ********************)
ecut and [
eqmod 256*cf244 2**32*(c0244+c0753) 1043969,
eqmod 256*cf500 2**32*(c0500+c1009) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200189b0; PC = 0x8002728 *)
mov L0x200189b0 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018bb0; PC = 0x800272c *)
mov L0x20018bb0 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b39c; Value = 0xffffd335; PC = 0x8002738 *)
mov r4 L0x2001b39c;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b79c; Value = 0x0007b105; PC = 0x800273c *)
mov r5 L0x2001b79c;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001bb9c; Value = 0xfff8650d; PC = 0x8002740 *)
mov r6 L0x2001bb9c;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bf9c; Value = 0xfff87a4a; PC = 0x8002744 *)
mov r7 L0x2001bf9c;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 496 ********************)

ghost c0245@sint32,c0501@sint32,c0757@sint32,c1013@sint32 :
and [c0245=r4, c0501=r5, c0757=r6, c1013=r7] && true;

ecut and [
eqmod (c0245*x**245+c0501*x**501+c0757*x**757+c1013*x**1013)
      (4*L0x2001b39c*x**245) [1043969, x**256 - 1],
eqmod (c0245*x**245+c0501*x**501+c0757*x**757+c1013*x**1013)
      (4*L0x2001b79c*x**245) [1043969, x**256 + 1],
eqmod (c0245*x**245+c0501*x**501+c0757*x**757+c1013*x**1013)
      (4*L0x2001bb9c*x**245) [1043969, x**256 - 554923],
eqmod (c0245*x**245+c0501*x**501+c0757*x**757+c1013*x**1013)
      (4*L0x2001bf9c*x**245) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf245@sint32 : cf245 = r4 && cf245 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf501@sint32 : cf501 = r5 && cf501 = r5;

(******************** CUT 497 ********************)
ecut and [
eqmod 256*cf245 2**32*(c0245+c0754) 1043969,
eqmod 256*cf501 2**32*(c0501+c1010) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200189b2; PC = 0x80027b0 *)
mov L0x200189b2 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018bb2; PC = 0x80027b4 *)
mov L0x20018bb2 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b3a0; Value = 0xffedeccb; PC = 0x8002628 *)
mov r4 L0x2001b3a0;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b7a0; Value = 0xfff9abb1; PC = 0x800262c *)
mov r5 L0x2001b7a0;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001bba0; Value = 0x00035dcf; PC = 0x8002630 *)
mov r6 L0x2001bba0;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bfa0; Value = 0xfffbf824; PC = 0x8002634 *)
mov r7 L0x2001bfa0;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 498 ********************)

ghost c0246@sint32,c0502@sint32,c0758@sint32,c1014@sint32 :
and [c0246=r4, c0502=r5, c0758=r6, c1014=r7] && true;

ecut and [
eqmod (c0246*x**246+c0502*x**502+c0758*x**758+c1014*x**1014)
      (4*L0x2001b3a0*x**246) [1043969, x**256 - 1],
eqmod (c0246*x**246+c0502*x**502+c0758*x**758+c1014*x**1014)
      (4*L0x2001b7a0*x**246) [1043969, x**256 + 1],
eqmod (c0246*x**246+c0502*x**502+c0758*x**758+c1014*x**1014)
      (4*L0x2001bba0*x**246) [1043969, x**256 - 554923],
eqmod (c0246*x**246+c0502*x**502+c0758*x**758+c1014*x**1014)
      (4*L0x2001bfa0*x**246) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf246@sint32 : cf246 = r4 && cf246 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf502@sint32 : cf502 = r5 && cf502 = r5;

(******************** CUT 499 ********************)
ecut and [
eqmod 256*cf246 2**32*(c0246+c0755) 1043969,
eqmod 256*cf502 2**32*(c0502+c1011) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200189b4; PC = 0x80026a0 *)
mov L0x200189b4 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018bb4; PC = 0x80026a4 *)
mov L0x20018bb4 r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b3a4; Value = 0xfff0e241; PC = 0x80026b0 *)
mov r4 L0x2001b3a4;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b7a4; Value = 0x00018ba0; PC = 0x80026b4 *)
mov r5 L0x2001b7a4;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001bba4; Value = 0x000277ff; PC = 0x80026b8 *)
mov r6 L0x2001bba4;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bfa4; Value = 0xfff82dfb; PC = 0x80026bc *)
mov r7 L0x2001bfa4;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 500 ********************)

ghost c0247@sint32,c0503@sint32,c0759@sint32,c1015@sint32 :
and [c0247=r4, c0503=r5, c0759=r6, c1015=r7] && true;

ecut and [
eqmod (c0247*x**247+c0503*x**503+c0759*x**759+c1015*x**1015)
      (4*L0x2001b3a4*x**247) [1043969, x**256 - 1],
eqmod (c0247*x**247+c0503*x**503+c0759*x**759+c1015*x**1015)
      (4*L0x2001b7a4*x**247) [1043969, x**256 + 1],
eqmod (c0247*x**247+c0503*x**503+c0759*x**759+c1015*x**1015)
      (4*L0x2001bba4*x**247) [1043969, x**256 - 554923],
eqmod (c0247*x**247+c0503*x**503+c0759*x**759+c1015*x**1015)
      (4*L0x2001bfa4*x**247) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf247@sint32 : cf247 = r4 && cf247 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf503@sint32 : cf503 = r5 && cf503 = r5;

(******************** CUT 501 ********************)
ecut and [
eqmod 256*cf247 2**32*(c0247+c0756) 1043969,
eqmod 256*cf503 2**32*(c0503+c1012) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200189b6; PC = 0x8002728 *)
mov L0x200189b6 r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018bb6; PC = 0x800272c *)
mov L0x20018bb6 r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b3a8; Value = 0x00062238; PC = 0x8002738 *)
mov r4 L0x2001b3a8;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b7a8; Value = 0xfffd96b8; PC = 0x800273c *)
mov r5 L0x2001b7a8;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001bba8; Value = 0x0002808c; PC = 0x8002740 *)
mov r6 L0x2001bba8;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bfa8; Value = 0x0001aaef; PC = 0x8002744 *)
mov r7 L0x2001bfa8;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 502 ********************)

ghost c0248@sint32,c0504@sint32,c0760@sint32,c1016@sint32 :
and [c0248=r4, c0504=r5, c0760=r6, c1016=r7] && true;

ecut and [
eqmod (c0248*x**248+c0504*x**504+c0760*x**760+c1016*x**1016)
      (4*L0x2001b3a8*x**248) [1043969, x**256 - 1],
eqmod (c0248*x**248+c0504*x**504+c0760*x**760+c1016*x**1016)
      (4*L0x2001b7a8*x**248) [1043969, x**256 + 1],
eqmod (c0248*x**248+c0504*x**504+c0760*x**760+c1016*x**1016)
      (4*L0x2001bba8*x**248) [1043969, x**256 - 554923],
eqmod (c0248*x**248+c0504*x**504+c0760*x**760+c1016*x**1016)
      (4*L0x2001bfa8*x**248) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf248@sint32 : cf248 = r4 && cf248 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf504@sint32 : cf504 = r5 && cf504 = r5;

(******************** CUT 503 ********************)
ecut and [
eqmod 256*cf248 2**32*(c0248+c0757) 1043969,
eqmod 256*cf504 2**32*(c0504+c1013) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200189b8; PC = 0x80027b0 *)
mov L0x200189b8 r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018bb8; PC = 0x80027b4 *)
mov L0x20018bb8 r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x8002624 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b3ac; Value = 0x0001f455; PC = 0x8002628 *)
mov r4 L0x2001b3ac;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b7ac; Value = 0x00041242; PC = 0x800262c *)
mov r5 L0x2001b7ac;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001bbac; Value = 0x00013b46; PC = 0x8002630 *)
mov r6 L0x2001bbac;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bfac; Value = 0x0005617f; PC = 0x8002634 *)
mov r7 L0x2001bfac;
(* add	r4, r5                                      #! PC = 0x8002638 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800263a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800263c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002640 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002644 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002648 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800264c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002650 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002652 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002654 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002658 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 504 ********************)

ghost c0249@sint32,c0505@sint32,c0761@sint32,c1017@sint32 :
and [c0249=r4, c0505=r5, c0761=r6, c1017=r7] && true;

(* NOTE: product of two degree 508 polynomials *)
assume c1017 = 0 && true;

ecut and [
c1017 = 0,
eqmod (c0249*x**249+c0505*x**505+c0761*x**761+c1017*x**1017)
      (4*L0x2001b3ac*x**249) [1043969, x**256 - 1],
eqmod (c0249*x**249+c0505*x**505+c0761*x**761+c1017*x**1017)
      (4*L0x2001b7ac*x**249) [1043969, x**256 + 1],
eqmod (c0249*x**249+c0505*x**505+c0761*x**761+c1017*x**1017)
      (4*L0x2001bbac*x**249) [1043969, x**256 - 554923],
eqmod (c0249*x**249+c0505*x**505+c0761*x**761+c1017*x**1017)
      (4*L0x2001bfac*x**249) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x800265c *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002660 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002662 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002664 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002668 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800266c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002670 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002674 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002678 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800267c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002680 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002682 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002686 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf249@sint32 : cf249 = r4 && cf249 = r4;

(* it	lt                                           #! PC = 0x8002688 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800268c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800268e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002692 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf505@sint32 : cf505 = r5 && cf505 = r5;

(******************** CUT 505 ********************)
ecut and [
eqmod 256*cf249 2**32*(c0249+c0758) 1043969,
eqmod 256*cf505 2**32*(c0505+c1014) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002694 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002698 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x800269c *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200189ba; PC = 0x80026a0 *)
mov L0x200189ba r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018bba; PC = 0x80026a4 *)
mov L0x20018bba r5;
(* vmov	s6, s7, r6, r7                             #! PC = 0x80026a8 *)
mov s6 r6;
mov s7 r7;
(* vmov	r11, s0                                    #! PC = 0x80026ac *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b3b0; Value = 0x00066445; PC = 0x80026b0 *)
mov r4 L0x2001b3b0;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b7b0; Value = 0x00021d1f; PC = 0x80026b4 *)
mov r5 L0x2001b7b0;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001bbb0; Value = 0x00048f7b; PC = 0x80026b8 *)
mov r6 L0x2001bbb0;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bfb0; Value = 0x00004855; PC = 0x80026bc *)
mov r7 L0x2001bfb0;
(* add	r4, r5                                      #! PC = 0x80026c0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80026c2 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80026c4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80026c8 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80026cc *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80026d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80026d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80026d8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80026da *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80026dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80026e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 506 ********************)

ghost c0250@sint32,c0506@sint32,c0762@sint32,c1018@sint32 :
and [c0250=r4, c0506=r5, c0762=r6, c1018=r7] && true;

(* NOTE: product of two degree 508 polynomials *)
assume c1018 = 0 && true;

ecut and [
c1018 = 0,
eqmod (c0250*x**250+c0506*x**506+c0762*x**762+c1018*x**1018)
      (4*L0x2001b3b0*x**250) [1043969, x**256 - 1],
eqmod (c0250*x**250+c0506*x**506+c0762*x**762+c1018*x**1018)
      (4*L0x2001b7b0*x**250) [1043969, x**256 + 1],
eqmod (c0250*x**250+c0506*x**506+c0762*x**762+c1018*x**1018)
      (4*L0x2001bbb0*x**250) [1043969, x**256 - 554923],
eqmod (c0250*x**250+c0506*x**506+c0762*x**762+c1018*x**1018)
      (4*L0x2001bfb0*x**250) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x80026e4 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x80026e8 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80026ea *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80026ec *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80026f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x80026f8 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80026fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002700 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002704 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002708 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800270a *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800270e *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf250@sint32 : cf250 = r4 && cf250 = r4;

(* it	lt                                           #! PC = 0x8002710 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002714 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002716 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x800271a *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf506@sint32 : cf506 = r5 && cf506 = r5;

(******************** CUT 507 ********************)
ecut and [
eqmod 256*cf250 2**32*(c0250+c0759) 1043969,
eqmod 256*cf506 2**32*(c0506+c1015) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x800271c *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002720 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002724 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200189bc; PC = 0x8002728 *)
mov L0x200189bc r4;
(* strh.w	r5, [r0, #514]	; 0x202                   #! EA = L0x20018bbc; PC = 0x800272c *)
mov L0x20018bbc r5;
(* vmov	s10, s11, r6, r7                           #! PC = 0x8002730 *)
mov s10 r6;
mov s11 r7;
(* vmov	r11, s0                                    #! PC = 0x8002734 *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b3b4; Value = 0x0007c318; PC = 0x8002738 *)
mov r4 L0x2001b3b4;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b7b4; Value = 0xfffe1daa; PC = 0x800273c *)
mov r5 L0x2001b7b4;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001bbb4; Value = 0xfffce0c2; PC = 0x8002740 *)
mov r6 L0x2001bbb4;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bfb4; Value = 0x0001841a; PC = 0x8002744 *)
mov r7 L0x2001bfb4;
(* add	r4, r5                                      #! PC = 0x8002748 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800274a *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x800274c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002750 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002754 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002758 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800275c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002760 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002762 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002764 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002768 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 508 ********************)

ghost c0251@sint32,c0507@sint32,c0763@sint32,c1019@sint32 :
and [c0251=r4, c0507=r5, c0763=r6, c1019=r7] && true;

(* NOTE: product of two degree 508 polynomials *)
assume c1019 = 0 && true;

ecut and [
c1019 = 0,
eqmod (c0251*x**251+c0507*x**507+c0763*x**763+c1019*x**1019)
      (4*L0x2001b3b4*x**251) [1043969, x**256 - 1],
eqmod (c0251*x**251+c0507*x**507+c0763*x**763+c1019*x**1019)
      (4*L0x2001b7b4*x**251) [1043969, x**256 + 1],
eqmod (c0251*x**251+c0507*x**507+c0763*x**763+c1019*x**1019)
      (4*L0x2001bbb4*x**251) [1043969, x**256 - 554923],
eqmod (c0251*x**251+c0507*x**507+c0763*x**763+c1019*x**1019)
      (4*L0x2001bfb4*x**251) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x800276c *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002770 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002772 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002774 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002778 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800277c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002780 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002784 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002788 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x800278c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002790 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002792 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002796 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf251@sint32 : cf251 = r4 && cf251 = r4;

(* it	lt                                           #! PC = 0x8002798 *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x800279c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800279e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80027a2 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf507@sint32 : cf507 = r5 && cf507 = r5;

(******************** CUT 509 ********************)
ecut and [
eqmod 256*cf251 2**32*(c0251+c0760) 1043969,
eqmod 256*cf507 2**32*(c0507+c1016) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80027a4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80027a8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80027ac *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200189be; PC = 0x80027b0 *)
mov L0x200189be r4;
(* strh.w	r5, [r0, #516]	; 0x204                   #! EA = L0x20018bbe; PC = 0x80027b4 *)
mov L0x20018bbe r5;
(* vmov	s14, s15, r6, r7                           #! PC = 0x80027b8 *)
mov s14 r6;
mov s15 r7;
(* vmov	r11, s0                                    #! PC = 0x80027bc *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x80027c0 *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80027c4 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80027c8 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r12, s2                                    #! PC = 0x80027cc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80027d0 *)
(* cmp.w r0, $2v *)
nop;
(* #bne.w	0x8002624 <_final_map>                   #! PC = 0x80027d4 *)
#bne.w	0x8002624 <_final_map>                   #! 0x80027d4 = 0x80027d4;
(* vmov	r11, s0                                    #! PC = 0x80027d8 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b3b8; Value = 0x0001d39e; PC = 0x80027dc *)
mov r4 L0x2001b3b8;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b7b8; Value = 0x000695ad; PC = 0x80027e0 *)
mov r5 L0x2001b7b8;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001bbb8; Value = 0xfff96c7c; PC = 0x80027e4 *)
mov r6 L0x2001bbb8;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bfb8; Value = 0x00037950; PC = 0x80027e8 *)
mov r7 L0x2001bfb8;
(* add	r4, r5                                      #! PC = 0x80027ec *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80027ee *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80027f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80027f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80027f8 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80027fc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8002800 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002804 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8002806 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002808 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800280c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 510 ********************)
ghost c0252@sint32,c0508@sint32,c0764@sint32,c1020@sint32 :
and [c0252=r4, c0508=r5, c0764=r6, c1020=r7] && true;

(* NOTE: product of two degree 508 polynomials *)
assume c1020 = 0 && true;

ecut and [
c1020 = 0,
eqmod (c0252*x**252+c0508*x**508+c0764*x**764+c1020*x**1020)
      (4*L0x2001b3b8*x**252) [1043969, x**256 - 1],
eqmod (c0252*x**252+c0508*x**508+c0764*x**764+c1020*x**1020)
      (4*L0x2001b7b8*x**252) [1043969, x**256 + 1],
eqmod (c0252*x**252+c0508*x**508+c0764*x**764+c1020*x**1020)
      (4*L0x2001bbb8*x**252) [1043969, x**256 - 554923],
eqmod (c0252*x**252+c0508*x**508+c0764*x**764+c1020*x**1020)
      (4*L0x2001bfb8*x**252) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x8002810 *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x8002814 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x8002816 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002818 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x800281c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8002820 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002824 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800282c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002830 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002834 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002836 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x800283a *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf252@sint32 : cf252 = r4 && cf252 = r4;

(* it	lt                                           #! PC = 0x800283c *)
(* it lt *)
nop;
(* cmp	r5, r1                                      #! PC = 0x8002840 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002842 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002846 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf508@sint32 : cf508 = r5 && cf508 = r5;

(******************** CUT 511 ********************)
ecut and [
eqmod 256*cf252 2**32*(c0252+c0761) 1043969,
eqmod 256*cf508 2**32*(c0508+c1017) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002848 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x800284c *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002850 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200189c0; PC = 0x8002854 *)
mov L0x200189c0 r4;
(* strh.w	r5, [r0, #512]	; 0x200                   #! EA = L0x20018bc0; PC = 0x8002858 *)
mov L0x20018bc0 r5;
(* vmov	s6, r6                                     #! PC = 0x800285c *)
mov s6 r6;
(* vmov	r11, s0                                    #! PC = 0x8002860 *)
mov r11 s0;
(* ldr.w	r4, [r11, #4]                             #! EA = L0x2001b3bc; Value = 0xfff05d99; PC = 0x8002864 *)
mov r4 L0x2001b3bc;
(* ldr.w	r5, [r11, #1028]	; 0x404                  #! EA = L0x2001b7bc; Value = 0x0005a7d8; PC = 0x8002868 *)
mov r5 L0x2001b7bc;
(* ldr.w	r6, [r11, #2052]	; 0x804                  #! EA = L0x2001bbbc; Value = 0xfffa344f; PC = 0x800286c *)
mov r6 L0x2001bbbc;
(* ldr.w	r7, [r11, #3076]	; 0xc04                  #! EA = L0x2001bfbc; Value = 0x0001d31e; PC = 0x8002870 *)
mov r7 L0x2001bfbc;
(* add	r4, r5                                      #! PC = 0x8002874 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8002876 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8002878 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x800287c *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x8002880 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002884 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8002888 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x800288c *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x800288e *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8002890 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002894 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 512 ********************)

ghost c0253@sint32,c0509@sint32,c0765@sint32,c1021@sint32 :
and [c0253=r4, c0509=r5, c0765=r6, c1021=r7] && true;

(* NOTE: product of two degree 508 polynomials *)
assume c1021 = 0 && true;

ecut and [
c1021 = 0,
eqmod (c0253*x**253+c0509*x**509+c0765*x**765+c1021*x**1021)
      (4*L0x2001b3bc*x**253) [1043969, x**256 - 1],
eqmod (c0253*x**253+c0509*x**509+c0765*x**765+c1021*x**1021)
      (4*L0x2001b7bc*x**253) [1043969, x**256 + 1],
eqmod (c0253*x**253+c0509*x**509+c0765*x**765+c1021*x**1021)
      (4*L0x2001bbbc*x**253) [1043969, x**256 - 554923],
eqmod (c0253*x**253+c0509*x**509+c0765*x**765+c1021*x**1021)
      (4*L0x2001bfbc*x**253) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s10, s11                         #! PC = 0x8002898 *)
mov r10 s10;
mov r11 s11;
(* add	r4, r10                                     #! PC = 0x800289c *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x800289e *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80028a0 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80028a4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80028a8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x80028ac *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x80028b0 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x80028b2 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x80028b6 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf253@sint32 : cf253 = r4 && cf253 = r4;

(* it	lt                                           #! PC = 0x80028b8 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x80028bc *)
mov r4_o r4;
split dontcare r4 r4 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200189c2; PC = 0x80028c0 *)
mov L0x200189c2 r4;
(* sub.w	r0, r0, #504	; 0x1f8                      #! PC = 0x80028c4 *)
subs dontcare r0 r0 504@uint32;
(* vmov	r11, s4                                    #! PC = 0x80028c8 *)
mov r11 s4;
(* add.w	r5, r5, r11                               #! PC = 0x80028cc *)
add r5 r5 r11;
(* smull	r12, r5, r5, r9                           #! PC = 0x80028d0 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80028d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80028d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x80028dc *)
split r1 dclsr r3 1;
(* cmp	r5, r1                                      #! PC = 0x80028e0 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x80028e2 *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x80028e6 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf000@sint32 : cf000 = r5 && cf000 = r5;

(******************** CUT 513 ********************)
ecut and [
eqmod 256*cf253 2**32*(c0253+c0762) 1043969,
eqmod 256*cf000 2**32*(c0000+c0509+c1018) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x80028e8 *)
(* it lt *)
nop;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x80028ec *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r5, [r0]                                 #! EA = L0x200187c8; PC = 0x80028f0 *)
mov L0x200187c8 r5;
(* add.w	r0, r0, #504	; 0x1f8                      #! PC = 0x80028f4 *)
adds dontcare r0 r0 504@uint32;
(* vmov	s10, r6                                    #! PC = 0x80028f8 *)
mov s10 r6;
(* vmov	r11, s0                                    #! PC = 0x80028fc *)
mov r11 s0;
(* ldr.w	r4, [r11, #8]                             #! EA = L0x2001b3c0; Value = 0xffebaa99; PC = 0x8002900 *)
mov r4 L0x2001b3c0;
(* ldr.w	r5, [r11, #1032]	; 0x408                  #! EA = L0x2001b7c0; Value = 0x0007ad56; PC = 0x8002904 *)
mov r5 L0x2001b7c0;
(* ldr.w	r6, [r11, #2056]	; 0x808                  #! EA = L0x2001bbc0; Value = 0x0003184a; PC = 0x8002908 *)
mov r6 L0x2001bbc0;
(* ldr.w	r7, [r11, #3080]	; 0xc08                  #! EA = L0x2001bfc0; Value = 0x000560fa; PC = 0x800290c *)
mov r7 L0x2001bfc0;
(* add	r4, r5                                      #! PC = 0x8002910 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8002912 *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8002914 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8002918 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x800291c *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x8002920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8002924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8002928 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x800292a *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800292c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8002930 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 514 ********************)

ghost c0254@sint32,c0510@sint32,c0766@sint32,c1022@sint32 :
and [c0254=r4, c0510=r5, c0766=r6, c1022=r7] && true;

(* NOTE: product of two degree 508 polynomials *)
assume c1022 = 0 && true;

ecut and [
c1022 = 0,
eqmod (c0254*x**254+c0510*x**510+c0766*x**766+c1022*x**1022)
      (4*L0x2001b3c0*x**254) [1043969, x**256 - 1],
eqmod (c0254*x**254+c0510*x**510+c0766*x**766+c1022*x**1022)
      (4*L0x2001b7c0*x**254) [1043969, x**256 + 1],
eqmod (c0254*x**254+c0510*x**510+c0766*x**766+c1022*x**1022)
      (4*L0x2001bbc0*x**254) [1043969, x**256 - 554923],
eqmod (c0254*x**254+c0510*x**510+c0766*x**766+c1022*x**1022)
      (4*L0x2001bfc0*x**254) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s14, s15                         #! PC = 0x8002934 *)
mov r10 s14;
mov r11 s15;
(* add	r4, r10                                     #! PC = 0x8002938 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x800293a *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x800293c *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002940 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8002944 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002948 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x800294c *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x800294e *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002952 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf254@sint32 : cf254 = r4 && cf254 = r4;

(* it	lt                                           #! PC = 0x8002954 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002958 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* strh.w	r4, [r0, #4]                             #! EA = L0x200189c4; PC = 0x800295c *)
mov L0x200189c4 r4;
(* sub.w	r0, r0, #504	; 0x1f8                      #! PC = 0x8002960 *)
subs dontcare r0 r0 504@uint32;
(* vmov	r11, s8                                    #! PC = 0x8002964 *)
mov r11 s8;
(* add.w	r5, r5, r11                               #! PC = 0x8002968 *)
add r5 r5 r11;
(* smull	r12, r5, r5, r9                           #! PC = 0x800296c *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002970 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002974 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002978 *)
split r1 dclsr r3 1;
(* cmp	r5, r1                                      #! PC = 0x800297c *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x800297e *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002982 *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf001@sint32 : cf001 = r5 && cf001 = r5;

(******************** CUT 515 ********************)
ecut and [
eqmod 256*cf254 2**32*(c0254+c0763) 1043969,
eqmod 256*cf001 2**32*(c0001+c0510+c1019) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002984 *)
(* it lt *)
nop;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002988 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r5, [r0, #2]                             #! EA = L0x200187ca; PC = 0x800298c *)
mov L0x200187ca r5;
(* add.w	r0, r0, #504	; 0x1f8                      #! PC = 0x8002990 *)
adds dontcare r0 r0 504@uint32;
(* vmov	s14, r6                                    #! PC = 0x8002994 *)
mov s14 r6;
(* vmov	r11, s0                                    #! PC = 0x8002998 *)
mov r11 s0;
(* add.w	r11, r11, #12                             #! PC = 0x800299c *)
adds dontcare r11 r11 12@uint32;
(* vmov	s0, r11                                    #! PC = 0x80029a0 *)
mov s0 r11;
(* add.w	r0, r0, #6                                #! PC = 0x80029a4 *)
adds dontcare r0 r0 6@uint32;
(* vmov	r11, s0                                    #! PC = 0x80029a8 *)
mov r11 s0;
(* ldr.w	r4, [r11]                                 #! EA = L0x2001b3c4; Value = 0x0001c97a; PC = 0x80029ac *)
mov r4 L0x2001b3c4;
(* ldr.w	r5, [r11, #1024]	; 0x400                  #! EA = L0x2001b7c4; Value = 0xfffc2112; PC = 0x80029b0 *)
mov r5 L0x2001b7c4;
(* ldr.w	r6, [r11, #2048]	; 0x800                  #! EA = L0x2001bbc4; Value = 0x000083a1; PC = 0x80029b4 *)
mov r6 L0x2001bbc4;
(* ldr.w	r7, [r11, #3072]	; 0xc00                  #! EA = L0x2001bfc4; Value = 0xfff9feac; PC = 0x80029b8 *)
mov r7 L0x2001bfc4;
(* add	r4, r5                                      #! PC = 0x80029bc *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80029be *)
add r6 r6 r7;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80029c0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80029c4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* smull	r12, r7, r7, r8                           #! PC = 0x80029c8 *)
smull r7 r12 r7 r8;
(* mul.w	lr, r12, r2                               #! PC = 0x80029cc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80029d0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80029d4 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80029d6 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80029d8 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80029dc *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;


(******************** CUT 516 ********************)

ghost c0255@sint32,c0511@sint32,c0767@sint32,c1023@sint32 :
and [c0255=r4, c0511=r5, c0767=r6, c1023=r7] && true;

(* NOTE: product of two degree 508 polynomials *)
assume c1023 = 0 && true;

ecut and [
c1023 = 0,
eqmod (c0255*x**255+c0511*x**511+c0767*x**767+c1023*x**1023)
      (4*L0x2001b3c4*x**255) [1043969, x**256 - 1],
eqmod (c0255*x**255+c0511*x**511+c0767*x**767+c1023*x**1023)
      (4*L0x2001b7c4*x**255) [1043969, x**256 + 1],
eqmod (c0255*x**255+c0511*x**511+c0767*x**767+c1023*x**1023)
      (4*L0x2001bbc4*x**255) [1043969, x**256 - 554923],
eqmod (c0255*x**255+c0511*x**511+c0767*x**767+c1023*x**1023)
      (4*L0x2001bfc4*x**255) [1043969, x**256 + 554923]
];


(* vmov	r10, r11, s6, s7                           #! PC = 0x80029e0 *)
mov r10 s6;
mov r11 s7;
(* add	r4, r10                                     #! PC = 0x80029e4 *)
add r4 r4 r10;
(* add	r5, r11                                     #! PC = 0x80029e6 *)
add r5 r5 r11;
(* smull	r12, r4, r4, r9                           #! PC = 0x80029e8 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x80029ec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x80029f0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x80029f4 *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x80029f8 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x80029fa *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x80029fe *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf255@sint32 : cf255 = r4 && cf255 = r4;

(* it	lt                                           #! PC = 0x8002a00 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002a04 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* strh.w	r4, [r0]                                 #! EA = L0x200189c6; PC = 0x8002a08 *)
mov L0x200189c6 r4;
(* sub.w	r0, r0, #510	; 0x1fe                      #! PC = 0x8002a0c *)
subs dontcare r0 r0 510@uint32;
(* vmov	r11, s12                                   #! PC = 0x8002a10 *)
mov r11 s12;
(* add.w	r5, r5, r11                               #! PC = 0x8002a14 *)
add r5 r5 r11;
(* smull	r12, r5, r5, r9                           #! PC = 0x8002a18 *)
smull r5 r12 r5 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002a1c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8002a20 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002a24 *)
split r1 dclsr r3 1;
(* cmp	r5, r1                                      #! PC = 0x8002a28 *)
subc carry cmp r5 r1;
sub r5_s r5 r3;
cmov r5 carry r5_s r5;
(* it	ge                                           #! PC = 0x8002a2a *)
(* it ge *)
nop;
(* cmn	r5, r1                                      #! PC = 0x8002a2e *)
adds carry cmn r5 r1;
add r5_a r5 r3;
cmov r5 carry r5 r5_a;
ghost cf002@sint32 : cf002 = r5 && cf002 = r5;

(******************** CUT 517 ********************)
(* NOTE: need FIX *)
ecut and [
eqmod 256*cf255 2**32*(c0255+c0764) 1043969,
eqmod 256*cf002 2**32*(c0002+c0511+c1017) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002a30 *)
(* it lt *)
nop;
(* ubfx	r5, r5, #0, #11                            #! PC = 0x8002a34 *)
mov r5_o r5;
split dontcare r5 r5 11;
(* strh.w	r5, [r0, #4]                             #! EA = L0x200187cc; PC = 0x8002a38 *)
mov L0x200187cc r5;
(* add.w	r0, r0, #510	; 0x1fe                      #! PC = 0x8002a3c *)
adds dontcare r0 r0 510@uint32;
(* vmov	s6, r6                                     #! PC = 0x8002a40 *)
mov s6 r6;
(* vmov	r4, r5, s5, s6                             #! PC = 0x8002a44 *)
mov r4 s5;
mov r5 s6;
(* vmov	r6, r7, s9, s10                            #! PC = 0x8002a48 *)
mov r6 s9;
mov r7 s10;
(* vmov	r8, r10, s13, s14                          #! PC = 0x8002a4c *)
mov r8 s13;
mov r10 s14;
(* add	r4, r7                                      #! PC = 0x8002a50 *)
add r4 r4 r7;
(* add	r6, r10                                     #! PC = 0x8002a52 *)
add r6 r6 r10;
(* add.w	r8, r8, r5                                #! PC = 0x8002a54 *)
add r8 r8 r5;
(* smull	r12, r4, r4, r9                           #! PC = 0x8002a58 *)
smull r4 r12 r4 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002a5c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8002a60 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r6, r6, r9                           #! PC = 0x8002a64 *)
smull r6 r12 r6 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002a68 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8002a6c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r9                           #! PC = 0x8002a70 *)
smull r8 r12 r8 r9;
(* mul.w	lr, r12, r2                               #! PC = 0x8002a74 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8002a78 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* mov.w	r1, r3, lsr #1                            #! PC = 0x8002a7c *)
split r1 dclsr r3 1;
(* cmp	r4, r1                                      #! PC = 0x8002a80 *)
subc carry cmp r4 r1;
sub r4_s r4 r3;
cmov r4 carry r4_s r4;
(* it	ge                                           #! PC = 0x8002a82 *)
(* it ge *)
nop;
(* cmn	r4, r1                                      #! PC = 0x8002a86 *)
adds carry cmn r4 r1;
add r4_a r4 r3;
cmov r4 carry r4 r4_a;
ghost cf256@sint32 : cf256 = r4 && cf256 = r4;

(* it	lt                                           #! PC = 0x8002a88 *)
(* it lt *)
nop;
(* cmp	r6, r1                                      #! PC = 0x8002a8c *)
subc carry cmp r6 r1;
sub r6_s r6 r3;
cmov r6 carry r6_s r6;
(* it	ge                                           #! PC = 0x8002a8e *)
(* it ge *)
nop;
(* cmn	r6, r1                                      #! PC = 0x8002a92 *)
adds carry cmn r6 r1;
add r6_a r6 r3;
cmov r6 carry r6 r6_a;
ghost cf257@sint32 : cf257 = r6 && cf257 = r6;

(* it	lt                                           #! PC = 0x8002a94 *)
(* it lt *)
nop;
(* cmp	r8, r1                                      #! PC = 0x8002a98 *)
subc carry cmp r8 r1;
sub r8_s r8 r3;
cmov r8 carry r8_s r8;
(* it	ge                                           #! PC = 0x8002a9a *)
(* it ge *)
nop;
(* cmn.w	r8, r1                                    #! PC = 0x8002aa0 *)
adds carry cmn r8 r1;
add r8_a r8 r3;
cmov r8 carry r8 r8_a;
ghost cf258@sint32 : cf258 = r8 && cf258 = r8;

(******************** CUT 518 ********************)
ecut and [
eqmod 256*cf256 2**32*(c0256+c0765) 1043969,
eqmod 256*cf257 2**32*(c0257+c0766) 1043969,
eqmod 256*cf258 2**32*(c0258+c0767) 1043969
] prove with [ all ghosts ];

(* it	lt                                           #! PC = 0x8002aa4 *)
(* it lt *)
nop;
(* ubfx	r4, r4, #0, #11                            #! PC = 0x8002aa8 *)
mov r4_o r4;
split dontcare r4 r4 11;
(* ubfx	r6, r6, #0, #11                            #! PC = 0x8002aac *)
mov r6_o r6;
split dontcare r6 r6 11;
(* ubfx	r8, r8, #0, #11                            #! PC = 0x8002ab0 *)
mov r8_o r8;
split dontcare r8 r8 11;
(* strh.w	r4, [r0, #2]                             #! EA = L0x200189c8; PC = 0x8002ab4 *)
mov L0x200189c8 r4;
(* strh.w	r6, [r0, #4]                             #! EA = L0x200189ca; PC = 0x8002ab8 *)
mov L0x200189ca r6;
(* strh.w	r8, [r0, #6]                             #! EA = L0x200189cc; PC = 0x8002abc *)
mov L0x200189cc r8;
(* #ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, r10, r11, r12, pc}#! EA = L0x20017798; Value = 0x200009b8; PC = 0x8002ac0 *)
#ldmia.w	sp!, {%%r4, %%r5, %%r6, %%r7, %%r8, %%r9, %%r10, %%r11, %%r12, pc}#! L0x20017798 = L0x20017798; 0x200009b8 = 0x200009b8; 0x8002ac0 = 0x8002ac0;
(* #bl	0x8002be4 <hal_get_time>                    #! PC = 0x8000276 *)
#bl	0x8002be4 <hal_get_time>                    #! 0x8000276 = 0x8000276;
(* #! -> SP = 0x200177c0 *)
#! 0x200177c0 = 0x200177c0;
(* #push	{r4, r5, r6, lr}                           #! PC = 0x8002be4 *)
#push	{%%r4, %%r5, %%r6, %%lr}                           #! 0x8002be4 = 0x8002be4;

(******************** CUT 519 ********************)


ecut and [
eqmod (c0000*x**   0+c0256*x** 256+c0512*x** 512+c0768*x** 768)
      (4*L0x2001afc8*x**  0) [ 1043969, x**256 - 1 ],
eqmod (c0001*x**   1+c0257*x** 257+c0513*x** 513+c0769*x** 769)
      (4*L0x2001afcc*x**  1) [ 1043969, x**256 - 1 ],
eqmod (c0002*x**   2+c0258*x** 258+c0514*x** 514+c0770*x** 770)
      (4*L0x2001afd0*x**  2) [ 1043969, x**256 - 1 ],
eqmod (c0003*x**   3+c0259*x** 259+c0515*x** 515+c0771*x** 771)
      (4*L0x2001afd4*x**  3) [ 1043969, x**256 - 1 ],
eqmod (c0004*x**   4+c0260*x** 260+c0516*x** 516+c0772*x** 772)
      (4*L0x2001afd8*x**  4) [ 1043969, x**256 - 1 ],
eqmod (c0005*x**   5+c0261*x** 261+c0517*x** 517+c0773*x** 773)
      (4*L0x2001afdc*x**  5) [ 1043969, x**256 - 1 ],
eqmod (c0006*x**   6+c0262*x** 262+c0518*x** 518+c0774*x** 774)
      (4*L0x2001afe0*x**  6) [ 1043969, x**256 - 1 ],
eqmod (c0007*x**   7+c0263*x** 263+c0519*x** 519+c0775*x** 775)
      (4*L0x2001afe4*x**  7) [ 1043969, x**256 - 1 ],
eqmod (c0008*x**   8+c0264*x** 264+c0520*x** 520+c0776*x** 776)
      (4*L0x2001afe8*x**  8) [ 1043969, x**256 - 1 ],
eqmod (c0009*x**   9+c0265*x** 265+c0521*x** 521+c0777*x** 777)
      (4*L0x2001afec*x**  9) [ 1043969, x**256 - 1 ],
eqmod (c0010*x**  10+c0266*x** 266+c0522*x** 522+c0778*x** 778)
      (4*L0x2001aff0*x** 10) [ 1043969, x**256 - 1 ],
eqmod (c0011*x**  11+c0267*x** 267+c0523*x** 523+c0779*x** 779)
      (4*L0x2001aff4*x** 11) [ 1043969, x**256 - 1 ],
eqmod (c0012*x**  12+c0268*x** 268+c0524*x** 524+c0780*x** 780)
      (4*L0x2001aff8*x** 12) [ 1043969, x**256 - 1 ],
eqmod (c0013*x**  13+c0269*x** 269+c0525*x** 525+c0781*x** 781)
      (4*L0x2001affc*x** 13) [ 1043969, x**256 - 1 ],
eqmod (c0014*x**  14+c0270*x** 270+c0526*x** 526+c0782*x** 782)
      (4*L0x2001b000*x** 14) [ 1043969, x**256 - 1 ],
eqmod (c0015*x**  15+c0271*x** 271+c0527*x** 527+c0783*x** 783)
      (4*L0x2001b004*x** 15) [ 1043969, x**256 - 1 ],
eqmod (c0016*x**  16+c0272*x** 272+c0528*x** 528+c0784*x** 784)
      (4*L0x2001b008*x** 16) [ 1043969, x**256 - 1 ],
eqmod (c0017*x**  17+c0273*x** 273+c0529*x** 529+c0785*x** 785)
      (4*L0x2001b00c*x** 17) [ 1043969, x**256 - 1 ],
eqmod (c0018*x**  18+c0274*x** 274+c0530*x** 530+c0786*x** 786)
      (4*L0x2001b010*x** 18) [ 1043969, x**256 - 1 ],
eqmod (c0019*x**  19+c0275*x** 275+c0531*x** 531+c0787*x** 787)
      (4*L0x2001b014*x** 19) [ 1043969, x**256 - 1 ],
eqmod (c0020*x**  20+c0276*x** 276+c0532*x** 532+c0788*x** 788)
      (4*L0x2001b018*x** 20) [ 1043969, x**256 - 1 ],
eqmod (c0021*x**  21+c0277*x** 277+c0533*x** 533+c0789*x** 789)
      (4*L0x2001b01c*x** 21) [ 1043969, x**256 - 1 ],
eqmod (c0022*x**  22+c0278*x** 278+c0534*x** 534+c0790*x** 790)
      (4*L0x2001b020*x** 22) [ 1043969, x**256 - 1 ],
eqmod (c0023*x**  23+c0279*x** 279+c0535*x** 535+c0791*x** 791)
      (4*L0x2001b024*x** 23) [ 1043969, x**256 - 1 ],
eqmod (c0024*x**  24+c0280*x** 280+c0536*x** 536+c0792*x** 792)
      (4*L0x2001b028*x** 24) [ 1043969, x**256 - 1 ],
eqmod (c0025*x**  25+c0281*x** 281+c0537*x** 537+c0793*x** 793)
      (4*L0x2001b02c*x** 25) [ 1043969, x**256 - 1 ],
eqmod (c0026*x**  26+c0282*x** 282+c0538*x** 538+c0794*x** 794)
      (4*L0x2001b030*x** 26) [ 1043969, x**256 - 1 ],
eqmod (c0027*x**  27+c0283*x** 283+c0539*x** 539+c0795*x** 795)
      (4*L0x2001b034*x** 27) [ 1043969, x**256 - 1 ],
eqmod (c0028*x**  28+c0284*x** 284+c0540*x** 540+c0796*x** 796)
      (4*L0x2001b038*x** 28) [ 1043969, x**256 - 1 ],
eqmod (c0029*x**  29+c0285*x** 285+c0541*x** 541+c0797*x** 797)
      (4*L0x2001b03c*x** 29) [ 1043969, x**256 - 1 ],
eqmod (c0030*x**  30+c0286*x** 286+c0542*x** 542+c0798*x** 798)
      (4*L0x2001b040*x** 30) [ 1043969, x**256 - 1 ],
eqmod (c0031*x**  31+c0287*x** 287+c0543*x** 543+c0799*x** 799)
      (4*L0x2001b044*x** 31) [ 1043969, x**256 - 1 ],
eqmod (c0032*x**  32+c0288*x** 288+c0544*x** 544+c0800*x** 800)
      (4*L0x2001b048*x** 32) [ 1043969, x**256 - 1 ],
eqmod (c0033*x**  33+c0289*x** 289+c0545*x** 545+c0801*x** 801)
      (4*L0x2001b04c*x** 33) [ 1043969, x**256 - 1 ],
eqmod (c0034*x**  34+c0290*x** 290+c0546*x** 546+c0802*x** 802)
      (4*L0x2001b050*x** 34) [ 1043969, x**256 - 1 ],
eqmod (c0035*x**  35+c0291*x** 291+c0547*x** 547+c0803*x** 803)
      (4*L0x2001b054*x** 35) [ 1043969, x**256 - 1 ],
eqmod (c0036*x**  36+c0292*x** 292+c0548*x** 548+c0804*x** 804)
      (4*L0x2001b058*x** 36) [ 1043969, x**256 - 1 ],
eqmod (c0037*x**  37+c0293*x** 293+c0549*x** 549+c0805*x** 805)
      (4*L0x2001b05c*x** 37) [ 1043969, x**256 - 1 ],
eqmod (c0038*x**  38+c0294*x** 294+c0550*x** 550+c0806*x** 806)
      (4*L0x2001b060*x** 38) [ 1043969, x**256 - 1 ],
eqmod (c0039*x**  39+c0295*x** 295+c0551*x** 551+c0807*x** 807)
      (4*L0x2001b064*x** 39) [ 1043969, x**256 - 1 ],
eqmod (c0040*x**  40+c0296*x** 296+c0552*x** 552+c0808*x** 808)
      (4*L0x2001b068*x** 40) [ 1043969, x**256 - 1 ],
eqmod (c0041*x**  41+c0297*x** 297+c0553*x** 553+c0809*x** 809)
      (4*L0x2001b06c*x** 41) [ 1043969, x**256 - 1 ],
eqmod (c0042*x**  42+c0298*x** 298+c0554*x** 554+c0810*x** 810)
      (4*L0x2001b070*x** 42) [ 1043969, x**256 - 1 ],
eqmod (c0043*x**  43+c0299*x** 299+c0555*x** 555+c0811*x** 811)
      (4*L0x2001b074*x** 43) [ 1043969, x**256 - 1 ],
eqmod (c0044*x**  44+c0300*x** 300+c0556*x** 556+c0812*x** 812)
      (4*L0x2001b078*x** 44) [ 1043969, x**256 - 1 ],
eqmod (c0045*x**  45+c0301*x** 301+c0557*x** 557+c0813*x** 813)
      (4*L0x2001b07c*x** 45) [ 1043969, x**256 - 1 ],
eqmod (c0046*x**  46+c0302*x** 302+c0558*x** 558+c0814*x** 814)
      (4*L0x2001b080*x** 46) [ 1043969, x**256 - 1 ],
eqmod (c0047*x**  47+c0303*x** 303+c0559*x** 559+c0815*x** 815)
      (4*L0x2001b084*x** 47) [ 1043969, x**256 - 1 ],
eqmod (c0048*x**  48+c0304*x** 304+c0560*x** 560+c0816*x** 816)
      (4*L0x2001b088*x** 48) [ 1043969, x**256 - 1 ],
eqmod (c0049*x**  49+c0305*x** 305+c0561*x** 561+c0817*x** 817)
      (4*L0x2001b08c*x** 49) [ 1043969, x**256 - 1 ],
eqmod (c0050*x**  50+c0306*x** 306+c0562*x** 562+c0818*x** 818)
      (4*L0x2001b090*x** 50) [ 1043969, x**256 - 1 ],
eqmod (c0051*x**  51+c0307*x** 307+c0563*x** 563+c0819*x** 819)
      (4*L0x2001b094*x** 51) [ 1043969, x**256 - 1 ],
eqmod (c0052*x**  52+c0308*x** 308+c0564*x** 564+c0820*x** 820)
      (4*L0x2001b098*x** 52) [ 1043969, x**256 - 1 ],
eqmod (c0053*x**  53+c0309*x** 309+c0565*x** 565+c0821*x** 821)
      (4*L0x2001b09c*x** 53) [ 1043969, x**256 - 1 ],
eqmod (c0054*x**  54+c0310*x** 310+c0566*x** 566+c0822*x** 822)
      (4*L0x2001b0a0*x** 54) [ 1043969, x**256 - 1 ],
eqmod (c0055*x**  55+c0311*x** 311+c0567*x** 567+c0823*x** 823)
      (4*L0x2001b0a4*x** 55) [ 1043969, x**256 - 1 ],
eqmod (c0056*x**  56+c0312*x** 312+c0568*x** 568+c0824*x** 824)
      (4*L0x2001b0a8*x** 56) [ 1043969, x**256 - 1 ],
eqmod (c0057*x**  57+c0313*x** 313+c0569*x** 569+c0825*x** 825)
      (4*L0x2001b0ac*x** 57) [ 1043969, x**256 - 1 ],
eqmod (c0058*x**  58+c0314*x** 314+c0570*x** 570+c0826*x** 826)
      (4*L0x2001b0b0*x** 58) [ 1043969, x**256 - 1 ],
eqmod (c0059*x**  59+c0315*x** 315+c0571*x** 571+c0827*x** 827)
      (4*L0x2001b0b4*x** 59) [ 1043969, x**256 - 1 ],
eqmod (c0060*x**  60+c0316*x** 316+c0572*x** 572+c0828*x** 828)
      (4*L0x2001b0b8*x** 60) [ 1043969, x**256 - 1 ],
eqmod (c0061*x**  61+c0317*x** 317+c0573*x** 573+c0829*x** 829)
      (4*L0x2001b0bc*x** 61) [ 1043969, x**256 - 1 ],
eqmod (c0062*x**  62+c0318*x** 318+c0574*x** 574+c0830*x** 830)
      (4*L0x2001b0c0*x** 62) [ 1043969, x**256 - 1 ],
eqmod (c0063*x**  63+c0319*x** 319+c0575*x** 575+c0831*x** 831)
      (4*L0x2001b0c4*x** 63) [ 1043969, x**256 - 1 ],
eqmod (c0064*x**  64+c0320*x** 320+c0576*x** 576+c0832*x** 832)
      (4*L0x2001b0c8*x** 64) [ 1043969, x**256 - 1 ],
eqmod (c0065*x**  65+c0321*x** 321+c0577*x** 577+c0833*x** 833)
      (4*L0x2001b0cc*x** 65) [ 1043969, x**256 - 1 ],
eqmod (c0066*x**  66+c0322*x** 322+c0578*x** 578+c0834*x** 834)
      (4*L0x2001b0d0*x** 66) [ 1043969, x**256 - 1 ],
eqmod (c0067*x**  67+c0323*x** 323+c0579*x** 579+c0835*x** 835)
      (4*L0x2001b0d4*x** 67) [ 1043969, x**256 - 1 ],
eqmod (c0068*x**  68+c0324*x** 324+c0580*x** 580+c0836*x** 836)
      (4*L0x2001b0d8*x** 68) [ 1043969, x**256 - 1 ],
eqmod (c0069*x**  69+c0325*x** 325+c0581*x** 581+c0837*x** 837)
      (4*L0x2001b0dc*x** 69) [ 1043969, x**256 - 1 ],
eqmod (c0070*x**  70+c0326*x** 326+c0582*x** 582+c0838*x** 838)
      (4*L0x2001b0e0*x** 70) [ 1043969, x**256 - 1 ],
eqmod (c0071*x**  71+c0327*x** 327+c0583*x** 583+c0839*x** 839)
      (4*L0x2001b0e4*x** 71) [ 1043969, x**256 - 1 ],
eqmod (c0072*x**  72+c0328*x** 328+c0584*x** 584+c0840*x** 840)
      (4*L0x2001b0e8*x** 72) [ 1043969, x**256 - 1 ],
eqmod (c0073*x**  73+c0329*x** 329+c0585*x** 585+c0841*x** 841)
      (4*L0x2001b0ec*x** 73) [ 1043969, x**256 - 1 ],
eqmod (c0074*x**  74+c0330*x** 330+c0586*x** 586+c0842*x** 842)
      (4*L0x2001b0f0*x** 74) [ 1043969, x**256 - 1 ],
eqmod (c0075*x**  75+c0331*x** 331+c0587*x** 587+c0843*x** 843)
      (4*L0x2001b0f4*x** 75) [ 1043969, x**256 - 1 ],
eqmod (c0076*x**  76+c0332*x** 332+c0588*x** 588+c0844*x** 844)
      (4*L0x2001b0f8*x** 76) [ 1043969, x**256 - 1 ],
eqmod (c0077*x**  77+c0333*x** 333+c0589*x** 589+c0845*x** 845)
      (4*L0x2001b0fc*x** 77) [ 1043969, x**256 - 1 ],
eqmod (c0078*x**  78+c0334*x** 334+c0590*x** 590+c0846*x** 846)
      (4*L0x2001b100*x** 78) [ 1043969, x**256 - 1 ],
eqmod (c0079*x**  79+c0335*x** 335+c0591*x** 591+c0847*x** 847)
      (4*L0x2001b104*x** 79) [ 1043969, x**256 - 1 ],
eqmod (c0080*x**  80+c0336*x** 336+c0592*x** 592+c0848*x** 848)
      (4*L0x2001b108*x** 80) [ 1043969, x**256 - 1 ],
eqmod (c0081*x**  81+c0337*x** 337+c0593*x** 593+c0849*x** 849)
      (4*L0x2001b10c*x** 81) [ 1043969, x**256 - 1 ],
eqmod (c0082*x**  82+c0338*x** 338+c0594*x** 594+c0850*x** 850)
      (4*L0x2001b110*x** 82) [ 1043969, x**256 - 1 ],
eqmod (c0083*x**  83+c0339*x** 339+c0595*x** 595+c0851*x** 851)
      (4*L0x2001b114*x** 83) [ 1043969, x**256 - 1 ],
eqmod (c0084*x**  84+c0340*x** 340+c0596*x** 596+c0852*x** 852)
      (4*L0x2001b118*x** 84) [ 1043969, x**256 - 1 ],
eqmod (c0085*x**  85+c0341*x** 341+c0597*x** 597+c0853*x** 853)
      (4*L0x2001b11c*x** 85) [ 1043969, x**256 - 1 ],
eqmod (c0086*x**  86+c0342*x** 342+c0598*x** 598+c0854*x** 854)
      (4*L0x2001b120*x** 86) [ 1043969, x**256 - 1 ],
eqmod (c0087*x**  87+c0343*x** 343+c0599*x** 599+c0855*x** 855)
      (4*L0x2001b124*x** 87) [ 1043969, x**256 - 1 ],
eqmod (c0088*x**  88+c0344*x** 344+c0600*x** 600+c0856*x** 856)
      (4*L0x2001b128*x** 88) [ 1043969, x**256 - 1 ],
eqmod (c0089*x**  89+c0345*x** 345+c0601*x** 601+c0857*x** 857)
      (4*L0x2001b12c*x** 89) [ 1043969, x**256 - 1 ],
eqmod (c0090*x**  90+c0346*x** 346+c0602*x** 602+c0858*x** 858)
      (4*L0x2001b130*x** 90) [ 1043969, x**256 - 1 ],
eqmod (c0091*x**  91+c0347*x** 347+c0603*x** 603+c0859*x** 859)
      (4*L0x2001b134*x** 91) [ 1043969, x**256 - 1 ],
eqmod (c0092*x**  92+c0348*x** 348+c0604*x** 604+c0860*x** 860)
      (4*L0x2001b138*x** 92) [ 1043969, x**256 - 1 ],
eqmod (c0093*x**  93+c0349*x** 349+c0605*x** 605+c0861*x** 861)
      (4*L0x2001b13c*x** 93) [ 1043969, x**256 - 1 ],
eqmod (c0094*x**  94+c0350*x** 350+c0606*x** 606+c0862*x** 862)
      (4*L0x2001b140*x** 94) [ 1043969, x**256 - 1 ],
eqmod (c0095*x**  95+c0351*x** 351+c0607*x** 607+c0863*x** 863)
      (4*L0x2001b144*x** 95) [ 1043969, x**256 - 1 ],
eqmod (c0096*x**  96+c0352*x** 352+c0608*x** 608+c0864*x** 864)
      (4*L0x2001b148*x** 96) [ 1043969, x**256 - 1 ],
eqmod (c0097*x**  97+c0353*x** 353+c0609*x** 609+c0865*x** 865)
      (4*L0x2001b14c*x** 97) [ 1043969, x**256 - 1 ],
eqmod (c0098*x**  98+c0354*x** 354+c0610*x** 610+c0866*x** 866)
      (4*L0x2001b150*x** 98) [ 1043969, x**256 - 1 ],
eqmod (c0099*x**  99+c0355*x** 355+c0611*x** 611+c0867*x** 867)
      (4*L0x2001b154*x** 99) [ 1043969, x**256 - 1 ],
eqmod (c0100*x** 100+c0356*x** 356+c0612*x** 612+c0868*x** 868)
      (4*L0x2001b158*x**100) [ 1043969, x**256 - 1 ],
eqmod (c0101*x** 101+c0357*x** 357+c0613*x** 613+c0869*x** 869)
      (4*L0x2001b15c*x**101) [ 1043969, x**256 - 1 ],
eqmod (c0102*x** 102+c0358*x** 358+c0614*x** 614+c0870*x** 870)
      (4*L0x2001b160*x**102) [ 1043969, x**256 - 1 ],
eqmod (c0103*x** 103+c0359*x** 359+c0615*x** 615+c0871*x** 871)
      (4*L0x2001b164*x**103) [ 1043969, x**256 - 1 ],
eqmod (c0104*x** 104+c0360*x** 360+c0616*x** 616+c0872*x** 872)
      (4*L0x2001b168*x**104) [ 1043969, x**256 - 1 ],
eqmod (c0105*x** 105+c0361*x** 361+c0617*x** 617+c0873*x** 873)
      (4*L0x2001b16c*x**105) [ 1043969, x**256 - 1 ],
eqmod (c0106*x** 106+c0362*x** 362+c0618*x** 618+c0874*x** 874)
      (4*L0x2001b170*x**106) [ 1043969, x**256 - 1 ],
eqmod (c0107*x** 107+c0363*x** 363+c0619*x** 619+c0875*x** 875)
      (4*L0x2001b174*x**107) [ 1043969, x**256 - 1 ],
eqmod (c0108*x** 108+c0364*x** 364+c0620*x** 620+c0876*x** 876)
      (4*L0x2001b178*x**108) [ 1043969, x**256 - 1 ],
eqmod (c0109*x** 109+c0365*x** 365+c0621*x** 621+c0877*x** 877)
      (4*L0x2001b17c*x**109) [ 1043969, x**256 - 1 ],
eqmod (c0110*x** 110+c0366*x** 366+c0622*x** 622+c0878*x** 878)
      (4*L0x2001b180*x**110) [ 1043969, x**256 - 1 ],
eqmod (c0111*x** 111+c0367*x** 367+c0623*x** 623+c0879*x** 879)
      (4*L0x2001b184*x**111) [ 1043969, x**256 - 1 ],
eqmod (c0112*x** 112+c0368*x** 368+c0624*x** 624+c0880*x** 880)
      (4*L0x2001b188*x**112) [ 1043969, x**256 - 1 ],
eqmod (c0113*x** 113+c0369*x** 369+c0625*x** 625+c0881*x** 881)
      (4*L0x2001b18c*x**113) [ 1043969, x**256 - 1 ],
eqmod (c0114*x** 114+c0370*x** 370+c0626*x** 626+c0882*x** 882)
      (4*L0x2001b190*x**114) [ 1043969, x**256 - 1 ],
eqmod (c0115*x** 115+c0371*x** 371+c0627*x** 627+c0883*x** 883)
      (4*L0x2001b194*x**115) [ 1043969, x**256 - 1 ],
eqmod (c0116*x** 116+c0372*x** 372+c0628*x** 628+c0884*x** 884)
      (4*L0x2001b198*x**116) [ 1043969, x**256 - 1 ],
eqmod (c0117*x** 117+c0373*x** 373+c0629*x** 629+c0885*x** 885)
      (4*L0x2001b19c*x**117) [ 1043969, x**256 - 1 ],
eqmod (c0118*x** 118+c0374*x** 374+c0630*x** 630+c0886*x** 886)
      (4*L0x2001b1a0*x**118) [ 1043969, x**256 - 1 ],
eqmod (c0119*x** 119+c0375*x** 375+c0631*x** 631+c0887*x** 887)
      (4*L0x2001b1a4*x**119) [ 1043969, x**256 - 1 ],
eqmod (c0120*x** 120+c0376*x** 376+c0632*x** 632+c0888*x** 888)
      (4*L0x2001b1a8*x**120) [ 1043969, x**256 - 1 ],
eqmod (c0121*x** 121+c0377*x** 377+c0633*x** 633+c0889*x** 889)
      (4*L0x2001b1ac*x**121) [ 1043969, x**256 - 1 ],
eqmod (c0122*x** 122+c0378*x** 378+c0634*x** 634+c0890*x** 890)
      (4*L0x2001b1b0*x**122) [ 1043969, x**256 - 1 ],
eqmod (c0123*x** 123+c0379*x** 379+c0635*x** 635+c0891*x** 891)
      (4*L0x2001b1b4*x**123) [ 1043969, x**256 - 1 ],
eqmod (c0124*x** 124+c0380*x** 380+c0636*x** 636+c0892*x** 892)
      (4*L0x2001b1b8*x**124) [ 1043969, x**256 - 1 ],
eqmod (c0125*x** 125+c0381*x** 381+c0637*x** 637+c0893*x** 893)
      (4*L0x2001b1bc*x**125) [ 1043969, x**256 - 1 ],
eqmod (c0126*x** 126+c0382*x** 382+c0638*x** 638+c0894*x** 894)
      (4*L0x2001b1c0*x**126) [ 1043969, x**256 - 1 ],
eqmod (c0127*x** 127+c0383*x** 383+c0639*x** 639+c0895*x** 895)
      (4*L0x2001b1c4*x**127) [ 1043969, x**256 - 1 ],
eqmod (c0128*x** 128+c0384*x** 384+c0640*x** 640+c0896*x** 896)
      (4*L0x2001b1c8*x**128) [ 1043969, x**256 - 1 ],
eqmod (c0129*x** 129+c0385*x** 385+c0641*x** 641+c0897*x** 897)
      (4*L0x2001b1cc*x**129) [ 1043969, x**256 - 1 ],
eqmod (c0130*x** 130+c0386*x** 386+c0642*x** 642+c0898*x** 898)
      (4*L0x2001b1d0*x**130) [ 1043969, x**256 - 1 ],
eqmod (c0131*x** 131+c0387*x** 387+c0643*x** 643+c0899*x** 899)
      (4*L0x2001b1d4*x**131) [ 1043969, x**256 - 1 ],
eqmod (c0132*x** 132+c0388*x** 388+c0644*x** 644+c0900*x** 900)
      (4*L0x2001b1d8*x**132) [ 1043969, x**256 - 1 ],
eqmod (c0133*x** 133+c0389*x** 389+c0645*x** 645+c0901*x** 901)
      (4*L0x2001b1dc*x**133) [ 1043969, x**256 - 1 ],
eqmod (c0134*x** 134+c0390*x** 390+c0646*x** 646+c0902*x** 902)
      (4*L0x2001b1e0*x**134) [ 1043969, x**256 - 1 ],
eqmod (c0135*x** 135+c0391*x** 391+c0647*x** 647+c0903*x** 903)
      (4*L0x2001b1e4*x**135) [ 1043969, x**256 - 1 ],
eqmod (c0136*x** 136+c0392*x** 392+c0648*x** 648+c0904*x** 904)
      (4*L0x2001b1e8*x**136) [ 1043969, x**256 - 1 ],
eqmod (c0137*x** 137+c0393*x** 393+c0649*x** 649+c0905*x** 905)
      (4*L0x2001b1ec*x**137) [ 1043969, x**256 - 1 ],
eqmod (c0138*x** 138+c0394*x** 394+c0650*x** 650+c0906*x** 906)
      (4*L0x2001b1f0*x**138) [ 1043969, x**256 - 1 ],
eqmod (c0139*x** 139+c0395*x** 395+c0651*x** 651+c0907*x** 907)
      (4*L0x2001b1f4*x**139) [ 1043969, x**256 - 1 ],
eqmod (c0140*x** 140+c0396*x** 396+c0652*x** 652+c0908*x** 908)
      (4*L0x2001b1f8*x**140) [ 1043969, x**256 - 1 ],
eqmod (c0141*x** 141+c0397*x** 397+c0653*x** 653+c0909*x** 909)
      (4*L0x2001b1fc*x**141) [ 1043969, x**256 - 1 ],
eqmod (c0142*x** 142+c0398*x** 398+c0654*x** 654+c0910*x** 910)
      (4*L0x2001b200*x**142) [ 1043969, x**256 - 1 ],
eqmod (c0143*x** 143+c0399*x** 399+c0655*x** 655+c0911*x** 911)
      (4*L0x2001b204*x**143) [ 1043969, x**256 - 1 ],
eqmod (c0144*x** 144+c0400*x** 400+c0656*x** 656+c0912*x** 912)
      (4*L0x2001b208*x**144) [ 1043969, x**256 - 1 ],
eqmod (c0145*x** 145+c0401*x** 401+c0657*x** 657+c0913*x** 913)
      (4*L0x2001b20c*x**145) [ 1043969, x**256 - 1 ],
eqmod (c0146*x** 146+c0402*x** 402+c0658*x** 658+c0914*x** 914)
      (4*L0x2001b210*x**146) [ 1043969, x**256 - 1 ],
eqmod (c0147*x** 147+c0403*x** 403+c0659*x** 659+c0915*x** 915)
      (4*L0x2001b214*x**147) [ 1043969, x**256 - 1 ],
eqmod (c0148*x** 148+c0404*x** 404+c0660*x** 660+c0916*x** 916)
      (4*L0x2001b218*x**148) [ 1043969, x**256 - 1 ],
eqmod (c0149*x** 149+c0405*x** 405+c0661*x** 661+c0917*x** 917)
      (4*L0x2001b21c*x**149) [ 1043969, x**256 - 1 ],
eqmod (c0150*x** 150+c0406*x** 406+c0662*x** 662+c0918*x** 918)
      (4*L0x2001b220*x**150) [ 1043969, x**256 - 1 ],
eqmod (c0151*x** 151+c0407*x** 407+c0663*x** 663+c0919*x** 919)
      (4*L0x2001b224*x**151) [ 1043969, x**256 - 1 ],
eqmod (c0152*x** 152+c0408*x** 408+c0664*x** 664+c0920*x** 920)
      (4*L0x2001b228*x**152) [ 1043969, x**256 - 1 ],
eqmod (c0153*x** 153+c0409*x** 409+c0665*x** 665+c0921*x** 921)
      (4*L0x2001b22c*x**153) [ 1043969, x**256 - 1 ],
eqmod (c0154*x** 154+c0410*x** 410+c0666*x** 666+c0922*x** 922)
      (4*L0x2001b230*x**154) [ 1043969, x**256 - 1 ],
eqmod (c0155*x** 155+c0411*x** 411+c0667*x** 667+c0923*x** 923)
      (4*L0x2001b234*x**155) [ 1043969, x**256 - 1 ],
eqmod (c0156*x** 156+c0412*x** 412+c0668*x** 668+c0924*x** 924)
      (4*L0x2001b238*x**156) [ 1043969, x**256 - 1 ],
eqmod (c0157*x** 157+c0413*x** 413+c0669*x** 669+c0925*x** 925)
      (4*L0x2001b23c*x**157) [ 1043969, x**256 - 1 ],
eqmod (c0158*x** 158+c0414*x** 414+c0670*x** 670+c0926*x** 926)
      (4*L0x2001b240*x**158) [ 1043969, x**256 - 1 ],
eqmod (c0159*x** 159+c0415*x** 415+c0671*x** 671+c0927*x** 927)
      (4*L0x2001b244*x**159) [ 1043969, x**256 - 1 ],
eqmod (c0160*x** 160+c0416*x** 416+c0672*x** 672+c0928*x** 928)
      (4*L0x2001b248*x**160) [ 1043969, x**256 - 1 ],
eqmod (c0161*x** 161+c0417*x** 417+c0673*x** 673+c0929*x** 929)
      (4*L0x2001b24c*x**161) [ 1043969, x**256 - 1 ],
eqmod (c0162*x** 162+c0418*x** 418+c0674*x** 674+c0930*x** 930)
      (4*L0x2001b250*x**162) [ 1043969, x**256 - 1 ],
eqmod (c0163*x** 163+c0419*x** 419+c0675*x** 675+c0931*x** 931)
      (4*L0x2001b254*x**163) [ 1043969, x**256 - 1 ],
eqmod (c0164*x** 164+c0420*x** 420+c0676*x** 676+c0932*x** 932)
      (4*L0x2001b258*x**164) [ 1043969, x**256 - 1 ],
eqmod (c0165*x** 165+c0421*x** 421+c0677*x** 677+c0933*x** 933)
      (4*L0x2001b25c*x**165) [ 1043969, x**256 - 1 ],
eqmod (c0166*x** 166+c0422*x** 422+c0678*x** 678+c0934*x** 934)
      (4*L0x2001b260*x**166) [ 1043969, x**256 - 1 ],
eqmod (c0167*x** 167+c0423*x** 423+c0679*x** 679+c0935*x** 935)
      (4*L0x2001b264*x**167) [ 1043969, x**256 - 1 ],
eqmod (c0168*x** 168+c0424*x** 424+c0680*x** 680+c0936*x** 936)
      (4*L0x2001b268*x**168) [ 1043969, x**256 - 1 ],
eqmod (c0169*x** 169+c0425*x** 425+c0681*x** 681+c0937*x** 937)
      (4*L0x2001b26c*x**169) [ 1043969, x**256 - 1 ],
eqmod (c0170*x** 170+c0426*x** 426+c0682*x** 682+c0938*x** 938)
      (4*L0x2001b270*x**170) [ 1043969, x**256 - 1 ],
eqmod (c0171*x** 171+c0427*x** 427+c0683*x** 683+c0939*x** 939)
      (4*L0x2001b274*x**171) [ 1043969, x**256 - 1 ],
eqmod (c0172*x** 172+c0428*x** 428+c0684*x** 684+c0940*x** 940)
      (4*L0x2001b278*x**172) [ 1043969, x**256 - 1 ],
eqmod (c0173*x** 173+c0429*x** 429+c0685*x** 685+c0941*x** 941)
      (4*L0x2001b27c*x**173) [ 1043969, x**256 - 1 ],
eqmod (c0174*x** 174+c0430*x** 430+c0686*x** 686+c0942*x** 942)
      (4*L0x2001b280*x**174) [ 1043969, x**256 - 1 ],
eqmod (c0175*x** 175+c0431*x** 431+c0687*x** 687+c0943*x** 943)
      (4*L0x2001b284*x**175) [ 1043969, x**256 - 1 ],
eqmod (c0176*x** 176+c0432*x** 432+c0688*x** 688+c0944*x** 944)
      (4*L0x2001b288*x**176) [ 1043969, x**256 - 1 ],
eqmod (c0177*x** 177+c0433*x** 433+c0689*x** 689+c0945*x** 945)
      (4*L0x2001b28c*x**177) [ 1043969, x**256 - 1 ],
eqmod (c0178*x** 178+c0434*x** 434+c0690*x** 690+c0946*x** 946)
      (4*L0x2001b290*x**178) [ 1043969, x**256 - 1 ],
eqmod (c0179*x** 179+c0435*x** 435+c0691*x** 691+c0947*x** 947)
      (4*L0x2001b294*x**179) [ 1043969, x**256 - 1 ],
eqmod (c0180*x** 180+c0436*x** 436+c0692*x** 692+c0948*x** 948)
      (4*L0x2001b298*x**180) [ 1043969, x**256 - 1 ],
eqmod (c0181*x** 181+c0437*x** 437+c0693*x** 693+c0949*x** 949)
      (4*L0x2001b29c*x**181) [ 1043969, x**256 - 1 ],
eqmod (c0182*x** 182+c0438*x** 438+c0694*x** 694+c0950*x** 950)
      (4*L0x2001b2a0*x**182) [ 1043969, x**256 - 1 ],
eqmod (c0183*x** 183+c0439*x** 439+c0695*x** 695+c0951*x** 951)
      (4*L0x2001b2a4*x**183) [ 1043969, x**256 - 1 ],
eqmod (c0184*x** 184+c0440*x** 440+c0696*x** 696+c0952*x** 952)
      (4*L0x2001b2a8*x**184) [ 1043969, x**256 - 1 ],
eqmod (c0185*x** 185+c0441*x** 441+c0697*x** 697+c0953*x** 953)
      (4*L0x2001b2ac*x**185) [ 1043969, x**256 - 1 ],
eqmod (c0186*x** 186+c0442*x** 442+c0698*x** 698+c0954*x** 954)
      (4*L0x2001b2b0*x**186) [ 1043969, x**256 - 1 ],
eqmod (c0187*x** 187+c0443*x** 443+c0699*x** 699+c0955*x** 955)
      (4*L0x2001b2b4*x**187) [ 1043969, x**256 - 1 ],
eqmod (c0188*x** 188+c0444*x** 444+c0700*x** 700+c0956*x** 956)
      (4*L0x2001b2b8*x**188) [ 1043969, x**256 - 1 ],
eqmod (c0189*x** 189+c0445*x** 445+c0701*x** 701+c0957*x** 957)
      (4*L0x2001b2bc*x**189) [ 1043969, x**256 - 1 ],
eqmod (c0190*x** 190+c0446*x** 446+c0702*x** 702+c0958*x** 958)
      (4*L0x2001b2c0*x**190) [ 1043969, x**256 - 1 ],
eqmod (c0191*x** 191+c0447*x** 447+c0703*x** 703+c0959*x** 959)
      (4*L0x2001b2c4*x**191) [ 1043969, x**256 - 1 ],
eqmod (c0192*x** 192+c0448*x** 448+c0704*x** 704+c0960*x** 960)
      (4*L0x2001b2c8*x**192) [ 1043969, x**256 - 1 ],
eqmod (c0193*x** 193+c0449*x** 449+c0705*x** 705+c0961*x** 961)
      (4*L0x2001b2cc*x**193) [ 1043969, x**256 - 1 ],
eqmod (c0194*x** 194+c0450*x** 450+c0706*x** 706+c0962*x** 962)
      (4*L0x2001b2d0*x**194) [ 1043969, x**256 - 1 ],
eqmod (c0195*x** 195+c0451*x** 451+c0707*x** 707+c0963*x** 963)
      (4*L0x2001b2d4*x**195) [ 1043969, x**256 - 1 ],
eqmod (c0196*x** 196+c0452*x** 452+c0708*x** 708+c0964*x** 964)
      (4*L0x2001b2d8*x**196) [ 1043969, x**256 - 1 ],
eqmod (c0197*x** 197+c0453*x** 453+c0709*x** 709+c0965*x** 965)
      (4*L0x2001b2dc*x**197) [ 1043969, x**256 - 1 ],
eqmod (c0198*x** 198+c0454*x** 454+c0710*x** 710+c0966*x** 966)
      (4*L0x2001b2e0*x**198) [ 1043969, x**256 - 1 ],
eqmod (c0199*x** 199+c0455*x** 455+c0711*x** 711+c0967*x** 967)
      (4*L0x2001b2e4*x**199) [ 1043969, x**256 - 1 ],
eqmod (c0200*x** 200+c0456*x** 456+c0712*x** 712+c0968*x** 968)
      (4*L0x2001b2e8*x**200) [ 1043969, x**256 - 1 ],
eqmod (c0201*x** 201+c0457*x** 457+c0713*x** 713+c0969*x** 969)
      (4*L0x2001b2ec*x**201) [ 1043969, x**256 - 1 ],
eqmod (c0202*x** 202+c0458*x** 458+c0714*x** 714+c0970*x** 970)
      (4*L0x2001b2f0*x**202) [ 1043969, x**256 - 1 ],
eqmod (c0203*x** 203+c0459*x** 459+c0715*x** 715+c0971*x** 971)
      (4*L0x2001b2f4*x**203) [ 1043969, x**256 - 1 ],
eqmod (c0204*x** 204+c0460*x** 460+c0716*x** 716+c0972*x** 972)
      (4*L0x2001b2f8*x**204) [ 1043969, x**256 - 1 ],
eqmod (c0205*x** 205+c0461*x** 461+c0717*x** 717+c0973*x** 973)
      (4*L0x2001b2fc*x**205) [ 1043969, x**256 - 1 ],
eqmod (c0206*x** 206+c0462*x** 462+c0718*x** 718+c0974*x** 974)
      (4*L0x2001b300*x**206) [ 1043969, x**256 - 1 ],
eqmod (c0207*x** 207+c0463*x** 463+c0719*x** 719+c0975*x** 975)
      (4*L0x2001b304*x**207) [ 1043969, x**256 - 1 ],
eqmod (c0208*x** 208+c0464*x** 464+c0720*x** 720+c0976*x** 976)
      (4*L0x2001b308*x**208) [ 1043969, x**256 - 1 ],
eqmod (c0209*x** 209+c0465*x** 465+c0721*x** 721+c0977*x** 977)
      (4*L0x2001b30c*x**209) [ 1043969, x**256 - 1 ],
eqmod (c0210*x** 210+c0466*x** 466+c0722*x** 722+c0978*x** 978)
      (4*L0x2001b310*x**210) [ 1043969, x**256 - 1 ],
eqmod (c0211*x** 211+c0467*x** 467+c0723*x** 723+c0979*x** 979)
      (4*L0x2001b314*x**211) [ 1043969, x**256 - 1 ],
eqmod (c0212*x** 212+c0468*x** 468+c0724*x** 724+c0980*x** 980)
      (4*L0x2001b318*x**212) [ 1043969, x**256 - 1 ],
eqmod (c0213*x** 213+c0469*x** 469+c0725*x** 725+c0981*x** 981)
      (4*L0x2001b31c*x**213) [ 1043969, x**256 - 1 ],
eqmod (c0214*x** 214+c0470*x** 470+c0726*x** 726+c0982*x** 982)
      (4*L0x2001b320*x**214) [ 1043969, x**256 - 1 ],
eqmod (c0215*x** 215+c0471*x** 471+c0727*x** 727+c0983*x** 983)
      (4*L0x2001b324*x**215) [ 1043969, x**256 - 1 ],
eqmod (c0216*x** 216+c0472*x** 472+c0728*x** 728+c0984*x** 984)
      (4*L0x2001b328*x**216) [ 1043969, x**256 - 1 ],
eqmod (c0217*x** 217+c0473*x** 473+c0729*x** 729+c0985*x** 985)
      (4*L0x2001b32c*x**217) [ 1043969, x**256 - 1 ],
eqmod (c0218*x** 218+c0474*x** 474+c0730*x** 730+c0986*x** 986)
      (4*L0x2001b330*x**218) [ 1043969, x**256 - 1 ],
eqmod (c0219*x** 219+c0475*x** 475+c0731*x** 731+c0987*x** 987)
      (4*L0x2001b334*x**219) [ 1043969, x**256 - 1 ],
eqmod (c0220*x** 220+c0476*x** 476+c0732*x** 732+c0988*x** 988)
      (4*L0x2001b338*x**220) [ 1043969, x**256 - 1 ],
eqmod (c0221*x** 221+c0477*x** 477+c0733*x** 733+c0989*x** 989)
      (4*L0x2001b33c*x**221) [ 1043969, x**256 - 1 ],
eqmod (c0222*x** 222+c0478*x** 478+c0734*x** 734+c0990*x** 990)
      (4*L0x2001b340*x**222) [ 1043969, x**256 - 1 ],
eqmod (c0223*x** 223+c0479*x** 479+c0735*x** 735+c0991*x** 991)
      (4*L0x2001b344*x**223) [ 1043969, x**256 - 1 ],
eqmod (c0224*x** 224+c0480*x** 480+c0736*x** 736+c0992*x** 992)
      (4*L0x2001b348*x**224) [ 1043969, x**256 - 1 ],
eqmod (c0225*x** 225+c0481*x** 481+c0737*x** 737+c0993*x** 993)
      (4*L0x2001b34c*x**225) [ 1043969, x**256 - 1 ],
eqmod (c0226*x** 226+c0482*x** 482+c0738*x** 738+c0994*x** 994)
      (4*L0x2001b350*x**226) [ 1043969, x**256 - 1 ],
eqmod (c0227*x** 227+c0483*x** 483+c0739*x** 739+c0995*x** 995)
      (4*L0x2001b354*x**227) [ 1043969, x**256 - 1 ],
eqmod (c0228*x** 228+c0484*x** 484+c0740*x** 740+c0996*x** 996)
      (4*L0x2001b358*x**228) [ 1043969, x**256 - 1 ],
eqmod (c0229*x** 229+c0485*x** 485+c0741*x** 741+c0997*x** 997)
      (4*L0x2001b35c*x**229) [ 1043969, x**256 - 1 ],
eqmod (c0230*x** 230+c0486*x** 486+c0742*x** 742+c0998*x** 998)
      (4*L0x2001b360*x**230) [ 1043969, x**256 - 1 ],
eqmod (c0231*x** 231+c0487*x** 487+c0743*x** 743+c0999*x** 999)
      (4*L0x2001b364*x**231) [ 1043969, x**256 - 1 ],
eqmod (c0232*x** 232+c0488*x** 488+c0744*x** 744+c1000*x**1000)
      (4*L0x2001b368*x**232) [ 1043969, x**256 - 1 ],
eqmod (c0233*x** 233+c0489*x** 489+c0745*x** 745+c1001*x**1001)
      (4*L0x2001b36c*x**233) [ 1043969, x**256 - 1 ],
eqmod (c0234*x** 234+c0490*x** 490+c0746*x** 746+c1002*x**1002)
      (4*L0x2001b370*x**234) [ 1043969, x**256 - 1 ],
eqmod (c0235*x** 235+c0491*x** 491+c0747*x** 747+c1003*x**1003)
      (4*L0x2001b374*x**235) [ 1043969, x**256 - 1 ],
eqmod (c0236*x** 236+c0492*x** 492+c0748*x** 748+c1004*x**1004)
      (4*L0x2001b378*x**236) [ 1043969, x**256 - 1 ],
eqmod (c0237*x** 237+c0493*x** 493+c0749*x** 749+c1005*x**1005)
      (4*L0x2001b37c*x**237) [ 1043969, x**256 - 1 ],
eqmod (c0238*x** 238+c0494*x** 494+c0750*x** 750+c1006*x**1006)
      (4*L0x2001b380*x**238) [ 1043969, x**256 - 1 ],
eqmod (c0239*x** 239+c0495*x** 495+c0751*x** 751+c1007*x**1007)
      (4*L0x2001b384*x**239) [ 1043969, x**256 - 1 ],
eqmod (c0240*x** 240+c0496*x** 496+c0752*x** 752+c1008*x**1008)
      (4*L0x2001b388*x**240) [ 1043969, x**256 - 1 ],
eqmod (c0241*x** 241+c0497*x** 497+c0753*x** 753+c1009*x**1009)
      (4*L0x2001b38c*x**241) [ 1043969, x**256 - 1 ],
eqmod (c0242*x** 242+c0498*x** 498+c0754*x** 754+c1010*x**1010)
      (4*L0x2001b390*x**242) [ 1043969, x**256 - 1 ],
eqmod (c0243*x** 243+c0499*x** 499+c0755*x** 755+c1011*x**1011)
      (4*L0x2001b394*x**243) [ 1043969, x**256 - 1 ],
eqmod (c0244*x** 244+c0500*x** 500+c0756*x** 756+c1012*x**1012)
      (4*L0x2001b398*x**244) [ 1043969, x**256 - 1 ],
eqmod (c0245*x** 245+c0501*x** 501+c0757*x** 757+c1013*x**1013)
      (4*L0x2001b39c*x**245) [ 1043969, x**256 - 1 ],
eqmod (c0246*x** 246+c0502*x** 502+c0758*x** 758+c1014*x**1014)
      (4*L0x2001b3a0*x**246) [ 1043969, x**256 - 1 ],
eqmod (c0247*x** 247+c0503*x** 503+c0759*x** 759+c1015*x**1015)
      (4*L0x2001b3a4*x**247) [ 1043969, x**256 - 1 ],
eqmod (c0248*x** 248+c0504*x** 504+c0760*x** 760+c1016*x**1016)
      (4*L0x2001b3a8*x**248) [ 1043969, x**256 - 1 ],
eqmod (c0249*x** 249+c0505*x** 505+c0761*x** 761+c1017*x**1017)
      (4*L0x2001b3ac*x**249) [ 1043969, x**256 - 1 ],
eqmod (c0250*x** 250+c0506*x** 506+c0762*x** 762+c1018*x**1018)
      (4*L0x2001b3b0*x**250) [ 1043969, x**256 - 1 ],
eqmod (c0251*x** 251+c0507*x** 507+c0763*x** 763+c1019*x**1019)
      (4*L0x2001b3b4*x**251) [ 1043969, x**256 - 1 ],
eqmod (c0252*x** 252+c0508*x** 508+c0764*x** 764+c1020*x**1020)
      (4*L0x2001b3b8*x**252) [ 1043969, x**256 - 1 ],
eqmod (c0253*x** 253+c0509*x** 509+c0765*x** 765+c1021*x**1021)
      (4*L0x2001b3bc*x**253) [ 1043969, x**256 - 1 ],
eqmod (c0254*x** 254+c0510*x** 510+c0766*x** 766+c1022*x**1022)
      (4*L0x2001b3c0*x**254) [ 1043969, x**256 - 1 ],
eqmod (c0255*x** 255+c0511*x** 511+c0767*x** 767+c1023*x**1023)
      (4*L0x2001b3c4*x**255) [ 1043969, x**256 - 1 ]
] prove with [ cuts [ 
  9,  10,  11,  12,  14,  16,  18,  20,  22,  24,  26,  28,
 30,  32,  34,  36,  38,  40,  42,  44,  46,  48,  50,  52,
 54,  56,  58,  60,  62,  64,  66,  68,  70,  72,  74,  76,
 78,  80,  82,  84,  86,  88,  90,  92,  94,  96,  98, 100,
102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124,
126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148,
150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172,
174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196,
198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220,
222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244,
246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268,
270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292,
294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316,
318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340,
342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364,
366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388,
390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412,
414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436,
438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460,
462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484,
486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508,
510, 512, 514, 516 ] ];



(******************** CUT 520 ********************)


ecut and [
eqmod (c0000*x**   0+c0256*x** 256+c0512*x** 512+c0768*x** 768)
      (4*L0x2001b3c8*x**  0) [ 1043969, x**256 + 1 ],
eqmod (c0001*x**   1+c0257*x** 257+c0513*x** 513+c0769*x** 769)
      (4*L0x2001b3cc*x**  1) [ 1043969, x**256 + 1 ],
eqmod (c0002*x**   2+c0258*x** 258+c0514*x** 514+c0770*x** 770)
      (4*L0x2001b3d0*x**  2) [ 1043969, x**256 + 1 ],
eqmod (c0003*x**   3+c0259*x** 259+c0515*x** 515+c0771*x** 771)
      (4*L0x2001b3d4*x**  3) [ 1043969, x**256 + 1 ],
eqmod (c0004*x**   4+c0260*x** 260+c0516*x** 516+c0772*x** 772)
      (4*L0x2001b3d8*x**  4) [ 1043969, x**256 + 1 ],
eqmod (c0005*x**   5+c0261*x** 261+c0517*x** 517+c0773*x** 773)
      (4*L0x2001b3dc*x**  5) [ 1043969, x**256 + 1 ],
eqmod (c0006*x**   6+c0262*x** 262+c0518*x** 518+c0774*x** 774)
      (4*L0x2001b3e0*x**  6) [ 1043969, x**256 + 1 ],
eqmod (c0007*x**   7+c0263*x** 263+c0519*x** 519+c0775*x** 775)
      (4*L0x2001b3e4*x**  7) [ 1043969, x**256 + 1 ],
eqmod (c0008*x**   8+c0264*x** 264+c0520*x** 520+c0776*x** 776)
      (4*L0x2001b3e8*x**  8) [ 1043969, x**256 + 1 ],
eqmod (c0009*x**   9+c0265*x** 265+c0521*x** 521+c0777*x** 777)
      (4*L0x2001b3ec*x**  9) [ 1043969, x**256 + 1 ],
eqmod (c0010*x**  10+c0266*x** 266+c0522*x** 522+c0778*x** 778)
      (4*L0x2001b3f0*x** 10) [ 1043969, x**256 + 1 ],
eqmod (c0011*x**  11+c0267*x** 267+c0523*x** 523+c0779*x** 779)
      (4*L0x2001b3f4*x** 11) [ 1043969, x**256 + 1 ],
eqmod (c0012*x**  12+c0268*x** 268+c0524*x** 524+c0780*x** 780)
      (4*L0x2001b3f8*x** 12) [ 1043969, x**256 + 1 ],
eqmod (c0013*x**  13+c0269*x** 269+c0525*x** 525+c0781*x** 781)
      (4*L0x2001b3fc*x** 13) [ 1043969, x**256 + 1 ],
eqmod (c0014*x**  14+c0270*x** 270+c0526*x** 526+c0782*x** 782)
      (4*L0x2001b400*x** 14) [ 1043969, x**256 + 1 ],
eqmod (c0015*x**  15+c0271*x** 271+c0527*x** 527+c0783*x** 783)
      (4*L0x2001b404*x** 15) [ 1043969, x**256 + 1 ],
eqmod (c0016*x**  16+c0272*x** 272+c0528*x** 528+c0784*x** 784)
      (4*L0x2001b408*x** 16) [ 1043969, x**256 + 1 ],
eqmod (c0017*x**  17+c0273*x** 273+c0529*x** 529+c0785*x** 785)
      (4*L0x2001b40c*x** 17) [ 1043969, x**256 + 1 ],
eqmod (c0018*x**  18+c0274*x** 274+c0530*x** 530+c0786*x** 786)
      (4*L0x2001b410*x** 18) [ 1043969, x**256 + 1 ],
eqmod (c0019*x**  19+c0275*x** 275+c0531*x** 531+c0787*x** 787)
      (4*L0x2001b414*x** 19) [ 1043969, x**256 + 1 ],
eqmod (c0020*x**  20+c0276*x** 276+c0532*x** 532+c0788*x** 788)
      (4*L0x2001b418*x** 20) [ 1043969, x**256 + 1 ],
eqmod (c0021*x**  21+c0277*x** 277+c0533*x** 533+c0789*x** 789)
      (4*L0x2001b41c*x** 21) [ 1043969, x**256 + 1 ],
eqmod (c0022*x**  22+c0278*x** 278+c0534*x** 534+c0790*x** 790)
      (4*L0x2001b420*x** 22) [ 1043969, x**256 + 1 ],
eqmod (c0023*x**  23+c0279*x** 279+c0535*x** 535+c0791*x** 791)
      (4*L0x2001b424*x** 23) [ 1043969, x**256 + 1 ],
eqmod (c0024*x**  24+c0280*x** 280+c0536*x** 536+c0792*x** 792)
      (4*L0x2001b428*x** 24) [ 1043969, x**256 + 1 ],
eqmod (c0025*x**  25+c0281*x** 281+c0537*x** 537+c0793*x** 793)
      (4*L0x2001b42c*x** 25) [ 1043969, x**256 + 1 ],
eqmod (c0026*x**  26+c0282*x** 282+c0538*x** 538+c0794*x** 794)
      (4*L0x2001b430*x** 26) [ 1043969, x**256 + 1 ],
eqmod (c0027*x**  27+c0283*x** 283+c0539*x** 539+c0795*x** 795)
      (4*L0x2001b434*x** 27) [ 1043969, x**256 + 1 ],
eqmod (c0028*x**  28+c0284*x** 284+c0540*x** 540+c0796*x** 796)
      (4*L0x2001b438*x** 28) [ 1043969, x**256 + 1 ],
eqmod (c0029*x**  29+c0285*x** 285+c0541*x** 541+c0797*x** 797)
      (4*L0x2001b43c*x** 29) [ 1043969, x**256 + 1 ],
eqmod (c0030*x**  30+c0286*x** 286+c0542*x** 542+c0798*x** 798)
      (4*L0x2001b440*x** 30) [ 1043969, x**256 + 1 ],
eqmod (c0031*x**  31+c0287*x** 287+c0543*x** 543+c0799*x** 799)
      (4*L0x2001b444*x** 31) [ 1043969, x**256 + 1 ],
eqmod (c0032*x**  32+c0288*x** 288+c0544*x** 544+c0800*x** 800)
      (4*L0x2001b448*x** 32) [ 1043969, x**256 + 1 ],
eqmod (c0033*x**  33+c0289*x** 289+c0545*x** 545+c0801*x** 801)
      (4*L0x2001b44c*x** 33) [ 1043969, x**256 + 1 ],
eqmod (c0034*x**  34+c0290*x** 290+c0546*x** 546+c0802*x** 802)
      (4*L0x2001b450*x** 34) [ 1043969, x**256 + 1 ],
eqmod (c0035*x**  35+c0291*x** 291+c0547*x** 547+c0803*x** 803)
      (4*L0x2001b454*x** 35) [ 1043969, x**256 + 1 ],
eqmod (c0036*x**  36+c0292*x** 292+c0548*x** 548+c0804*x** 804)
      (4*L0x2001b458*x** 36) [ 1043969, x**256 + 1 ],
eqmod (c0037*x**  37+c0293*x** 293+c0549*x** 549+c0805*x** 805)
      (4*L0x2001b45c*x** 37) [ 1043969, x**256 + 1 ],
eqmod (c0038*x**  38+c0294*x** 294+c0550*x** 550+c0806*x** 806)
      (4*L0x2001b460*x** 38) [ 1043969, x**256 + 1 ],
eqmod (c0039*x**  39+c0295*x** 295+c0551*x** 551+c0807*x** 807)
      (4*L0x2001b464*x** 39) [ 1043969, x**256 + 1 ],
eqmod (c0040*x**  40+c0296*x** 296+c0552*x** 552+c0808*x** 808)
      (4*L0x2001b468*x** 40) [ 1043969, x**256 + 1 ],
eqmod (c0041*x**  41+c0297*x** 297+c0553*x** 553+c0809*x** 809)
      (4*L0x2001b46c*x** 41) [ 1043969, x**256 + 1 ],
eqmod (c0042*x**  42+c0298*x** 298+c0554*x** 554+c0810*x** 810)
      (4*L0x2001b470*x** 42) [ 1043969, x**256 + 1 ],
eqmod (c0043*x**  43+c0299*x** 299+c0555*x** 555+c0811*x** 811)
      (4*L0x2001b474*x** 43) [ 1043969, x**256 + 1 ],
eqmod (c0044*x**  44+c0300*x** 300+c0556*x** 556+c0812*x** 812)
      (4*L0x2001b478*x** 44) [ 1043969, x**256 + 1 ],
eqmod (c0045*x**  45+c0301*x** 301+c0557*x** 557+c0813*x** 813)
      (4*L0x2001b47c*x** 45) [ 1043969, x**256 + 1 ],
eqmod (c0046*x**  46+c0302*x** 302+c0558*x** 558+c0814*x** 814)
      (4*L0x2001b480*x** 46) [ 1043969, x**256 + 1 ],
eqmod (c0047*x**  47+c0303*x** 303+c0559*x** 559+c0815*x** 815)
      (4*L0x2001b484*x** 47) [ 1043969, x**256 + 1 ],
eqmod (c0048*x**  48+c0304*x** 304+c0560*x** 560+c0816*x** 816)
      (4*L0x2001b488*x** 48) [ 1043969, x**256 + 1 ],
eqmod (c0049*x**  49+c0305*x** 305+c0561*x** 561+c0817*x** 817)
      (4*L0x2001b48c*x** 49) [ 1043969, x**256 + 1 ],
eqmod (c0050*x**  50+c0306*x** 306+c0562*x** 562+c0818*x** 818)
      (4*L0x2001b490*x** 50) [ 1043969, x**256 + 1 ],
eqmod (c0051*x**  51+c0307*x** 307+c0563*x** 563+c0819*x** 819)
      (4*L0x2001b494*x** 51) [ 1043969, x**256 + 1 ],
eqmod (c0052*x**  52+c0308*x** 308+c0564*x** 564+c0820*x** 820)
      (4*L0x2001b498*x** 52) [ 1043969, x**256 + 1 ],
eqmod (c0053*x**  53+c0309*x** 309+c0565*x** 565+c0821*x** 821)
      (4*L0x2001b49c*x** 53) [ 1043969, x**256 + 1 ],
eqmod (c0054*x**  54+c0310*x** 310+c0566*x** 566+c0822*x** 822)
      (4*L0x2001b4a0*x** 54) [ 1043969, x**256 + 1 ],
eqmod (c0055*x**  55+c0311*x** 311+c0567*x** 567+c0823*x** 823)
      (4*L0x2001b4a4*x** 55) [ 1043969, x**256 + 1 ],
eqmod (c0056*x**  56+c0312*x** 312+c0568*x** 568+c0824*x** 824)
      (4*L0x2001b4a8*x** 56) [ 1043969, x**256 + 1 ],
eqmod (c0057*x**  57+c0313*x** 313+c0569*x** 569+c0825*x** 825)
      (4*L0x2001b4ac*x** 57) [ 1043969, x**256 + 1 ],
eqmod (c0058*x**  58+c0314*x** 314+c0570*x** 570+c0826*x** 826)
      (4*L0x2001b4b0*x** 58) [ 1043969, x**256 + 1 ],
eqmod (c0059*x**  59+c0315*x** 315+c0571*x** 571+c0827*x** 827)
      (4*L0x2001b4b4*x** 59) [ 1043969, x**256 + 1 ],
eqmod (c0060*x**  60+c0316*x** 316+c0572*x** 572+c0828*x** 828)
      (4*L0x2001b4b8*x** 60) [ 1043969, x**256 + 1 ],
eqmod (c0061*x**  61+c0317*x** 317+c0573*x** 573+c0829*x** 829)
      (4*L0x2001b4bc*x** 61) [ 1043969, x**256 + 1 ],
eqmod (c0062*x**  62+c0318*x** 318+c0574*x** 574+c0830*x** 830)
      (4*L0x2001b4c0*x** 62) [ 1043969, x**256 + 1 ],
eqmod (c0063*x**  63+c0319*x** 319+c0575*x** 575+c0831*x** 831)
      (4*L0x2001b4c4*x** 63) [ 1043969, x**256 + 1 ],
eqmod (c0064*x**  64+c0320*x** 320+c0576*x** 576+c0832*x** 832)
      (4*L0x2001b4c8*x** 64) [ 1043969, x**256 + 1 ],
eqmod (c0065*x**  65+c0321*x** 321+c0577*x** 577+c0833*x** 833)
      (4*L0x2001b4cc*x** 65) [ 1043969, x**256 + 1 ],
eqmod (c0066*x**  66+c0322*x** 322+c0578*x** 578+c0834*x** 834)
      (4*L0x2001b4d0*x** 66) [ 1043969, x**256 + 1 ],
eqmod (c0067*x**  67+c0323*x** 323+c0579*x** 579+c0835*x** 835)
      (4*L0x2001b4d4*x** 67) [ 1043969, x**256 + 1 ],
eqmod (c0068*x**  68+c0324*x** 324+c0580*x** 580+c0836*x** 836)
      (4*L0x2001b4d8*x** 68) [ 1043969, x**256 + 1 ],
eqmod (c0069*x**  69+c0325*x** 325+c0581*x** 581+c0837*x** 837)
      (4*L0x2001b4dc*x** 69) [ 1043969, x**256 + 1 ],
eqmod (c0070*x**  70+c0326*x** 326+c0582*x** 582+c0838*x** 838)
      (4*L0x2001b4e0*x** 70) [ 1043969, x**256 + 1 ],
eqmod (c0071*x**  71+c0327*x** 327+c0583*x** 583+c0839*x** 839)
      (4*L0x2001b4e4*x** 71) [ 1043969, x**256 + 1 ],
eqmod (c0072*x**  72+c0328*x** 328+c0584*x** 584+c0840*x** 840)
      (4*L0x2001b4e8*x** 72) [ 1043969, x**256 + 1 ],
eqmod (c0073*x**  73+c0329*x** 329+c0585*x** 585+c0841*x** 841)
      (4*L0x2001b4ec*x** 73) [ 1043969, x**256 + 1 ],
eqmod (c0074*x**  74+c0330*x** 330+c0586*x** 586+c0842*x** 842)
      (4*L0x2001b4f0*x** 74) [ 1043969, x**256 + 1 ],
eqmod (c0075*x**  75+c0331*x** 331+c0587*x** 587+c0843*x** 843)
      (4*L0x2001b4f4*x** 75) [ 1043969, x**256 + 1 ],
eqmod (c0076*x**  76+c0332*x** 332+c0588*x** 588+c0844*x** 844)
      (4*L0x2001b4f8*x** 76) [ 1043969, x**256 + 1 ],
eqmod (c0077*x**  77+c0333*x** 333+c0589*x** 589+c0845*x** 845)
      (4*L0x2001b4fc*x** 77) [ 1043969, x**256 + 1 ],
eqmod (c0078*x**  78+c0334*x** 334+c0590*x** 590+c0846*x** 846)
      (4*L0x2001b500*x** 78) [ 1043969, x**256 + 1 ],
eqmod (c0079*x**  79+c0335*x** 335+c0591*x** 591+c0847*x** 847)
      (4*L0x2001b504*x** 79) [ 1043969, x**256 + 1 ],
eqmod (c0080*x**  80+c0336*x** 336+c0592*x** 592+c0848*x** 848)
      (4*L0x2001b508*x** 80) [ 1043969, x**256 + 1 ],
eqmod (c0081*x**  81+c0337*x** 337+c0593*x** 593+c0849*x** 849)
      (4*L0x2001b50c*x** 81) [ 1043969, x**256 + 1 ],
eqmod (c0082*x**  82+c0338*x** 338+c0594*x** 594+c0850*x** 850)
      (4*L0x2001b510*x** 82) [ 1043969, x**256 + 1 ],
eqmod (c0083*x**  83+c0339*x** 339+c0595*x** 595+c0851*x** 851)
      (4*L0x2001b514*x** 83) [ 1043969, x**256 + 1 ],
eqmod (c0084*x**  84+c0340*x** 340+c0596*x** 596+c0852*x** 852)
      (4*L0x2001b518*x** 84) [ 1043969, x**256 + 1 ],
eqmod (c0085*x**  85+c0341*x** 341+c0597*x** 597+c0853*x** 853)
      (4*L0x2001b51c*x** 85) [ 1043969, x**256 + 1 ],
eqmod (c0086*x**  86+c0342*x** 342+c0598*x** 598+c0854*x** 854)
      (4*L0x2001b520*x** 86) [ 1043969, x**256 + 1 ],
eqmod (c0087*x**  87+c0343*x** 343+c0599*x** 599+c0855*x** 855)
      (4*L0x2001b524*x** 87) [ 1043969, x**256 + 1 ],
eqmod (c0088*x**  88+c0344*x** 344+c0600*x** 600+c0856*x** 856)
      (4*L0x2001b528*x** 88) [ 1043969, x**256 + 1 ],
eqmod (c0089*x**  89+c0345*x** 345+c0601*x** 601+c0857*x** 857)
      (4*L0x2001b52c*x** 89) [ 1043969, x**256 + 1 ],
eqmod (c0090*x**  90+c0346*x** 346+c0602*x** 602+c0858*x** 858)
      (4*L0x2001b530*x** 90) [ 1043969, x**256 + 1 ],
eqmod (c0091*x**  91+c0347*x** 347+c0603*x** 603+c0859*x** 859)
      (4*L0x2001b534*x** 91) [ 1043969, x**256 + 1 ],
eqmod (c0092*x**  92+c0348*x** 348+c0604*x** 604+c0860*x** 860)
      (4*L0x2001b538*x** 92) [ 1043969, x**256 + 1 ],
eqmod (c0093*x**  93+c0349*x** 349+c0605*x** 605+c0861*x** 861)
      (4*L0x2001b53c*x** 93) [ 1043969, x**256 + 1 ],
eqmod (c0094*x**  94+c0350*x** 350+c0606*x** 606+c0862*x** 862)
      (4*L0x2001b540*x** 94) [ 1043969, x**256 + 1 ],
eqmod (c0095*x**  95+c0351*x** 351+c0607*x** 607+c0863*x** 863)
      (4*L0x2001b544*x** 95) [ 1043969, x**256 + 1 ],
eqmod (c0096*x**  96+c0352*x** 352+c0608*x** 608+c0864*x** 864)
      (4*L0x2001b548*x** 96) [ 1043969, x**256 + 1 ],
eqmod (c0097*x**  97+c0353*x** 353+c0609*x** 609+c0865*x** 865)
      (4*L0x2001b54c*x** 97) [ 1043969, x**256 + 1 ],
eqmod (c0098*x**  98+c0354*x** 354+c0610*x** 610+c0866*x** 866)
      (4*L0x2001b550*x** 98) [ 1043969, x**256 + 1 ],
eqmod (c0099*x**  99+c0355*x** 355+c0611*x** 611+c0867*x** 867)
      (4*L0x2001b554*x** 99) [ 1043969, x**256 + 1 ],
eqmod (c0100*x** 100+c0356*x** 356+c0612*x** 612+c0868*x** 868)
      (4*L0x2001b558*x**100) [ 1043969, x**256 + 1 ],
eqmod (c0101*x** 101+c0357*x** 357+c0613*x** 613+c0869*x** 869)
      (4*L0x2001b55c*x**101) [ 1043969, x**256 + 1 ],
eqmod (c0102*x** 102+c0358*x** 358+c0614*x** 614+c0870*x** 870)
      (4*L0x2001b560*x**102) [ 1043969, x**256 + 1 ],
eqmod (c0103*x** 103+c0359*x** 359+c0615*x** 615+c0871*x** 871)
      (4*L0x2001b564*x**103) [ 1043969, x**256 + 1 ],
eqmod (c0104*x** 104+c0360*x** 360+c0616*x** 616+c0872*x** 872)
      (4*L0x2001b568*x**104) [ 1043969, x**256 + 1 ],
eqmod (c0105*x** 105+c0361*x** 361+c0617*x** 617+c0873*x** 873)
      (4*L0x2001b56c*x**105) [ 1043969, x**256 + 1 ],
eqmod (c0106*x** 106+c0362*x** 362+c0618*x** 618+c0874*x** 874)
      (4*L0x2001b570*x**106) [ 1043969, x**256 + 1 ],
eqmod (c0107*x** 107+c0363*x** 363+c0619*x** 619+c0875*x** 875)
      (4*L0x2001b574*x**107) [ 1043969, x**256 + 1 ],
eqmod (c0108*x** 108+c0364*x** 364+c0620*x** 620+c0876*x** 876)
      (4*L0x2001b578*x**108) [ 1043969, x**256 + 1 ],
eqmod (c0109*x** 109+c0365*x** 365+c0621*x** 621+c0877*x** 877)
      (4*L0x2001b57c*x**109) [ 1043969, x**256 + 1 ],
eqmod (c0110*x** 110+c0366*x** 366+c0622*x** 622+c0878*x** 878)
      (4*L0x2001b580*x**110) [ 1043969, x**256 + 1 ],
eqmod (c0111*x** 111+c0367*x** 367+c0623*x** 623+c0879*x** 879)
      (4*L0x2001b584*x**111) [ 1043969, x**256 + 1 ],
eqmod (c0112*x** 112+c0368*x** 368+c0624*x** 624+c0880*x** 880)
      (4*L0x2001b588*x**112) [ 1043969, x**256 + 1 ],
eqmod (c0113*x** 113+c0369*x** 369+c0625*x** 625+c0881*x** 881)
      (4*L0x2001b58c*x**113) [ 1043969, x**256 + 1 ],
eqmod (c0114*x** 114+c0370*x** 370+c0626*x** 626+c0882*x** 882)
      (4*L0x2001b590*x**114) [ 1043969, x**256 + 1 ],
eqmod (c0115*x** 115+c0371*x** 371+c0627*x** 627+c0883*x** 883)
      (4*L0x2001b594*x**115) [ 1043969, x**256 + 1 ],
eqmod (c0116*x** 116+c0372*x** 372+c0628*x** 628+c0884*x** 884)
      (4*L0x2001b598*x**116) [ 1043969, x**256 + 1 ],
eqmod (c0117*x** 117+c0373*x** 373+c0629*x** 629+c0885*x** 885)
      (4*L0x2001b59c*x**117) [ 1043969, x**256 + 1 ],
eqmod (c0118*x** 118+c0374*x** 374+c0630*x** 630+c0886*x** 886)
      (4*L0x2001b5a0*x**118) [ 1043969, x**256 + 1 ],
eqmod (c0119*x** 119+c0375*x** 375+c0631*x** 631+c0887*x** 887)
      (4*L0x2001b5a4*x**119) [ 1043969, x**256 + 1 ],
eqmod (c0120*x** 120+c0376*x** 376+c0632*x** 632+c0888*x** 888)
      (4*L0x2001b5a8*x**120) [ 1043969, x**256 + 1 ],
eqmod (c0121*x** 121+c0377*x** 377+c0633*x** 633+c0889*x** 889)
      (4*L0x2001b5ac*x**121) [ 1043969, x**256 + 1 ],
eqmod (c0122*x** 122+c0378*x** 378+c0634*x** 634+c0890*x** 890)
      (4*L0x2001b5b0*x**122) [ 1043969, x**256 + 1 ],
eqmod (c0123*x** 123+c0379*x** 379+c0635*x** 635+c0891*x** 891)
      (4*L0x2001b5b4*x**123) [ 1043969, x**256 + 1 ],
eqmod (c0124*x** 124+c0380*x** 380+c0636*x** 636+c0892*x** 892)
      (4*L0x2001b5b8*x**124) [ 1043969, x**256 + 1 ],
eqmod (c0125*x** 125+c0381*x** 381+c0637*x** 637+c0893*x** 893)
      (4*L0x2001b5bc*x**125) [ 1043969, x**256 + 1 ],
eqmod (c0126*x** 126+c0382*x** 382+c0638*x** 638+c0894*x** 894)
      (4*L0x2001b5c0*x**126) [ 1043969, x**256 + 1 ],
eqmod (c0127*x** 127+c0383*x** 383+c0639*x** 639+c0895*x** 895)
      (4*L0x2001b5c4*x**127) [ 1043969, x**256 + 1 ],
eqmod (c0128*x** 128+c0384*x** 384+c0640*x** 640+c0896*x** 896)
      (4*L0x2001b5c8*x**128) [ 1043969, x**256 + 1 ],
eqmod (c0129*x** 129+c0385*x** 385+c0641*x** 641+c0897*x** 897)
      (4*L0x2001b5cc*x**129) [ 1043969, x**256 + 1 ],
eqmod (c0130*x** 130+c0386*x** 386+c0642*x** 642+c0898*x** 898)
      (4*L0x2001b5d0*x**130) [ 1043969, x**256 + 1 ],
eqmod (c0131*x** 131+c0387*x** 387+c0643*x** 643+c0899*x** 899)
      (4*L0x2001b5d4*x**131) [ 1043969, x**256 + 1 ],
eqmod (c0132*x** 132+c0388*x** 388+c0644*x** 644+c0900*x** 900)
      (4*L0x2001b5d8*x**132) [ 1043969, x**256 + 1 ],
eqmod (c0133*x** 133+c0389*x** 389+c0645*x** 645+c0901*x** 901)
      (4*L0x2001b5dc*x**133) [ 1043969, x**256 + 1 ],
eqmod (c0134*x** 134+c0390*x** 390+c0646*x** 646+c0902*x** 902)
      (4*L0x2001b5e0*x**134) [ 1043969, x**256 + 1 ],
eqmod (c0135*x** 135+c0391*x** 391+c0647*x** 647+c0903*x** 903)
      (4*L0x2001b5e4*x**135) [ 1043969, x**256 + 1 ],
eqmod (c0136*x** 136+c0392*x** 392+c0648*x** 648+c0904*x** 904)
      (4*L0x2001b5e8*x**136) [ 1043969, x**256 + 1 ],
eqmod (c0137*x** 137+c0393*x** 393+c0649*x** 649+c0905*x** 905)
      (4*L0x2001b5ec*x**137) [ 1043969, x**256 + 1 ],
eqmod (c0138*x** 138+c0394*x** 394+c0650*x** 650+c0906*x** 906)
      (4*L0x2001b5f0*x**138) [ 1043969, x**256 + 1 ],
eqmod (c0139*x** 139+c0395*x** 395+c0651*x** 651+c0907*x** 907)
      (4*L0x2001b5f4*x**139) [ 1043969, x**256 + 1 ],
eqmod (c0140*x** 140+c0396*x** 396+c0652*x** 652+c0908*x** 908)
      (4*L0x2001b5f8*x**140) [ 1043969, x**256 + 1 ],
eqmod (c0141*x** 141+c0397*x** 397+c0653*x** 653+c0909*x** 909)
      (4*L0x2001b5fc*x**141) [ 1043969, x**256 + 1 ],
eqmod (c0142*x** 142+c0398*x** 398+c0654*x** 654+c0910*x** 910)
      (4*L0x2001b600*x**142) [ 1043969, x**256 + 1 ],
eqmod (c0143*x** 143+c0399*x** 399+c0655*x** 655+c0911*x** 911)
      (4*L0x2001b604*x**143) [ 1043969, x**256 + 1 ],
eqmod (c0144*x** 144+c0400*x** 400+c0656*x** 656+c0912*x** 912)
      (4*L0x2001b608*x**144) [ 1043969, x**256 + 1 ],
eqmod (c0145*x** 145+c0401*x** 401+c0657*x** 657+c0913*x** 913)
      (4*L0x2001b60c*x**145) [ 1043969, x**256 + 1 ],
eqmod (c0146*x** 146+c0402*x** 402+c0658*x** 658+c0914*x** 914)
      (4*L0x2001b610*x**146) [ 1043969, x**256 + 1 ],
eqmod (c0147*x** 147+c0403*x** 403+c0659*x** 659+c0915*x** 915)
      (4*L0x2001b614*x**147) [ 1043969, x**256 + 1 ],
eqmod (c0148*x** 148+c0404*x** 404+c0660*x** 660+c0916*x** 916)
      (4*L0x2001b618*x**148) [ 1043969, x**256 + 1 ],
eqmod (c0149*x** 149+c0405*x** 405+c0661*x** 661+c0917*x** 917)
      (4*L0x2001b61c*x**149) [ 1043969, x**256 + 1 ],
eqmod (c0150*x** 150+c0406*x** 406+c0662*x** 662+c0918*x** 918)
      (4*L0x2001b620*x**150) [ 1043969, x**256 + 1 ],
eqmod (c0151*x** 151+c0407*x** 407+c0663*x** 663+c0919*x** 919)
      (4*L0x2001b624*x**151) [ 1043969, x**256 + 1 ],
eqmod (c0152*x** 152+c0408*x** 408+c0664*x** 664+c0920*x** 920)
      (4*L0x2001b628*x**152) [ 1043969, x**256 + 1 ],
eqmod (c0153*x** 153+c0409*x** 409+c0665*x** 665+c0921*x** 921)
      (4*L0x2001b62c*x**153) [ 1043969, x**256 + 1 ],
eqmod (c0154*x** 154+c0410*x** 410+c0666*x** 666+c0922*x** 922)
      (4*L0x2001b630*x**154) [ 1043969, x**256 + 1 ],
eqmod (c0155*x** 155+c0411*x** 411+c0667*x** 667+c0923*x** 923)
      (4*L0x2001b634*x**155) [ 1043969, x**256 + 1 ],
eqmod (c0156*x** 156+c0412*x** 412+c0668*x** 668+c0924*x** 924)
      (4*L0x2001b638*x**156) [ 1043969, x**256 + 1 ],
eqmod (c0157*x** 157+c0413*x** 413+c0669*x** 669+c0925*x** 925)
      (4*L0x2001b63c*x**157) [ 1043969, x**256 + 1 ],
eqmod (c0158*x** 158+c0414*x** 414+c0670*x** 670+c0926*x** 926)
      (4*L0x2001b640*x**158) [ 1043969, x**256 + 1 ],
eqmod (c0159*x** 159+c0415*x** 415+c0671*x** 671+c0927*x** 927)
      (4*L0x2001b644*x**159) [ 1043969, x**256 + 1 ],
eqmod (c0160*x** 160+c0416*x** 416+c0672*x** 672+c0928*x** 928)
      (4*L0x2001b648*x**160) [ 1043969, x**256 + 1 ],
eqmod (c0161*x** 161+c0417*x** 417+c0673*x** 673+c0929*x** 929)
      (4*L0x2001b64c*x**161) [ 1043969, x**256 + 1 ],
eqmod (c0162*x** 162+c0418*x** 418+c0674*x** 674+c0930*x** 930)
      (4*L0x2001b650*x**162) [ 1043969, x**256 + 1 ],
eqmod (c0163*x** 163+c0419*x** 419+c0675*x** 675+c0931*x** 931)
      (4*L0x2001b654*x**163) [ 1043969, x**256 + 1 ],
eqmod (c0164*x** 164+c0420*x** 420+c0676*x** 676+c0932*x** 932)
      (4*L0x2001b658*x**164) [ 1043969, x**256 + 1 ],
eqmod (c0165*x** 165+c0421*x** 421+c0677*x** 677+c0933*x** 933)
      (4*L0x2001b65c*x**165) [ 1043969, x**256 + 1 ],
eqmod (c0166*x** 166+c0422*x** 422+c0678*x** 678+c0934*x** 934)
      (4*L0x2001b660*x**166) [ 1043969, x**256 + 1 ],
eqmod (c0167*x** 167+c0423*x** 423+c0679*x** 679+c0935*x** 935)
      (4*L0x2001b664*x**167) [ 1043969, x**256 + 1 ],
eqmod (c0168*x** 168+c0424*x** 424+c0680*x** 680+c0936*x** 936)
      (4*L0x2001b668*x**168) [ 1043969, x**256 + 1 ],
eqmod (c0169*x** 169+c0425*x** 425+c0681*x** 681+c0937*x** 937)
      (4*L0x2001b66c*x**169) [ 1043969, x**256 + 1 ],
eqmod (c0170*x** 170+c0426*x** 426+c0682*x** 682+c0938*x** 938)
      (4*L0x2001b670*x**170) [ 1043969, x**256 + 1 ],
eqmod (c0171*x** 171+c0427*x** 427+c0683*x** 683+c0939*x** 939)
      (4*L0x2001b674*x**171) [ 1043969, x**256 + 1 ],
eqmod (c0172*x** 172+c0428*x** 428+c0684*x** 684+c0940*x** 940)
      (4*L0x2001b678*x**172) [ 1043969, x**256 + 1 ],
eqmod (c0173*x** 173+c0429*x** 429+c0685*x** 685+c0941*x** 941)
      (4*L0x2001b67c*x**173) [ 1043969, x**256 + 1 ],
eqmod (c0174*x** 174+c0430*x** 430+c0686*x** 686+c0942*x** 942)
      (4*L0x2001b680*x**174) [ 1043969, x**256 + 1 ],
eqmod (c0175*x** 175+c0431*x** 431+c0687*x** 687+c0943*x** 943)
      (4*L0x2001b684*x**175) [ 1043969, x**256 + 1 ],
eqmod (c0176*x** 176+c0432*x** 432+c0688*x** 688+c0944*x** 944)
      (4*L0x2001b688*x**176) [ 1043969, x**256 + 1 ],
eqmod (c0177*x** 177+c0433*x** 433+c0689*x** 689+c0945*x** 945)
      (4*L0x2001b68c*x**177) [ 1043969, x**256 + 1 ],
eqmod (c0178*x** 178+c0434*x** 434+c0690*x** 690+c0946*x** 946)
      (4*L0x2001b690*x**178) [ 1043969, x**256 + 1 ],
eqmod (c0179*x** 179+c0435*x** 435+c0691*x** 691+c0947*x** 947)
      (4*L0x2001b694*x**179) [ 1043969, x**256 + 1 ],
eqmod (c0180*x** 180+c0436*x** 436+c0692*x** 692+c0948*x** 948)
      (4*L0x2001b698*x**180) [ 1043969, x**256 + 1 ],
eqmod (c0181*x** 181+c0437*x** 437+c0693*x** 693+c0949*x** 949)
      (4*L0x2001b69c*x**181) [ 1043969, x**256 + 1 ],
eqmod (c0182*x** 182+c0438*x** 438+c0694*x** 694+c0950*x** 950)
      (4*L0x2001b6a0*x**182) [ 1043969, x**256 + 1 ],
eqmod (c0183*x** 183+c0439*x** 439+c0695*x** 695+c0951*x** 951)
      (4*L0x2001b6a4*x**183) [ 1043969, x**256 + 1 ],
eqmod (c0184*x** 184+c0440*x** 440+c0696*x** 696+c0952*x** 952)
      (4*L0x2001b6a8*x**184) [ 1043969, x**256 + 1 ],
eqmod (c0185*x** 185+c0441*x** 441+c0697*x** 697+c0953*x** 953)
      (4*L0x2001b6ac*x**185) [ 1043969, x**256 + 1 ],
eqmod (c0186*x** 186+c0442*x** 442+c0698*x** 698+c0954*x** 954)
      (4*L0x2001b6b0*x**186) [ 1043969, x**256 + 1 ],
eqmod (c0187*x** 187+c0443*x** 443+c0699*x** 699+c0955*x** 955)
      (4*L0x2001b6b4*x**187) [ 1043969, x**256 + 1 ],
eqmod (c0188*x** 188+c0444*x** 444+c0700*x** 700+c0956*x** 956)
      (4*L0x2001b6b8*x**188) [ 1043969, x**256 + 1 ],
eqmod (c0189*x** 189+c0445*x** 445+c0701*x** 701+c0957*x** 957)
      (4*L0x2001b6bc*x**189) [ 1043969, x**256 + 1 ],
eqmod (c0190*x** 190+c0446*x** 446+c0702*x** 702+c0958*x** 958)
      (4*L0x2001b6c0*x**190) [ 1043969, x**256 + 1 ],
eqmod (c0191*x** 191+c0447*x** 447+c0703*x** 703+c0959*x** 959)
      (4*L0x2001b6c4*x**191) [ 1043969, x**256 + 1 ],
eqmod (c0192*x** 192+c0448*x** 448+c0704*x** 704+c0960*x** 960)
      (4*L0x2001b6c8*x**192) [ 1043969, x**256 + 1 ],
eqmod (c0193*x** 193+c0449*x** 449+c0705*x** 705+c0961*x** 961)
      (4*L0x2001b6cc*x**193) [ 1043969, x**256 + 1 ],
eqmod (c0194*x** 194+c0450*x** 450+c0706*x** 706+c0962*x** 962)
      (4*L0x2001b6d0*x**194) [ 1043969, x**256 + 1 ],
eqmod (c0195*x** 195+c0451*x** 451+c0707*x** 707+c0963*x** 963)
      (4*L0x2001b6d4*x**195) [ 1043969, x**256 + 1 ],
eqmod (c0196*x** 196+c0452*x** 452+c0708*x** 708+c0964*x** 964)
      (4*L0x2001b6d8*x**196) [ 1043969, x**256 + 1 ],
eqmod (c0197*x** 197+c0453*x** 453+c0709*x** 709+c0965*x** 965)
      (4*L0x2001b6dc*x**197) [ 1043969, x**256 + 1 ],
eqmod (c0198*x** 198+c0454*x** 454+c0710*x** 710+c0966*x** 966)
      (4*L0x2001b6e0*x**198) [ 1043969, x**256 + 1 ],
eqmod (c0199*x** 199+c0455*x** 455+c0711*x** 711+c0967*x** 967)
      (4*L0x2001b6e4*x**199) [ 1043969, x**256 + 1 ],
eqmod (c0200*x** 200+c0456*x** 456+c0712*x** 712+c0968*x** 968)
      (4*L0x2001b6e8*x**200) [ 1043969, x**256 + 1 ],
eqmod (c0201*x** 201+c0457*x** 457+c0713*x** 713+c0969*x** 969)
      (4*L0x2001b6ec*x**201) [ 1043969, x**256 + 1 ],
eqmod (c0202*x** 202+c0458*x** 458+c0714*x** 714+c0970*x** 970)
      (4*L0x2001b6f0*x**202) [ 1043969, x**256 + 1 ],
eqmod (c0203*x** 203+c0459*x** 459+c0715*x** 715+c0971*x** 971)
      (4*L0x2001b6f4*x**203) [ 1043969, x**256 + 1 ],
eqmod (c0204*x** 204+c0460*x** 460+c0716*x** 716+c0972*x** 972)
      (4*L0x2001b6f8*x**204) [ 1043969, x**256 + 1 ],
eqmod (c0205*x** 205+c0461*x** 461+c0717*x** 717+c0973*x** 973)
      (4*L0x2001b6fc*x**205) [ 1043969, x**256 + 1 ],
eqmod (c0206*x** 206+c0462*x** 462+c0718*x** 718+c0974*x** 974)
      (4*L0x2001b700*x**206) [ 1043969, x**256 + 1 ],
eqmod (c0207*x** 207+c0463*x** 463+c0719*x** 719+c0975*x** 975)
      (4*L0x2001b704*x**207) [ 1043969, x**256 + 1 ],
eqmod (c0208*x** 208+c0464*x** 464+c0720*x** 720+c0976*x** 976)
      (4*L0x2001b708*x**208) [ 1043969, x**256 + 1 ],
eqmod (c0209*x** 209+c0465*x** 465+c0721*x** 721+c0977*x** 977)
      (4*L0x2001b70c*x**209) [ 1043969, x**256 + 1 ],
eqmod (c0210*x** 210+c0466*x** 466+c0722*x** 722+c0978*x** 978)
      (4*L0x2001b710*x**210) [ 1043969, x**256 + 1 ],
eqmod (c0211*x** 211+c0467*x** 467+c0723*x** 723+c0979*x** 979)
      (4*L0x2001b714*x**211) [ 1043969, x**256 + 1 ],
eqmod (c0212*x** 212+c0468*x** 468+c0724*x** 724+c0980*x** 980)
      (4*L0x2001b718*x**212) [ 1043969, x**256 + 1 ],
eqmod (c0213*x** 213+c0469*x** 469+c0725*x** 725+c0981*x** 981)
      (4*L0x2001b71c*x**213) [ 1043969, x**256 + 1 ],
eqmod (c0214*x** 214+c0470*x** 470+c0726*x** 726+c0982*x** 982)
      (4*L0x2001b720*x**214) [ 1043969, x**256 + 1 ],
eqmod (c0215*x** 215+c0471*x** 471+c0727*x** 727+c0983*x** 983)
      (4*L0x2001b724*x**215) [ 1043969, x**256 + 1 ],
eqmod (c0216*x** 216+c0472*x** 472+c0728*x** 728+c0984*x** 984)
      (4*L0x2001b728*x**216) [ 1043969, x**256 + 1 ],
eqmod (c0217*x** 217+c0473*x** 473+c0729*x** 729+c0985*x** 985)
      (4*L0x2001b72c*x**217) [ 1043969, x**256 + 1 ],
eqmod (c0218*x** 218+c0474*x** 474+c0730*x** 730+c0986*x** 986)
      (4*L0x2001b730*x**218) [ 1043969, x**256 + 1 ],
eqmod (c0219*x** 219+c0475*x** 475+c0731*x** 731+c0987*x** 987)
      (4*L0x2001b734*x**219) [ 1043969, x**256 + 1 ],
eqmod (c0220*x** 220+c0476*x** 476+c0732*x** 732+c0988*x** 988)
      (4*L0x2001b738*x**220) [ 1043969, x**256 + 1 ],
eqmod (c0221*x** 221+c0477*x** 477+c0733*x** 733+c0989*x** 989)
      (4*L0x2001b73c*x**221) [ 1043969, x**256 + 1 ],
eqmod (c0222*x** 222+c0478*x** 478+c0734*x** 734+c0990*x** 990)
      (4*L0x2001b740*x**222) [ 1043969, x**256 + 1 ],
eqmod (c0223*x** 223+c0479*x** 479+c0735*x** 735+c0991*x** 991)
      (4*L0x2001b744*x**223) [ 1043969, x**256 + 1 ],
eqmod (c0224*x** 224+c0480*x** 480+c0736*x** 736+c0992*x** 992)
      (4*L0x2001b748*x**224) [ 1043969, x**256 + 1 ],
eqmod (c0225*x** 225+c0481*x** 481+c0737*x** 737+c0993*x** 993)
      (4*L0x2001b74c*x**225) [ 1043969, x**256 + 1 ],
eqmod (c0226*x** 226+c0482*x** 482+c0738*x** 738+c0994*x** 994)
      (4*L0x2001b750*x**226) [ 1043969, x**256 + 1 ],
eqmod (c0227*x** 227+c0483*x** 483+c0739*x** 739+c0995*x** 995)
      (4*L0x2001b754*x**227) [ 1043969, x**256 + 1 ],
eqmod (c0228*x** 228+c0484*x** 484+c0740*x** 740+c0996*x** 996)
      (4*L0x2001b758*x**228) [ 1043969, x**256 + 1 ],
eqmod (c0229*x** 229+c0485*x** 485+c0741*x** 741+c0997*x** 997)
      (4*L0x2001b75c*x**229) [ 1043969, x**256 + 1 ],
eqmod (c0230*x** 230+c0486*x** 486+c0742*x** 742+c0998*x** 998)
      (4*L0x2001b760*x**230) [ 1043969, x**256 + 1 ],
eqmod (c0231*x** 231+c0487*x** 487+c0743*x** 743+c0999*x** 999)
      (4*L0x2001b764*x**231) [ 1043969, x**256 + 1 ],
eqmod (c0232*x** 232+c0488*x** 488+c0744*x** 744+c1000*x**1000)
      (4*L0x2001b768*x**232) [ 1043969, x**256 + 1 ],
eqmod (c0233*x** 233+c0489*x** 489+c0745*x** 745+c1001*x**1001)
      (4*L0x2001b76c*x**233) [ 1043969, x**256 + 1 ],
eqmod (c0234*x** 234+c0490*x** 490+c0746*x** 746+c1002*x**1002)
      (4*L0x2001b770*x**234) [ 1043969, x**256 + 1 ],
eqmod (c0235*x** 235+c0491*x** 491+c0747*x** 747+c1003*x**1003)
      (4*L0x2001b774*x**235) [ 1043969, x**256 + 1 ],
eqmod (c0236*x** 236+c0492*x** 492+c0748*x** 748+c1004*x**1004)
      (4*L0x2001b778*x**236) [ 1043969, x**256 + 1 ],
eqmod (c0237*x** 237+c0493*x** 493+c0749*x** 749+c1005*x**1005)
      (4*L0x2001b77c*x**237) [ 1043969, x**256 + 1 ],
eqmod (c0238*x** 238+c0494*x** 494+c0750*x** 750+c1006*x**1006)
      (4*L0x2001b780*x**238) [ 1043969, x**256 + 1 ],
eqmod (c0239*x** 239+c0495*x** 495+c0751*x** 751+c1007*x**1007)
      (4*L0x2001b784*x**239) [ 1043969, x**256 + 1 ],
eqmod (c0240*x** 240+c0496*x** 496+c0752*x** 752+c1008*x**1008)
      (4*L0x2001b788*x**240) [ 1043969, x**256 + 1 ],
eqmod (c0241*x** 241+c0497*x** 497+c0753*x** 753+c1009*x**1009)
      (4*L0x2001b78c*x**241) [ 1043969, x**256 + 1 ],
eqmod (c0242*x** 242+c0498*x** 498+c0754*x** 754+c1010*x**1010)
      (4*L0x2001b790*x**242) [ 1043969, x**256 + 1 ],
eqmod (c0243*x** 243+c0499*x** 499+c0755*x** 755+c1011*x**1011)
      (4*L0x2001b794*x**243) [ 1043969, x**256 + 1 ],
eqmod (c0244*x** 244+c0500*x** 500+c0756*x** 756+c1012*x**1012)
      (4*L0x2001b798*x**244) [ 1043969, x**256 + 1 ],
eqmod (c0245*x** 245+c0501*x** 501+c0757*x** 757+c1013*x**1013)
      (4*L0x2001b79c*x**245) [ 1043969, x**256 + 1 ],
eqmod (c0246*x** 246+c0502*x** 502+c0758*x** 758+c1014*x**1014)
      (4*L0x2001b7a0*x**246) [ 1043969, x**256 + 1 ],
eqmod (c0247*x** 247+c0503*x** 503+c0759*x** 759+c1015*x**1015)
      (4*L0x2001b7a4*x**247) [ 1043969, x**256 + 1 ],
eqmod (c0248*x** 248+c0504*x** 504+c0760*x** 760+c1016*x**1016)
      (4*L0x2001b7a8*x**248) [ 1043969, x**256 + 1 ],
eqmod (c0249*x** 249+c0505*x** 505+c0761*x** 761+c1017*x**1017)
      (4*L0x2001b7ac*x**249) [ 1043969, x**256 + 1 ],
eqmod (c0250*x** 250+c0506*x** 506+c0762*x** 762+c1018*x**1018)
      (4*L0x2001b7b0*x**250) [ 1043969, x**256 + 1 ],
eqmod (c0251*x** 251+c0507*x** 507+c0763*x** 763+c1019*x**1019)
      (4*L0x2001b7b4*x**251) [ 1043969, x**256 + 1 ],
eqmod (c0252*x** 252+c0508*x** 508+c0764*x** 764+c1020*x**1020)
      (4*L0x2001b7b8*x**252) [ 1043969, x**256 + 1 ],
eqmod (c0253*x** 253+c0509*x** 509+c0765*x** 765+c1021*x**1021)
      (4*L0x2001b7bc*x**253) [ 1043969, x**256 + 1 ],
eqmod (c0254*x** 254+c0510*x** 510+c0766*x** 766+c1022*x**1022)
      (4*L0x2001b7c0*x**254) [ 1043969, x**256 + 1 ],
eqmod (c0255*x** 255+c0511*x** 511+c0767*x** 767+c1023*x**1023)
      (4*L0x2001b7c4*x**255) [ 1043969, x**256 + 1 ]
] prove with [ cuts [ 
  9,  10,  11,  12,  14,  16,  18,  20,  22,  24,  26,  28,
 30,  32,  34,  36,  38,  40,  42,  44,  46,  48,  50,  52,
 54,  56,  58,  60,  62,  64,  66,  68,  70,  72,  74,  76,
 78,  80,  82,  84,  86,  88,  90,  92,  94,  96,  98, 100,
102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124,
126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148,
150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172,
174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196,
198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220,
222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244,
246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268,
270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292,
294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316,
318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340,
342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364,
366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388,
390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412,
414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436,
438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460,
462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484,
486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508,
510, 512, 514, 516 ] ];



(******************** CUT 521 ********************)


ecut and [
eqmod (c0000*x**   0+c0256*x** 256+c0512*x** 512+c0768*x** 768)
      (4*L0x2001b7c8*x**  0) [ 1043969, x**256 - 554923 ],
eqmod (c0001*x**   1+c0257*x** 257+c0513*x** 513+c0769*x** 769)
      (4*L0x2001b7cc*x**  1) [ 1043969, x**256 - 554923 ],
eqmod (c0002*x**   2+c0258*x** 258+c0514*x** 514+c0770*x** 770)
      (4*L0x2001b7d0*x**  2) [ 1043969, x**256 - 554923 ],
eqmod (c0003*x**   3+c0259*x** 259+c0515*x** 515+c0771*x** 771)
      (4*L0x2001b7d4*x**  3) [ 1043969, x**256 - 554923 ],
eqmod (c0004*x**   4+c0260*x** 260+c0516*x** 516+c0772*x** 772)
      (4*L0x2001b7d8*x**  4) [ 1043969, x**256 - 554923 ],
eqmod (c0005*x**   5+c0261*x** 261+c0517*x** 517+c0773*x** 773)
      (4*L0x2001b7dc*x**  5) [ 1043969, x**256 - 554923 ],
eqmod (c0006*x**   6+c0262*x** 262+c0518*x** 518+c0774*x** 774)
      (4*L0x2001b7e0*x**  6) [ 1043969, x**256 - 554923 ],
eqmod (c0007*x**   7+c0263*x** 263+c0519*x** 519+c0775*x** 775)
      (4*L0x2001b7e4*x**  7) [ 1043969, x**256 - 554923 ],
eqmod (c0008*x**   8+c0264*x** 264+c0520*x** 520+c0776*x** 776)
      (4*L0x2001b7e8*x**  8) [ 1043969, x**256 - 554923 ],
eqmod (c0009*x**   9+c0265*x** 265+c0521*x** 521+c0777*x** 777)
      (4*L0x2001b7ec*x**  9) [ 1043969, x**256 - 554923 ],
eqmod (c0010*x**  10+c0266*x** 266+c0522*x** 522+c0778*x** 778)
      (4*L0x2001b7f0*x** 10) [ 1043969, x**256 - 554923 ],
eqmod (c0011*x**  11+c0267*x** 267+c0523*x** 523+c0779*x** 779)
      (4*L0x2001b7f4*x** 11) [ 1043969, x**256 - 554923 ],
eqmod (c0012*x**  12+c0268*x** 268+c0524*x** 524+c0780*x** 780)
      (4*L0x2001b7f8*x** 12) [ 1043969, x**256 - 554923 ],
eqmod (c0013*x**  13+c0269*x** 269+c0525*x** 525+c0781*x** 781)
      (4*L0x2001b7fc*x** 13) [ 1043969, x**256 - 554923 ],
eqmod (c0014*x**  14+c0270*x** 270+c0526*x** 526+c0782*x** 782)
      (4*L0x2001b800*x** 14) [ 1043969, x**256 - 554923 ],
eqmod (c0015*x**  15+c0271*x** 271+c0527*x** 527+c0783*x** 783)
      (4*L0x2001b804*x** 15) [ 1043969, x**256 - 554923 ],
eqmod (c0016*x**  16+c0272*x** 272+c0528*x** 528+c0784*x** 784)
      (4*L0x2001b808*x** 16) [ 1043969, x**256 - 554923 ],
eqmod (c0017*x**  17+c0273*x** 273+c0529*x** 529+c0785*x** 785)
      (4*L0x2001b80c*x** 17) [ 1043969, x**256 - 554923 ],
eqmod (c0018*x**  18+c0274*x** 274+c0530*x** 530+c0786*x** 786)
      (4*L0x2001b810*x** 18) [ 1043969, x**256 - 554923 ],
eqmod (c0019*x**  19+c0275*x** 275+c0531*x** 531+c0787*x** 787)
      (4*L0x2001b814*x** 19) [ 1043969, x**256 - 554923 ],
eqmod (c0020*x**  20+c0276*x** 276+c0532*x** 532+c0788*x** 788)
      (4*L0x2001b818*x** 20) [ 1043969, x**256 - 554923 ],
eqmod (c0021*x**  21+c0277*x** 277+c0533*x** 533+c0789*x** 789)
      (4*L0x2001b81c*x** 21) [ 1043969, x**256 - 554923 ],
eqmod (c0022*x**  22+c0278*x** 278+c0534*x** 534+c0790*x** 790)
      (4*L0x2001b820*x** 22) [ 1043969, x**256 - 554923 ],
eqmod (c0023*x**  23+c0279*x** 279+c0535*x** 535+c0791*x** 791)
      (4*L0x2001b824*x** 23) [ 1043969, x**256 - 554923 ],
eqmod (c0024*x**  24+c0280*x** 280+c0536*x** 536+c0792*x** 792)
      (4*L0x2001b828*x** 24) [ 1043969, x**256 - 554923 ],
eqmod (c0025*x**  25+c0281*x** 281+c0537*x** 537+c0793*x** 793)
      (4*L0x2001b82c*x** 25) [ 1043969, x**256 - 554923 ],
eqmod (c0026*x**  26+c0282*x** 282+c0538*x** 538+c0794*x** 794)
      (4*L0x2001b830*x** 26) [ 1043969, x**256 - 554923 ],
eqmod (c0027*x**  27+c0283*x** 283+c0539*x** 539+c0795*x** 795)
      (4*L0x2001b834*x** 27) [ 1043969, x**256 - 554923 ],
eqmod (c0028*x**  28+c0284*x** 284+c0540*x** 540+c0796*x** 796)
      (4*L0x2001b838*x** 28) [ 1043969, x**256 - 554923 ],
eqmod (c0029*x**  29+c0285*x** 285+c0541*x** 541+c0797*x** 797)
      (4*L0x2001b83c*x** 29) [ 1043969, x**256 - 554923 ],
eqmod (c0030*x**  30+c0286*x** 286+c0542*x** 542+c0798*x** 798)
      (4*L0x2001b840*x** 30) [ 1043969, x**256 - 554923 ],
eqmod (c0031*x**  31+c0287*x** 287+c0543*x** 543+c0799*x** 799)
      (4*L0x2001b844*x** 31) [ 1043969, x**256 - 554923 ],
eqmod (c0032*x**  32+c0288*x** 288+c0544*x** 544+c0800*x** 800)
      (4*L0x2001b848*x** 32) [ 1043969, x**256 - 554923 ],
eqmod (c0033*x**  33+c0289*x** 289+c0545*x** 545+c0801*x** 801)
      (4*L0x2001b84c*x** 33) [ 1043969, x**256 - 554923 ],
eqmod (c0034*x**  34+c0290*x** 290+c0546*x** 546+c0802*x** 802)
      (4*L0x2001b850*x** 34) [ 1043969, x**256 - 554923 ],
eqmod (c0035*x**  35+c0291*x** 291+c0547*x** 547+c0803*x** 803)
      (4*L0x2001b854*x** 35) [ 1043969, x**256 - 554923 ],
eqmod (c0036*x**  36+c0292*x** 292+c0548*x** 548+c0804*x** 804)
      (4*L0x2001b858*x** 36) [ 1043969, x**256 - 554923 ],
eqmod (c0037*x**  37+c0293*x** 293+c0549*x** 549+c0805*x** 805)
      (4*L0x2001b85c*x** 37) [ 1043969, x**256 - 554923 ],
eqmod (c0038*x**  38+c0294*x** 294+c0550*x** 550+c0806*x** 806)
      (4*L0x2001b860*x** 38) [ 1043969, x**256 - 554923 ],
eqmod (c0039*x**  39+c0295*x** 295+c0551*x** 551+c0807*x** 807)
      (4*L0x2001b864*x** 39) [ 1043969, x**256 - 554923 ],
eqmod (c0040*x**  40+c0296*x** 296+c0552*x** 552+c0808*x** 808)
      (4*L0x2001b868*x** 40) [ 1043969, x**256 - 554923 ],
eqmod (c0041*x**  41+c0297*x** 297+c0553*x** 553+c0809*x** 809)
      (4*L0x2001b86c*x** 41) [ 1043969, x**256 - 554923 ],
eqmod (c0042*x**  42+c0298*x** 298+c0554*x** 554+c0810*x** 810)
      (4*L0x2001b870*x** 42) [ 1043969, x**256 - 554923 ],
eqmod (c0043*x**  43+c0299*x** 299+c0555*x** 555+c0811*x** 811)
      (4*L0x2001b874*x** 43) [ 1043969, x**256 - 554923 ],
eqmod (c0044*x**  44+c0300*x** 300+c0556*x** 556+c0812*x** 812)
      (4*L0x2001b878*x** 44) [ 1043969, x**256 - 554923 ],
eqmod (c0045*x**  45+c0301*x** 301+c0557*x** 557+c0813*x** 813)
      (4*L0x2001b87c*x** 45) [ 1043969, x**256 - 554923 ],
eqmod (c0046*x**  46+c0302*x** 302+c0558*x** 558+c0814*x** 814)
      (4*L0x2001b880*x** 46) [ 1043969, x**256 - 554923 ],
eqmod (c0047*x**  47+c0303*x** 303+c0559*x** 559+c0815*x** 815)
      (4*L0x2001b884*x** 47) [ 1043969, x**256 - 554923 ],
eqmod (c0048*x**  48+c0304*x** 304+c0560*x** 560+c0816*x** 816)
      (4*L0x2001b888*x** 48) [ 1043969, x**256 - 554923 ],
eqmod (c0049*x**  49+c0305*x** 305+c0561*x** 561+c0817*x** 817)
      (4*L0x2001b88c*x** 49) [ 1043969, x**256 - 554923 ],
eqmod (c0050*x**  50+c0306*x** 306+c0562*x** 562+c0818*x** 818)
      (4*L0x2001b890*x** 50) [ 1043969, x**256 - 554923 ],
eqmod (c0051*x**  51+c0307*x** 307+c0563*x** 563+c0819*x** 819)
      (4*L0x2001b894*x** 51) [ 1043969, x**256 - 554923 ],
eqmod (c0052*x**  52+c0308*x** 308+c0564*x** 564+c0820*x** 820)
      (4*L0x2001b898*x** 52) [ 1043969, x**256 - 554923 ],
eqmod (c0053*x**  53+c0309*x** 309+c0565*x** 565+c0821*x** 821)
      (4*L0x2001b89c*x** 53) [ 1043969, x**256 - 554923 ],
eqmod (c0054*x**  54+c0310*x** 310+c0566*x** 566+c0822*x** 822)
      (4*L0x2001b8a0*x** 54) [ 1043969, x**256 - 554923 ],
eqmod (c0055*x**  55+c0311*x** 311+c0567*x** 567+c0823*x** 823)
      (4*L0x2001b8a4*x** 55) [ 1043969, x**256 - 554923 ],
eqmod (c0056*x**  56+c0312*x** 312+c0568*x** 568+c0824*x** 824)
      (4*L0x2001b8a8*x** 56) [ 1043969, x**256 - 554923 ],
eqmod (c0057*x**  57+c0313*x** 313+c0569*x** 569+c0825*x** 825)
      (4*L0x2001b8ac*x** 57) [ 1043969, x**256 - 554923 ],
eqmod (c0058*x**  58+c0314*x** 314+c0570*x** 570+c0826*x** 826)
      (4*L0x2001b8b0*x** 58) [ 1043969, x**256 - 554923 ],
eqmod (c0059*x**  59+c0315*x** 315+c0571*x** 571+c0827*x** 827)
      (4*L0x2001b8b4*x** 59) [ 1043969, x**256 - 554923 ],
eqmod (c0060*x**  60+c0316*x** 316+c0572*x** 572+c0828*x** 828)
      (4*L0x2001b8b8*x** 60) [ 1043969, x**256 - 554923 ],
eqmod (c0061*x**  61+c0317*x** 317+c0573*x** 573+c0829*x** 829)
      (4*L0x2001b8bc*x** 61) [ 1043969, x**256 - 554923 ],
eqmod (c0062*x**  62+c0318*x** 318+c0574*x** 574+c0830*x** 830)
      (4*L0x2001b8c0*x** 62) [ 1043969, x**256 - 554923 ],
eqmod (c0063*x**  63+c0319*x** 319+c0575*x** 575+c0831*x** 831)
      (4*L0x2001b8c4*x** 63) [ 1043969, x**256 - 554923 ],
eqmod (c0064*x**  64+c0320*x** 320+c0576*x** 576+c0832*x** 832)
      (4*L0x2001b8c8*x** 64) [ 1043969, x**256 - 554923 ],
eqmod (c0065*x**  65+c0321*x** 321+c0577*x** 577+c0833*x** 833)
      (4*L0x2001b8cc*x** 65) [ 1043969, x**256 - 554923 ],
eqmod (c0066*x**  66+c0322*x** 322+c0578*x** 578+c0834*x** 834)
      (4*L0x2001b8d0*x** 66) [ 1043969, x**256 - 554923 ],
eqmod (c0067*x**  67+c0323*x** 323+c0579*x** 579+c0835*x** 835)
      (4*L0x2001b8d4*x** 67) [ 1043969, x**256 - 554923 ],
eqmod (c0068*x**  68+c0324*x** 324+c0580*x** 580+c0836*x** 836)
      (4*L0x2001b8d8*x** 68) [ 1043969, x**256 - 554923 ],
eqmod (c0069*x**  69+c0325*x** 325+c0581*x** 581+c0837*x** 837)
      (4*L0x2001b8dc*x** 69) [ 1043969, x**256 - 554923 ],
eqmod (c0070*x**  70+c0326*x** 326+c0582*x** 582+c0838*x** 838)
      (4*L0x2001b8e0*x** 70) [ 1043969, x**256 - 554923 ],
eqmod (c0071*x**  71+c0327*x** 327+c0583*x** 583+c0839*x** 839)
      (4*L0x2001b8e4*x** 71) [ 1043969, x**256 - 554923 ],
eqmod (c0072*x**  72+c0328*x** 328+c0584*x** 584+c0840*x** 840)
      (4*L0x2001b8e8*x** 72) [ 1043969, x**256 - 554923 ],
eqmod (c0073*x**  73+c0329*x** 329+c0585*x** 585+c0841*x** 841)
      (4*L0x2001b8ec*x** 73) [ 1043969, x**256 - 554923 ],
eqmod (c0074*x**  74+c0330*x** 330+c0586*x** 586+c0842*x** 842)
      (4*L0x2001b8f0*x** 74) [ 1043969, x**256 - 554923 ],
eqmod (c0075*x**  75+c0331*x** 331+c0587*x** 587+c0843*x** 843)
      (4*L0x2001b8f4*x** 75) [ 1043969, x**256 - 554923 ],
eqmod (c0076*x**  76+c0332*x** 332+c0588*x** 588+c0844*x** 844)
      (4*L0x2001b8f8*x** 76) [ 1043969, x**256 - 554923 ],
eqmod (c0077*x**  77+c0333*x** 333+c0589*x** 589+c0845*x** 845)
      (4*L0x2001b8fc*x** 77) [ 1043969, x**256 - 554923 ],
eqmod (c0078*x**  78+c0334*x** 334+c0590*x** 590+c0846*x** 846)
      (4*L0x2001b900*x** 78) [ 1043969, x**256 - 554923 ],
eqmod (c0079*x**  79+c0335*x** 335+c0591*x** 591+c0847*x** 847)
      (4*L0x2001b904*x** 79) [ 1043969, x**256 - 554923 ],
eqmod (c0080*x**  80+c0336*x** 336+c0592*x** 592+c0848*x** 848)
      (4*L0x2001b908*x** 80) [ 1043969, x**256 - 554923 ],
eqmod (c0081*x**  81+c0337*x** 337+c0593*x** 593+c0849*x** 849)
      (4*L0x2001b90c*x** 81) [ 1043969, x**256 - 554923 ],
eqmod (c0082*x**  82+c0338*x** 338+c0594*x** 594+c0850*x** 850)
      (4*L0x2001b910*x** 82) [ 1043969, x**256 - 554923 ],
eqmod (c0083*x**  83+c0339*x** 339+c0595*x** 595+c0851*x** 851)
      (4*L0x2001b914*x** 83) [ 1043969, x**256 - 554923 ],
eqmod (c0084*x**  84+c0340*x** 340+c0596*x** 596+c0852*x** 852)
      (4*L0x2001b918*x** 84) [ 1043969, x**256 - 554923 ],
eqmod (c0085*x**  85+c0341*x** 341+c0597*x** 597+c0853*x** 853)
      (4*L0x2001b91c*x** 85) [ 1043969, x**256 - 554923 ],
eqmod (c0086*x**  86+c0342*x** 342+c0598*x** 598+c0854*x** 854)
      (4*L0x2001b920*x** 86) [ 1043969, x**256 - 554923 ],
eqmod (c0087*x**  87+c0343*x** 343+c0599*x** 599+c0855*x** 855)
      (4*L0x2001b924*x** 87) [ 1043969, x**256 - 554923 ],
eqmod (c0088*x**  88+c0344*x** 344+c0600*x** 600+c0856*x** 856)
      (4*L0x2001b928*x** 88) [ 1043969, x**256 - 554923 ],
eqmod (c0089*x**  89+c0345*x** 345+c0601*x** 601+c0857*x** 857)
      (4*L0x2001b92c*x** 89) [ 1043969, x**256 - 554923 ],
eqmod (c0090*x**  90+c0346*x** 346+c0602*x** 602+c0858*x** 858)
      (4*L0x2001b930*x** 90) [ 1043969, x**256 - 554923 ],
eqmod (c0091*x**  91+c0347*x** 347+c0603*x** 603+c0859*x** 859)
      (4*L0x2001b934*x** 91) [ 1043969, x**256 - 554923 ],
eqmod (c0092*x**  92+c0348*x** 348+c0604*x** 604+c0860*x** 860)
      (4*L0x2001b938*x** 92) [ 1043969, x**256 - 554923 ],
eqmod (c0093*x**  93+c0349*x** 349+c0605*x** 605+c0861*x** 861)
      (4*L0x2001b93c*x** 93) [ 1043969, x**256 - 554923 ],
eqmod (c0094*x**  94+c0350*x** 350+c0606*x** 606+c0862*x** 862)
      (4*L0x2001b940*x** 94) [ 1043969, x**256 - 554923 ],
eqmod (c0095*x**  95+c0351*x** 351+c0607*x** 607+c0863*x** 863)
      (4*L0x2001b944*x** 95) [ 1043969, x**256 - 554923 ],
eqmod (c0096*x**  96+c0352*x** 352+c0608*x** 608+c0864*x** 864)
      (4*L0x2001b948*x** 96) [ 1043969, x**256 - 554923 ],
eqmod (c0097*x**  97+c0353*x** 353+c0609*x** 609+c0865*x** 865)
      (4*L0x2001b94c*x** 97) [ 1043969, x**256 - 554923 ],
eqmod (c0098*x**  98+c0354*x** 354+c0610*x** 610+c0866*x** 866)
      (4*L0x2001b950*x** 98) [ 1043969, x**256 - 554923 ],
eqmod (c0099*x**  99+c0355*x** 355+c0611*x** 611+c0867*x** 867)
      (4*L0x2001b954*x** 99) [ 1043969, x**256 - 554923 ],
eqmod (c0100*x** 100+c0356*x** 356+c0612*x** 612+c0868*x** 868)
      (4*L0x2001b958*x**100) [ 1043969, x**256 - 554923 ],
eqmod (c0101*x** 101+c0357*x** 357+c0613*x** 613+c0869*x** 869)
      (4*L0x2001b95c*x**101) [ 1043969, x**256 - 554923 ],
eqmod (c0102*x** 102+c0358*x** 358+c0614*x** 614+c0870*x** 870)
      (4*L0x2001b960*x**102) [ 1043969, x**256 - 554923 ],
eqmod (c0103*x** 103+c0359*x** 359+c0615*x** 615+c0871*x** 871)
      (4*L0x2001b964*x**103) [ 1043969, x**256 - 554923 ],
eqmod (c0104*x** 104+c0360*x** 360+c0616*x** 616+c0872*x** 872)
      (4*L0x2001b968*x**104) [ 1043969, x**256 - 554923 ],
eqmod (c0105*x** 105+c0361*x** 361+c0617*x** 617+c0873*x** 873)
      (4*L0x2001b96c*x**105) [ 1043969, x**256 - 554923 ],
eqmod (c0106*x** 106+c0362*x** 362+c0618*x** 618+c0874*x** 874)
      (4*L0x2001b970*x**106) [ 1043969, x**256 - 554923 ],
eqmod (c0107*x** 107+c0363*x** 363+c0619*x** 619+c0875*x** 875)
      (4*L0x2001b974*x**107) [ 1043969, x**256 - 554923 ],
eqmod (c0108*x** 108+c0364*x** 364+c0620*x** 620+c0876*x** 876)
      (4*L0x2001b978*x**108) [ 1043969, x**256 - 554923 ],
eqmod (c0109*x** 109+c0365*x** 365+c0621*x** 621+c0877*x** 877)
      (4*L0x2001b97c*x**109) [ 1043969, x**256 - 554923 ],
eqmod (c0110*x** 110+c0366*x** 366+c0622*x** 622+c0878*x** 878)
      (4*L0x2001b980*x**110) [ 1043969, x**256 - 554923 ],
eqmod (c0111*x** 111+c0367*x** 367+c0623*x** 623+c0879*x** 879)
      (4*L0x2001b984*x**111) [ 1043969, x**256 - 554923 ],
eqmod (c0112*x** 112+c0368*x** 368+c0624*x** 624+c0880*x** 880)
      (4*L0x2001b988*x**112) [ 1043969, x**256 - 554923 ],
eqmod (c0113*x** 113+c0369*x** 369+c0625*x** 625+c0881*x** 881)
      (4*L0x2001b98c*x**113) [ 1043969, x**256 - 554923 ],
eqmod (c0114*x** 114+c0370*x** 370+c0626*x** 626+c0882*x** 882)
      (4*L0x2001b990*x**114) [ 1043969, x**256 - 554923 ],
eqmod (c0115*x** 115+c0371*x** 371+c0627*x** 627+c0883*x** 883)
      (4*L0x2001b994*x**115) [ 1043969, x**256 - 554923 ],
eqmod (c0116*x** 116+c0372*x** 372+c0628*x** 628+c0884*x** 884)
      (4*L0x2001b998*x**116) [ 1043969, x**256 - 554923 ],
eqmod (c0117*x** 117+c0373*x** 373+c0629*x** 629+c0885*x** 885)
      (4*L0x2001b99c*x**117) [ 1043969, x**256 - 554923 ],
eqmod (c0118*x** 118+c0374*x** 374+c0630*x** 630+c0886*x** 886)
      (4*L0x2001b9a0*x**118) [ 1043969, x**256 - 554923 ],
eqmod (c0119*x** 119+c0375*x** 375+c0631*x** 631+c0887*x** 887)
      (4*L0x2001b9a4*x**119) [ 1043969, x**256 - 554923 ],
eqmod (c0120*x** 120+c0376*x** 376+c0632*x** 632+c0888*x** 888)
      (4*L0x2001b9a8*x**120) [ 1043969, x**256 - 554923 ],
eqmod (c0121*x** 121+c0377*x** 377+c0633*x** 633+c0889*x** 889)
      (4*L0x2001b9ac*x**121) [ 1043969, x**256 - 554923 ],
eqmod (c0122*x** 122+c0378*x** 378+c0634*x** 634+c0890*x** 890)
      (4*L0x2001b9b0*x**122) [ 1043969, x**256 - 554923 ],
eqmod (c0123*x** 123+c0379*x** 379+c0635*x** 635+c0891*x** 891)
      (4*L0x2001b9b4*x**123) [ 1043969, x**256 - 554923 ],
eqmod (c0124*x** 124+c0380*x** 380+c0636*x** 636+c0892*x** 892)
      (4*L0x2001b9b8*x**124) [ 1043969, x**256 - 554923 ],
eqmod (c0125*x** 125+c0381*x** 381+c0637*x** 637+c0893*x** 893)
      (4*L0x2001b9bc*x**125) [ 1043969, x**256 - 554923 ],
eqmod (c0126*x** 126+c0382*x** 382+c0638*x** 638+c0894*x** 894)
      (4*L0x2001b9c0*x**126) [ 1043969, x**256 - 554923 ],
eqmod (c0127*x** 127+c0383*x** 383+c0639*x** 639+c0895*x** 895)
      (4*L0x2001b9c4*x**127) [ 1043969, x**256 - 554923 ],
eqmod (c0128*x** 128+c0384*x** 384+c0640*x** 640+c0896*x** 896)
      (4*L0x2001b9c8*x**128) [ 1043969, x**256 - 554923 ],
eqmod (c0129*x** 129+c0385*x** 385+c0641*x** 641+c0897*x** 897)
      (4*L0x2001b9cc*x**129) [ 1043969, x**256 - 554923 ],
eqmod (c0130*x** 130+c0386*x** 386+c0642*x** 642+c0898*x** 898)
      (4*L0x2001b9d0*x**130) [ 1043969, x**256 - 554923 ],
eqmod (c0131*x** 131+c0387*x** 387+c0643*x** 643+c0899*x** 899)
      (4*L0x2001b9d4*x**131) [ 1043969, x**256 - 554923 ],
eqmod (c0132*x** 132+c0388*x** 388+c0644*x** 644+c0900*x** 900)
      (4*L0x2001b9d8*x**132) [ 1043969, x**256 - 554923 ],
eqmod (c0133*x** 133+c0389*x** 389+c0645*x** 645+c0901*x** 901)
      (4*L0x2001b9dc*x**133) [ 1043969, x**256 - 554923 ],
eqmod (c0134*x** 134+c0390*x** 390+c0646*x** 646+c0902*x** 902)
      (4*L0x2001b9e0*x**134) [ 1043969, x**256 - 554923 ],
eqmod (c0135*x** 135+c0391*x** 391+c0647*x** 647+c0903*x** 903)
      (4*L0x2001b9e4*x**135) [ 1043969, x**256 - 554923 ],
eqmod (c0136*x** 136+c0392*x** 392+c0648*x** 648+c0904*x** 904)
      (4*L0x2001b9e8*x**136) [ 1043969, x**256 - 554923 ],
eqmod (c0137*x** 137+c0393*x** 393+c0649*x** 649+c0905*x** 905)
      (4*L0x2001b9ec*x**137) [ 1043969, x**256 - 554923 ],
eqmod (c0138*x** 138+c0394*x** 394+c0650*x** 650+c0906*x** 906)
      (4*L0x2001b9f0*x**138) [ 1043969, x**256 - 554923 ],
eqmod (c0139*x** 139+c0395*x** 395+c0651*x** 651+c0907*x** 907)
      (4*L0x2001b9f4*x**139) [ 1043969, x**256 - 554923 ],
eqmod (c0140*x** 140+c0396*x** 396+c0652*x** 652+c0908*x** 908)
      (4*L0x2001b9f8*x**140) [ 1043969, x**256 - 554923 ],
eqmod (c0141*x** 141+c0397*x** 397+c0653*x** 653+c0909*x** 909)
      (4*L0x2001b9fc*x**141) [ 1043969, x**256 - 554923 ],
eqmod (c0142*x** 142+c0398*x** 398+c0654*x** 654+c0910*x** 910)
      (4*L0x2001ba00*x**142) [ 1043969, x**256 - 554923 ],
eqmod (c0143*x** 143+c0399*x** 399+c0655*x** 655+c0911*x** 911)
      (4*L0x2001ba04*x**143) [ 1043969, x**256 - 554923 ],
eqmod (c0144*x** 144+c0400*x** 400+c0656*x** 656+c0912*x** 912)
      (4*L0x2001ba08*x**144) [ 1043969, x**256 - 554923 ],
eqmod (c0145*x** 145+c0401*x** 401+c0657*x** 657+c0913*x** 913)
      (4*L0x2001ba0c*x**145) [ 1043969, x**256 - 554923 ],
eqmod (c0146*x** 146+c0402*x** 402+c0658*x** 658+c0914*x** 914)
      (4*L0x2001ba10*x**146) [ 1043969, x**256 - 554923 ],
eqmod (c0147*x** 147+c0403*x** 403+c0659*x** 659+c0915*x** 915)
      (4*L0x2001ba14*x**147) [ 1043969, x**256 - 554923 ],
eqmod (c0148*x** 148+c0404*x** 404+c0660*x** 660+c0916*x** 916)
      (4*L0x2001ba18*x**148) [ 1043969, x**256 - 554923 ],
eqmod (c0149*x** 149+c0405*x** 405+c0661*x** 661+c0917*x** 917)
      (4*L0x2001ba1c*x**149) [ 1043969, x**256 - 554923 ],
eqmod (c0150*x** 150+c0406*x** 406+c0662*x** 662+c0918*x** 918)
      (4*L0x2001ba20*x**150) [ 1043969, x**256 - 554923 ],
eqmod (c0151*x** 151+c0407*x** 407+c0663*x** 663+c0919*x** 919)
      (4*L0x2001ba24*x**151) [ 1043969, x**256 - 554923 ],
eqmod (c0152*x** 152+c0408*x** 408+c0664*x** 664+c0920*x** 920)
      (4*L0x2001ba28*x**152) [ 1043969, x**256 - 554923 ],
eqmod (c0153*x** 153+c0409*x** 409+c0665*x** 665+c0921*x** 921)
      (4*L0x2001ba2c*x**153) [ 1043969, x**256 - 554923 ],
eqmod (c0154*x** 154+c0410*x** 410+c0666*x** 666+c0922*x** 922)
      (4*L0x2001ba30*x**154) [ 1043969, x**256 - 554923 ],
eqmod (c0155*x** 155+c0411*x** 411+c0667*x** 667+c0923*x** 923)
      (4*L0x2001ba34*x**155) [ 1043969, x**256 - 554923 ],
eqmod (c0156*x** 156+c0412*x** 412+c0668*x** 668+c0924*x** 924)
      (4*L0x2001ba38*x**156) [ 1043969, x**256 - 554923 ],
eqmod (c0157*x** 157+c0413*x** 413+c0669*x** 669+c0925*x** 925)
      (4*L0x2001ba3c*x**157) [ 1043969, x**256 - 554923 ],
eqmod (c0158*x** 158+c0414*x** 414+c0670*x** 670+c0926*x** 926)
      (4*L0x2001ba40*x**158) [ 1043969, x**256 - 554923 ],
eqmod (c0159*x** 159+c0415*x** 415+c0671*x** 671+c0927*x** 927)
      (4*L0x2001ba44*x**159) [ 1043969, x**256 - 554923 ],
eqmod (c0160*x** 160+c0416*x** 416+c0672*x** 672+c0928*x** 928)
      (4*L0x2001ba48*x**160) [ 1043969, x**256 - 554923 ],
eqmod (c0161*x** 161+c0417*x** 417+c0673*x** 673+c0929*x** 929)
      (4*L0x2001ba4c*x**161) [ 1043969, x**256 - 554923 ],
eqmod (c0162*x** 162+c0418*x** 418+c0674*x** 674+c0930*x** 930)
      (4*L0x2001ba50*x**162) [ 1043969, x**256 - 554923 ],
eqmod (c0163*x** 163+c0419*x** 419+c0675*x** 675+c0931*x** 931)
      (4*L0x2001ba54*x**163) [ 1043969, x**256 - 554923 ],
eqmod (c0164*x** 164+c0420*x** 420+c0676*x** 676+c0932*x** 932)
      (4*L0x2001ba58*x**164) [ 1043969, x**256 - 554923 ],
eqmod (c0165*x** 165+c0421*x** 421+c0677*x** 677+c0933*x** 933)
      (4*L0x2001ba5c*x**165) [ 1043969, x**256 - 554923 ],
eqmod (c0166*x** 166+c0422*x** 422+c0678*x** 678+c0934*x** 934)
      (4*L0x2001ba60*x**166) [ 1043969, x**256 - 554923 ],
eqmod (c0167*x** 167+c0423*x** 423+c0679*x** 679+c0935*x** 935)
      (4*L0x2001ba64*x**167) [ 1043969, x**256 - 554923 ],
eqmod (c0168*x** 168+c0424*x** 424+c0680*x** 680+c0936*x** 936)
      (4*L0x2001ba68*x**168) [ 1043969, x**256 - 554923 ],
eqmod (c0169*x** 169+c0425*x** 425+c0681*x** 681+c0937*x** 937)
      (4*L0x2001ba6c*x**169) [ 1043969, x**256 - 554923 ],
eqmod (c0170*x** 170+c0426*x** 426+c0682*x** 682+c0938*x** 938)
      (4*L0x2001ba70*x**170) [ 1043969, x**256 - 554923 ],
eqmod (c0171*x** 171+c0427*x** 427+c0683*x** 683+c0939*x** 939)
      (4*L0x2001ba74*x**171) [ 1043969, x**256 - 554923 ],
eqmod (c0172*x** 172+c0428*x** 428+c0684*x** 684+c0940*x** 940)
      (4*L0x2001ba78*x**172) [ 1043969, x**256 - 554923 ],
eqmod (c0173*x** 173+c0429*x** 429+c0685*x** 685+c0941*x** 941)
      (4*L0x2001ba7c*x**173) [ 1043969, x**256 - 554923 ],
eqmod (c0174*x** 174+c0430*x** 430+c0686*x** 686+c0942*x** 942)
      (4*L0x2001ba80*x**174) [ 1043969, x**256 - 554923 ],
eqmod (c0175*x** 175+c0431*x** 431+c0687*x** 687+c0943*x** 943)
      (4*L0x2001ba84*x**175) [ 1043969, x**256 - 554923 ],
eqmod (c0176*x** 176+c0432*x** 432+c0688*x** 688+c0944*x** 944)
      (4*L0x2001ba88*x**176) [ 1043969, x**256 - 554923 ],
eqmod (c0177*x** 177+c0433*x** 433+c0689*x** 689+c0945*x** 945)
      (4*L0x2001ba8c*x**177) [ 1043969, x**256 - 554923 ],
eqmod (c0178*x** 178+c0434*x** 434+c0690*x** 690+c0946*x** 946)
      (4*L0x2001ba90*x**178) [ 1043969, x**256 - 554923 ],
eqmod (c0179*x** 179+c0435*x** 435+c0691*x** 691+c0947*x** 947)
      (4*L0x2001ba94*x**179) [ 1043969, x**256 - 554923 ],
eqmod (c0180*x** 180+c0436*x** 436+c0692*x** 692+c0948*x** 948)
      (4*L0x2001ba98*x**180) [ 1043969, x**256 - 554923 ],
eqmod (c0181*x** 181+c0437*x** 437+c0693*x** 693+c0949*x** 949)
      (4*L0x2001ba9c*x**181) [ 1043969, x**256 - 554923 ],
eqmod (c0182*x** 182+c0438*x** 438+c0694*x** 694+c0950*x** 950)
      (4*L0x2001baa0*x**182) [ 1043969, x**256 - 554923 ],
eqmod (c0183*x** 183+c0439*x** 439+c0695*x** 695+c0951*x** 951)
      (4*L0x2001baa4*x**183) [ 1043969, x**256 - 554923 ],
eqmod (c0184*x** 184+c0440*x** 440+c0696*x** 696+c0952*x** 952)
      (4*L0x2001baa8*x**184) [ 1043969, x**256 - 554923 ],
eqmod (c0185*x** 185+c0441*x** 441+c0697*x** 697+c0953*x** 953)
      (4*L0x2001baac*x**185) [ 1043969, x**256 - 554923 ],
eqmod (c0186*x** 186+c0442*x** 442+c0698*x** 698+c0954*x** 954)
      (4*L0x2001bab0*x**186) [ 1043969, x**256 - 554923 ],
eqmod (c0187*x** 187+c0443*x** 443+c0699*x** 699+c0955*x** 955)
      (4*L0x2001bab4*x**187) [ 1043969, x**256 - 554923 ],
eqmod (c0188*x** 188+c0444*x** 444+c0700*x** 700+c0956*x** 956)
      (4*L0x2001bab8*x**188) [ 1043969, x**256 - 554923 ],
eqmod (c0189*x** 189+c0445*x** 445+c0701*x** 701+c0957*x** 957)
      (4*L0x2001babc*x**189) [ 1043969, x**256 - 554923 ],
eqmod (c0190*x** 190+c0446*x** 446+c0702*x** 702+c0958*x** 958)
      (4*L0x2001bac0*x**190) [ 1043969, x**256 - 554923 ],
eqmod (c0191*x** 191+c0447*x** 447+c0703*x** 703+c0959*x** 959)
      (4*L0x2001bac4*x**191) [ 1043969, x**256 - 554923 ],
eqmod (c0192*x** 192+c0448*x** 448+c0704*x** 704+c0960*x** 960)
      (4*L0x2001bac8*x**192) [ 1043969, x**256 - 554923 ],
eqmod (c0193*x** 193+c0449*x** 449+c0705*x** 705+c0961*x** 961)
      (4*L0x2001bacc*x**193) [ 1043969, x**256 - 554923 ],
eqmod (c0194*x** 194+c0450*x** 450+c0706*x** 706+c0962*x** 962)
      (4*L0x2001bad0*x**194) [ 1043969, x**256 - 554923 ],
eqmod (c0195*x** 195+c0451*x** 451+c0707*x** 707+c0963*x** 963)
      (4*L0x2001bad4*x**195) [ 1043969, x**256 - 554923 ],
eqmod (c0196*x** 196+c0452*x** 452+c0708*x** 708+c0964*x** 964)
      (4*L0x2001bad8*x**196) [ 1043969, x**256 - 554923 ],
eqmod (c0197*x** 197+c0453*x** 453+c0709*x** 709+c0965*x** 965)
      (4*L0x2001badc*x**197) [ 1043969, x**256 - 554923 ],
eqmod (c0198*x** 198+c0454*x** 454+c0710*x** 710+c0966*x** 966)
      (4*L0x2001bae0*x**198) [ 1043969, x**256 - 554923 ],
eqmod (c0199*x** 199+c0455*x** 455+c0711*x** 711+c0967*x** 967)
      (4*L0x2001bae4*x**199) [ 1043969, x**256 - 554923 ],
eqmod (c0200*x** 200+c0456*x** 456+c0712*x** 712+c0968*x** 968)
      (4*L0x2001bae8*x**200) [ 1043969, x**256 - 554923 ],
eqmod (c0201*x** 201+c0457*x** 457+c0713*x** 713+c0969*x** 969)
      (4*L0x2001baec*x**201) [ 1043969, x**256 - 554923 ],
eqmod (c0202*x** 202+c0458*x** 458+c0714*x** 714+c0970*x** 970)
      (4*L0x2001baf0*x**202) [ 1043969, x**256 - 554923 ],
eqmod (c0203*x** 203+c0459*x** 459+c0715*x** 715+c0971*x** 971)
      (4*L0x2001baf4*x**203) [ 1043969, x**256 - 554923 ],
eqmod (c0204*x** 204+c0460*x** 460+c0716*x** 716+c0972*x** 972)
      (4*L0x2001baf8*x**204) [ 1043969, x**256 - 554923 ],
eqmod (c0205*x** 205+c0461*x** 461+c0717*x** 717+c0973*x** 973)
      (4*L0x2001bafc*x**205) [ 1043969, x**256 - 554923 ],
eqmod (c0206*x** 206+c0462*x** 462+c0718*x** 718+c0974*x** 974)
      (4*L0x2001bb00*x**206) [ 1043969, x**256 - 554923 ],
eqmod (c0207*x** 207+c0463*x** 463+c0719*x** 719+c0975*x** 975)
      (4*L0x2001bb04*x**207) [ 1043969, x**256 - 554923 ],
eqmod (c0208*x** 208+c0464*x** 464+c0720*x** 720+c0976*x** 976)
      (4*L0x2001bb08*x**208) [ 1043969, x**256 - 554923 ],
eqmod (c0209*x** 209+c0465*x** 465+c0721*x** 721+c0977*x** 977)
      (4*L0x2001bb0c*x**209) [ 1043969, x**256 - 554923 ],
eqmod (c0210*x** 210+c0466*x** 466+c0722*x** 722+c0978*x** 978)
      (4*L0x2001bb10*x**210) [ 1043969, x**256 - 554923 ],
eqmod (c0211*x** 211+c0467*x** 467+c0723*x** 723+c0979*x** 979)
      (4*L0x2001bb14*x**211) [ 1043969, x**256 - 554923 ],
eqmod (c0212*x** 212+c0468*x** 468+c0724*x** 724+c0980*x** 980)
      (4*L0x2001bb18*x**212) [ 1043969, x**256 - 554923 ],
eqmod (c0213*x** 213+c0469*x** 469+c0725*x** 725+c0981*x** 981)
      (4*L0x2001bb1c*x**213) [ 1043969, x**256 - 554923 ],
eqmod (c0214*x** 214+c0470*x** 470+c0726*x** 726+c0982*x** 982)
      (4*L0x2001bb20*x**214) [ 1043969, x**256 - 554923 ],
eqmod (c0215*x** 215+c0471*x** 471+c0727*x** 727+c0983*x** 983)
      (4*L0x2001bb24*x**215) [ 1043969, x**256 - 554923 ],
eqmod (c0216*x** 216+c0472*x** 472+c0728*x** 728+c0984*x** 984)
      (4*L0x2001bb28*x**216) [ 1043969, x**256 - 554923 ],
eqmod (c0217*x** 217+c0473*x** 473+c0729*x** 729+c0985*x** 985)
      (4*L0x2001bb2c*x**217) [ 1043969, x**256 - 554923 ],
eqmod (c0218*x** 218+c0474*x** 474+c0730*x** 730+c0986*x** 986)
      (4*L0x2001bb30*x**218) [ 1043969, x**256 - 554923 ],
eqmod (c0219*x** 219+c0475*x** 475+c0731*x** 731+c0987*x** 987)
      (4*L0x2001bb34*x**219) [ 1043969, x**256 - 554923 ],
eqmod (c0220*x** 220+c0476*x** 476+c0732*x** 732+c0988*x** 988)
      (4*L0x2001bb38*x**220) [ 1043969, x**256 - 554923 ],
eqmod (c0221*x** 221+c0477*x** 477+c0733*x** 733+c0989*x** 989)
      (4*L0x2001bb3c*x**221) [ 1043969, x**256 - 554923 ],
eqmod (c0222*x** 222+c0478*x** 478+c0734*x** 734+c0990*x** 990)
      (4*L0x2001bb40*x**222) [ 1043969, x**256 - 554923 ],
eqmod (c0223*x** 223+c0479*x** 479+c0735*x** 735+c0991*x** 991)
      (4*L0x2001bb44*x**223) [ 1043969, x**256 - 554923 ],
eqmod (c0224*x** 224+c0480*x** 480+c0736*x** 736+c0992*x** 992)
      (4*L0x2001bb48*x**224) [ 1043969, x**256 - 554923 ],
eqmod (c0225*x** 225+c0481*x** 481+c0737*x** 737+c0993*x** 993)
      (4*L0x2001bb4c*x**225) [ 1043969, x**256 - 554923 ],
eqmod (c0226*x** 226+c0482*x** 482+c0738*x** 738+c0994*x** 994)
      (4*L0x2001bb50*x**226) [ 1043969, x**256 - 554923 ],
eqmod (c0227*x** 227+c0483*x** 483+c0739*x** 739+c0995*x** 995)
      (4*L0x2001bb54*x**227) [ 1043969, x**256 - 554923 ],
eqmod (c0228*x** 228+c0484*x** 484+c0740*x** 740+c0996*x** 996)
      (4*L0x2001bb58*x**228) [ 1043969, x**256 - 554923 ],
eqmod (c0229*x** 229+c0485*x** 485+c0741*x** 741+c0997*x** 997)
      (4*L0x2001bb5c*x**229) [ 1043969, x**256 - 554923 ],
eqmod (c0230*x** 230+c0486*x** 486+c0742*x** 742+c0998*x** 998)
      (4*L0x2001bb60*x**230) [ 1043969, x**256 - 554923 ],
eqmod (c0231*x** 231+c0487*x** 487+c0743*x** 743+c0999*x** 999)
      (4*L0x2001bb64*x**231) [ 1043969, x**256 - 554923 ],
eqmod (c0232*x** 232+c0488*x** 488+c0744*x** 744+c1000*x**1000)
      (4*L0x2001bb68*x**232) [ 1043969, x**256 - 554923 ],
eqmod (c0233*x** 233+c0489*x** 489+c0745*x** 745+c1001*x**1001)
      (4*L0x2001bb6c*x**233) [ 1043969, x**256 - 554923 ],
eqmod (c0234*x** 234+c0490*x** 490+c0746*x** 746+c1002*x**1002)
      (4*L0x2001bb70*x**234) [ 1043969, x**256 - 554923 ],
eqmod (c0235*x** 235+c0491*x** 491+c0747*x** 747+c1003*x**1003)
      (4*L0x2001bb74*x**235) [ 1043969, x**256 - 554923 ],
eqmod (c0236*x** 236+c0492*x** 492+c0748*x** 748+c1004*x**1004)
      (4*L0x2001bb78*x**236) [ 1043969, x**256 - 554923 ],
eqmod (c0237*x** 237+c0493*x** 493+c0749*x** 749+c1005*x**1005)
      (4*L0x2001bb7c*x**237) [ 1043969, x**256 - 554923 ],
eqmod (c0238*x** 238+c0494*x** 494+c0750*x** 750+c1006*x**1006)
      (4*L0x2001bb80*x**238) [ 1043969, x**256 - 554923 ],
eqmod (c0239*x** 239+c0495*x** 495+c0751*x** 751+c1007*x**1007)
      (4*L0x2001bb84*x**239) [ 1043969, x**256 - 554923 ],
eqmod (c0240*x** 240+c0496*x** 496+c0752*x** 752+c1008*x**1008)
      (4*L0x2001bb88*x**240) [ 1043969, x**256 - 554923 ],
eqmod (c0241*x** 241+c0497*x** 497+c0753*x** 753+c1009*x**1009)
      (4*L0x2001bb8c*x**241) [ 1043969, x**256 - 554923 ],
eqmod (c0242*x** 242+c0498*x** 498+c0754*x** 754+c1010*x**1010)
      (4*L0x2001bb90*x**242) [ 1043969, x**256 - 554923 ],
eqmod (c0243*x** 243+c0499*x** 499+c0755*x** 755+c1011*x**1011)
      (4*L0x2001bb94*x**243) [ 1043969, x**256 - 554923 ],
eqmod (c0244*x** 244+c0500*x** 500+c0756*x** 756+c1012*x**1012)
      (4*L0x2001bb98*x**244) [ 1043969, x**256 - 554923 ],
eqmod (c0245*x** 245+c0501*x** 501+c0757*x** 757+c1013*x**1013)
      (4*L0x2001bb9c*x**245) [ 1043969, x**256 - 554923 ],
eqmod (c0246*x** 246+c0502*x** 502+c0758*x** 758+c1014*x**1014)
      (4*L0x2001bba0*x**246) [ 1043969, x**256 - 554923 ],
eqmod (c0247*x** 247+c0503*x** 503+c0759*x** 759+c1015*x**1015)
      (4*L0x2001bba4*x**247) [ 1043969, x**256 - 554923 ],
eqmod (c0248*x** 248+c0504*x** 504+c0760*x** 760+c1016*x**1016)
      (4*L0x2001bba8*x**248) [ 1043969, x**256 - 554923 ],
eqmod (c0249*x** 249+c0505*x** 505+c0761*x** 761+c1017*x**1017)
      (4*L0x2001bbac*x**249) [ 1043969, x**256 - 554923 ],
eqmod (c0250*x** 250+c0506*x** 506+c0762*x** 762+c1018*x**1018)
      (4*L0x2001bbb0*x**250) [ 1043969, x**256 - 554923 ],
eqmod (c0251*x** 251+c0507*x** 507+c0763*x** 763+c1019*x**1019)
      (4*L0x2001bbb4*x**251) [ 1043969, x**256 - 554923 ],
eqmod (c0252*x** 252+c0508*x** 508+c0764*x** 764+c1020*x**1020)
      (4*L0x2001bbb8*x**252) [ 1043969, x**256 - 554923 ],
eqmod (c0253*x** 253+c0509*x** 509+c0765*x** 765+c1021*x**1021)
      (4*L0x2001bbbc*x**253) [ 1043969, x**256 - 554923 ],
eqmod (c0254*x** 254+c0510*x** 510+c0766*x** 766+c1022*x**1022)
      (4*L0x2001bbc0*x**254) [ 1043969, x**256 - 554923 ],
eqmod (c0255*x** 255+c0511*x** 511+c0767*x** 767+c1023*x**1023)
      (4*L0x2001bbc4*x**255) [ 1043969, x**256 - 554923 ]
] prove with [ cuts [ 
  9,  10,  11,  12,  14,  16,  18,  20,  22,  24,  26,  28,
 30,  32,  34,  36,  38,  40,  42,  44,  46,  48,  50,  52,
 54,  56,  58,  60,  62,  64,  66,  68,  70,  72,  74,  76,
 78,  80,  82,  84,  86,  88,  90,  92,  94,  96,  98, 100,
102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124,
126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148,
150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172,
174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196,
198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220,
222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244,
246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268,
270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292,
294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316,
318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340,
342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364,
366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388,
390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412,
414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436,
438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460,
462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484,
486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508,
510, 512, 514, 516 ] ];



(******************** CUT 522 ********************)


ecut and [
eqmod (c0000*x**   0+c0256*x** 256+c0512*x** 512+c0768*x** 768)
      (4*L0x2001bbc8*x**  0) [ 1043969, x**256 + 554923 ],
eqmod (c0001*x**   1+c0257*x** 257+c0513*x** 513+c0769*x** 769)
      (4*L0x2001bbcc*x**  1) [ 1043969, x**256 + 554923 ],
eqmod (c0002*x**   2+c0258*x** 258+c0514*x** 514+c0770*x** 770)
      (4*L0x2001bbd0*x**  2) [ 1043969, x**256 + 554923 ],
eqmod (c0003*x**   3+c0259*x** 259+c0515*x** 515+c0771*x** 771)
      (4*L0x2001bbd4*x**  3) [ 1043969, x**256 + 554923 ],
eqmod (c0004*x**   4+c0260*x** 260+c0516*x** 516+c0772*x** 772)
      (4*L0x2001bbd8*x**  4) [ 1043969, x**256 + 554923 ],
eqmod (c0005*x**   5+c0261*x** 261+c0517*x** 517+c0773*x** 773)
      (4*L0x2001bbdc*x**  5) [ 1043969, x**256 + 554923 ],
eqmod (c0006*x**   6+c0262*x** 262+c0518*x** 518+c0774*x** 774)
      (4*L0x2001bbe0*x**  6) [ 1043969, x**256 + 554923 ],
eqmod (c0007*x**   7+c0263*x** 263+c0519*x** 519+c0775*x** 775)
      (4*L0x2001bbe4*x**  7) [ 1043969, x**256 + 554923 ],
eqmod (c0008*x**   8+c0264*x** 264+c0520*x** 520+c0776*x** 776)
      (4*L0x2001bbe8*x**  8) [ 1043969, x**256 + 554923 ],
eqmod (c0009*x**   9+c0265*x** 265+c0521*x** 521+c0777*x** 777)
      (4*L0x2001bbec*x**  9) [ 1043969, x**256 + 554923 ],
eqmod (c0010*x**  10+c0266*x** 266+c0522*x** 522+c0778*x** 778)
      (4*L0x2001bbf0*x** 10) [ 1043969, x**256 + 554923 ],
eqmod (c0011*x**  11+c0267*x** 267+c0523*x** 523+c0779*x** 779)
      (4*L0x2001bbf4*x** 11) [ 1043969, x**256 + 554923 ],
eqmod (c0012*x**  12+c0268*x** 268+c0524*x** 524+c0780*x** 780)
      (4*L0x2001bbf8*x** 12) [ 1043969, x**256 + 554923 ],
eqmod (c0013*x**  13+c0269*x** 269+c0525*x** 525+c0781*x** 781)
      (4*L0x2001bbfc*x** 13) [ 1043969, x**256 + 554923 ],
eqmod (c0014*x**  14+c0270*x** 270+c0526*x** 526+c0782*x** 782)
      (4*L0x2001bc00*x** 14) [ 1043969, x**256 + 554923 ],
eqmod (c0015*x**  15+c0271*x** 271+c0527*x** 527+c0783*x** 783)
      (4*L0x2001bc04*x** 15) [ 1043969, x**256 + 554923 ],
eqmod (c0016*x**  16+c0272*x** 272+c0528*x** 528+c0784*x** 784)
      (4*L0x2001bc08*x** 16) [ 1043969, x**256 + 554923 ],
eqmod (c0017*x**  17+c0273*x** 273+c0529*x** 529+c0785*x** 785)
      (4*L0x2001bc0c*x** 17) [ 1043969, x**256 + 554923 ],
eqmod (c0018*x**  18+c0274*x** 274+c0530*x** 530+c0786*x** 786)
      (4*L0x2001bc10*x** 18) [ 1043969, x**256 + 554923 ],
eqmod (c0019*x**  19+c0275*x** 275+c0531*x** 531+c0787*x** 787)
      (4*L0x2001bc14*x** 19) [ 1043969, x**256 + 554923 ],
eqmod (c0020*x**  20+c0276*x** 276+c0532*x** 532+c0788*x** 788)
      (4*L0x2001bc18*x** 20) [ 1043969, x**256 + 554923 ],
eqmod (c0021*x**  21+c0277*x** 277+c0533*x** 533+c0789*x** 789)
      (4*L0x2001bc1c*x** 21) [ 1043969, x**256 + 554923 ],
eqmod (c0022*x**  22+c0278*x** 278+c0534*x** 534+c0790*x** 790)
      (4*L0x2001bc20*x** 22) [ 1043969, x**256 + 554923 ],
eqmod (c0023*x**  23+c0279*x** 279+c0535*x** 535+c0791*x** 791)
      (4*L0x2001bc24*x** 23) [ 1043969, x**256 + 554923 ],
eqmod (c0024*x**  24+c0280*x** 280+c0536*x** 536+c0792*x** 792)
      (4*L0x2001bc28*x** 24) [ 1043969, x**256 + 554923 ],
eqmod (c0025*x**  25+c0281*x** 281+c0537*x** 537+c0793*x** 793)
      (4*L0x2001bc2c*x** 25) [ 1043969, x**256 + 554923 ],
eqmod (c0026*x**  26+c0282*x** 282+c0538*x** 538+c0794*x** 794)
      (4*L0x2001bc30*x** 26) [ 1043969, x**256 + 554923 ],
eqmod (c0027*x**  27+c0283*x** 283+c0539*x** 539+c0795*x** 795)
      (4*L0x2001bc34*x** 27) [ 1043969, x**256 + 554923 ],
eqmod (c0028*x**  28+c0284*x** 284+c0540*x** 540+c0796*x** 796)
      (4*L0x2001bc38*x** 28) [ 1043969, x**256 + 554923 ],
eqmod (c0029*x**  29+c0285*x** 285+c0541*x** 541+c0797*x** 797)
      (4*L0x2001bc3c*x** 29) [ 1043969, x**256 + 554923 ],
eqmod (c0030*x**  30+c0286*x** 286+c0542*x** 542+c0798*x** 798)
      (4*L0x2001bc40*x** 30) [ 1043969, x**256 + 554923 ],
eqmod (c0031*x**  31+c0287*x** 287+c0543*x** 543+c0799*x** 799)
      (4*L0x2001bc44*x** 31) [ 1043969, x**256 + 554923 ],
eqmod (c0032*x**  32+c0288*x** 288+c0544*x** 544+c0800*x** 800)
      (4*L0x2001bc48*x** 32) [ 1043969, x**256 + 554923 ],
eqmod (c0033*x**  33+c0289*x** 289+c0545*x** 545+c0801*x** 801)
      (4*L0x2001bc4c*x** 33) [ 1043969, x**256 + 554923 ],
eqmod (c0034*x**  34+c0290*x** 290+c0546*x** 546+c0802*x** 802)
      (4*L0x2001bc50*x** 34) [ 1043969, x**256 + 554923 ],
eqmod (c0035*x**  35+c0291*x** 291+c0547*x** 547+c0803*x** 803)
      (4*L0x2001bc54*x** 35) [ 1043969, x**256 + 554923 ],
eqmod (c0036*x**  36+c0292*x** 292+c0548*x** 548+c0804*x** 804)
      (4*L0x2001bc58*x** 36) [ 1043969, x**256 + 554923 ],
eqmod (c0037*x**  37+c0293*x** 293+c0549*x** 549+c0805*x** 805)
      (4*L0x2001bc5c*x** 37) [ 1043969, x**256 + 554923 ],
eqmod (c0038*x**  38+c0294*x** 294+c0550*x** 550+c0806*x** 806)
      (4*L0x2001bc60*x** 38) [ 1043969, x**256 + 554923 ],
eqmod (c0039*x**  39+c0295*x** 295+c0551*x** 551+c0807*x** 807)
      (4*L0x2001bc64*x** 39) [ 1043969, x**256 + 554923 ],
eqmod (c0040*x**  40+c0296*x** 296+c0552*x** 552+c0808*x** 808)
      (4*L0x2001bc68*x** 40) [ 1043969, x**256 + 554923 ],
eqmod (c0041*x**  41+c0297*x** 297+c0553*x** 553+c0809*x** 809)
      (4*L0x2001bc6c*x** 41) [ 1043969, x**256 + 554923 ],
eqmod (c0042*x**  42+c0298*x** 298+c0554*x** 554+c0810*x** 810)
      (4*L0x2001bc70*x** 42) [ 1043969, x**256 + 554923 ],
eqmod (c0043*x**  43+c0299*x** 299+c0555*x** 555+c0811*x** 811)
      (4*L0x2001bc74*x** 43) [ 1043969, x**256 + 554923 ],
eqmod (c0044*x**  44+c0300*x** 300+c0556*x** 556+c0812*x** 812)
      (4*L0x2001bc78*x** 44) [ 1043969, x**256 + 554923 ],
eqmod (c0045*x**  45+c0301*x** 301+c0557*x** 557+c0813*x** 813)
      (4*L0x2001bc7c*x** 45) [ 1043969, x**256 + 554923 ],
eqmod (c0046*x**  46+c0302*x** 302+c0558*x** 558+c0814*x** 814)
      (4*L0x2001bc80*x** 46) [ 1043969, x**256 + 554923 ],
eqmod (c0047*x**  47+c0303*x** 303+c0559*x** 559+c0815*x** 815)
      (4*L0x2001bc84*x** 47) [ 1043969, x**256 + 554923 ],
eqmod (c0048*x**  48+c0304*x** 304+c0560*x** 560+c0816*x** 816)
      (4*L0x2001bc88*x** 48) [ 1043969, x**256 + 554923 ],
eqmod (c0049*x**  49+c0305*x** 305+c0561*x** 561+c0817*x** 817)
      (4*L0x2001bc8c*x** 49) [ 1043969, x**256 + 554923 ],
eqmod (c0050*x**  50+c0306*x** 306+c0562*x** 562+c0818*x** 818)
      (4*L0x2001bc90*x** 50) [ 1043969, x**256 + 554923 ],
eqmod (c0051*x**  51+c0307*x** 307+c0563*x** 563+c0819*x** 819)
      (4*L0x2001bc94*x** 51) [ 1043969, x**256 + 554923 ],
eqmod (c0052*x**  52+c0308*x** 308+c0564*x** 564+c0820*x** 820)
      (4*L0x2001bc98*x** 52) [ 1043969, x**256 + 554923 ],
eqmod (c0053*x**  53+c0309*x** 309+c0565*x** 565+c0821*x** 821)
      (4*L0x2001bc9c*x** 53) [ 1043969, x**256 + 554923 ],
eqmod (c0054*x**  54+c0310*x** 310+c0566*x** 566+c0822*x** 822)
      (4*L0x2001bca0*x** 54) [ 1043969, x**256 + 554923 ],
eqmod (c0055*x**  55+c0311*x** 311+c0567*x** 567+c0823*x** 823)
      (4*L0x2001bca4*x** 55) [ 1043969, x**256 + 554923 ],
eqmod (c0056*x**  56+c0312*x** 312+c0568*x** 568+c0824*x** 824)
      (4*L0x2001bca8*x** 56) [ 1043969, x**256 + 554923 ],
eqmod (c0057*x**  57+c0313*x** 313+c0569*x** 569+c0825*x** 825)
      (4*L0x2001bcac*x** 57) [ 1043969, x**256 + 554923 ],
eqmod (c0058*x**  58+c0314*x** 314+c0570*x** 570+c0826*x** 826)
      (4*L0x2001bcb0*x** 58) [ 1043969, x**256 + 554923 ],
eqmod (c0059*x**  59+c0315*x** 315+c0571*x** 571+c0827*x** 827)
      (4*L0x2001bcb4*x** 59) [ 1043969, x**256 + 554923 ],
eqmod (c0060*x**  60+c0316*x** 316+c0572*x** 572+c0828*x** 828)
      (4*L0x2001bcb8*x** 60) [ 1043969, x**256 + 554923 ],
eqmod (c0061*x**  61+c0317*x** 317+c0573*x** 573+c0829*x** 829)
      (4*L0x2001bcbc*x** 61) [ 1043969, x**256 + 554923 ],
eqmod (c0062*x**  62+c0318*x** 318+c0574*x** 574+c0830*x** 830)
      (4*L0x2001bcc0*x** 62) [ 1043969, x**256 + 554923 ],
eqmod (c0063*x**  63+c0319*x** 319+c0575*x** 575+c0831*x** 831)
      (4*L0x2001bcc4*x** 63) [ 1043969, x**256 + 554923 ],
eqmod (c0064*x**  64+c0320*x** 320+c0576*x** 576+c0832*x** 832)
      (4*L0x2001bcc8*x** 64) [ 1043969, x**256 + 554923 ],
eqmod (c0065*x**  65+c0321*x** 321+c0577*x** 577+c0833*x** 833)
      (4*L0x2001bccc*x** 65) [ 1043969, x**256 + 554923 ],
eqmod (c0066*x**  66+c0322*x** 322+c0578*x** 578+c0834*x** 834)
      (4*L0x2001bcd0*x** 66) [ 1043969, x**256 + 554923 ],
eqmod (c0067*x**  67+c0323*x** 323+c0579*x** 579+c0835*x** 835)
      (4*L0x2001bcd4*x** 67) [ 1043969, x**256 + 554923 ],
eqmod (c0068*x**  68+c0324*x** 324+c0580*x** 580+c0836*x** 836)
      (4*L0x2001bcd8*x** 68) [ 1043969, x**256 + 554923 ],
eqmod (c0069*x**  69+c0325*x** 325+c0581*x** 581+c0837*x** 837)
      (4*L0x2001bcdc*x** 69) [ 1043969, x**256 + 554923 ],
eqmod (c0070*x**  70+c0326*x** 326+c0582*x** 582+c0838*x** 838)
      (4*L0x2001bce0*x** 70) [ 1043969, x**256 + 554923 ],
eqmod (c0071*x**  71+c0327*x** 327+c0583*x** 583+c0839*x** 839)
      (4*L0x2001bce4*x** 71) [ 1043969, x**256 + 554923 ],
eqmod (c0072*x**  72+c0328*x** 328+c0584*x** 584+c0840*x** 840)
      (4*L0x2001bce8*x** 72) [ 1043969, x**256 + 554923 ],
eqmod (c0073*x**  73+c0329*x** 329+c0585*x** 585+c0841*x** 841)
      (4*L0x2001bcec*x** 73) [ 1043969, x**256 + 554923 ],
eqmod (c0074*x**  74+c0330*x** 330+c0586*x** 586+c0842*x** 842)
      (4*L0x2001bcf0*x** 74) [ 1043969, x**256 + 554923 ],
eqmod (c0075*x**  75+c0331*x** 331+c0587*x** 587+c0843*x** 843)
      (4*L0x2001bcf4*x** 75) [ 1043969, x**256 + 554923 ],
eqmod (c0076*x**  76+c0332*x** 332+c0588*x** 588+c0844*x** 844)
      (4*L0x2001bcf8*x** 76) [ 1043969, x**256 + 554923 ],
eqmod (c0077*x**  77+c0333*x** 333+c0589*x** 589+c0845*x** 845)
      (4*L0x2001bcfc*x** 77) [ 1043969, x**256 + 554923 ],
eqmod (c0078*x**  78+c0334*x** 334+c0590*x** 590+c0846*x** 846)
      (4*L0x2001bd00*x** 78) [ 1043969, x**256 + 554923 ],
eqmod (c0079*x**  79+c0335*x** 335+c0591*x** 591+c0847*x** 847)
      (4*L0x2001bd04*x** 79) [ 1043969, x**256 + 554923 ],
eqmod (c0080*x**  80+c0336*x** 336+c0592*x** 592+c0848*x** 848)
      (4*L0x2001bd08*x** 80) [ 1043969, x**256 + 554923 ],
eqmod (c0081*x**  81+c0337*x** 337+c0593*x** 593+c0849*x** 849)
      (4*L0x2001bd0c*x** 81) [ 1043969, x**256 + 554923 ],
eqmod (c0082*x**  82+c0338*x** 338+c0594*x** 594+c0850*x** 850)
      (4*L0x2001bd10*x** 82) [ 1043969, x**256 + 554923 ],
eqmod (c0083*x**  83+c0339*x** 339+c0595*x** 595+c0851*x** 851)
      (4*L0x2001bd14*x** 83) [ 1043969, x**256 + 554923 ],
eqmod (c0084*x**  84+c0340*x** 340+c0596*x** 596+c0852*x** 852)
      (4*L0x2001bd18*x** 84) [ 1043969, x**256 + 554923 ],
eqmod (c0085*x**  85+c0341*x** 341+c0597*x** 597+c0853*x** 853)
      (4*L0x2001bd1c*x** 85) [ 1043969, x**256 + 554923 ],
eqmod (c0086*x**  86+c0342*x** 342+c0598*x** 598+c0854*x** 854)
      (4*L0x2001bd20*x** 86) [ 1043969, x**256 + 554923 ],
eqmod (c0087*x**  87+c0343*x** 343+c0599*x** 599+c0855*x** 855)
      (4*L0x2001bd24*x** 87) [ 1043969, x**256 + 554923 ],
eqmod (c0088*x**  88+c0344*x** 344+c0600*x** 600+c0856*x** 856)
      (4*L0x2001bd28*x** 88) [ 1043969, x**256 + 554923 ],
eqmod (c0089*x**  89+c0345*x** 345+c0601*x** 601+c0857*x** 857)
      (4*L0x2001bd2c*x** 89) [ 1043969, x**256 + 554923 ],
eqmod (c0090*x**  90+c0346*x** 346+c0602*x** 602+c0858*x** 858)
      (4*L0x2001bd30*x** 90) [ 1043969, x**256 + 554923 ],
eqmod (c0091*x**  91+c0347*x** 347+c0603*x** 603+c0859*x** 859)
      (4*L0x2001bd34*x** 91) [ 1043969, x**256 + 554923 ],
eqmod (c0092*x**  92+c0348*x** 348+c0604*x** 604+c0860*x** 860)
      (4*L0x2001bd38*x** 92) [ 1043969, x**256 + 554923 ],
eqmod (c0093*x**  93+c0349*x** 349+c0605*x** 605+c0861*x** 861)
      (4*L0x2001bd3c*x** 93) [ 1043969, x**256 + 554923 ],
eqmod (c0094*x**  94+c0350*x** 350+c0606*x** 606+c0862*x** 862)
      (4*L0x2001bd40*x** 94) [ 1043969, x**256 + 554923 ],
eqmod (c0095*x**  95+c0351*x** 351+c0607*x** 607+c0863*x** 863)
      (4*L0x2001bd44*x** 95) [ 1043969, x**256 + 554923 ],
eqmod (c0096*x**  96+c0352*x** 352+c0608*x** 608+c0864*x** 864)
      (4*L0x2001bd48*x** 96) [ 1043969, x**256 + 554923 ],
eqmod (c0097*x**  97+c0353*x** 353+c0609*x** 609+c0865*x** 865)
      (4*L0x2001bd4c*x** 97) [ 1043969, x**256 + 554923 ],
eqmod (c0098*x**  98+c0354*x** 354+c0610*x** 610+c0866*x** 866)
      (4*L0x2001bd50*x** 98) [ 1043969, x**256 + 554923 ],
eqmod (c0099*x**  99+c0355*x** 355+c0611*x** 611+c0867*x** 867)
      (4*L0x2001bd54*x** 99) [ 1043969, x**256 + 554923 ],
eqmod (c0100*x** 100+c0356*x** 356+c0612*x** 612+c0868*x** 868)
      (4*L0x2001bd58*x**100) [ 1043969, x**256 + 554923 ],
eqmod (c0101*x** 101+c0357*x** 357+c0613*x** 613+c0869*x** 869)
      (4*L0x2001bd5c*x**101) [ 1043969, x**256 + 554923 ],
eqmod (c0102*x** 102+c0358*x** 358+c0614*x** 614+c0870*x** 870)
      (4*L0x2001bd60*x**102) [ 1043969, x**256 + 554923 ],
eqmod (c0103*x** 103+c0359*x** 359+c0615*x** 615+c0871*x** 871)
      (4*L0x2001bd64*x**103) [ 1043969, x**256 + 554923 ],
eqmod (c0104*x** 104+c0360*x** 360+c0616*x** 616+c0872*x** 872)
      (4*L0x2001bd68*x**104) [ 1043969, x**256 + 554923 ],
eqmod (c0105*x** 105+c0361*x** 361+c0617*x** 617+c0873*x** 873)
      (4*L0x2001bd6c*x**105) [ 1043969, x**256 + 554923 ],
eqmod (c0106*x** 106+c0362*x** 362+c0618*x** 618+c0874*x** 874)
      (4*L0x2001bd70*x**106) [ 1043969, x**256 + 554923 ],
eqmod (c0107*x** 107+c0363*x** 363+c0619*x** 619+c0875*x** 875)
      (4*L0x2001bd74*x**107) [ 1043969, x**256 + 554923 ],
eqmod (c0108*x** 108+c0364*x** 364+c0620*x** 620+c0876*x** 876)
      (4*L0x2001bd78*x**108) [ 1043969, x**256 + 554923 ],
eqmod (c0109*x** 109+c0365*x** 365+c0621*x** 621+c0877*x** 877)
      (4*L0x2001bd7c*x**109) [ 1043969, x**256 + 554923 ],
eqmod (c0110*x** 110+c0366*x** 366+c0622*x** 622+c0878*x** 878)
      (4*L0x2001bd80*x**110) [ 1043969, x**256 + 554923 ],
eqmod (c0111*x** 111+c0367*x** 367+c0623*x** 623+c0879*x** 879)
      (4*L0x2001bd84*x**111) [ 1043969, x**256 + 554923 ],
eqmod (c0112*x** 112+c0368*x** 368+c0624*x** 624+c0880*x** 880)
      (4*L0x2001bd88*x**112) [ 1043969, x**256 + 554923 ],
eqmod (c0113*x** 113+c0369*x** 369+c0625*x** 625+c0881*x** 881)
      (4*L0x2001bd8c*x**113) [ 1043969, x**256 + 554923 ],
eqmod (c0114*x** 114+c0370*x** 370+c0626*x** 626+c0882*x** 882)
      (4*L0x2001bd90*x**114) [ 1043969, x**256 + 554923 ],
eqmod (c0115*x** 115+c0371*x** 371+c0627*x** 627+c0883*x** 883)
      (4*L0x2001bd94*x**115) [ 1043969, x**256 + 554923 ],
eqmod (c0116*x** 116+c0372*x** 372+c0628*x** 628+c0884*x** 884)
      (4*L0x2001bd98*x**116) [ 1043969, x**256 + 554923 ],
eqmod (c0117*x** 117+c0373*x** 373+c0629*x** 629+c0885*x** 885)
      (4*L0x2001bd9c*x**117) [ 1043969, x**256 + 554923 ],
eqmod (c0118*x** 118+c0374*x** 374+c0630*x** 630+c0886*x** 886)
      (4*L0x2001bda0*x**118) [ 1043969, x**256 + 554923 ],
eqmod (c0119*x** 119+c0375*x** 375+c0631*x** 631+c0887*x** 887)
      (4*L0x2001bda4*x**119) [ 1043969, x**256 + 554923 ],
eqmod (c0120*x** 120+c0376*x** 376+c0632*x** 632+c0888*x** 888)
      (4*L0x2001bda8*x**120) [ 1043969, x**256 + 554923 ],
eqmod (c0121*x** 121+c0377*x** 377+c0633*x** 633+c0889*x** 889)
      (4*L0x2001bdac*x**121) [ 1043969, x**256 + 554923 ],
eqmod (c0122*x** 122+c0378*x** 378+c0634*x** 634+c0890*x** 890)
      (4*L0x2001bdb0*x**122) [ 1043969, x**256 + 554923 ],
eqmod (c0123*x** 123+c0379*x** 379+c0635*x** 635+c0891*x** 891)
      (4*L0x2001bdb4*x**123) [ 1043969, x**256 + 554923 ],
eqmod (c0124*x** 124+c0380*x** 380+c0636*x** 636+c0892*x** 892)
      (4*L0x2001bdb8*x**124) [ 1043969, x**256 + 554923 ],
eqmod (c0125*x** 125+c0381*x** 381+c0637*x** 637+c0893*x** 893)
      (4*L0x2001bdbc*x**125) [ 1043969, x**256 + 554923 ],
eqmod (c0126*x** 126+c0382*x** 382+c0638*x** 638+c0894*x** 894)
      (4*L0x2001bdc0*x**126) [ 1043969, x**256 + 554923 ],
eqmod (c0127*x** 127+c0383*x** 383+c0639*x** 639+c0895*x** 895)
      (4*L0x2001bdc4*x**127) [ 1043969, x**256 + 554923 ],
eqmod (c0128*x** 128+c0384*x** 384+c0640*x** 640+c0896*x** 896)
      (4*L0x2001bdc8*x**128) [ 1043969, x**256 + 554923 ],
eqmod (c0129*x** 129+c0385*x** 385+c0641*x** 641+c0897*x** 897)
      (4*L0x2001bdcc*x**129) [ 1043969, x**256 + 554923 ],
eqmod (c0130*x** 130+c0386*x** 386+c0642*x** 642+c0898*x** 898)
      (4*L0x2001bdd0*x**130) [ 1043969, x**256 + 554923 ],
eqmod (c0131*x** 131+c0387*x** 387+c0643*x** 643+c0899*x** 899)
      (4*L0x2001bdd4*x**131) [ 1043969, x**256 + 554923 ],
eqmod (c0132*x** 132+c0388*x** 388+c0644*x** 644+c0900*x** 900)
      (4*L0x2001bdd8*x**132) [ 1043969, x**256 + 554923 ],
eqmod (c0133*x** 133+c0389*x** 389+c0645*x** 645+c0901*x** 901)
      (4*L0x2001bddc*x**133) [ 1043969, x**256 + 554923 ],
eqmod (c0134*x** 134+c0390*x** 390+c0646*x** 646+c0902*x** 902)
      (4*L0x2001bde0*x**134) [ 1043969, x**256 + 554923 ],
eqmod (c0135*x** 135+c0391*x** 391+c0647*x** 647+c0903*x** 903)
      (4*L0x2001bde4*x**135) [ 1043969, x**256 + 554923 ],
eqmod (c0136*x** 136+c0392*x** 392+c0648*x** 648+c0904*x** 904)
      (4*L0x2001bde8*x**136) [ 1043969, x**256 + 554923 ],
eqmod (c0137*x** 137+c0393*x** 393+c0649*x** 649+c0905*x** 905)
      (4*L0x2001bdec*x**137) [ 1043969, x**256 + 554923 ],
eqmod (c0138*x** 138+c0394*x** 394+c0650*x** 650+c0906*x** 906)
      (4*L0x2001bdf0*x**138) [ 1043969, x**256 + 554923 ],
eqmod (c0139*x** 139+c0395*x** 395+c0651*x** 651+c0907*x** 907)
      (4*L0x2001bdf4*x**139) [ 1043969, x**256 + 554923 ],
eqmod (c0140*x** 140+c0396*x** 396+c0652*x** 652+c0908*x** 908)
      (4*L0x2001bdf8*x**140) [ 1043969, x**256 + 554923 ],
eqmod (c0141*x** 141+c0397*x** 397+c0653*x** 653+c0909*x** 909)
      (4*L0x2001bdfc*x**141) [ 1043969, x**256 + 554923 ],
eqmod (c0142*x** 142+c0398*x** 398+c0654*x** 654+c0910*x** 910)
      (4*L0x2001be00*x**142) [ 1043969, x**256 + 554923 ],
eqmod (c0143*x** 143+c0399*x** 399+c0655*x** 655+c0911*x** 911)
      (4*L0x2001be04*x**143) [ 1043969, x**256 + 554923 ],
eqmod (c0144*x** 144+c0400*x** 400+c0656*x** 656+c0912*x** 912)
      (4*L0x2001be08*x**144) [ 1043969, x**256 + 554923 ],
eqmod (c0145*x** 145+c0401*x** 401+c0657*x** 657+c0913*x** 913)
      (4*L0x2001be0c*x**145) [ 1043969, x**256 + 554923 ],
eqmod (c0146*x** 146+c0402*x** 402+c0658*x** 658+c0914*x** 914)
      (4*L0x2001be10*x**146) [ 1043969, x**256 + 554923 ],
eqmod (c0147*x** 147+c0403*x** 403+c0659*x** 659+c0915*x** 915)
      (4*L0x2001be14*x**147) [ 1043969, x**256 + 554923 ],
eqmod (c0148*x** 148+c0404*x** 404+c0660*x** 660+c0916*x** 916)
      (4*L0x2001be18*x**148) [ 1043969, x**256 + 554923 ],
eqmod (c0149*x** 149+c0405*x** 405+c0661*x** 661+c0917*x** 917)
      (4*L0x2001be1c*x**149) [ 1043969, x**256 + 554923 ],
eqmod (c0150*x** 150+c0406*x** 406+c0662*x** 662+c0918*x** 918)
      (4*L0x2001be20*x**150) [ 1043969, x**256 + 554923 ],
eqmod (c0151*x** 151+c0407*x** 407+c0663*x** 663+c0919*x** 919)
      (4*L0x2001be24*x**151) [ 1043969, x**256 + 554923 ],
eqmod (c0152*x** 152+c0408*x** 408+c0664*x** 664+c0920*x** 920)
      (4*L0x2001be28*x**152) [ 1043969, x**256 + 554923 ],
eqmod (c0153*x** 153+c0409*x** 409+c0665*x** 665+c0921*x** 921)
      (4*L0x2001be2c*x**153) [ 1043969, x**256 + 554923 ],
eqmod (c0154*x** 154+c0410*x** 410+c0666*x** 666+c0922*x** 922)
      (4*L0x2001be30*x**154) [ 1043969, x**256 + 554923 ],
eqmod (c0155*x** 155+c0411*x** 411+c0667*x** 667+c0923*x** 923)
      (4*L0x2001be34*x**155) [ 1043969, x**256 + 554923 ],
eqmod (c0156*x** 156+c0412*x** 412+c0668*x** 668+c0924*x** 924)
      (4*L0x2001be38*x**156) [ 1043969, x**256 + 554923 ],
eqmod (c0157*x** 157+c0413*x** 413+c0669*x** 669+c0925*x** 925)
      (4*L0x2001be3c*x**157) [ 1043969, x**256 + 554923 ],
eqmod (c0158*x** 158+c0414*x** 414+c0670*x** 670+c0926*x** 926)
      (4*L0x2001be40*x**158) [ 1043969, x**256 + 554923 ],
eqmod (c0159*x** 159+c0415*x** 415+c0671*x** 671+c0927*x** 927)
      (4*L0x2001be44*x**159) [ 1043969, x**256 + 554923 ],
eqmod (c0160*x** 160+c0416*x** 416+c0672*x** 672+c0928*x** 928)
      (4*L0x2001be48*x**160) [ 1043969, x**256 + 554923 ],
eqmod (c0161*x** 161+c0417*x** 417+c0673*x** 673+c0929*x** 929)
      (4*L0x2001be4c*x**161) [ 1043969, x**256 + 554923 ],
eqmod (c0162*x** 162+c0418*x** 418+c0674*x** 674+c0930*x** 930)
      (4*L0x2001be50*x**162) [ 1043969, x**256 + 554923 ],
eqmod (c0163*x** 163+c0419*x** 419+c0675*x** 675+c0931*x** 931)
      (4*L0x2001be54*x**163) [ 1043969, x**256 + 554923 ],
eqmod (c0164*x** 164+c0420*x** 420+c0676*x** 676+c0932*x** 932)
      (4*L0x2001be58*x**164) [ 1043969, x**256 + 554923 ],
eqmod (c0165*x** 165+c0421*x** 421+c0677*x** 677+c0933*x** 933)
      (4*L0x2001be5c*x**165) [ 1043969, x**256 + 554923 ],
eqmod (c0166*x** 166+c0422*x** 422+c0678*x** 678+c0934*x** 934)
      (4*L0x2001be60*x**166) [ 1043969, x**256 + 554923 ],
eqmod (c0167*x** 167+c0423*x** 423+c0679*x** 679+c0935*x** 935)
      (4*L0x2001be64*x**167) [ 1043969, x**256 + 554923 ],
eqmod (c0168*x** 168+c0424*x** 424+c0680*x** 680+c0936*x** 936)
      (4*L0x2001be68*x**168) [ 1043969, x**256 + 554923 ],
eqmod (c0169*x** 169+c0425*x** 425+c0681*x** 681+c0937*x** 937)
      (4*L0x2001be6c*x**169) [ 1043969, x**256 + 554923 ],
eqmod (c0170*x** 170+c0426*x** 426+c0682*x** 682+c0938*x** 938)
      (4*L0x2001be70*x**170) [ 1043969, x**256 + 554923 ],
eqmod (c0171*x** 171+c0427*x** 427+c0683*x** 683+c0939*x** 939)
      (4*L0x2001be74*x**171) [ 1043969, x**256 + 554923 ],
eqmod (c0172*x** 172+c0428*x** 428+c0684*x** 684+c0940*x** 940)
      (4*L0x2001be78*x**172) [ 1043969, x**256 + 554923 ],
eqmod (c0173*x** 173+c0429*x** 429+c0685*x** 685+c0941*x** 941)
      (4*L0x2001be7c*x**173) [ 1043969, x**256 + 554923 ],
eqmod (c0174*x** 174+c0430*x** 430+c0686*x** 686+c0942*x** 942)
      (4*L0x2001be80*x**174) [ 1043969, x**256 + 554923 ],
eqmod (c0175*x** 175+c0431*x** 431+c0687*x** 687+c0943*x** 943)
      (4*L0x2001be84*x**175) [ 1043969, x**256 + 554923 ],
eqmod (c0176*x** 176+c0432*x** 432+c0688*x** 688+c0944*x** 944)
      (4*L0x2001be88*x**176) [ 1043969, x**256 + 554923 ],
eqmod (c0177*x** 177+c0433*x** 433+c0689*x** 689+c0945*x** 945)
      (4*L0x2001be8c*x**177) [ 1043969, x**256 + 554923 ],
eqmod (c0178*x** 178+c0434*x** 434+c0690*x** 690+c0946*x** 946)
      (4*L0x2001be90*x**178) [ 1043969, x**256 + 554923 ],
eqmod (c0179*x** 179+c0435*x** 435+c0691*x** 691+c0947*x** 947)
      (4*L0x2001be94*x**179) [ 1043969, x**256 + 554923 ],
eqmod (c0180*x** 180+c0436*x** 436+c0692*x** 692+c0948*x** 948)
      (4*L0x2001be98*x**180) [ 1043969, x**256 + 554923 ],
eqmod (c0181*x** 181+c0437*x** 437+c0693*x** 693+c0949*x** 949)
      (4*L0x2001be9c*x**181) [ 1043969, x**256 + 554923 ],
eqmod (c0182*x** 182+c0438*x** 438+c0694*x** 694+c0950*x** 950)
      (4*L0x2001bea0*x**182) [ 1043969, x**256 + 554923 ],
eqmod (c0183*x** 183+c0439*x** 439+c0695*x** 695+c0951*x** 951)
      (4*L0x2001bea4*x**183) [ 1043969, x**256 + 554923 ],
eqmod (c0184*x** 184+c0440*x** 440+c0696*x** 696+c0952*x** 952)
      (4*L0x2001bea8*x**184) [ 1043969, x**256 + 554923 ],
eqmod (c0185*x** 185+c0441*x** 441+c0697*x** 697+c0953*x** 953)
      (4*L0x2001beac*x**185) [ 1043969, x**256 + 554923 ],
eqmod (c0186*x** 186+c0442*x** 442+c0698*x** 698+c0954*x** 954)
      (4*L0x2001beb0*x**186) [ 1043969, x**256 + 554923 ],
eqmod (c0187*x** 187+c0443*x** 443+c0699*x** 699+c0955*x** 955)
      (4*L0x2001beb4*x**187) [ 1043969, x**256 + 554923 ],
eqmod (c0188*x** 188+c0444*x** 444+c0700*x** 700+c0956*x** 956)
      (4*L0x2001beb8*x**188) [ 1043969, x**256 + 554923 ],
eqmod (c0189*x** 189+c0445*x** 445+c0701*x** 701+c0957*x** 957)
      (4*L0x2001bebc*x**189) [ 1043969, x**256 + 554923 ],
eqmod (c0190*x** 190+c0446*x** 446+c0702*x** 702+c0958*x** 958)
      (4*L0x2001bec0*x**190) [ 1043969, x**256 + 554923 ],
eqmod (c0191*x** 191+c0447*x** 447+c0703*x** 703+c0959*x** 959)
      (4*L0x2001bec4*x**191) [ 1043969, x**256 + 554923 ],
eqmod (c0192*x** 192+c0448*x** 448+c0704*x** 704+c0960*x** 960)
      (4*L0x2001bec8*x**192) [ 1043969, x**256 + 554923 ],
eqmod (c0193*x** 193+c0449*x** 449+c0705*x** 705+c0961*x** 961)
      (4*L0x2001becc*x**193) [ 1043969, x**256 + 554923 ],
eqmod (c0194*x** 194+c0450*x** 450+c0706*x** 706+c0962*x** 962)
      (4*L0x2001bed0*x**194) [ 1043969, x**256 + 554923 ],
eqmod (c0195*x** 195+c0451*x** 451+c0707*x** 707+c0963*x** 963)
      (4*L0x2001bed4*x**195) [ 1043969, x**256 + 554923 ],
eqmod (c0196*x** 196+c0452*x** 452+c0708*x** 708+c0964*x** 964)
      (4*L0x2001bed8*x**196) [ 1043969, x**256 + 554923 ],
eqmod (c0197*x** 197+c0453*x** 453+c0709*x** 709+c0965*x** 965)
      (4*L0x2001bedc*x**197) [ 1043969, x**256 + 554923 ],
eqmod (c0198*x** 198+c0454*x** 454+c0710*x** 710+c0966*x** 966)
      (4*L0x2001bee0*x**198) [ 1043969, x**256 + 554923 ],
eqmod (c0199*x** 199+c0455*x** 455+c0711*x** 711+c0967*x** 967)
      (4*L0x2001bee4*x**199) [ 1043969, x**256 + 554923 ],
eqmod (c0200*x** 200+c0456*x** 456+c0712*x** 712+c0968*x** 968)
      (4*L0x2001bee8*x**200) [ 1043969, x**256 + 554923 ],
eqmod (c0201*x** 201+c0457*x** 457+c0713*x** 713+c0969*x** 969)
      (4*L0x2001beec*x**201) [ 1043969, x**256 + 554923 ],
eqmod (c0202*x** 202+c0458*x** 458+c0714*x** 714+c0970*x** 970)
      (4*L0x2001bef0*x**202) [ 1043969, x**256 + 554923 ],
eqmod (c0203*x** 203+c0459*x** 459+c0715*x** 715+c0971*x** 971)
      (4*L0x2001bef4*x**203) [ 1043969, x**256 + 554923 ],
eqmod (c0204*x** 204+c0460*x** 460+c0716*x** 716+c0972*x** 972)
      (4*L0x2001bef8*x**204) [ 1043969, x**256 + 554923 ],
eqmod (c0205*x** 205+c0461*x** 461+c0717*x** 717+c0973*x** 973)
      (4*L0x2001befc*x**205) [ 1043969, x**256 + 554923 ],
eqmod (c0206*x** 206+c0462*x** 462+c0718*x** 718+c0974*x** 974)
      (4*L0x2001bf00*x**206) [ 1043969, x**256 + 554923 ],
eqmod (c0207*x** 207+c0463*x** 463+c0719*x** 719+c0975*x** 975)
      (4*L0x2001bf04*x**207) [ 1043969, x**256 + 554923 ],
eqmod (c0208*x** 208+c0464*x** 464+c0720*x** 720+c0976*x** 976)
      (4*L0x2001bf08*x**208) [ 1043969, x**256 + 554923 ],
eqmod (c0209*x** 209+c0465*x** 465+c0721*x** 721+c0977*x** 977)
      (4*L0x2001bf0c*x**209) [ 1043969, x**256 + 554923 ],
eqmod (c0210*x** 210+c0466*x** 466+c0722*x** 722+c0978*x** 978)
      (4*L0x2001bf10*x**210) [ 1043969, x**256 + 554923 ],
eqmod (c0211*x** 211+c0467*x** 467+c0723*x** 723+c0979*x** 979)
      (4*L0x2001bf14*x**211) [ 1043969, x**256 + 554923 ],
eqmod (c0212*x** 212+c0468*x** 468+c0724*x** 724+c0980*x** 980)
      (4*L0x2001bf18*x**212) [ 1043969, x**256 + 554923 ],
eqmod (c0213*x** 213+c0469*x** 469+c0725*x** 725+c0981*x** 981)
      (4*L0x2001bf1c*x**213) [ 1043969, x**256 + 554923 ],
eqmod (c0214*x** 214+c0470*x** 470+c0726*x** 726+c0982*x** 982)
      (4*L0x2001bf20*x**214) [ 1043969, x**256 + 554923 ],
eqmod (c0215*x** 215+c0471*x** 471+c0727*x** 727+c0983*x** 983)
      (4*L0x2001bf24*x**215) [ 1043969, x**256 + 554923 ],
eqmod (c0216*x** 216+c0472*x** 472+c0728*x** 728+c0984*x** 984)
      (4*L0x2001bf28*x**216) [ 1043969, x**256 + 554923 ],
eqmod (c0217*x** 217+c0473*x** 473+c0729*x** 729+c0985*x** 985)
      (4*L0x2001bf2c*x**217) [ 1043969, x**256 + 554923 ],
eqmod (c0218*x** 218+c0474*x** 474+c0730*x** 730+c0986*x** 986)
      (4*L0x2001bf30*x**218) [ 1043969, x**256 + 554923 ],
eqmod (c0219*x** 219+c0475*x** 475+c0731*x** 731+c0987*x** 987)
      (4*L0x2001bf34*x**219) [ 1043969, x**256 + 554923 ],
eqmod (c0220*x** 220+c0476*x** 476+c0732*x** 732+c0988*x** 988)
      (4*L0x2001bf38*x**220) [ 1043969, x**256 + 554923 ],
eqmod (c0221*x** 221+c0477*x** 477+c0733*x** 733+c0989*x** 989)
      (4*L0x2001bf3c*x**221) [ 1043969, x**256 + 554923 ],
eqmod (c0222*x** 222+c0478*x** 478+c0734*x** 734+c0990*x** 990)
      (4*L0x2001bf40*x**222) [ 1043969, x**256 + 554923 ],
eqmod (c0223*x** 223+c0479*x** 479+c0735*x** 735+c0991*x** 991)
      (4*L0x2001bf44*x**223) [ 1043969, x**256 + 554923 ],
eqmod (c0224*x** 224+c0480*x** 480+c0736*x** 736+c0992*x** 992)
      (4*L0x2001bf48*x**224) [ 1043969, x**256 + 554923 ],
eqmod (c0225*x** 225+c0481*x** 481+c0737*x** 737+c0993*x** 993)
      (4*L0x2001bf4c*x**225) [ 1043969, x**256 + 554923 ],
eqmod (c0226*x** 226+c0482*x** 482+c0738*x** 738+c0994*x** 994)
      (4*L0x2001bf50*x**226) [ 1043969, x**256 + 554923 ],
eqmod (c0227*x** 227+c0483*x** 483+c0739*x** 739+c0995*x** 995)
      (4*L0x2001bf54*x**227) [ 1043969, x**256 + 554923 ],
eqmod (c0228*x** 228+c0484*x** 484+c0740*x** 740+c0996*x** 996)
      (4*L0x2001bf58*x**228) [ 1043969, x**256 + 554923 ],
eqmod (c0229*x** 229+c0485*x** 485+c0741*x** 741+c0997*x** 997)
      (4*L0x2001bf5c*x**229) [ 1043969, x**256 + 554923 ],
eqmod (c0230*x** 230+c0486*x** 486+c0742*x** 742+c0998*x** 998)
      (4*L0x2001bf60*x**230) [ 1043969, x**256 + 554923 ],
eqmod (c0231*x** 231+c0487*x** 487+c0743*x** 743+c0999*x** 999)
      (4*L0x2001bf64*x**231) [ 1043969, x**256 + 554923 ],
eqmod (c0232*x** 232+c0488*x** 488+c0744*x** 744+c1000*x**1000)
      (4*L0x2001bf68*x**232) [ 1043969, x**256 + 554923 ],
eqmod (c0233*x** 233+c0489*x** 489+c0745*x** 745+c1001*x**1001)
      (4*L0x2001bf6c*x**233) [ 1043969, x**256 + 554923 ],
eqmod (c0234*x** 234+c0490*x** 490+c0746*x** 746+c1002*x**1002)
      (4*L0x2001bf70*x**234) [ 1043969, x**256 + 554923 ],
eqmod (c0235*x** 235+c0491*x** 491+c0747*x** 747+c1003*x**1003)
      (4*L0x2001bf74*x**235) [ 1043969, x**256 + 554923 ],
eqmod (c0236*x** 236+c0492*x** 492+c0748*x** 748+c1004*x**1004)
      (4*L0x2001bf78*x**236) [ 1043969, x**256 + 554923 ],
eqmod (c0237*x** 237+c0493*x** 493+c0749*x** 749+c1005*x**1005)
      (4*L0x2001bf7c*x**237) [ 1043969, x**256 + 554923 ],
eqmod (c0238*x** 238+c0494*x** 494+c0750*x** 750+c1006*x**1006)
      (4*L0x2001bf80*x**238) [ 1043969, x**256 + 554923 ],
eqmod (c0239*x** 239+c0495*x** 495+c0751*x** 751+c1007*x**1007)
      (4*L0x2001bf84*x**239) [ 1043969, x**256 + 554923 ],
eqmod (c0240*x** 240+c0496*x** 496+c0752*x** 752+c1008*x**1008)
      (4*L0x2001bf88*x**240) [ 1043969, x**256 + 554923 ],
eqmod (c0241*x** 241+c0497*x** 497+c0753*x** 753+c1009*x**1009)
      (4*L0x2001bf8c*x**241) [ 1043969, x**256 + 554923 ],
eqmod (c0242*x** 242+c0498*x** 498+c0754*x** 754+c1010*x**1010)
      (4*L0x2001bf90*x**242) [ 1043969, x**256 + 554923 ],
eqmod (c0243*x** 243+c0499*x** 499+c0755*x** 755+c1011*x**1011)
      (4*L0x2001bf94*x**243) [ 1043969, x**256 + 554923 ],
eqmod (c0244*x** 244+c0500*x** 500+c0756*x** 756+c1012*x**1012)
      (4*L0x2001bf98*x**244) [ 1043969, x**256 + 554923 ],
eqmod (c0245*x** 245+c0501*x** 501+c0757*x** 757+c1013*x**1013)
      (4*L0x2001bf9c*x**245) [ 1043969, x**256 + 554923 ],
eqmod (c0246*x** 246+c0502*x** 502+c0758*x** 758+c1014*x**1014)
      (4*L0x2001bfa0*x**246) [ 1043969, x**256 + 554923 ],
eqmod (c0247*x** 247+c0503*x** 503+c0759*x** 759+c1015*x**1015)
      (4*L0x2001bfa4*x**247) [ 1043969, x**256 + 554923 ],
eqmod (c0248*x** 248+c0504*x** 504+c0760*x** 760+c1016*x**1016)
      (4*L0x2001bfa8*x**248) [ 1043969, x**256 + 554923 ],
eqmod (c0249*x** 249+c0505*x** 505+c0761*x** 761+c1017*x**1017)
      (4*L0x2001bfac*x**249) [ 1043969, x**256 + 554923 ],
eqmod (c0250*x** 250+c0506*x** 506+c0762*x** 762+c1018*x**1018)
      (4*L0x2001bfb0*x**250) [ 1043969, x**256 + 554923 ],
eqmod (c0251*x** 251+c0507*x** 507+c0763*x** 763+c1019*x**1019)
      (4*L0x2001bfb4*x**251) [ 1043969, x**256 + 554923 ],
eqmod (c0252*x** 252+c0508*x** 508+c0764*x** 764+c1020*x**1020)
      (4*L0x2001bfb8*x**252) [ 1043969, x**256 + 554923 ],
eqmod (c0253*x** 253+c0509*x** 509+c0765*x** 765+c1021*x**1021)
      (4*L0x2001bfbc*x**253) [ 1043969, x**256 + 554923 ],
eqmod (c0254*x** 254+c0510*x** 510+c0766*x** 766+c1022*x**1022)
      (4*L0x2001bfc0*x**254) [ 1043969, x**256 + 554923 ],
eqmod (c0255*x** 255+c0511*x** 511+c0767*x** 767+c1023*x**1023)
      (4*L0x2001bfc4*x**255) [ 1043969, x**256 + 554923 ]
] prove with [ cuts [ 
  9,  10,  11,  12,  14,  16,  18,  20,  22,  24,  26,  28,
 30,  32,  34,  36,  38,  40,  42,  44,  46,  48,  50,  52,
 54,  56,  58,  60,  62,  64,  66,  68,  70,  72,  74,  76,
 78,  80,  82,  84,  86,  88,  90,  92,  94,  96,  98, 100,
102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124,
126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148,
150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172,
174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196,
198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220,
222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244,
246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268,
270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292,
294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316,
318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340,
342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364,
366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388,
390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412,
414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436,
438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460,
462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484,
486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508,
510, 512, 514, 516 ] ];



(**************** output c poly ****************)

ghost cpoly@bit : cpoly = 
c0000*x**   0+c0001*x**   1+c0002*x**   2+c0003*x**   3+
c0004*x**   4+c0005*x**   5+c0006*x**   6+c0007*x**   7+
c0008*x**   8+c0009*x**   9+c0010*x**  10+c0011*x**  11+
c0012*x**  12+c0013*x**  13+c0014*x**  14+c0015*x**  15+
c0016*x**  16+c0017*x**  17+c0018*x**  18+c0019*x**  19+
c0020*x**  20+c0021*x**  21+c0022*x**  22+c0023*x**  23+
c0024*x**  24+c0025*x**  25+c0026*x**  26+c0027*x**  27+
c0028*x**  28+c0029*x**  29+c0030*x**  30+c0031*x**  31+
c0032*x**  32+c0033*x**  33+c0034*x**  34+c0035*x**  35+
c0036*x**  36+c0037*x**  37+c0038*x**  38+c0039*x**  39+
c0040*x**  40+c0041*x**  41+c0042*x**  42+c0043*x**  43+
c0044*x**  44+c0045*x**  45+c0046*x**  46+c0047*x**  47+
c0048*x**  48+c0049*x**  49+c0050*x**  50+c0051*x**  51+
c0052*x**  52+c0053*x**  53+c0054*x**  54+c0055*x**  55+
c0056*x**  56+c0057*x**  57+c0058*x**  58+c0059*x**  59+
c0060*x**  60+c0061*x**  61+c0062*x**  62+c0063*x**  63+
c0064*x**  64+c0065*x**  65+c0066*x**  66+c0067*x**  67+
c0068*x**  68+c0069*x**  69+c0070*x**  70+c0071*x**  71+
c0072*x**  72+c0073*x**  73+c0074*x**  74+c0075*x**  75+
c0076*x**  76+c0077*x**  77+c0078*x**  78+c0079*x**  79+
c0080*x**  80+c0081*x**  81+c0082*x**  82+c0083*x**  83+
c0084*x**  84+c0085*x**  85+c0086*x**  86+c0087*x**  87+
c0088*x**  88+c0089*x**  89+c0090*x**  90+c0091*x**  91+
c0092*x**  92+c0093*x**  93+c0094*x**  94+c0095*x**  95+
c0096*x**  96+c0097*x**  97+c0098*x**  98+c0099*x**  99+
c0100*x** 100+c0101*x** 101+c0102*x** 102+c0103*x** 103+
c0104*x** 104+c0105*x** 105+c0106*x** 106+c0107*x** 107+
c0108*x** 108+c0109*x** 109+c0110*x** 110+c0111*x** 111+
c0112*x** 112+c0113*x** 113+c0114*x** 114+c0115*x** 115+
c0116*x** 116+c0117*x** 117+c0118*x** 118+c0119*x** 119+
c0120*x** 120+c0121*x** 121+c0122*x** 122+c0123*x** 123+
c0124*x** 124+c0125*x** 125+c0126*x** 126+c0127*x** 127+
c0128*x** 128+c0129*x** 129+c0130*x** 130+c0131*x** 131+
c0132*x** 132+c0133*x** 133+c0134*x** 134+c0135*x** 135+
c0136*x** 136+c0137*x** 137+c0138*x** 138+c0139*x** 139+
c0140*x** 140+c0141*x** 141+c0142*x** 142+c0143*x** 143+
c0144*x** 144+c0145*x** 145+c0146*x** 146+c0147*x** 147+
c0148*x** 148+c0149*x** 149+c0150*x** 150+c0151*x** 151+
c0152*x** 152+c0153*x** 153+c0154*x** 154+c0155*x** 155+
c0156*x** 156+c0157*x** 157+c0158*x** 158+c0159*x** 159+
c0160*x** 160+c0161*x** 161+c0162*x** 162+c0163*x** 163+
c0164*x** 164+c0165*x** 165+c0166*x** 166+c0167*x** 167+
c0168*x** 168+c0169*x** 169+c0170*x** 170+c0171*x** 171+
c0172*x** 172+c0173*x** 173+c0174*x** 174+c0175*x** 175+
c0176*x** 176+c0177*x** 177+c0178*x** 178+c0179*x** 179+
c0180*x** 180+c0181*x** 181+c0182*x** 182+c0183*x** 183+
c0184*x** 184+c0185*x** 185+c0186*x** 186+c0187*x** 187+
c0188*x** 188+c0189*x** 189+c0190*x** 190+c0191*x** 191+
c0192*x** 192+c0193*x** 193+c0194*x** 194+c0195*x** 195+
c0196*x** 196+c0197*x** 197+c0198*x** 198+c0199*x** 199+
c0200*x** 200+c0201*x** 201+c0202*x** 202+c0203*x** 203+
c0204*x** 204+c0205*x** 205+c0206*x** 206+c0207*x** 207+
c0208*x** 208+c0209*x** 209+c0210*x** 210+c0211*x** 211+
c0212*x** 212+c0213*x** 213+c0214*x** 214+c0215*x** 215+
c0216*x** 216+c0217*x** 217+c0218*x** 218+c0219*x** 219+
c0220*x** 220+c0221*x** 221+c0222*x** 222+c0223*x** 223+
c0224*x** 224+c0225*x** 225+c0226*x** 226+c0227*x** 227+
c0228*x** 228+c0229*x** 229+c0230*x** 230+c0231*x** 231+
c0232*x** 232+c0233*x** 233+c0234*x** 234+c0235*x** 235+
c0236*x** 236+c0237*x** 237+c0238*x** 238+c0239*x** 239+
c0240*x** 240+c0241*x** 241+c0242*x** 242+c0243*x** 243+
c0244*x** 244+c0245*x** 245+c0246*x** 246+c0247*x** 247+
c0248*x** 248+c0249*x** 249+c0250*x** 250+c0251*x** 251+
c0252*x** 252+c0253*x** 253+c0254*x** 254+c0255*x** 255+
c0256*x** 256+c0257*x** 257+c0258*x** 258+c0259*x** 259+
c0260*x** 260+c0261*x** 261+c0262*x** 262+c0263*x** 263+
c0264*x** 264+c0265*x** 265+c0266*x** 266+c0267*x** 267+
c0268*x** 268+c0269*x** 269+c0270*x** 270+c0271*x** 271+
c0272*x** 272+c0273*x** 273+c0274*x** 274+c0275*x** 275+
c0276*x** 276+c0277*x** 277+c0278*x** 278+c0279*x** 279+
c0280*x** 280+c0281*x** 281+c0282*x** 282+c0283*x** 283+
c0284*x** 284+c0285*x** 285+c0286*x** 286+c0287*x** 287+
c0288*x** 288+c0289*x** 289+c0290*x** 290+c0291*x** 291+
c0292*x** 292+c0293*x** 293+c0294*x** 294+c0295*x** 295+
c0296*x** 296+c0297*x** 297+c0298*x** 298+c0299*x** 299+
c0300*x** 300+c0301*x** 301+c0302*x** 302+c0303*x** 303+
c0304*x** 304+c0305*x** 305+c0306*x** 306+c0307*x** 307+
c0308*x** 308+c0309*x** 309+c0310*x** 310+c0311*x** 311+
c0312*x** 312+c0313*x** 313+c0314*x** 314+c0315*x** 315+
c0316*x** 316+c0317*x** 317+c0318*x** 318+c0319*x** 319+
c0320*x** 320+c0321*x** 321+c0322*x** 322+c0323*x** 323+
c0324*x** 324+c0325*x** 325+c0326*x** 326+c0327*x** 327+
c0328*x** 328+c0329*x** 329+c0330*x** 330+c0331*x** 331+
c0332*x** 332+c0333*x** 333+c0334*x** 334+c0335*x** 335+
c0336*x** 336+c0337*x** 337+c0338*x** 338+c0339*x** 339+
c0340*x** 340+c0341*x** 341+c0342*x** 342+c0343*x** 343+
c0344*x** 344+c0345*x** 345+c0346*x** 346+c0347*x** 347+
c0348*x** 348+c0349*x** 349+c0350*x** 350+c0351*x** 351+
c0352*x** 352+c0353*x** 353+c0354*x** 354+c0355*x** 355+
c0356*x** 356+c0357*x** 357+c0358*x** 358+c0359*x** 359+
c0360*x** 360+c0361*x** 361+c0362*x** 362+c0363*x** 363+
c0364*x** 364+c0365*x** 365+c0366*x** 366+c0367*x** 367+
c0368*x** 368+c0369*x** 369+c0370*x** 370+c0371*x** 371+
c0372*x** 372+c0373*x** 373+c0374*x** 374+c0375*x** 375+
c0376*x** 376+c0377*x** 377+c0378*x** 378+c0379*x** 379+
c0380*x** 380+c0381*x** 381+c0382*x** 382+c0383*x** 383+
c0384*x** 384+c0385*x** 385+c0386*x** 386+c0387*x** 387+
c0388*x** 388+c0389*x** 389+c0390*x** 390+c0391*x** 391+
c0392*x** 392+c0393*x** 393+c0394*x** 394+c0395*x** 395+
c0396*x** 396+c0397*x** 397+c0398*x** 398+c0399*x** 399+
c0400*x** 400+c0401*x** 401+c0402*x** 402+c0403*x** 403+
c0404*x** 404+c0405*x** 405+c0406*x** 406+c0407*x** 407+
c0408*x** 408+c0409*x** 409+c0410*x** 410+c0411*x** 411+
c0412*x** 412+c0413*x** 413+c0414*x** 414+c0415*x** 415+
c0416*x** 416+c0417*x** 417+c0418*x** 418+c0419*x** 419+
c0420*x** 420+c0421*x** 421+c0422*x** 422+c0423*x** 423+
c0424*x** 424+c0425*x** 425+c0426*x** 426+c0427*x** 427+
c0428*x** 428+c0429*x** 429+c0430*x** 430+c0431*x** 431+
c0432*x** 432+c0433*x** 433+c0434*x** 434+c0435*x** 435+
c0436*x** 436+c0437*x** 437+c0438*x** 438+c0439*x** 439+
c0440*x** 440+c0441*x** 441+c0442*x** 442+c0443*x** 443+
c0444*x** 444+c0445*x** 445+c0446*x** 446+c0447*x** 447+
c0448*x** 448+c0449*x** 449+c0450*x** 450+c0451*x** 451+
c0452*x** 452+c0453*x** 453+c0454*x** 454+c0455*x** 455+
c0456*x** 456+c0457*x** 457+c0458*x** 458+c0459*x** 459+
c0460*x** 460+c0461*x** 461+c0462*x** 462+c0463*x** 463+
c0464*x** 464+c0465*x** 465+c0466*x** 466+c0467*x** 467+
c0468*x** 468+c0469*x** 469+c0470*x** 470+c0471*x** 471+
c0472*x** 472+c0473*x** 473+c0474*x** 474+c0475*x** 475+
c0476*x** 476+c0477*x** 477+c0478*x** 478+c0479*x** 479+
c0480*x** 480+c0481*x** 481+c0482*x** 482+c0483*x** 483+
c0484*x** 484+c0485*x** 485+c0486*x** 486+c0487*x** 487+
c0488*x** 488+c0489*x** 489+c0490*x** 490+c0491*x** 491+
c0492*x** 492+c0493*x** 493+c0494*x** 494+c0495*x** 495+
c0496*x** 496+c0497*x** 497+c0498*x** 498+c0499*x** 499+
c0500*x** 500+c0501*x** 501+c0502*x** 502+c0503*x** 503+
c0504*x** 504+c0505*x** 505+c0506*x** 506+c0507*x** 507+
c0508*x** 508+c0509*x** 509+c0510*x** 510+c0511*x** 511+
c0512*x** 512+c0513*x** 513+c0514*x** 514+c0515*x** 515+
c0516*x** 516+c0517*x** 517+c0518*x** 518+c0519*x** 519+
c0520*x** 520+c0521*x** 521+c0522*x** 522+c0523*x** 523+
c0524*x** 524+c0525*x** 525+c0526*x** 526+c0527*x** 527+
c0528*x** 528+c0529*x** 529+c0530*x** 530+c0531*x** 531+
c0532*x** 532+c0533*x** 533+c0534*x** 534+c0535*x** 535+
c0536*x** 536+c0537*x** 537+c0538*x** 538+c0539*x** 539+
c0540*x** 540+c0541*x** 541+c0542*x** 542+c0543*x** 543+
c0544*x** 544+c0545*x** 545+c0546*x** 546+c0547*x** 547+
c0548*x** 548+c0549*x** 549+c0550*x** 550+c0551*x** 551+
c0552*x** 552+c0553*x** 553+c0554*x** 554+c0555*x** 555+
c0556*x** 556+c0557*x** 557+c0558*x** 558+c0559*x** 559+
c0560*x** 560+c0561*x** 561+c0562*x** 562+c0563*x** 563+
c0564*x** 564+c0565*x** 565+c0566*x** 566+c0567*x** 567+
c0568*x** 568+c0569*x** 569+c0570*x** 570+c0571*x** 571+
c0572*x** 572+c0573*x** 573+c0574*x** 574+c0575*x** 575+
c0576*x** 576+c0577*x** 577+c0578*x** 578+c0579*x** 579+
c0580*x** 580+c0581*x** 581+c0582*x** 582+c0583*x** 583+
c0584*x** 584+c0585*x** 585+c0586*x** 586+c0587*x** 587+
c0588*x** 588+c0589*x** 589+c0590*x** 590+c0591*x** 591+
c0592*x** 592+c0593*x** 593+c0594*x** 594+c0595*x** 595+
c0596*x** 596+c0597*x** 597+c0598*x** 598+c0599*x** 599+
c0600*x** 600+c0601*x** 601+c0602*x** 602+c0603*x** 603+
c0604*x** 604+c0605*x** 605+c0606*x** 606+c0607*x** 607+
c0608*x** 608+c0609*x** 609+c0610*x** 610+c0611*x** 611+
c0612*x** 612+c0613*x** 613+c0614*x** 614+c0615*x** 615+
c0616*x** 616+c0617*x** 617+c0618*x** 618+c0619*x** 619+
c0620*x** 620+c0621*x** 621+c0622*x** 622+c0623*x** 623+
c0624*x** 624+c0625*x** 625+c0626*x** 626+c0627*x** 627+
c0628*x** 628+c0629*x** 629+c0630*x** 630+c0631*x** 631+
c0632*x** 632+c0633*x** 633+c0634*x** 634+c0635*x** 635+
c0636*x** 636+c0637*x** 637+c0638*x** 638+c0639*x** 639+
c0640*x** 640+c0641*x** 641+c0642*x** 642+c0643*x** 643+
c0644*x** 644+c0645*x** 645+c0646*x** 646+c0647*x** 647+
c0648*x** 648+c0649*x** 649+c0650*x** 650+c0651*x** 651+
c0652*x** 652+c0653*x** 653+c0654*x** 654+c0655*x** 655+
c0656*x** 656+c0657*x** 657+c0658*x** 658+c0659*x** 659+
c0660*x** 660+c0661*x** 661+c0662*x** 662+c0663*x** 663+
c0664*x** 664+c0665*x** 665+c0666*x** 666+c0667*x** 667+
c0668*x** 668+c0669*x** 669+c0670*x** 670+c0671*x** 671+
c0672*x** 672+c0673*x** 673+c0674*x** 674+c0675*x** 675+
c0676*x** 676+c0677*x** 677+c0678*x** 678+c0679*x** 679+
c0680*x** 680+c0681*x** 681+c0682*x** 682+c0683*x** 683+
c0684*x** 684+c0685*x** 685+c0686*x** 686+c0687*x** 687+
c0688*x** 688+c0689*x** 689+c0690*x** 690+c0691*x** 691+
c0692*x** 692+c0693*x** 693+c0694*x** 694+c0695*x** 695+
c0696*x** 696+c0697*x** 697+c0698*x** 698+c0699*x** 699+
c0700*x** 700+c0701*x** 701+c0702*x** 702+c0703*x** 703+
c0704*x** 704+c0705*x** 705+c0706*x** 706+c0707*x** 707+
c0708*x** 708+c0709*x** 709+c0710*x** 710+c0711*x** 711+
c0712*x** 712+c0713*x** 713+c0714*x** 714+c0715*x** 715+
c0716*x** 716+c0717*x** 717+c0718*x** 718+c0719*x** 719+
c0720*x** 720+c0721*x** 721+c0722*x** 722+c0723*x** 723+
c0724*x** 724+c0725*x** 725+c0726*x** 726+c0727*x** 727+
c0728*x** 728+c0729*x** 729+c0730*x** 730+c0731*x** 731+
c0732*x** 732+c0733*x** 733+c0734*x** 734+c0735*x** 735+
c0736*x** 736+c0737*x** 737+c0738*x** 738+c0739*x** 739+
c0740*x** 740+c0741*x** 741+c0742*x** 742+c0743*x** 743+
c0744*x** 744+c0745*x** 745+c0746*x** 746+c0747*x** 747+
c0748*x** 748+c0749*x** 749+c0750*x** 750+c0751*x** 751+
c0752*x** 752+c0753*x** 753+c0754*x** 754+c0755*x** 755+
c0756*x** 756+c0757*x** 757+c0758*x** 758+c0759*x** 759+
c0760*x** 760+c0761*x** 761+c0762*x** 762+c0763*x** 763+
c0764*x** 764+c0765*x** 765+c0766*x** 766+c0767*x** 767+
c0768*x** 768+c0769*x** 769+c0770*x** 770+c0771*x** 771+
c0772*x** 772+c0773*x** 773+c0774*x** 774+c0775*x** 775+
c0776*x** 776+c0777*x** 777+c0778*x** 778+c0779*x** 779+
c0780*x** 780+c0781*x** 781+c0782*x** 782+c0783*x** 783+
c0784*x** 784+c0785*x** 785+c0786*x** 786+c0787*x** 787+
c0788*x** 788+c0789*x** 789+c0790*x** 790+c0791*x** 791+
c0792*x** 792+c0793*x** 793+c0794*x** 794+c0795*x** 795+
c0796*x** 796+c0797*x** 797+c0798*x** 798+c0799*x** 799+
c0800*x** 800+c0801*x** 801+c0802*x** 802+c0803*x** 803+
c0804*x** 804+c0805*x** 805+c0806*x** 806+c0807*x** 807+
c0808*x** 808+c0809*x** 809+c0810*x** 810+c0811*x** 811+
c0812*x** 812+c0813*x** 813+c0814*x** 814+c0815*x** 815+
c0816*x** 816+c0817*x** 817+c0818*x** 818+c0819*x** 819+
c0820*x** 820+c0821*x** 821+c0822*x** 822+c0823*x** 823+
c0824*x** 824+c0825*x** 825+c0826*x** 826+c0827*x** 827+
c0828*x** 828+c0829*x** 829+c0830*x** 830+c0831*x** 831+
c0832*x** 832+c0833*x** 833+c0834*x** 834+c0835*x** 835+
c0836*x** 836+c0837*x** 837+c0838*x** 838+c0839*x** 839+
c0840*x** 840+c0841*x** 841+c0842*x** 842+c0843*x** 843+
c0844*x** 844+c0845*x** 845+c0846*x** 846+c0847*x** 847+
c0848*x** 848+c0849*x** 849+c0850*x** 850+c0851*x** 851+
c0852*x** 852+c0853*x** 853+c0854*x** 854+c0855*x** 855+
c0856*x** 856+c0857*x** 857+c0858*x** 858+c0859*x** 859+
c0860*x** 860+c0861*x** 861+c0862*x** 862+c0863*x** 863+
c0864*x** 864+c0865*x** 865+c0866*x** 866+c0867*x** 867+
c0868*x** 868+c0869*x** 869+c0870*x** 870+c0871*x** 871+
c0872*x** 872+c0873*x** 873+c0874*x** 874+c0875*x** 875+
c0876*x** 876+c0877*x** 877+c0878*x** 878+c0879*x** 879+
c0880*x** 880+c0881*x** 881+c0882*x** 882+c0883*x** 883+
c0884*x** 884+c0885*x** 885+c0886*x** 886+c0887*x** 887+
c0888*x** 888+c0889*x** 889+c0890*x** 890+c0891*x** 891+
c0892*x** 892+c0893*x** 893+c0894*x** 894+c0895*x** 895+
c0896*x** 896+c0897*x** 897+c0898*x** 898+c0899*x** 899+
c0900*x** 900+c0901*x** 901+c0902*x** 902+c0903*x** 903+
c0904*x** 904+c0905*x** 905+c0906*x** 906+c0907*x** 907+
c0908*x** 908+c0909*x** 909+c0910*x** 910+c0911*x** 911+
c0912*x** 912+c0913*x** 913+c0914*x** 914+c0915*x** 915+
c0916*x** 916+c0917*x** 917+c0918*x** 918+c0919*x** 919+
c0920*x** 920+c0921*x** 921+c0922*x** 922+c0923*x** 923+
c0924*x** 924+c0925*x** 925+c0926*x** 926+c0927*x** 927+
c0928*x** 928+c0929*x** 929+c0930*x** 930+c0931*x** 931+
c0932*x** 932+c0933*x** 933+c0934*x** 934+c0935*x** 935+
c0936*x** 936+c0937*x** 937+c0938*x** 938+c0939*x** 939+
c0940*x** 940+c0941*x** 941+c0942*x** 942+c0943*x** 943+
c0944*x** 944+c0945*x** 945+c0946*x** 946+c0947*x** 947+
c0948*x** 948+c0949*x** 949+c0950*x** 950+c0951*x** 951+
c0952*x** 952+c0953*x** 953+c0954*x** 954+c0955*x** 955+
c0956*x** 956+c0957*x** 957+c0958*x** 958+c0959*x** 959+
c0960*x** 960+c0961*x** 961+c0962*x** 962+c0963*x** 963+
c0964*x** 964+c0965*x** 965+c0966*x** 966+c0967*x** 967+
c0968*x** 968+c0969*x** 969+c0970*x** 970+c0971*x** 971+
c0972*x** 972+c0973*x** 973+c0974*x** 974+c0975*x** 975+
c0976*x** 976+c0977*x** 977+c0978*x** 978+c0979*x** 979+
c0980*x** 980+c0981*x** 981+c0982*x** 982+c0983*x** 983+
c0984*x** 984+c0985*x** 985+c0986*x** 986+c0987*x** 987+
c0988*x** 988+c0989*x** 989+c0990*x** 990+c0991*x** 991+
c0992*x** 992+c0993*x** 993+c0994*x** 994+c0995*x** 995+
c0996*x** 996+c0997*x** 997+c0998*x** 998+c0999*x** 999+
c1000*x**1000+c1001*x**1001+c1002*x**1002+c1003*x**1003+
c1004*x**1004+c1005*x**1005+c1006*x**1006+c1007*x**1007+
c1008*x**1008+c1009*x**1009+c1010*x**1010+c1011*x**1011+
c1012*x**1012+c1013*x**1013+c1014*x**1014+c1015*x**1015+
c1016*x**1016+c1017*x**1017+c1018*x**1018+c1019*x**1019+
c1020*x**1020+c1021*x**1021+c1022*x**1022+c1023*x**1023
&& true;



(******************** CUT 523 ********************)


ecut eqmod cpoly 4*segment0 [ 1043969, x**256 - 1 ]
prove with [ all ghosts, cuts [ 0, 519 ] ];



(******************** CUT 524 ********************)


ecut and [
eqmod (256*inp_poly00**2) cpoly [1043969, x**4 - 1],
eqmod (256*inp_poly01**2) cpoly [1043969, x**4 - 1043968],
eqmod (256*inp_poly02**2) cpoly [1043969, x**4 - 554923],
eqmod (256*inp_poly03**2) cpoly [1043969, x**4 - 489046],
eqmod (256*inp_poly04**2) cpoly [1043969, x**4 - 287998],
eqmod (256*inp_poly05**2) cpoly [1043969, x**4 - 755971],
eqmod (256*inp_poly06**2) cpoly [1043969, x**4 - 719789],
eqmod (256*inp_poly07**2) cpoly [1043969, x**4 - 324180],
eqmod (256*inp_poly08**2) cpoly [1043969, x**4 - 29512],
eqmod (256*inp_poly09**2) cpoly [1043969, x**4 - 1014457],
eqmod (256*inp_poly0a**2) cpoly [1043969, x**4 - 145873],
eqmod (256*inp_poly0b**2) cpoly [1043969, x**4 - 898096],
eqmod (256*inp_poly0c**2) cpoly [1043969, x**4 - 445347],
eqmod (256*inp_poly0d**2) cpoly [1043969, x**4 - 598622],
eqmod (256*inp_poly0e**2) cpoly [1043969, x**4 - 775725],
eqmod (256*inp_poly0f**2) cpoly [1043969, x**4 - 268244],
eqmod (256*inp_poly10**2) cpoly [1043969, x**4 - 754540],
eqmod (256*inp_poly11**2) cpoly [1043969, x**4 - 289429],
eqmod (256*inp_poly12**2) cpoly [1043969, x**4 - 689776],
eqmod (256*inp_poly13**2) cpoly [1043969, x**4 - 354193],
eqmod (256*inp_poly14**2) cpoly [1043969, x**4 - 731663],
eqmod (256*inp_poly15**2) cpoly [1043969, x**4 - 312306],
eqmod (256*inp_poly16**2) cpoly [1043969, x**4 - 379345],
eqmod (256*inp_poly17**2) cpoly [1043969, x**4 - 664624],
eqmod (256*inp_poly18**2) cpoly [1043969, x**4 - 125710],
eqmod (256*inp_poly19**2) cpoly [1043969, x**4 - 918259],
eqmod (256*inp_poly1a**2) cpoly [1043969, x**4 - 317781],
eqmod (256*inp_poly1b**2) cpoly [1043969, x**4 - 726188],
eqmod (256*inp_poly1c**2) cpoly [1043969, x**4 - 427629],
eqmod (256*inp_poly1d**2) cpoly [1043969, x**4 - 616340],
eqmod (256*inp_poly1e**2) cpoly [1043969, x**4 - 750053],
eqmod (256*inp_poly1f**2) cpoly [1043969, x**4 - 293916],
eqmod (256*inp_poly20**2) cpoly [1043969, x**4 - 587782],
eqmod (256*inp_poly21**2) cpoly [1043969, x**4 - 456187],
eqmod (256*inp_poly22**2) cpoly [1043969, x**4 - 252302],
eqmod (256*inp_poly23**2) cpoly [1043969, x**4 - 791667],
eqmod (256*inp_poly24**2) cpoly [1043969, x**4 - 467086],
eqmod (256*inp_poly25**2) cpoly [1043969, x**4 - 576883],
eqmod (256*inp_poly26**2) cpoly [1043969, x**4 - 141058],
eqmod (256*inp_poly27**2) cpoly [1043969, x**4 - 902911],
eqmod (256*inp_poly28**2) cpoly [1043969, x**4 - 33480],
eqmod (256*inp_poly29**2) cpoly [1043969, x**4 - 1010489],
eqmod (256*inp_poly2a**2) cpoly [1043969, x**4 - 349716],
eqmod (256*inp_poly2b**2) cpoly [1043969, x**4 - 694253],
eqmod (256*inp_poly2c**2) cpoly [1043969, x**4 - 75356],
eqmod (256*inp_poly2d**2) cpoly [1043969, x**4 - 968613],
eqmod (256*inp_poly2e**2) cpoly [1043969, x**4 - 599293],
eqmod (256*inp_poly2f**2) cpoly [1043969, x**4 - 444676],
eqmod (256*inp_poly30**2) cpoly [1043969, x**4 - 899855],
eqmod (256*inp_poly31**2) cpoly [1043969, x**4 - 144114],
eqmod (256*inp_poly32**2) cpoly [1043969, x**4 - 28054],
eqmod (256*inp_poly33**2) cpoly [1043969, x**4 - 1015915],
eqmod (256*inp_poly34**2) cpoly [1043969, x**4 - 531761],
eqmod (256*inp_poly35**2) cpoly [1043969, x**4 - 512208],
eqmod (256*inp_poly36**2) cpoly [1043969, x**4 - 219801],
eqmod (256*inp_poly37**2) cpoly [1043969, x**4 - 824168],
eqmod (256*inp_poly38**2) cpoly [1043969, x**4 - 37338],
eqmod (256*inp_poly39**2) cpoly [1043969, x**4 - 1006631],
eqmod (256*inp_poly3a**2) cpoly [1043969, x**4 - 62231],
eqmod (256*inp_poly3b**2) cpoly [1043969, x**4 - 981738],
eqmod (256*inp_poly3c**2) cpoly [1043969, x**4 - 388624],
eqmod (256*inp_poly3d**2) cpoly [1043969, x**4 - 655345],
eqmod (256*inp_poly3e**2) cpoly [1043969, x**4 - 587715],
eqmod (256*inp_poly3f**2) cpoly [1043969, x**4 - 456254]
] prove with [ cuts [ 1 ] ];



(******************** CUT 525 ********************)


ecut true;



(******************** CUT 526 ********************)


ecut eqmod cpoly 4*segment1 [ 1043969, x**256 + 1 ]
prove with [ all ghosts, cuts [ 2, 520 ] ];



(******************** CUT 527 ********************)


ecut and [
eqmod (256*inp_poly40**2) cpoly [1043969, x**4 - 1013205],
eqmod (256*inp_poly41**2) cpoly [1043969, x**4 - 30764],
eqmod (256*inp_poly42**2) cpoly [1043969, x**4 - 373885],
eqmod (256*inp_poly43**2) cpoly [1043969, x**4 - 670084],
eqmod (256*inp_poly44**2) cpoly [1043969, x**4 - 194431],
eqmod (256*inp_poly45**2) cpoly [1043969, x**4 - 849538],
eqmod (256*inp_poly46**2) cpoly [1043969, x**4 - 37663],
eqmod (256*inp_poly47**2) cpoly [1043969, x**4 - 1006306],
eqmod (256*inp_poly48**2) cpoly [1043969, x**4 - 345862],
eqmod (256*inp_poly49**2) cpoly [1043969, x**4 - 698107],
eqmod (256*inp_poly4a**2) cpoly [1043969, x**4 - 385759],
eqmod (256*inp_poly4b**2) cpoly [1043969, x**4 - 658210],
eqmod (256*inp_poly4c**2) cpoly [1043969, x**4 - 394048],
eqmod (256*inp_poly4d**2) cpoly [1043969, x**4 - 649921],
eqmod (256*inp_poly4e**2) cpoly [1043969, x**4 - 727440],
eqmod (256*inp_poly4f**2) cpoly [1043969, x**4 - 316529],
eqmod (256*inp_poly50**2) cpoly [1043969, x**4 - 1026124],
eqmod (256*inp_poly51**2) cpoly [1043969, x**4 - 17845],
eqmod (256*inp_poly52**2) cpoly [1043969, x**4 - 488999],
eqmod (256*inp_poly53**2) cpoly [1043969, x**4 - 554970],
eqmod (256*inp_poly54**2) cpoly [1043969, x**4 - 135077],
eqmod (256*inp_poly55**2) cpoly [1043969, x**4 - 908892],
eqmod (256*inp_poly56**2) cpoly [1043969, x**4 - 359871],
eqmod (256*inp_poly57**2) cpoly [1043969, x**4 - 684098],
eqmod (256*inp_poly58**2) cpoly [1043969, x**4 - 562705],
eqmod (256*inp_poly59**2) cpoly [1043969, x**4 - 481264],
eqmod (256*inp_poly5a**2) cpoly [1043969, x**4 - 555001],
eqmod (256*inp_poly5b**2) cpoly [1043969, x**4 - 488968],
eqmod (256*inp_poly5c**2) cpoly [1043969, x**4 - 518782],
eqmod (256*inp_poly5d**2) cpoly [1043969, x**4 - 525187],
eqmod (256*inp_poly5e**2) cpoly [1043969, x**4 - 216315],
eqmod (256*inp_poly5f**2) cpoly [1043969, x**4 - 827654],
eqmod (256*inp_poly60**2) cpoly [1043969, x**4 - 61601],
eqmod (256*inp_poly61**2) cpoly [1043969, x**4 - 982368],
eqmod (256*inp_poly62**2) cpoly [1043969, x**4 - 90787],
eqmod (256*inp_poly63**2) cpoly [1043969, x**4 - 953182],
eqmod (256*inp_poly64**2) cpoly [1043969, x**4 - 799581],
eqmod (256*inp_poly65**2) cpoly [1043969, x**4 - 244388],
eqmod (256*inp_poly66**2) cpoly [1043969, x**4 - 270821],
eqmod (256*inp_poly67**2) cpoly [1043969, x**4 - 773148],
eqmod (256*inp_poly68**2) cpoly [1043969, x**4 - 418683],
eqmod (256*inp_poly69**2) cpoly [1043969, x**4 - 625286],
eqmod (256*inp_poly6a**2) cpoly [1043969, x**4 - 481490],
eqmod (256*inp_poly6b**2) cpoly [1043969, x**4 - 562479],
eqmod (256*inp_poly6c**2) cpoly [1043969, x**4 - 403165],
eqmod (256*inp_poly6d**2) cpoly [1043969, x**4 - 640804],
eqmod (256*inp_poly6e**2) cpoly [1043969, x**4 - 886657],
eqmod (256*inp_poly6f**2) cpoly [1043969, x**4 - 157312],
eqmod (256*inp_poly70**2) cpoly [1043969, x**4 - 830722],
eqmod (256*inp_poly71**2) cpoly [1043969, x**4 - 213247],
eqmod (256*inp_poly72**2) cpoly [1043969, x**4 - 309107],
eqmod (256*inp_poly73**2) cpoly [1043969, x**4 - 734862],
eqmod (256*inp_poly74**2) cpoly [1043969, x**4 - 942795],
eqmod (256*inp_poly75**2) cpoly [1043969, x**4 - 101174],
eqmod (256*inp_poly76**2) cpoly [1043969, x**4 - 873218],
eqmod (256*inp_poly77**2) cpoly [1043969, x**4 - 170751],
eqmod (256*inp_poly78**2) cpoly [1043969, x**4 - 743637],
eqmod (256*inp_poly79**2) cpoly [1043969, x**4 - 300332],
eqmod (256*inp_poly7a**2) cpoly [1043969, x**4 - 164662],
eqmod (256*inp_poly7b**2) cpoly [1043969, x**4 - 879307],
eqmod (256*inp_poly7c**2) cpoly [1043969, x**4 - 948221],
eqmod (256*inp_poly7d**2) cpoly [1043969, x**4 - 95748],
eqmod (256*inp_poly7e**2) cpoly [1043969, x**4 - 34851],
eqmod (256*inp_poly7f**2) cpoly [1043969, x**4 - 1009118]
] prove with [ cuts [ 3 ] ];



(******************** CUT 528 ********************)


ecut true;



(******************** CUT 529 ********************)


ecut eqmod cpoly 4*segment2 [ 1043969, x**256 - 554923 ]
prove with [ all ghosts, cuts [ 4, 521 ] ];



(******************** CUT 530 ********************)


ecut and [
eqmod (256*inp_poly80**2) cpoly [1043969, x**4 - 941631],
eqmod (256*inp_poly81**2) cpoly [1043969, x**4 - 102338],
eqmod (256*inp_poly82**2) cpoly [1043969, x**4 - 115688],
eqmod (256*inp_poly83**2) cpoly [1043969, x**4 - 928281],
eqmod (256*inp_poly84**2) cpoly [1043969, x**4 - 193484],
eqmod (256*inp_poly85**2) cpoly [1043969, x**4 - 850485],
eqmod (256*inp_poly86**2) cpoly [1043969, x**4 - 685958],
eqmod (256*inp_poly87**2) cpoly [1043969, x**4 - 358011],
eqmod (256*inp_poly88**2) cpoly [1043969, x**4 - 3261],
eqmod (256*inp_poly89**2) cpoly [1043969, x**4 - 1040708],
eqmod (256*inp_poly8a**2) cpoly [1043969, x**4 - 405626],
eqmod (256*inp_poly8b**2) cpoly [1043969, x**4 - 638343],
eqmod (256*inp_poly8c**2) cpoly [1043969, x**4 - 633347],
eqmod (256*inp_poly8d**2) cpoly [1043969, x**4 - 410622],
eqmod (256*inp_poly8e**2) cpoly [1043969, x**4 - 389617],
eqmod (256*inp_poly8f**2) cpoly [1043969, x**4 - 654352],
eqmod (256*inp_poly90**2) cpoly [1043969, x**4 - 96534],
eqmod (256*inp_poly91**2) cpoly [1043969, x**4 - 947435],
eqmod (256*inp_poly92**2) cpoly [1043969, x**4 - 799554],
eqmod (256*inp_poly93**2) cpoly [1043969, x**4 - 244415],
eqmod (256*inp_poly94**2) cpoly [1043969, x**4 - 704462],
eqmod (256*inp_poly95**2) cpoly [1043969, x**4 - 339507],
eqmod (256*inp_poly96**2) cpoly [1043969, x**4 - 666593],
eqmod (256*inp_poly97**2) cpoly [1043969, x**4 - 377376],
eqmod (256*inp_poly98**2) cpoly [1043969, x**4 - 963976],
eqmod (256*inp_poly99**2) cpoly [1043969, x**4 - 79993],
eqmod (256*inp_poly9a**2) cpoly [1043969, x**4 - 650310],
eqmod (256*inp_poly9b**2) cpoly [1043969, x**4 - 393659],
eqmod (256*inp_poly9c**2) cpoly [1043969, x**4 - 483878],
eqmod (256*inp_poly9d**2) cpoly [1043969, x**4 - 560091],
eqmod (256*inp_poly9e**2) cpoly [1043969, x**4 - 984749],
eqmod (256*inp_poly9f**2) cpoly [1043969, x**4 - 59220],
eqmod (256*inp_polya0**2) cpoly [1043969, x**4 - 15495],
eqmod (256*inp_polya1**2) cpoly [1043969, x**4 - 1028474],
eqmod (256*inp_polya2**2) cpoly [1043969, x**4 - 403201],
eqmod (256*inp_polya3**2) cpoly [1043969, x**4 - 640768],
eqmod (256*inp_polya4**2) cpoly [1043969, x**4 - 605504],
eqmod (256*inp_polya5**2) cpoly [1043969, x**4 - 438465],
eqmod (256*inp_polya6**2) cpoly [1043969, x**4 - 409728],
eqmod (256*inp_polya7**2) cpoly [1043969, x**4 - 634241],
eqmod (256*inp_polya8**2) cpoly [1043969, x**4 - 30018],
eqmod (256*inp_polya9**2) cpoly [1043969, x**4 - 1013951],
eqmod (256*inp_polyaa**2) cpoly [1043969, x**4 - 109250],
eqmod (256*inp_polyab**2) cpoly [1043969, x**4 - 934719],
eqmod (256*inp_polyac**2) cpoly [1043969, x**4 - 16675],
eqmod (256*inp_polyad**2) cpoly [1043969, x**4 - 1027294],
eqmod (256*inp_polyae**2) cpoly [1043969, x**4 - 643778],
eqmod (256*inp_polyaf**2) cpoly [1043969, x**4 - 400191],
eqmod (256*inp_polyb0**2) cpoly [1043969, x**4 - 188469],
eqmod (256*inp_polyb1**2) cpoly [1043969, x**4 - 855500],
eqmod (256*inp_polyb2**2) cpoly [1043969, x**4 - 968467],
eqmod (256*inp_polyb3**2) cpoly [1043969, x**4 - 75502],
eqmod (256*inp_polyb4**2) cpoly [1043969, x**4 - 658814],
eqmod (256*inp_polyb5**2) cpoly [1043969, x**4 - 385155],
eqmod (256*inp_polyb6**2) cpoly [1043969, x**4 - 405305],
eqmod (256*inp_polyb7**2) cpoly [1043969, x**4 - 638664],
eqmod (256*inp_polyb8**2) cpoly [1043969, x**4 - 874265],
eqmod (256*inp_polyb9**2) cpoly [1043969, x**4 - 169704],
eqmod (256*inp_polyba**2) cpoly [1043969, x**4 - 658791],
eqmod (256*inp_polybb**2) cpoly [1043969, x**4 - 385178],
eqmod (256*inp_polybc**2) cpoly [1043969, x**4 - 40112],
eqmod (256*inp_polybd**2) cpoly [1043969, x**4 - 1003857],
eqmod (256*inp_polybe**2) cpoly [1043969, x**4 - 608327],
eqmod (256*inp_polybf**2) cpoly [1043969, x**4 - 435642]
] prove with [ cuts [ 5 ] ];



(******************** CUT 531 ********************)


ecut true;



(******************** CUT 532 ********************)


ecut eqmod cpoly 4*segment3 [ 1043969, x**256 + 554923 ]
prove with [ all ghosts, cuts [ 6, 522 ] ];



(******************** CUT 533 ********************)


ecut and [
eqmod (256*inp_polyc0**2) cpoly [1043969, x**4 - 759697],
eqmod (256*inp_polyc1**2) cpoly [1043969, x**4 - 284272],
eqmod (256*inp_polyc2**2) cpoly [1043969, x**4 - 908658],
eqmod (256*inp_polyc3**2) cpoly [1043969, x**4 - 135311],
eqmod (256*inp_polyc4**2) cpoly [1043969, x**4 - 369462],
eqmod (256*inp_polyc5**2) cpoly [1043969, x**4 - 674507],
eqmod (256*inp_polyc6**2) cpoly [1043969, x**4 - 1021423],
eqmod (256*inp_polyc7**2) cpoly [1043969, x**4 - 22546],
eqmod (256*inp_polyc8**2) cpoly [1043969, x**4 - 943589],
eqmod (256*inp_polyc9**2) cpoly [1043969, x**4 - 100380],
eqmod (256*inp_polyca**2) cpoly [1043969, x**4 - 927162],
eqmod (256*inp_polycb**2) cpoly [1043969, x**4 - 116807],
eqmod (256*inp_polycc**2) cpoly [1043969, x**4 - 350308],
eqmod (256*inp_polycd**2) cpoly [1043969, x**4 - 693661],
eqmod (256*inp_polyce**2) cpoly [1043969, x**4 - 674670],
eqmod (256*inp_polycf**2) cpoly [1043969, x**4 - 369299],
eqmod (256*inp_polyd0**2) cpoly [1043969, x**4 - 319829],
eqmod (256*inp_polyd1**2) cpoly [1043969, x**4 - 724140],
eqmod (256*inp_polyd2**2) cpoly [1043969, x**4 - 518322],
eqmod (256*inp_polyd3**2) cpoly [1043969, x**4 - 525647],
eqmod (256*inp_polyd4**2) cpoly [1043969, x**4 - 727472],
eqmod (256*inp_polyd5**2) cpoly [1043969, x**4 - 316497],
eqmod (256*inp_polyd6**2) cpoly [1043969, x**4 - 659984],
eqmod (256*inp_polyd7**2) cpoly [1043969, x**4 - 383985],
eqmod (256*inp_polyd8**2) cpoly [1043969, x**4 - 269719],
eqmod (256*inp_polyd9**2) cpoly [1043969, x**4 - 774250],
eqmod (256*inp_polyda**2) cpoly [1043969, x**4 - 485076],
eqmod (256*inp_polydb**2) cpoly [1043969, x**4 - 558893],
eqmod (256*inp_polydc**2) cpoly [1043969, x**4 - 975148],
eqmod (256*inp_polydd**2) cpoly [1043969, x**4 - 68821],
eqmod (256*inp_polyde**2) cpoly [1043969, x**4 - 118175],
eqmod (256*inp_polydf**2) cpoly [1043969, x**4 - 925794],
eqmod (256*inp_polye0**2) cpoly [1043969, x**4 - 405653],
eqmod (256*inp_polye1**2) cpoly [1043969, x**4 - 638316],
eqmod (256*inp_polye2**2) cpoly [1043969, x**4 - 364094],
eqmod (256*inp_polye3**2) cpoly [1043969, x**4 - 679875],
eqmod (256*inp_polye4**2) cpoly [1043969, x**4 - 857780],
eqmod (256*inp_polye5**2) cpoly [1043969, x**4 - 186189],
eqmod (256*inp_polye6**2) cpoly [1043969, x**4 - 9514],
eqmod (256*inp_polye7**2) cpoly [1043969, x**4 - 1034455],
eqmod (256*inp_polye8**2) cpoly [1043969, x**4 - 438813],
eqmod (256*inp_polye9**2) cpoly [1043969, x**4 - 605156],
eqmod (256*inp_polyea**2) cpoly [1043969, x**4 - 613180],
eqmod (256*inp_polyeb**2) cpoly [1043969, x**4 - 430789],
eqmod (256*inp_polyec**2) cpoly [1043969, x**4 - 643048],
eqmod (256*inp_polyed**2) cpoly [1043969, x**4 - 400921],
eqmod (256*inp_polyee**2) cpoly [1043969, x**4 - 993476],
eqmod (256*inp_polyef**2) cpoly [1043969, x**4 - 50493],
eqmod (256*inp_polyf0**2) cpoly [1043969, x**4 - 143510],
eqmod (256*inp_polyf1**2) cpoly [1043969, x**4 - 900459],
eqmod (256*inp_polyf2**2) cpoly [1043969, x**4 - 956472],
eqmod (256*inp_polyf3**2) cpoly [1043969, x**4 - 87497],
eqmod (256*inp_polyf4**2) cpoly [1043969, x**4 - 904239],
eqmod (256*inp_polyf5**2) cpoly [1043969, x**4 - 139730],
eqmod (256*inp_polyf6**2) cpoly [1043969, x**4 - 362716],
eqmod (256*inp_polyf7**2) cpoly [1043969, x**4 - 681253],
eqmod (256*inp_polyf8**2) cpoly [1043969, x**4 - 928856],
eqmod (256*inp_polyf9**2) cpoly [1043969, x**4 - 115113],
eqmod (256*inp_polyfa**2) cpoly [1043969, x**4 - 567842],
eqmod (256*inp_polyfb**2) cpoly [1043969, x**4 - 476127],
eqmod (256*inp_polyfc**2) cpoly [1043969, x**4 - 1009759],
eqmod (256*inp_polyfd**2) cpoly [1043969, x**4 - 34210],
eqmod (256*inp_polyfe**2) cpoly [1043969, x**4 - 660435],
eqmod (256*inp_polyff**2) cpoly [1043969, x**4 - 383534]
] prove with [ cuts [ 7 ] ];



(******************** CUT 534 ********************)

ecut and [
c1017 = 0, c1018 = 0, c1019 = 0, c1020 = 0,
c1021 = 0, c1022 = 0, c1023 = 0
] prove with [ cuts [ 504, 506, 508, 510, 512, 514, 516 ] ];


(**************** output cf poly ****************)

ghost cfpoly@bit : cfpoly =
cf000*x**  0+cf001*x**  1+cf002*x**  2+cf003*x**  3+
cf004*x**  4+cf005*x**  5+cf006*x**  6+cf007*x**  7+
cf008*x**  8+cf009*x**  9+cf010*x** 10+cf011*x** 11+
cf012*x** 12+cf013*x** 13+cf014*x** 14+cf015*x** 15+
cf016*x** 16+cf017*x** 17+cf018*x** 18+cf019*x** 19+
cf020*x** 20+cf021*x** 21+cf022*x** 22+cf023*x** 23+
cf024*x** 24+cf025*x** 25+cf026*x** 26+cf027*x** 27+
cf028*x** 28+cf029*x** 29+cf030*x** 30+cf031*x** 31+
cf032*x** 32+cf033*x** 33+cf034*x** 34+cf035*x** 35+
cf036*x** 36+cf037*x** 37+cf038*x** 38+cf039*x** 39+
cf040*x** 40+cf041*x** 41+cf042*x** 42+cf043*x** 43+
cf044*x** 44+cf045*x** 45+cf046*x** 46+cf047*x** 47+
cf048*x** 48+cf049*x** 49+cf050*x** 50+cf051*x** 51+
cf052*x** 52+cf053*x** 53+cf054*x** 54+cf055*x** 55+
cf056*x** 56+cf057*x** 57+cf058*x** 58+cf059*x** 59+
cf060*x** 60+cf061*x** 61+cf062*x** 62+cf063*x** 63+
cf064*x** 64+cf065*x** 65+cf066*x** 66+cf067*x** 67+
cf068*x** 68+cf069*x** 69+cf070*x** 70+cf071*x** 71+
cf072*x** 72+cf073*x** 73+cf074*x** 74+cf075*x** 75+
cf076*x** 76+cf077*x** 77+cf078*x** 78+cf079*x** 79+
cf080*x** 80+cf081*x** 81+cf082*x** 82+cf083*x** 83+
cf084*x** 84+cf085*x** 85+cf086*x** 86+cf087*x** 87+
cf088*x** 88+cf089*x** 89+cf090*x** 90+cf091*x** 91+
cf092*x** 92+cf093*x** 93+cf094*x** 94+cf095*x** 95+
cf096*x** 96+cf097*x** 97+cf098*x** 98+cf099*x** 99+
cf100*x**100+cf101*x**101+cf102*x**102+cf103*x**103+
cf104*x**104+cf105*x**105+cf106*x**106+cf107*x**107+
cf108*x**108+cf109*x**109+cf110*x**110+cf111*x**111+
cf112*x**112+cf113*x**113+cf114*x**114+cf115*x**115+
cf116*x**116+cf117*x**117+cf118*x**118+cf119*x**119+
cf120*x**120+cf121*x**121+cf122*x**122+cf123*x**123+
cf124*x**124+cf125*x**125+cf126*x**126+cf127*x**127+
cf128*x**128+cf129*x**129+cf130*x**130+cf131*x**131+
cf132*x**132+cf133*x**133+cf134*x**134+cf135*x**135+
cf136*x**136+cf137*x**137+cf138*x**138+cf139*x**139+
cf140*x**140+cf141*x**141+cf142*x**142+cf143*x**143+
cf144*x**144+cf145*x**145+cf146*x**146+cf147*x**147+
cf148*x**148+cf149*x**149+cf150*x**150+cf151*x**151+
cf152*x**152+cf153*x**153+cf154*x**154+cf155*x**155+
cf156*x**156+cf157*x**157+cf158*x**158+cf159*x**159+
cf160*x**160+cf161*x**161+cf162*x**162+cf163*x**163+
cf164*x**164+cf165*x**165+cf166*x**166+cf167*x**167+
cf168*x**168+cf169*x**169+cf170*x**170+cf171*x**171+
cf172*x**172+cf173*x**173+cf174*x**174+cf175*x**175+
cf176*x**176+cf177*x**177+cf178*x**178+cf179*x**179+
cf180*x**180+cf181*x**181+cf182*x**182+cf183*x**183+
cf184*x**184+cf185*x**185+cf186*x**186+cf187*x**187+
cf188*x**188+cf189*x**189+cf190*x**190+cf191*x**191+
cf192*x**192+cf193*x**193+cf194*x**194+cf195*x**195+
cf196*x**196+cf197*x**197+cf198*x**198+cf199*x**199+
cf200*x**200+cf201*x**201+cf202*x**202+cf203*x**203+
cf204*x**204+cf205*x**205+cf206*x**206+cf207*x**207+
cf208*x**208+cf209*x**209+cf210*x**210+cf211*x**211+
cf212*x**212+cf213*x**213+cf214*x**214+cf215*x**215+
cf216*x**216+cf217*x**217+cf218*x**218+cf219*x**219+
cf220*x**220+cf221*x**221+cf222*x**222+cf223*x**223+
cf224*x**224+cf225*x**225+cf226*x**226+cf227*x**227+
cf228*x**228+cf229*x**229+cf230*x**230+cf231*x**231+
cf232*x**232+cf233*x**233+cf234*x**234+cf235*x**235+
cf236*x**236+cf237*x**237+cf238*x**238+cf239*x**239+
cf240*x**240+cf241*x**241+cf242*x**242+cf243*x**243+
cf244*x**244+cf245*x**245+cf246*x**246+cf247*x**247+
cf248*x**248+cf249*x**249+cf250*x**250+cf251*x**251+
cf252*x**252+cf253*x**253+cf254*x**254+cf255*x**255+
cf256*x**256+cf257*x**257+cf258*x**258+cf259*x**259+
cf260*x**260+cf261*x**261+cf262*x**262+cf263*x**263+
cf264*x**264+cf265*x**265+cf266*x**266+cf267*x**267+
cf268*x**268+cf269*x**269+cf270*x**270+cf271*x**271+
cf272*x**272+cf273*x**273+cf274*x**274+cf275*x**275+
cf276*x**276+cf277*x**277+cf278*x**278+cf279*x**279+
cf280*x**280+cf281*x**281+cf282*x**282+cf283*x**283+
cf284*x**284+cf285*x**285+cf286*x**286+cf287*x**287+
cf288*x**288+cf289*x**289+cf290*x**290+cf291*x**291+
cf292*x**292+cf293*x**293+cf294*x**294+cf295*x**295+
cf296*x**296+cf297*x**297+cf298*x**298+cf299*x**299+
cf300*x**300+cf301*x**301+cf302*x**302+cf303*x**303+
cf304*x**304+cf305*x**305+cf306*x**306+cf307*x**307+
cf308*x**308+cf309*x**309+cf310*x**310+cf311*x**311+
cf312*x**312+cf313*x**313+cf314*x**314+cf315*x**315+
cf316*x**316+cf317*x**317+cf318*x**318+cf319*x**319+
cf320*x**320+cf321*x**321+cf322*x**322+cf323*x**323+
cf324*x**324+cf325*x**325+cf326*x**326+cf327*x**327+
cf328*x**328+cf329*x**329+cf330*x**330+cf331*x**331+
cf332*x**332+cf333*x**333+cf334*x**334+cf335*x**335+
cf336*x**336+cf337*x**337+cf338*x**338+cf339*x**339+
cf340*x**340+cf341*x**341+cf342*x**342+cf343*x**343+
cf344*x**344+cf345*x**345+cf346*x**346+cf347*x**347+
cf348*x**348+cf349*x**349+cf350*x**350+cf351*x**351+
cf352*x**352+cf353*x**353+cf354*x**354+cf355*x**355+
cf356*x**356+cf357*x**357+cf358*x**358+cf359*x**359+
cf360*x**360+cf361*x**361+cf362*x**362+cf363*x**363+
cf364*x**364+cf365*x**365+cf366*x**366+cf367*x**367+
cf368*x**368+cf369*x**369+cf370*x**370+cf371*x**371+
cf372*x**372+cf373*x**373+cf374*x**374+cf375*x**375+
cf376*x**376+cf377*x**377+cf378*x**378+cf379*x**379+
cf380*x**380+cf381*x**381+cf382*x**382+cf383*x**383+
cf384*x**384+cf385*x**385+cf386*x**386+cf387*x**387+
cf388*x**388+cf389*x**389+cf390*x**390+cf391*x**391+
cf392*x**392+cf393*x**393+cf394*x**394+cf395*x**395+
cf396*x**396+cf397*x**397+cf398*x**398+cf399*x**399+
cf400*x**400+cf401*x**401+cf402*x**402+cf403*x**403+
cf404*x**404+cf405*x**405+cf406*x**406+cf407*x**407+
cf408*x**408+cf409*x**409+cf410*x**410+cf411*x**411+
cf412*x**412+cf413*x**413+cf414*x**414+cf415*x**415+
cf416*x**416+cf417*x**417+cf418*x**418+cf419*x**419+
cf420*x**420+cf421*x**421+cf422*x**422+cf423*x**423+
cf424*x**424+cf425*x**425+cf426*x**426+cf427*x**427+
cf428*x**428+cf429*x**429+cf430*x**430+cf431*x**431+
cf432*x**432+cf433*x**433+cf434*x**434+cf435*x**435+
cf436*x**436+cf437*x**437+cf438*x**438+cf439*x**439+
cf440*x**440+cf441*x**441+cf442*x**442+cf443*x**443+
cf444*x**444+cf445*x**445+cf446*x**446+cf447*x**447+
cf448*x**448+cf449*x**449+cf450*x**450+cf451*x**451+
cf452*x**452+cf453*x**453+cf454*x**454+cf455*x**455+
cf456*x**456+cf457*x**457+cf458*x**458+cf459*x**459+
cf460*x**460+cf461*x**461+cf462*x**462+cf463*x**463+
cf464*x**464+cf465*x**465+cf466*x**466+cf467*x**467+
cf468*x**468+cf469*x**469+cf470*x**470+cf471*x**471+
cf472*x**472+cf473*x**473+cf474*x**474+cf475*x**475+
cf476*x**476+cf477*x**477+cf478*x**478+cf479*x**479+
cf480*x**480+cf481*x**481+cf482*x**482+cf483*x**483+
cf484*x**484+cf485*x**485+cf486*x**486+cf487*x**487+
cf488*x**488+cf489*x**489+cf490*x**490+cf491*x**491+
cf492*x**492+cf493*x**493+cf494*x**494+cf495*x**495+
cf496*x**496+cf497*x**497+cf498*x**498+cf499*x**499+
cf500*x**500+cf501*x**501+cf502*x**502+cf503*x**503+
cf504*x**504+cf505*x**505+cf506*x**506+cf507*x**507+
cf508*x**508
&& true;



(******************** CUT 535 ********************)

ecut eqmod 256*cfpoly 2**32*cpoly [ 1043969, x**509 - 1]
prove with [ all ghosts, cuts [
 13,  15,  17,  19,  21,  23,  25,  27,  29,  31,  33,  35,
 37,  39,  41,  43,  45,  47,  49,  51,  53,  55,  57,  59,
 61,  63,  65,  67,  69,  71,  73,  75,  77,  79,  81,  83,
 85,  87,  89,  91,  93,  95,  97,  99, 101, 103, 105, 107,
109, 111, 113, 115, 117, 119, 121, 123, 125, 127, 129, 131,
133, 135, 137, 139, 141, 143, 145, 147, 149, 151, 153, 155,
157, 159, 161, 163, 165, 167, 169, 171, 173, 175, 177, 179,
181, 183, 185, 187, 189, 191, 193, 195, 197, 199, 201, 203,
205, 207, 209, 211, 213, 215, 217, 219, 221, 223, 225, 227,
229, 231, 233, 235, 237, 239, 241, 243, 245, 247, 249, 251,
253, 255, 257, 259, 261, 263, 265, 267, 269, 271, 273, 275,
277, 279, 281, 283, 285, 287, 289, 291, 293, 295, 297, 299,
301, 303, 305, 307, 309, 311, 313, 315, 317, 319, 321, 323,
325, 327, 329, 331, 333, 335, 337, 339, 341, 343, 345, 347,
349, 351, 353, 355, 357, 359, 361, 363, 365, 367, 369, 371,
373, 375, 377, 379, 381, 383, 385, 387, 389, 391, 393, 395,
397, 399, 401, 403, 405, 407, 409, 411, 413, 415, 417, 419,
421, 423, 425, 427, 429, 431, 433, 435, 437, 439, 441, 443,
445, 447, 449, 451, 453, 455, 457, 459, 461, 463, 465, 467,
469, 471, 473, 475, 477, 479, 481, 483, 485, 487, 489, 491,
493, 495, 497, 499, 501, 503, 505, 507, 509, 511, 513, 515,
517, 518 ] ];


(**************** postcondition ****************)

{ and [
eqmod 256*cfpoly 2**32*cpoly [1043969, x**509 - 1],
eqmod (256*inp_poly00**2) cpoly [1043969, x**4 - 1],
eqmod (256*inp_poly01**2) cpoly [1043969, x**4 - 1043968],
eqmod (256*inp_poly02**2) cpoly [1043969, x**4 - 554923],
eqmod (256*inp_poly03**2) cpoly [1043969, x**4 - 489046],
eqmod (256*inp_poly04**2) cpoly [1043969, x**4 - 287998],
eqmod (256*inp_poly05**2) cpoly [1043969, x**4 - 755971],
eqmod (256*inp_poly06**2) cpoly [1043969, x**4 - 719789],
eqmod (256*inp_poly07**2) cpoly [1043969, x**4 - 324180],
eqmod (256*inp_poly08**2) cpoly [1043969, x**4 - 29512],
eqmod (256*inp_poly09**2) cpoly [1043969, x**4 - 1014457],
eqmod (256*inp_poly0a**2) cpoly [1043969, x**4 - 145873],
eqmod (256*inp_poly0b**2) cpoly [1043969, x**4 - 898096],
eqmod (256*inp_poly0c**2) cpoly [1043969, x**4 - 445347],
eqmod (256*inp_poly0d**2) cpoly [1043969, x**4 - 598622],
eqmod (256*inp_poly0e**2) cpoly [1043969, x**4 - 775725],
eqmod (256*inp_poly0f**2) cpoly [1043969, x**4 - 268244],
eqmod (256*inp_poly10**2) cpoly [1043969, x**4 - 754540],
eqmod (256*inp_poly11**2) cpoly [1043969, x**4 - 289429],
eqmod (256*inp_poly12**2) cpoly [1043969, x**4 - 689776],
eqmod (256*inp_poly13**2) cpoly [1043969, x**4 - 354193],
eqmod (256*inp_poly14**2) cpoly [1043969, x**4 - 731663],
eqmod (256*inp_poly15**2) cpoly [1043969, x**4 - 312306],
eqmod (256*inp_poly16**2) cpoly [1043969, x**4 - 379345],
eqmod (256*inp_poly17**2) cpoly [1043969, x**4 - 664624],
eqmod (256*inp_poly18**2) cpoly [1043969, x**4 - 125710],
eqmod (256*inp_poly19**2) cpoly [1043969, x**4 - 918259],
eqmod (256*inp_poly1a**2) cpoly [1043969, x**4 - 317781],
eqmod (256*inp_poly1b**2) cpoly [1043969, x**4 - 726188],
eqmod (256*inp_poly1c**2) cpoly [1043969, x**4 - 427629],
eqmod (256*inp_poly1d**2) cpoly [1043969, x**4 - 616340],
eqmod (256*inp_poly1e**2) cpoly [1043969, x**4 - 750053],
eqmod (256*inp_poly1f**2) cpoly [1043969, x**4 - 293916],
eqmod (256*inp_poly20**2) cpoly [1043969, x**4 - 587782],
eqmod (256*inp_poly21**2) cpoly [1043969, x**4 - 456187],
eqmod (256*inp_poly22**2) cpoly [1043969, x**4 - 252302],
eqmod (256*inp_poly23**2) cpoly [1043969, x**4 - 791667],
eqmod (256*inp_poly24**2) cpoly [1043969, x**4 - 467086],
eqmod (256*inp_poly25**2) cpoly [1043969, x**4 - 576883],
eqmod (256*inp_poly26**2) cpoly [1043969, x**4 - 141058],
eqmod (256*inp_poly27**2) cpoly [1043969, x**4 - 902911],
eqmod (256*inp_poly28**2) cpoly [1043969, x**4 - 33480],
eqmod (256*inp_poly29**2) cpoly [1043969, x**4 - 1010489],
eqmod (256*inp_poly2a**2) cpoly [1043969, x**4 - 349716],
eqmod (256*inp_poly2b**2) cpoly [1043969, x**4 - 694253],
eqmod (256*inp_poly2c**2) cpoly [1043969, x**4 - 75356],
eqmod (256*inp_poly2d**2) cpoly [1043969, x**4 - 968613],
eqmod (256*inp_poly2e**2) cpoly [1043969, x**4 - 599293],
eqmod (256*inp_poly2f**2) cpoly [1043969, x**4 - 444676],
eqmod (256*inp_poly30**2) cpoly [1043969, x**4 - 899855],
eqmod (256*inp_poly31**2) cpoly [1043969, x**4 - 144114],
eqmod (256*inp_poly32**2) cpoly [1043969, x**4 - 28054],
eqmod (256*inp_poly33**2) cpoly [1043969, x**4 - 1015915],
eqmod (256*inp_poly34**2) cpoly [1043969, x**4 - 531761],
eqmod (256*inp_poly35**2) cpoly [1043969, x**4 - 512208],
eqmod (256*inp_poly36**2) cpoly [1043969, x**4 - 219801],
eqmod (256*inp_poly37**2) cpoly [1043969, x**4 - 824168],
eqmod (256*inp_poly38**2) cpoly [1043969, x**4 - 37338],
eqmod (256*inp_poly39**2) cpoly [1043969, x**4 - 1006631],
eqmod (256*inp_poly3a**2) cpoly [1043969, x**4 - 62231],
eqmod (256*inp_poly3b**2) cpoly [1043969, x**4 - 981738],
eqmod (256*inp_poly3c**2) cpoly [1043969, x**4 - 388624],
eqmod (256*inp_poly3d**2) cpoly [1043969, x**4 - 655345],
eqmod (256*inp_poly3e**2) cpoly [1043969, x**4 - 587715],
eqmod (256*inp_poly3f**2) cpoly [1043969, x**4 - 456254]
,
eqmod (256*inp_poly40**2) cpoly [1043969, x**4 - 1013205],
eqmod (256*inp_poly41**2) cpoly [1043969, x**4 - 30764],
eqmod (256*inp_poly42**2) cpoly [1043969, x**4 - 373885],
eqmod (256*inp_poly43**2) cpoly [1043969, x**4 - 670084],
eqmod (256*inp_poly44**2) cpoly [1043969, x**4 - 194431],
eqmod (256*inp_poly45**2) cpoly [1043969, x**4 - 849538],
eqmod (256*inp_poly46**2) cpoly [1043969, x**4 - 37663],
eqmod (256*inp_poly47**2) cpoly [1043969, x**4 - 1006306],
eqmod (256*inp_poly48**2) cpoly [1043969, x**4 - 345862],
eqmod (256*inp_poly49**2) cpoly [1043969, x**4 - 698107],
eqmod (256*inp_poly4a**2) cpoly [1043969, x**4 - 385759],
eqmod (256*inp_poly4b**2) cpoly [1043969, x**4 - 658210],
eqmod (256*inp_poly4c**2) cpoly [1043969, x**4 - 394048],
eqmod (256*inp_poly4d**2) cpoly [1043969, x**4 - 649921],
eqmod (256*inp_poly4e**2) cpoly [1043969, x**4 - 727440],
eqmod (256*inp_poly4f**2) cpoly [1043969, x**4 - 316529],
eqmod (256*inp_poly50**2) cpoly [1043969, x**4 - 1026124],
eqmod (256*inp_poly51**2) cpoly [1043969, x**4 - 17845],
eqmod (256*inp_poly52**2) cpoly [1043969, x**4 - 488999],
eqmod (256*inp_poly53**2) cpoly [1043969, x**4 - 554970],
eqmod (256*inp_poly54**2) cpoly [1043969, x**4 - 135077],
eqmod (256*inp_poly55**2) cpoly [1043969, x**4 - 908892],
eqmod (256*inp_poly56**2) cpoly [1043969, x**4 - 359871],
eqmod (256*inp_poly57**2) cpoly [1043969, x**4 - 684098],
eqmod (256*inp_poly58**2) cpoly [1043969, x**4 - 562705],
eqmod (256*inp_poly59**2) cpoly [1043969, x**4 - 481264],
eqmod (256*inp_poly5a**2) cpoly [1043969, x**4 - 555001],
eqmod (256*inp_poly5b**2) cpoly [1043969, x**4 - 488968],
eqmod (256*inp_poly5c**2) cpoly [1043969, x**4 - 518782],
eqmod (256*inp_poly5d**2) cpoly [1043969, x**4 - 525187],
eqmod (256*inp_poly5e**2) cpoly [1043969, x**4 - 216315],
eqmod (256*inp_poly5f**2) cpoly [1043969, x**4 - 827654],
eqmod (256*inp_poly60**2) cpoly [1043969, x**4 - 61601],
eqmod (256*inp_poly61**2) cpoly [1043969, x**4 - 982368],
eqmod (256*inp_poly62**2) cpoly [1043969, x**4 - 90787],
eqmod (256*inp_poly63**2) cpoly [1043969, x**4 - 953182],
eqmod (256*inp_poly64**2) cpoly [1043969, x**4 - 799581],
eqmod (256*inp_poly65**2) cpoly [1043969, x**4 - 244388],
eqmod (256*inp_poly66**2) cpoly [1043969, x**4 - 270821],
eqmod (256*inp_poly67**2) cpoly [1043969, x**4 - 773148],
eqmod (256*inp_poly68**2) cpoly [1043969, x**4 - 418683],
eqmod (256*inp_poly69**2) cpoly [1043969, x**4 - 625286],
eqmod (256*inp_poly6a**2) cpoly [1043969, x**4 - 481490],
eqmod (256*inp_poly6b**2) cpoly [1043969, x**4 - 562479],
eqmod (256*inp_poly6c**2) cpoly [1043969, x**4 - 403165],
eqmod (256*inp_poly6d**2) cpoly [1043969, x**4 - 640804],
eqmod (256*inp_poly6e**2) cpoly [1043969, x**4 - 886657],
eqmod (256*inp_poly6f**2) cpoly [1043969, x**4 - 157312],
eqmod (256*inp_poly70**2) cpoly [1043969, x**4 - 830722],
eqmod (256*inp_poly71**2) cpoly [1043969, x**4 - 213247],
eqmod (256*inp_poly72**2) cpoly [1043969, x**4 - 309107],
eqmod (256*inp_poly73**2) cpoly [1043969, x**4 - 734862],
eqmod (256*inp_poly74**2) cpoly [1043969, x**4 - 942795],
eqmod (256*inp_poly75**2) cpoly [1043969, x**4 - 101174],
eqmod (256*inp_poly76**2) cpoly [1043969, x**4 - 873218],
eqmod (256*inp_poly77**2) cpoly [1043969, x**4 - 170751],
eqmod (256*inp_poly78**2) cpoly [1043969, x**4 - 743637],
eqmod (256*inp_poly79**2) cpoly [1043969, x**4 - 300332],
eqmod (256*inp_poly7a**2) cpoly [1043969, x**4 - 164662],
eqmod (256*inp_poly7b**2) cpoly [1043969, x**4 - 879307],
eqmod (256*inp_poly7c**2) cpoly [1043969, x**4 - 948221],
eqmod (256*inp_poly7d**2) cpoly [1043969, x**4 - 95748],
eqmod (256*inp_poly7e**2) cpoly [1043969, x**4 - 34851],
eqmod (256*inp_poly7f**2) cpoly [1043969, x**4 - 1009118]
,
eqmod (256*inp_poly80**2) cpoly [1043969, x**4 - 941631],
eqmod (256*inp_poly81**2) cpoly [1043969, x**4 - 102338],
eqmod (256*inp_poly82**2) cpoly [1043969, x**4 - 115688],
eqmod (256*inp_poly83**2) cpoly [1043969, x**4 - 928281],
eqmod (256*inp_poly84**2) cpoly [1043969, x**4 - 193484],
eqmod (256*inp_poly85**2) cpoly [1043969, x**4 - 850485],
eqmod (256*inp_poly86**2) cpoly [1043969, x**4 - 685958],
eqmod (256*inp_poly87**2) cpoly [1043969, x**4 - 358011],
eqmod (256*inp_poly88**2) cpoly [1043969, x**4 - 3261],
eqmod (256*inp_poly89**2) cpoly [1043969, x**4 - 1040708],
eqmod (256*inp_poly8a**2) cpoly [1043969, x**4 - 405626],
eqmod (256*inp_poly8b**2) cpoly [1043969, x**4 - 638343],
eqmod (256*inp_poly8c**2) cpoly [1043969, x**4 - 633347],
eqmod (256*inp_poly8d**2) cpoly [1043969, x**4 - 410622],
eqmod (256*inp_poly8e**2) cpoly [1043969, x**4 - 389617],
eqmod (256*inp_poly8f**2) cpoly [1043969, x**4 - 654352],
eqmod (256*inp_poly90**2) cpoly [1043969, x**4 - 96534],
eqmod (256*inp_poly91**2) cpoly [1043969, x**4 - 947435],
eqmod (256*inp_poly92**2) cpoly [1043969, x**4 - 799554],
eqmod (256*inp_poly93**2) cpoly [1043969, x**4 - 244415],
eqmod (256*inp_poly94**2) cpoly [1043969, x**4 - 704462],
eqmod (256*inp_poly95**2) cpoly [1043969, x**4 - 339507],
eqmod (256*inp_poly96**2) cpoly [1043969, x**4 - 666593],
eqmod (256*inp_poly97**2) cpoly [1043969, x**4 - 377376],
eqmod (256*inp_poly98**2) cpoly [1043969, x**4 - 963976],
eqmod (256*inp_poly99**2) cpoly [1043969, x**4 - 79993],
eqmod (256*inp_poly9a**2) cpoly [1043969, x**4 - 650310],
eqmod (256*inp_poly9b**2) cpoly [1043969, x**4 - 393659],
eqmod (256*inp_poly9c**2) cpoly [1043969, x**4 - 483878],
eqmod (256*inp_poly9d**2) cpoly [1043969, x**4 - 560091],
eqmod (256*inp_poly9e**2) cpoly [1043969, x**4 - 984749],
eqmod (256*inp_poly9f**2) cpoly [1043969, x**4 - 59220],
eqmod (256*inp_polya0**2) cpoly [1043969, x**4 - 15495],
eqmod (256*inp_polya1**2) cpoly [1043969, x**4 - 1028474],
eqmod (256*inp_polya2**2) cpoly [1043969, x**4 - 403201],
eqmod (256*inp_polya3**2) cpoly [1043969, x**4 - 640768],
eqmod (256*inp_polya4**2) cpoly [1043969, x**4 - 605504],
eqmod (256*inp_polya5**2) cpoly [1043969, x**4 - 438465],
eqmod (256*inp_polya6**2) cpoly [1043969, x**4 - 409728],
eqmod (256*inp_polya7**2) cpoly [1043969, x**4 - 634241],
eqmod (256*inp_polya8**2) cpoly [1043969, x**4 - 30018],
eqmod (256*inp_polya9**2) cpoly [1043969, x**4 - 1013951],
eqmod (256*inp_polyaa**2) cpoly [1043969, x**4 - 109250],
eqmod (256*inp_polyab**2) cpoly [1043969, x**4 - 934719],
eqmod (256*inp_polyac**2) cpoly [1043969, x**4 - 16675],
eqmod (256*inp_polyad**2) cpoly [1043969, x**4 - 1027294],
eqmod (256*inp_polyae**2) cpoly [1043969, x**4 - 643778],
eqmod (256*inp_polyaf**2) cpoly [1043969, x**4 - 400191],
eqmod (256*inp_polyb0**2) cpoly [1043969, x**4 - 188469],
eqmod (256*inp_polyb1**2) cpoly [1043969, x**4 - 855500],
eqmod (256*inp_polyb2**2) cpoly [1043969, x**4 - 968467],
eqmod (256*inp_polyb3**2) cpoly [1043969, x**4 - 75502],
eqmod (256*inp_polyb4**2) cpoly [1043969, x**4 - 658814],
eqmod (256*inp_polyb5**2) cpoly [1043969, x**4 - 385155],
eqmod (256*inp_polyb6**2) cpoly [1043969, x**4 - 405305],
eqmod (256*inp_polyb7**2) cpoly [1043969, x**4 - 638664],
eqmod (256*inp_polyb8**2) cpoly [1043969, x**4 - 874265],
eqmod (256*inp_polyb9**2) cpoly [1043969, x**4 - 169704],
eqmod (256*inp_polyba**2) cpoly [1043969, x**4 - 658791],
eqmod (256*inp_polybb**2) cpoly [1043969, x**4 - 385178],
eqmod (256*inp_polybc**2) cpoly [1043969, x**4 - 40112],
eqmod (256*inp_polybd**2) cpoly [1043969, x**4 - 1003857],
eqmod (256*inp_polybe**2) cpoly [1043969, x**4 - 608327],
eqmod (256*inp_polybf**2) cpoly [1043969, x**4 - 435642]
,
eqmod (256*inp_polyc0**2) cpoly [1043969, x**4 - 759697],
eqmod (256*inp_polyc1**2) cpoly [1043969, x**4 - 284272],
eqmod (256*inp_polyc2**2) cpoly [1043969, x**4 - 908658],
eqmod (256*inp_polyc3**2) cpoly [1043969, x**4 - 135311],
eqmod (256*inp_polyc4**2) cpoly [1043969, x**4 - 369462],
eqmod (256*inp_polyc5**2) cpoly [1043969, x**4 - 674507],
eqmod (256*inp_polyc6**2) cpoly [1043969, x**4 - 1021423],
eqmod (256*inp_polyc7**2) cpoly [1043969, x**4 - 22546],
eqmod (256*inp_polyc8**2) cpoly [1043969, x**4 - 943589],
eqmod (256*inp_polyc9**2) cpoly [1043969, x**4 - 100380],
eqmod (256*inp_polyca**2) cpoly [1043969, x**4 - 927162],
eqmod (256*inp_polycb**2) cpoly [1043969, x**4 - 116807],
eqmod (256*inp_polycc**2) cpoly [1043969, x**4 - 350308],
eqmod (256*inp_polycd**2) cpoly [1043969, x**4 - 693661],
eqmod (256*inp_polyce**2) cpoly [1043969, x**4 - 674670],
eqmod (256*inp_polycf**2) cpoly [1043969, x**4 - 369299],
eqmod (256*inp_polyd0**2) cpoly [1043969, x**4 - 319829],
eqmod (256*inp_polyd1**2) cpoly [1043969, x**4 - 724140],
eqmod (256*inp_polyd2**2) cpoly [1043969, x**4 - 518322],
eqmod (256*inp_polyd3**2) cpoly [1043969, x**4 - 525647],
eqmod (256*inp_polyd4**2) cpoly [1043969, x**4 - 727472],
eqmod (256*inp_polyd5**2) cpoly [1043969, x**4 - 316497],
eqmod (256*inp_polyd6**2) cpoly [1043969, x**4 - 659984],
eqmod (256*inp_polyd7**2) cpoly [1043969, x**4 - 383985],
eqmod (256*inp_polyd8**2) cpoly [1043969, x**4 - 269719],
eqmod (256*inp_polyd9**2) cpoly [1043969, x**4 - 774250],
eqmod (256*inp_polyda**2) cpoly [1043969, x**4 - 485076],
eqmod (256*inp_polydb**2) cpoly [1043969, x**4 - 558893],
eqmod (256*inp_polydc**2) cpoly [1043969, x**4 - 975148],
eqmod (256*inp_polydd**2) cpoly [1043969, x**4 - 68821],
eqmod (256*inp_polyde**2) cpoly [1043969, x**4 - 118175],
eqmod (256*inp_polydf**2) cpoly [1043969, x**4 - 925794],
eqmod (256*inp_polye0**2) cpoly [1043969, x**4 - 405653],
eqmod (256*inp_polye1**2) cpoly [1043969, x**4 - 638316],
eqmod (256*inp_polye2**2) cpoly [1043969, x**4 - 364094],
eqmod (256*inp_polye3**2) cpoly [1043969, x**4 - 679875],
eqmod (256*inp_polye4**2) cpoly [1043969, x**4 - 857780],
eqmod (256*inp_polye5**2) cpoly [1043969, x**4 - 186189],
eqmod (256*inp_polye6**2) cpoly [1043969, x**4 - 9514],
eqmod (256*inp_polye7**2) cpoly [1043969, x**4 - 1034455],
eqmod (256*inp_polye8**2) cpoly [1043969, x**4 - 438813],
eqmod (256*inp_polye9**2) cpoly [1043969, x**4 - 605156],
eqmod (256*inp_polyea**2) cpoly [1043969, x**4 - 613180],
eqmod (256*inp_polyeb**2) cpoly [1043969, x**4 - 430789],
eqmod (256*inp_polyec**2) cpoly [1043969, x**4 - 643048],
eqmod (256*inp_polyed**2) cpoly [1043969, x**4 - 400921],
eqmod (256*inp_polyee**2) cpoly [1043969, x**4 - 993476],
eqmod (256*inp_polyef**2) cpoly [1043969, x**4 - 50493],
eqmod (256*inp_polyf0**2) cpoly [1043969, x**4 - 143510],
eqmod (256*inp_polyf1**2) cpoly [1043969, x**4 - 900459],
eqmod (256*inp_polyf2**2) cpoly [1043969, x**4 - 956472],
eqmod (256*inp_polyf3**2) cpoly [1043969, x**4 - 87497],
eqmod (256*inp_polyf4**2) cpoly [1043969, x**4 - 904239],
eqmod (256*inp_polyf5**2) cpoly [1043969, x**4 - 139730],
eqmod (256*inp_polyf6**2) cpoly [1043969, x**4 - 362716],
eqmod (256*inp_polyf7**2) cpoly [1043969, x**4 - 681253],
eqmod (256*inp_polyf8**2) cpoly [1043969, x**4 - 928856],
eqmod (256*inp_polyf9**2) cpoly [1043969, x**4 - 115113],
eqmod (256*inp_polyfa**2) cpoly [1043969, x**4 - 567842],
eqmod (256*inp_polyfb**2) cpoly [1043969, x**4 - 476127],
eqmod (256*inp_polyfc**2) cpoly [1043969, x**4 - 1009759],
eqmod (256*inp_polyfd**2) cpoly [1043969, x**4 - 34210],
eqmod (256*inp_polyfe**2) cpoly [1043969, x**4 - 660435],
eqmod (256*inp_polyff**2) cpoly [1043969, x**4 - 383534]

] prove with [ cuts [ 524, 527, 530, 533 ] ] && and [
(-512)@32*1043969@32 <=s cf000, cf000 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf001, cf001 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf002, cf002 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf003, cf003 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf004, cf004 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf005, cf005 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf006, cf006 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf007, cf007 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf008, cf008 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf009, cf009 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf010, cf010 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf011, cf011 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf012, cf012 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf013, cf013 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf014, cf014 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf015, cf015 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf016, cf016 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf017, cf017 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf018, cf018 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf019, cf019 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf020, cf020 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf021, cf021 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf022, cf022 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf023, cf023 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf024, cf024 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf025, cf025 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf026, cf026 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf027, cf027 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf028, cf028 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf029, cf029 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf030, cf030 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf031, cf031 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf032, cf032 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf033, cf033 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf034, cf034 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf035, cf035 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf036, cf036 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf037, cf037 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf038, cf038 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf039, cf039 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf040, cf040 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf041, cf041 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf042, cf042 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf043, cf043 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf044, cf044 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf045, cf045 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf046, cf046 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf047, cf047 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf048, cf048 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf049, cf049 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf050, cf050 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf051, cf051 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf052, cf052 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf053, cf053 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf054, cf054 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf055, cf055 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf056, cf056 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf057, cf057 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf058, cf058 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf059, cf059 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf060, cf060 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf061, cf061 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf062, cf062 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf063, cf063 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf064, cf064 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf065, cf065 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf066, cf066 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf067, cf067 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf068, cf068 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf069, cf069 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf070, cf070 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf071, cf071 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf072, cf072 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf073, cf073 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf074, cf074 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf075, cf075 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf076, cf076 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf077, cf077 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf078, cf078 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf079, cf079 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf080, cf080 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf081, cf081 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf082, cf082 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf083, cf083 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf084, cf084 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf085, cf085 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf086, cf086 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf087, cf087 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf088, cf088 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf089, cf089 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf090, cf090 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf091, cf091 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf092, cf092 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf093, cf093 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf094, cf094 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf095, cf095 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf096, cf096 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf097, cf097 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf098, cf098 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf099, cf099 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf100, cf100 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf101, cf101 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf102, cf102 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf103, cf103 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf104, cf104 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf105, cf105 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf106, cf106 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf107, cf107 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf108, cf108 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf109, cf109 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf110, cf110 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf111, cf111 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf112, cf112 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf113, cf113 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf114, cf114 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf115, cf115 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf116, cf116 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf117, cf117 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf118, cf118 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf119, cf119 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf120, cf120 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf121, cf121 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf122, cf122 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf123, cf123 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf124, cf124 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf125, cf125 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf126, cf126 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf127, cf127 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf128, cf128 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf129, cf129 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf130, cf130 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf131, cf131 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf132, cf132 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf133, cf133 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf134, cf134 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf135, cf135 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf136, cf136 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf137, cf137 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf138, cf138 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf139, cf139 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf140, cf140 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf141, cf141 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf142, cf142 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf143, cf143 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf144, cf144 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf145, cf145 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf146, cf146 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf147, cf147 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf148, cf148 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf149, cf149 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf150, cf150 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf151, cf151 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf152, cf152 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf153, cf153 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf154, cf154 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf155, cf155 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf156, cf156 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf157, cf157 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf158, cf158 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf159, cf159 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf160, cf160 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf161, cf161 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf162, cf162 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf163, cf163 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf164, cf164 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf165, cf165 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf166, cf166 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf167, cf167 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf168, cf168 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf169, cf169 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf170, cf170 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf171, cf171 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf172, cf172 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf173, cf173 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf174, cf174 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf175, cf175 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf176, cf176 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf177, cf177 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf178, cf178 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf179, cf179 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf180, cf180 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf181, cf181 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf182, cf182 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf183, cf183 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf184, cf184 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf185, cf185 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf186, cf186 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf187, cf187 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf188, cf188 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf189, cf189 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf190, cf190 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf191, cf191 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf192, cf192 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf193, cf193 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf194, cf194 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf195, cf195 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf196, cf196 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf197, cf197 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf198, cf198 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf199, cf199 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf200, cf200 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf201, cf201 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf202, cf202 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf203, cf203 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf204, cf204 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf205, cf205 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf206, cf206 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf207, cf207 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf208, cf208 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf209, cf209 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf210, cf210 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf211, cf211 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf212, cf212 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf213, cf213 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf214, cf214 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf215, cf215 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf216, cf216 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf217, cf217 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf218, cf218 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf219, cf219 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf220, cf220 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf221, cf221 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf222, cf222 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf223, cf223 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf224, cf224 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf225, cf225 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf226, cf226 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf227, cf227 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf228, cf228 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf229, cf229 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf230, cf230 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf231, cf231 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf232, cf232 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf233, cf233 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf234, cf234 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf235, cf235 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf236, cf236 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf237, cf237 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf238, cf238 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf239, cf239 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf240, cf240 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf241, cf241 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf242, cf242 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf243, cf243 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf244, cf244 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf245, cf245 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf246, cf246 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf247, cf247 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf248, cf248 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf249, cf249 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf250, cf250 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf251, cf251 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf252, cf252 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf253, cf253 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf254, cf254 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf255, cf255 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf256, cf256 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf257, cf257 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf258, cf258 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf259, cf259 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf260, cf260 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf261, cf261 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf262, cf262 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf263, cf263 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf264, cf264 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf265, cf265 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf266, cf266 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf267, cf267 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf268, cf268 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf269, cf269 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf270, cf270 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf271, cf271 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf272, cf272 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf273, cf273 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf274, cf274 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf275, cf275 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf276, cf276 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf277, cf277 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf278, cf278 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf279, cf279 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf280, cf280 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf281, cf281 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf282, cf282 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf283, cf283 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf284, cf284 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf285, cf285 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf286, cf286 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf287, cf287 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf288, cf288 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf289, cf289 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf290, cf290 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf291, cf291 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf292, cf292 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf293, cf293 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf294, cf294 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf295, cf295 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf296, cf296 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf297, cf297 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf298, cf298 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf299, cf299 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf300, cf300 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf301, cf301 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf302, cf302 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf303, cf303 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf304, cf304 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf305, cf305 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf306, cf306 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf307, cf307 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf308, cf308 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf309, cf309 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf310, cf310 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf311, cf311 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf312, cf312 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf313, cf313 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf314, cf314 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf315, cf315 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf316, cf316 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf317, cf317 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf318, cf318 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf319, cf319 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf320, cf320 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf321, cf321 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf322, cf322 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf323, cf323 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf324, cf324 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf325, cf325 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf326, cf326 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf327, cf327 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf328, cf328 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf329, cf329 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf330, cf330 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf331, cf331 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf332, cf332 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf333, cf333 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf334, cf334 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf335, cf335 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf336, cf336 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf337, cf337 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf338, cf338 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf339, cf339 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf340, cf340 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf341, cf341 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf342, cf342 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf343, cf343 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf344, cf344 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf345, cf345 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf346, cf346 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf347, cf347 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf348, cf348 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf349, cf349 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf350, cf350 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf351, cf351 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf352, cf352 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf353, cf353 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf354, cf354 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf355, cf355 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf356, cf356 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf357, cf357 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf358, cf358 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf359, cf359 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf360, cf360 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf361, cf361 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf362, cf362 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf363, cf363 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf364, cf364 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf365, cf365 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf366, cf366 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf367, cf367 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf368, cf368 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf369, cf369 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf370, cf370 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf371, cf371 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf372, cf372 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf373, cf373 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf374, cf374 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf375, cf375 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf376, cf376 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf377, cf377 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf378, cf378 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf379, cf379 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf380, cf380 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf381, cf381 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf382, cf382 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf383, cf383 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf384, cf384 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf385, cf385 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf386, cf386 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf387, cf387 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf388, cf388 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf389, cf389 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf390, cf390 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf391, cf391 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf392, cf392 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf393, cf393 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf394, cf394 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf395, cf395 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf396, cf396 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf397, cf397 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf398, cf398 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf399, cf399 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf400, cf400 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf401, cf401 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf402, cf402 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf403, cf403 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf404, cf404 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf405, cf405 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf406, cf406 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf407, cf407 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf408, cf408 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf409, cf409 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf410, cf410 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf411, cf411 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf412, cf412 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf413, cf413 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf414, cf414 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf415, cf415 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf416, cf416 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf417, cf417 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf418, cf418 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf419, cf419 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf420, cf420 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf421, cf421 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf422, cf422 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf423, cf423 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf424, cf424 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf425, cf425 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf426, cf426 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf427, cf427 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf428, cf428 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf429, cf429 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf430, cf430 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf431, cf431 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf432, cf432 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf433, cf433 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf434, cf434 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf435, cf435 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf436, cf436 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf437, cf437 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf438, cf438 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf439, cf439 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf440, cf440 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf441, cf441 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf442, cf442 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf443, cf443 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf444, cf444 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf445, cf445 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf446, cf446 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf447, cf447 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf448, cf448 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf449, cf449 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf450, cf450 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf451, cf451 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf452, cf452 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf453, cf453 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf454, cf454 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf455, cf455 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf456, cf456 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf457, cf457 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf458, cf458 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf459, cf459 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf460, cf460 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf461, cf461 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf462, cf462 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf463, cf463 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf464, cf464 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf465, cf465 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf466, cf466 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf467, cf467 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf468, cf468 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf469, cf469 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf470, cf470 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf471, cf471 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf472, cf472 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf473, cf473 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf474, cf474 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf475, cf475 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf476, cf476 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf477, cf477 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf478, cf478 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf479, cf479 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf480, cf480 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf481, cf481 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf482, cf482 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf483, cf483 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf484, cf484 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf485, cf485 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf486, cf486 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf487, cf487 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf488, cf488 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf489, cf489 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf490, cf490 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf491, cf491 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf492, cf492 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf493, cf493 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf494, cf494 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf495, cf495 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf496, cf496 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf497, cf497 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf498, cf498 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf499, cf499 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf500, cf500 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf501, cf501 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf502, cf502 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf503, cf503 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf504, cf504 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf505, cf505 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf506, cf506 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf507, cf507 <=s 512@32*1043969@32,
(-512)@32*1043969@32 <=s cf508, cf508 <=s 512@32*1043969@32,
eqmod L0x200187c8 cf000 2048@32, eqmod L0x200187ca cf001 2048@32,
eqmod L0x200187cc cf002 2048@32, eqmod L0x200187ce cf003 2048@32,
eqmod L0x200187d0 cf004 2048@32, eqmod L0x200187d2 cf005 2048@32,
eqmod L0x200187d4 cf006 2048@32, eqmod L0x200187d6 cf007 2048@32,
eqmod L0x200187d8 cf008 2048@32, eqmod L0x200187da cf009 2048@32,
eqmod L0x200187dc cf010 2048@32, eqmod L0x200187de cf011 2048@32,
eqmod L0x200187e0 cf012 2048@32, eqmod L0x200187e2 cf013 2048@32,
eqmod L0x200187e4 cf014 2048@32, eqmod L0x200187e6 cf015 2048@32,
eqmod L0x200187e8 cf016 2048@32, eqmod L0x200187ea cf017 2048@32,
eqmod L0x200187ec cf018 2048@32, eqmod L0x200187ee cf019 2048@32,
eqmod L0x200187f0 cf020 2048@32, eqmod L0x200187f2 cf021 2048@32,
eqmod L0x200187f4 cf022 2048@32, eqmod L0x200187f6 cf023 2048@32,
eqmod L0x200187f8 cf024 2048@32, eqmod L0x200187fa cf025 2048@32,
eqmod L0x200187fc cf026 2048@32, eqmod L0x200187fe cf027 2048@32,
eqmod L0x20018800 cf028 2048@32, eqmod L0x20018802 cf029 2048@32,
eqmod L0x20018804 cf030 2048@32, eqmod L0x20018806 cf031 2048@32,
eqmod L0x20018808 cf032 2048@32, eqmod L0x2001880a cf033 2048@32,
eqmod L0x2001880c cf034 2048@32, eqmod L0x2001880e cf035 2048@32,
eqmod L0x20018810 cf036 2048@32, eqmod L0x20018812 cf037 2048@32,
eqmod L0x20018814 cf038 2048@32, eqmod L0x20018816 cf039 2048@32,
eqmod L0x20018818 cf040 2048@32, eqmod L0x2001881a cf041 2048@32,
eqmod L0x2001881c cf042 2048@32, eqmod L0x2001881e cf043 2048@32,
eqmod L0x20018820 cf044 2048@32, eqmod L0x20018822 cf045 2048@32,
eqmod L0x20018824 cf046 2048@32, eqmod L0x20018826 cf047 2048@32,
eqmod L0x20018828 cf048 2048@32, eqmod L0x2001882a cf049 2048@32,
eqmod L0x2001882c cf050 2048@32, eqmod L0x2001882e cf051 2048@32,
eqmod L0x20018830 cf052 2048@32, eqmod L0x20018832 cf053 2048@32,
eqmod L0x20018834 cf054 2048@32, eqmod L0x20018836 cf055 2048@32,
eqmod L0x20018838 cf056 2048@32, eqmod L0x2001883a cf057 2048@32,
eqmod L0x2001883c cf058 2048@32, eqmod L0x2001883e cf059 2048@32,
eqmod L0x20018840 cf060 2048@32, eqmod L0x20018842 cf061 2048@32,
eqmod L0x20018844 cf062 2048@32, eqmod L0x20018846 cf063 2048@32,
eqmod L0x20018848 cf064 2048@32, eqmod L0x2001884a cf065 2048@32,
eqmod L0x2001884c cf066 2048@32, eqmod L0x2001884e cf067 2048@32,
eqmod L0x20018850 cf068 2048@32, eqmod L0x20018852 cf069 2048@32,
eqmod L0x20018854 cf070 2048@32, eqmod L0x20018856 cf071 2048@32,
eqmod L0x20018858 cf072 2048@32, eqmod L0x2001885a cf073 2048@32,
eqmod L0x2001885c cf074 2048@32, eqmod L0x2001885e cf075 2048@32,
eqmod L0x20018860 cf076 2048@32, eqmod L0x20018862 cf077 2048@32,
eqmod L0x20018864 cf078 2048@32, eqmod L0x20018866 cf079 2048@32,
eqmod L0x20018868 cf080 2048@32, eqmod L0x2001886a cf081 2048@32,
eqmod L0x2001886c cf082 2048@32, eqmod L0x2001886e cf083 2048@32,
eqmod L0x20018870 cf084 2048@32, eqmod L0x20018872 cf085 2048@32,
eqmod L0x20018874 cf086 2048@32, eqmod L0x20018876 cf087 2048@32,
eqmod L0x20018878 cf088 2048@32, eqmod L0x2001887a cf089 2048@32,
eqmod L0x2001887c cf090 2048@32, eqmod L0x2001887e cf091 2048@32,
eqmod L0x20018880 cf092 2048@32, eqmod L0x20018882 cf093 2048@32,
eqmod L0x20018884 cf094 2048@32, eqmod L0x20018886 cf095 2048@32,
eqmod L0x20018888 cf096 2048@32, eqmod L0x2001888a cf097 2048@32,
eqmod L0x2001888c cf098 2048@32, eqmod L0x2001888e cf099 2048@32,
eqmod L0x20018890 cf100 2048@32, eqmod L0x20018892 cf101 2048@32,
eqmod L0x20018894 cf102 2048@32, eqmod L0x20018896 cf103 2048@32,
eqmod L0x20018898 cf104 2048@32, eqmod L0x2001889a cf105 2048@32,
eqmod L0x2001889c cf106 2048@32, eqmod L0x2001889e cf107 2048@32,
eqmod L0x200188a0 cf108 2048@32, eqmod L0x200188a2 cf109 2048@32,
eqmod L0x200188a4 cf110 2048@32, eqmod L0x200188a6 cf111 2048@32,
eqmod L0x200188a8 cf112 2048@32, eqmod L0x200188aa cf113 2048@32,
eqmod L0x200188ac cf114 2048@32, eqmod L0x200188ae cf115 2048@32,
eqmod L0x200188b0 cf116 2048@32, eqmod L0x200188b2 cf117 2048@32,
eqmod L0x200188b4 cf118 2048@32, eqmod L0x200188b6 cf119 2048@32,
eqmod L0x200188b8 cf120 2048@32, eqmod L0x200188ba cf121 2048@32,
eqmod L0x200188bc cf122 2048@32, eqmod L0x200188be cf123 2048@32,
eqmod L0x200188c0 cf124 2048@32, eqmod L0x200188c2 cf125 2048@32,
eqmod L0x200188c4 cf126 2048@32, eqmod L0x200188c6 cf127 2048@32,
eqmod L0x200188c8 cf128 2048@32, eqmod L0x200188ca cf129 2048@32,
eqmod L0x200188cc cf130 2048@32, eqmod L0x200188ce cf131 2048@32,
eqmod L0x200188d0 cf132 2048@32, eqmod L0x200188d2 cf133 2048@32,
eqmod L0x200188d4 cf134 2048@32, eqmod L0x200188d6 cf135 2048@32,
eqmod L0x200188d8 cf136 2048@32, eqmod L0x200188da cf137 2048@32,
eqmod L0x200188dc cf138 2048@32, eqmod L0x200188de cf139 2048@32,
eqmod L0x200188e0 cf140 2048@32, eqmod L0x200188e2 cf141 2048@32,
eqmod L0x200188e4 cf142 2048@32, eqmod L0x200188e6 cf143 2048@32,
eqmod L0x200188e8 cf144 2048@32, eqmod L0x200188ea cf145 2048@32,
eqmod L0x200188ec cf146 2048@32, eqmod L0x200188ee cf147 2048@32,
eqmod L0x200188f0 cf148 2048@32, eqmod L0x200188f2 cf149 2048@32,
eqmod L0x200188f4 cf150 2048@32, eqmod L0x200188f6 cf151 2048@32,
eqmod L0x200188f8 cf152 2048@32, eqmod L0x200188fa cf153 2048@32,
eqmod L0x200188fc cf154 2048@32, eqmod L0x200188fe cf155 2048@32,
eqmod L0x20018900 cf156 2048@32, eqmod L0x20018902 cf157 2048@32,
eqmod L0x20018904 cf158 2048@32, eqmod L0x20018906 cf159 2048@32,
eqmod L0x20018908 cf160 2048@32, eqmod L0x2001890a cf161 2048@32,
eqmod L0x2001890c cf162 2048@32, eqmod L0x2001890e cf163 2048@32,
eqmod L0x20018910 cf164 2048@32, eqmod L0x20018912 cf165 2048@32,
eqmod L0x20018914 cf166 2048@32, eqmod L0x20018916 cf167 2048@32,
eqmod L0x20018918 cf168 2048@32, eqmod L0x2001891a cf169 2048@32,
eqmod L0x2001891c cf170 2048@32, eqmod L0x2001891e cf171 2048@32,
eqmod L0x20018920 cf172 2048@32, eqmod L0x20018922 cf173 2048@32,
eqmod L0x20018924 cf174 2048@32, eqmod L0x20018926 cf175 2048@32,
eqmod L0x20018928 cf176 2048@32, eqmod L0x2001892a cf177 2048@32,
eqmod L0x2001892c cf178 2048@32, eqmod L0x2001892e cf179 2048@32,
eqmod L0x20018930 cf180 2048@32, eqmod L0x20018932 cf181 2048@32,
eqmod L0x20018934 cf182 2048@32, eqmod L0x20018936 cf183 2048@32,
eqmod L0x20018938 cf184 2048@32, eqmod L0x2001893a cf185 2048@32,
eqmod L0x2001893c cf186 2048@32, eqmod L0x2001893e cf187 2048@32,
eqmod L0x20018940 cf188 2048@32, eqmod L0x20018942 cf189 2048@32,
eqmod L0x20018944 cf190 2048@32, eqmod L0x20018946 cf191 2048@32,
eqmod L0x20018948 cf192 2048@32, eqmod L0x2001894a cf193 2048@32,
eqmod L0x2001894c cf194 2048@32, eqmod L0x2001894e cf195 2048@32,
eqmod L0x20018950 cf196 2048@32, eqmod L0x20018952 cf197 2048@32,
eqmod L0x20018954 cf198 2048@32, eqmod L0x20018956 cf199 2048@32,
eqmod L0x20018958 cf200 2048@32, eqmod L0x2001895a cf201 2048@32,
eqmod L0x2001895c cf202 2048@32, eqmod L0x2001895e cf203 2048@32,
eqmod L0x20018960 cf204 2048@32, eqmod L0x20018962 cf205 2048@32,
eqmod L0x20018964 cf206 2048@32, eqmod L0x20018966 cf207 2048@32,
eqmod L0x20018968 cf208 2048@32, eqmod L0x2001896a cf209 2048@32,
eqmod L0x2001896c cf210 2048@32, eqmod L0x2001896e cf211 2048@32,
eqmod L0x20018970 cf212 2048@32, eqmod L0x20018972 cf213 2048@32,
eqmod L0x20018974 cf214 2048@32, eqmod L0x20018976 cf215 2048@32,
eqmod L0x20018978 cf216 2048@32, eqmod L0x2001897a cf217 2048@32,
eqmod L0x2001897c cf218 2048@32, eqmod L0x2001897e cf219 2048@32,
eqmod L0x20018980 cf220 2048@32, eqmod L0x20018982 cf221 2048@32,
eqmod L0x20018984 cf222 2048@32, eqmod L0x20018986 cf223 2048@32,
eqmod L0x20018988 cf224 2048@32, eqmod L0x2001898a cf225 2048@32,
eqmod L0x2001898c cf226 2048@32, eqmod L0x2001898e cf227 2048@32,
eqmod L0x20018990 cf228 2048@32, eqmod L0x20018992 cf229 2048@32,
eqmod L0x20018994 cf230 2048@32, eqmod L0x20018996 cf231 2048@32,
eqmod L0x20018998 cf232 2048@32, eqmod L0x2001899a cf233 2048@32,
eqmod L0x2001899c cf234 2048@32, eqmod L0x2001899e cf235 2048@32,
eqmod L0x200189a0 cf236 2048@32, eqmod L0x200189a2 cf237 2048@32,
eqmod L0x200189a4 cf238 2048@32, eqmod L0x200189a6 cf239 2048@32,
eqmod L0x200189a8 cf240 2048@32, eqmod L0x200189aa cf241 2048@32,
eqmod L0x200189ac cf242 2048@32, eqmod L0x200189ae cf243 2048@32,
eqmod L0x200189b0 cf244 2048@32, eqmod L0x200189b2 cf245 2048@32,
eqmod L0x200189b4 cf246 2048@32, eqmod L0x200189b6 cf247 2048@32,
eqmod L0x200189b8 cf248 2048@32, eqmod L0x200189ba cf249 2048@32,
eqmod L0x200189bc cf250 2048@32, eqmod L0x200189be cf251 2048@32,
eqmod L0x200189c0 cf252 2048@32, eqmod L0x200189c2 cf253 2048@32,
eqmod L0x200189c4 cf254 2048@32, eqmod L0x200189c6 cf255 2048@32,
eqmod L0x200189c8 cf256 2048@32, eqmod L0x200189ca cf257 2048@32,
eqmod L0x200189cc cf258 2048@32, eqmod L0x200189ce cf259 2048@32,
eqmod L0x200189d0 cf260 2048@32, eqmod L0x200189d2 cf261 2048@32,
eqmod L0x200189d4 cf262 2048@32, eqmod L0x200189d6 cf263 2048@32,
eqmod L0x200189d8 cf264 2048@32, eqmod L0x200189da cf265 2048@32,
eqmod L0x200189dc cf266 2048@32, eqmod L0x200189de cf267 2048@32,
eqmod L0x200189e0 cf268 2048@32, eqmod L0x200189e2 cf269 2048@32,
eqmod L0x200189e4 cf270 2048@32, eqmod L0x200189e6 cf271 2048@32,
eqmod L0x200189e8 cf272 2048@32, eqmod L0x200189ea cf273 2048@32,
eqmod L0x200189ec cf274 2048@32, eqmod L0x200189ee cf275 2048@32,
eqmod L0x200189f0 cf276 2048@32, eqmod L0x200189f2 cf277 2048@32,
eqmod L0x200189f4 cf278 2048@32, eqmod L0x200189f6 cf279 2048@32,
eqmod L0x200189f8 cf280 2048@32, eqmod L0x200189fa cf281 2048@32,
eqmod L0x200189fc cf282 2048@32, eqmod L0x200189fe cf283 2048@32,
eqmod L0x20018a00 cf284 2048@32, eqmod L0x20018a02 cf285 2048@32,
eqmod L0x20018a04 cf286 2048@32, eqmod L0x20018a06 cf287 2048@32,
eqmod L0x20018a08 cf288 2048@32, eqmod L0x20018a0a cf289 2048@32,
eqmod L0x20018a0c cf290 2048@32, eqmod L0x20018a0e cf291 2048@32,
eqmod L0x20018a10 cf292 2048@32, eqmod L0x20018a12 cf293 2048@32,
eqmod L0x20018a14 cf294 2048@32, eqmod L0x20018a16 cf295 2048@32,
eqmod L0x20018a18 cf296 2048@32, eqmod L0x20018a1a cf297 2048@32,
eqmod L0x20018a1c cf298 2048@32, eqmod L0x20018a1e cf299 2048@32,
eqmod L0x20018a20 cf300 2048@32, eqmod L0x20018a22 cf301 2048@32,
eqmod L0x20018a24 cf302 2048@32, eqmod L0x20018a26 cf303 2048@32,
eqmod L0x20018a28 cf304 2048@32, eqmod L0x20018a2a cf305 2048@32,
eqmod L0x20018a2c cf306 2048@32, eqmod L0x20018a2e cf307 2048@32,
eqmod L0x20018a30 cf308 2048@32, eqmod L0x20018a32 cf309 2048@32,
eqmod L0x20018a34 cf310 2048@32, eqmod L0x20018a36 cf311 2048@32,
eqmod L0x20018a38 cf312 2048@32, eqmod L0x20018a3a cf313 2048@32,
eqmod L0x20018a3c cf314 2048@32, eqmod L0x20018a3e cf315 2048@32,
eqmod L0x20018a40 cf316 2048@32, eqmod L0x20018a42 cf317 2048@32,
eqmod L0x20018a44 cf318 2048@32, eqmod L0x20018a46 cf319 2048@32,
eqmod L0x20018a48 cf320 2048@32, eqmod L0x20018a4a cf321 2048@32,
eqmod L0x20018a4c cf322 2048@32, eqmod L0x20018a4e cf323 2048@32,
eqmod L0x20018a50 cf324 2048@32, eqmod L0x20018a52 cf325 2048@32,
eqmod L0x20018a54 cf326 2048@32, eqmod L0x20018a56 cf327 2048@32,
eqmod L0x20018a58 cf328 2048@32, eqmod L0x20018a5a cf329 2048@32,
eqmod L0x20018a5c cf330 2048@32, eqmod L0x20018a5e cf331 2048@32,
eqmod L0x20018a60 cf332 2048@32, eqmod L0x20018a62 cf333 2048@32,
eqmod L0x20018a64 cf334 2048@32, eqmod L0x20018a66 cf335 2048@32,
eqmod L0x20018a68 cf336 2048@32, eqmod L0x20018a6a cf337 2048@32,
eqmod L0x20018a6c cf338 2048@32, eqmod L0x20018a6e cf339 2048@32,
eqmod L0x20018a70 cf340 2048@32, eqmod L0x20018a72 cf341 2048@32,
eqmod L0x20018a74 cf342 2048@32, eqmod L0x20018a76 cf343 2048@32,
eqmod L0x20018a78 cf344 2048@32, eqmod L0x20018a7a cf345 2048@32,
eqmod L0x20018a7c cf346 2048@32, eqmod L0x20018a7e cf347 2048@32,
eqmod L0x20018a80 cf348 2048@32, eqmod L0x20018a82 cf349 2048@32,
eqmod L0x20018a84 cf350 2048@32, eqmod L0x20018a86 cf351 2048@32,
eqmod L0x20018a88 cf352 2048@32, eqmod L0x20018a8a cf353 2048@32,
eqmod L0x20018a8c cf354 2048@32, eqmod L0x20018a8e cf355 2048@32,
eqmod L0x20018a90 cf356 2048@32, eqmod L0x20018a92 cf357 2048@32,
eqmod L0x20018a94 cf358 2048@32, eqmod L0x20018a96 cf359 2048@32,
eqmod L0x20018a98 cf360 2048@32, eqmod L0x20018a9a cf361 2048@32,
eqmod L0x20018a9c cf362 2048@32, eqmod L0x20018a9e cf363 2048@32,
eqmod L0x20018aa0 cf364 2048@32, eqmod L0x20018aa2 cf365 2048@32,
eqmod L0x20018aa4 cf366 2048@32, eqmod L0x20018aa6 cf367 2048@32,
eqmod L0x20018aa8 cf368 2048@32, eqmod L0x20018aaa cf369 2048@32,
eqmod L0x20018aac cf370 2048@32, eqmod L0x20018aae cf371 2048@32,
eqmod L0x20018ab0 cf372 2048@32, eqmod L0x20018ab2 cf373 2048@32,
eqmod L0x20018ab4 cf374 2048@32, eqmod L0x20018ab6 cf375 2048@32,
eqmod L0x20018ab8 cf376 2048@32, eqmod L0x20018aba cf377 2048@32,
eqmod L0x20018abc cf378 2048@32, eqmod L0x20018abe cf379 2048@32,
eqmod L0x20018ac0 cf380 2048@32, eqmod L0x20018ac2 cf381 2048@32,
eqmod L0x20018ac4 cf382 2048@32, eqmod L0x20018ac6 cf383 2048@32,
eqmod L0x20018ac8 cf384 2048@32, eqmod L0x20018aca cf385 2048@32,
eqmod L0x20018acc cf386 2048@32, eqmod L0x20018ace cf387 2048@32,
eqmod L0x20018ad0 cf388 2048@32, eqmod L0x20018ad2 cf389 2048@32,
eqmod L0x20018ad4 cf390 2048@32, eqmod L0x20018ad6 cf391 2048@32,
eqmod L0x20018ad8 cf392 2048@32, eqmod L0x20018ada cf393 2048@32,
eqmod L0x20018adc cf394 2048@32, eqmod L0x20018ade cf395 2048@32,
eqmod L0x20018ae0 cf396 2048@32, eqmod L0x20018ae2 cf397 2048@32,
eqmod L0x20018ae4 cf398 2048@32, eqmod L0x20018ae6 cf399 2048@32,
eqmod L0x20018ae8 cf400 2048@32, eqmod L0x20018aea cf401 2048@32,
eqmod L0x20018aec cf402 2048@32, eqmod L0x20018aee cf403 2048@32,
eqmod L0x20018af0 cf404 2048@32, eqmod L0x20018af2 cf405 2048@32,
eqmod L0x20018af4 cf406 2048@32, eqmod L0x20018af6 cf407 2048@32,
eqmod L0x20018af8 cf408 2048@32, eqmod L0x20018afa cf409 2048@32,
eqmod L0x20018afc cf410 2048@32, eqmod L0x20018afe cf411 2048@32,
eqmod L0x20018b00 cf412 2048@32, eqmod L0x20018b02 cf413 2048@32,
eqmod L0x20018b04 cf414 2048@32, eqmod L0x20018b06 cf415 2048@32,
eqmod L0x20018b08 cf416 2048@32, eqmod L0x20018b0a cf417 2048@32,
eqmod L0x20018b0c cf418 2048@32, eqmod L0x20018b0e cf419 2048@32,
eqmod L0x20018b10 cf420 2048@32, eqmod L0x20018b12 cf421 2048@32,
eqmod L0x20018b14 cf422 2048@32, eqmod L0x20018b16 cf423 2048@32,
eqmod L0x20018b18 cf424 2048@32, eqmod L0x20018b1a cf425 2048@32,
eqmod L0x20018b1c cf426 2048@32, eqmod L0x20018b1e cf427 2048@32,
eqmod L0x20018b20 cf428 2048@32, eqmod L0x20018b22 cf429 2048@32,
eqmod L0x20018b24 cf430 2048@32, eqmod L0x20018b26 cf431 2048@32,
eqmod L0x20018b28 cf432 2048@32, eqmod L0x20018b2a cf433 2048@32,
eqmod L0x20018b2c cf434 2048@32, eqmod L0x20018b2e cf435 2048@32,
eqmod L0x20018b30 cf436 2048@32, eqmod L0x20018b32 cf437 2048@32,
eqmod L0x20018b34 cf438 2048@32, eqmod L0x20018b36 cf439 2048@32,
eqmod L0x20018b38 cf440 2048@32, eqmod L0x20018b3a cf441 2048@32,
eqmod L0x20018b3c cf442 2048@32, eqmod L0x20018b3e cf443 2048@32,
eqmod L0x20018b40 cf444 2048@32, eqmod L0x20018b42 cf445 2048@32,
eqmod L0x20018b44 cf446 2048@32, eqmod L0x20018b46 cf447 2048@32,
eqmod L0x20018b48 cf448 2048@32, eqmod L0x20018b4a cf449 2048@32,
eqmod L0x20018b4c cf450 2048@32, eqmod L0x20018b4e cf451 2048@32,
eqmod L0x20018b50 cf452 2048@32, eqmod L0x20018b52 cf453 2048@32,
eqmod L0x20018b54 cf454 2048@32, eqmod L0x20018b56 cf455 2048@32,
eqmod L0x20018b58 cf456 2048@32, eqmod L0x20018b5a cf457 2048@32,
eqmod L0x20018b5c cf458 2048@32, eqmod L0x20018b5e cf459 2048@32,
eqmod L0x20018b60 cf460 2048@32, eqmod L0x20018b62 cf461 2048@32,
eqmod L0x20018b64 cf462 2048@32, eqmod L0x20018b66 cf463 2048@32,
eqmod L0x20018b68 cf464 2048@32, eqmod L0x20018b6a cf465 2048@32,
eqmod L0x20018b6c cf466 2048@32, eqmod L0x20018b6e cf467 2048@32,
eqmod L0x20018b70 cf468 2048@32, eqmod L0x20018b72 cf469 2048@32,
eqmod L0x20018b74 cf470 2048@32, eqmod L0x20018b76 cf471 2048@32,
eqmod L0x20018b78 cf472 2048@32, eqmod L0x20018b7a cf473 2048@32,
eqmod L0x20018b7c cf474 2048@32, eqmod L0x20018b7e cf475 2048@32,
eqmod L0x20018b80 cf476 2048@32, eqmod L0x20018b82 cf477 2048@32,
eqmod L0x20018b84 cf478 2048@32, eqmod L0x20018b86 cf479 2048@32,
eqmod L0x20018b88 cf480 2048@32, eqmod L0x20018b8a cf481 2048@32,
eqmod L0x20018b8c cf482 2048@32, eqmod L0x20018b8e cf483 2048@32,
eqmod L0x20018b90 cf484 2048@32, eqmod L0x20018b92 cf485 2048@32,
eqmod L0x20018b94 cf486 2048@32, eqmod L0x20018b96 cf487 2048@32,
eqmod L0x20018b98 cf488 2048@32, eqmod L0x20018b9a cf489 2048@32,
eqmod L0x20018b9c cf490 2048@32, eqmod L0x20018b9e cf491 2048@32,
eqmod L0x20018ba0 cf492 2048@32, eqmod L0x20018ba2 cf493 2048@32,
eqmod L0x20018ba4 cf494 2048@32, eqmod L0x20018ba6 cf495 2048@32,
eqmod L0x20018ba8 cf496 2048@32, eqmod L0x20018baa cf497 2048@32,
eqmod L0x20018bac cf498 2048@32, eqmod L0x20018bae cf499 2048@32,
eqmod L0x20018bb0 cf500 2048@32, eqmod L0x20018bb2 cf501 2048@32,
eqmod L0x20018bb4 cf502 2048@32, eqmod L0x20018bb6 cf503 2048@32,
eqmod L0x20018bb8 cf504 2048@32, eqmod L0x20018bba cf505 2048@32,
eqmod L0x20018bbc cf506 2048@32, eqmod L0x20018bbe cf507 2048@32,
eqmod L0x20018bc0 cf508 2048@32
] }

