(* M2: cv.exe -v -isafety -enable_rewriting:eqmod -jobs 8 -slicing -abs_interp invntt_fast_plant.cl
Parsing CryptoLine file:                    [OK]            0.1493 seconds
Checking well-formedness:                   [OK]            0.0488 seconds

Procedure main
--------------
Transforming to SSA form:                   [OK]            0.0317 seconds
Normalizing specification:                  [OK]            0.0280 seconds
Rewriting assignments:                      [OK]            0.0295 seconds
Verifying program safety:                   [OK]            4.8408 seconds
Verifying range assertions:                 [OK]            0.0441 seconds
Verifying range specification:              [OK]            7.9180 seconds
Rewriting value-preserved casting:          [OK]            0.0213 seconds
Verifying algebraic assertions:             [OK]            186.9362 seconds
Verifying algebraic specification:          [OK]            18.7222 seconds
Procedure verification:                     [OK]            218.5880 seconds

Summary
-------
Verification result:                        [OK]            218.7866 seconds
*)
(* M2: cv.exe -v -isafety -enable_rewriting:eqmod -jobs 8 -slicing invntt_fast_plant.cl
Parsing CryptoLine file:                    [OK]            0.1463 seconds
Checking well-formedness:                   [OK]            0.0485 seconds

Procedure main
--------------
Transforming to SSA form:                   [OK]            0.0325 seconds
Normalizing specification:                  [OK]            0.0278 seconds
Rewriting assignments:                      [OK]            0.0295 seconds
Verifying program safety:                   [OK]            386.6614 seconds
Verifying range assertions:                 [OK]            0.0227 seconds
Verifying range specification:              [OK]            0.1660 seconds
Rewriting value-preserved casting:          [OK]            0.0169 seconds
Verifying algebraic assertions:             [OK]            148.3318 seconds
Verifying algebraic specification:          [OK]            16.7324 seconds
Procedure verification:                     [OK]            552.0374 seconds

Summary
-------
Verification result:                        [OK]            552.2327 seconds
*)
proc main (
int16 L0xbefff1c4,int16 L0xbefff1c6,int16 L0xbefff1c8,int16 L0xbefff1ca,
int16 L0xbefff1cc,int16 L0xbefff1ce,int16 L0xbefff1d0,int16 L0xbefff1d2,
int16 L0xbefff1d4,int16 L0xbefff1d6,int16 L0xbefff1d8,int16 L0xbefff1da,
int16 L0xbefff1dc,int16 L0xbefff1de,int16 L0xbefff1e0,int16 L0xbefff1e2,
int16 L0xbefff1e4,int16 L0xbefff1e6,int16 L0xbefff1e8,int16 L0xbefff1ea,
int16 L0xbefff1ec,int16 L0xbefff1ee,int16 L0xbefff1f0,int16 L0xbefff1f2,
int16 L0xbefff1f4,int16 L0xbefff1f6,int16 L0xbefff1f8,int16 L0xbefff1fa,
int16 L0xbefff1fc,int16 L0xbefff1fe,int16 L0xbefff200,int16 L0xbefff202,
int16 L0xbefff204,int16 L0xbefff206,int16 L0xbefff208,int16 L0xbefff20a,
int16 L0xbefff20c,int16 L0xbefff20e,int16 L0xbefff210,int16 L0xbefff212,
int16 L0xbefff214,int16 L0xbefff216,int16 L0xbefff218,int16 L0xbefff21a,
int16 L0xbefff21c,int16 L0xbefff21e,int16 L0xbefff220,int16 L0xbefff222,
int16 L0xbefff224,int16 L0xbefff226,int16 L0xbefff228,int16 L0xbefff22a,
int16 L0xbefff22c,int16 L0xbefff22e,int16 L0xbefff230,int16 L0xbefff232,
int16 L0xbefff234,int16 L0xbefff236,int16 L0xbefff238,int16 L0xbefff23a,
int16 L0xbefff23c,int16 L0xbefff23e,int16 L0xbefff240,int16 L0xbefff242,
int16 L0xbefff244,int16 L0xbefff246,int16 L0xbefff248,int16 L0xbefff24a,
int16 L0xbefff24c,int16 L0xbefff24e,int16 L0xbefff250,int16 L0xbefff252,
int16 L0xbefff254,int16 L0xbefff256,int16 L0xbefff258,int16 L0xbefff25a,
int16 L0xbefff25c,int16 L0xbefff25e,int16 L0xbefff260,int16 L0xbefff262,
int16 L0xbefff264,int16 L0xbefff266,int16 L0xbefff268,int16 L0xbefff26a,
int16 L0xbefff26c,int16 L0xbefff26e,int16 L0xbefff270,int16 L0xbefff272,
int16 L0xbefff274,int16 L0xbefff276,int16 L0xbefff278,int16 L0xbefff27a,
int16 L0xbefff27c,int16 L0xbefff27e,int16 L0xbefff280,int16 L0xbefff282,
int16 L0xbefff284,int16 L0xbefff286,int16 L0xbefff288,int16 L0xbefff28a,
int16 L0xbefff28c,int16 L0xbefff28e,int16 L0xbefff290,int16 L0xbefff292,
int16 L0xbefff294,int16 L0xbefff296,int16 L0xbefff298,int16 L0xbefff29a,
int16 L0xbefff29c,int16 L0xbefff29e,int16 L0xbefff2a0,int16 L0xbefff2a2,
int16 L0xbefff2a4,int16 L0xbefff2a6,int16 L0xbefff2a8,int16 L0xbefff2aa,
int16 L0xbefff2ac,int16 L0xbefff2ae,int16 L0xbefff2b0,int16 L0xbefff2b2,
int16 L0xbefff2b4,int16 L0xbefff2b6,int16 L0xbefff2b8,int16 L0xbefff2ba,
int16 L0xbefff2bc,int16 L0xbefff2be,int16 L0xbefff2c0,int16 L0xbefff2c2,
int16 L0xbefff2c4,int16 L0xbefff2c6,int16 L0xbefff2c8,int16 L0xbefff2ca,
int16 L0xbefff2cc,int16 L0xbefff2ce,int16 L0xbefff2d0,int16 L0xbefff2d2,
int16 L0xbefff2d4,int16 L0xbefff2d6,int16 L0xbefff2d8,int16 L0xbefff2da,
int16 L0xbefff2dc,int16 L0xbefff2de,int16 L0xbefff2e0,int16 L0xbefff2e2,
int16 L0xbefff2e4,int16 L0xbefff2e6,int16 L0xbefff2e8,int16 L0xbefff2ea,
int16 L0xbefff2ec,int16 L0xbefff2ee,int16 L0xbefff2f0,int16 L0xbefff2f2,
int16 L0xbefff2f4,int16 L0xbefff2f6,int16 L0xbefff2f8,int16 L0xbefff2fa,
int16 L0xbefff2fc,int16 L0xbefff2fe,int16 L0xbefff300,int16 L0xbefff302,
int16 L0xbefff304,int16 L0xbefff306,int16 L0xbefff308,int16 L0xbefff30a,
int16 L0xbefff30c,int16 L0xbefff30e,int16 L0xbefff310,int16 L0xbefff312,
int16 L0xbefff314,int16 L0xbefff316,int16 L0xbefff318,int16 L0xbefff31a,
int16 L0xbefff31c,int16 L0xbefff31e,int16 L0xbefff320,int16 L0xbefff322,
int16 L0xbefff324,int16 L0xbefff326,int16 L0xbefff328,int16 L0xbefff32a,
int16 L0xbefff32c,int16 L0xbefff32e,int16 L0xbefff330,int16 L0xbefff332,
int16 L0xbefff334,int16 L0xbefff336,int16 L0xbefff338,int16 L0xbefff33a,
int16 L0xbefff33c,int16 L0xbefff33e,int16 L0xbefff340,int16 L0xbefff342,
int16 L0xbefff344,int16 L0xbefff346,int16 L0xbefff348,int16 L0xbefff34a,
int16 L0xbefff34c,int16 L0xbefff34e,int16 L0xbefff350,int16 L0xbefff352,
int16 L0xbefff354,int16 L0xbefff356,int16 L0xbefff358,int16 L0xbefff35a,
int16 L0xbefff35c,int16 L0xbefff35e,int16 L0xbefff360,int16 L0xbefff362,
int16 L0xbefff364,int16 L0xbefff366,int16 L0xbefff368,int16 L0xbefff36a,
int16 L0xbefff36c,int16 L0xbefff36e,int16 L0xbefff370,int16 L0xbefff372,
int16 L0xbefff374,int16 L0xbefff376,int16 L0xbefff378,int16 L0xbefff37a,
int16 L0xbefff37c,int16 L0xbefff37e,int16 L0xbefff380,int16 L0xbefff382,
int16 L0xbefff384,int16 L0xbefff386,int16 L0xbefff388,int16 L0xbefff38a,
int16 L0xbefff38c,int16 L0xbefff38e,int16 L0xbefff390,int16 L0xbefff392,
int16 L0xbefff394,int16 L0xbefff396,int16 L0xbefff398,int16 L0xbefff39a,
int16 L0xbefff39c,int16 L0xbefff39e,int16 L0xbefff3a0,int16 L0xbefff3a2,
int16 L0xbefff3a4,int16 L0xbefff3a6,int16 L0xbefff3a8,int16 L0xbefff3aa,
int16 L0xbefff3ac,int16 L0xbefff3ae,int16 L0xbefff3b0,int16 L0xbefff3b2,
int16 L0xbefff3b4,int16 L0xbefff3b6,int16 L0xbefff3b8,int16 L0xbefff3ba,
int16 L0xbefff3bc,int16 L0xbefff3be,int16 L0xbefff3c0,int16 L0xbefff3c2,
int16 Q, int16 Q2, int16 NQ, int16 NQ2, int16 F, int16 X
) =
{
Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
eqmod (F**2) (poly X [L0xbefff1c4, L0xbefff1c6]) [Q, X**2 - 17**1  ] /\
eqmod (F**2) (poly X [L0xbefff1c8, L0xbefff1ca]) [Q, X**2 - 17**129] /\
eqmod (F**2) (poly X [L0xbefff1cc, L0xbefff1ce]) [Q, X**2 - 17**65 ] /\
eqmod (F**2) (poly X [L0xbefff1d0, L0xbefff1d2]) [Q, X**2 - 17**193] /\
eqmod (F**2) (poly X [L0xbefff1d4, L0xbefff1d6]) [Q, X**2 - 17**33 ] /\
eqmod (F**2) (poly X [L0xbefff1d8, L0xbefff1da]) [Q, X**2 - 17**161] /\
eqmod (F**2) (poly X [L0xbefff1dc, L0xbefff1de]) [Q, X**2 - 17**97 ] /\
eqmod (F**2) (poly X [L0xbefff1e0, L0xbefff1e2]) [Q, X**2 - 17**225] /\
eqmod (F**2) (poly X [L0xbefff1e4, L0xbefff1e6]) [Q, X**2 - 17**17 ] /\
eqmod (F**2) (poly X [L0xbefff1e8, L0xbefff1ea]) [Q, X**2 - 17**145] /\
eqmod (F**2) (poly X [L0xbefff1ec, L0xbefff1ee]) [Q, X**2 - 17**81 ] /\
eqmod (F**2) (poly X [L0xbefff1f0, L0xbefff1f2]) [Q, X**2 - 17**209] /\
eqmod (F**2) (poly X [L0xbefff1f4, L0xbefff1f6]) [Q, X**2 - 17**49 ] /\
eqmod (F**2) (poly X [L0xbefff1f8, L0xbefff1fa]) [Q, X**2 - 17**177] /\
eqmod (F**2) (poly X [L0xbefff1fc, L0xbefff1fe]) [Q, X**2 - 17**113] /\
eqmod (F**2) (poly X [L0xbefff200, L0xbefff202]) [Q, X**2 - 17**241] /\
eqmod (F**2) (poly X [L0xbefff204, L0xbefff206]) [Q, X**2 - 17**9  ] /\
eqmod (F**2) (poly X [L0xbefff208, L0xbefff20a]) [Q, X**2 - 17**137] /\
eqmod (F**2) (poly X [L0xbefff20c, L0xbefff20e]) [Q, X**2 - 17**73 ] /\
eqmod (F**2) (poly X [L0xbefff210, L0xbefff212]) [Q, X**2 - 17**201] /\
eqmod (F**2) (poly X [L0xbefff214, L0xbefff216]) [Q, X**2 - 17**41 ] /\
eqmod (F**2) (poly X [L0xbefff218, L0xbefff21a]) [Q, X**2 - 17**169] /\
eqmod (F**2) (poly X [L0xbefff21c, L0xbefff21e]) [Q, X**2 - 17**105] /\
eqmod (F**2) (poly X [L0xbefff220, L0xbefff222]) [Q, X**2 - 17**233] /\
eqmod (F**2) (poly X [L0xbefff224, L0xbefff226]) [Q, X**2 - 17**25 ] /\
eqmod (F**2) (poly X [L0xbefff228, L0xbefff22a]) [Q, X**2 - 17**153] /\
eqmod (F**2) (poly X [L0xbefff22c, L0xbefff22e]) [Q, X**2 - 17**89 ] /\
eqmod (F**2) (poly X [L0xbefff230, L0xbefff232]) [Q, X**2 - 17**217] /\
eqmod (F**2) (poly X [L0xbefff234, L0xbefff236]) [Q, X**2 - 17**57 ] /\
eqmod (F**2) (poly X [L0xbefff238, L0xbefff23a]) [Q, X**2 - 17**185] /\
eqmod (F**2) (poly X [L0xbefff23c, L0xbefff23e]) [Q, X**2 - 17**121] /\
eqmod (F**2) (poly X [L0xbefff240, L0xbefff242]) [Q, X**2 - 17**249] /\
eqmod (F**2) (poly X [L0xbefff244, L0xbefff246]) [Q, X**2 - 17**5  ] /\
eqmod (F**2) (poly X [L0xbefff248, L0xbefff24a]) [Q, X**2 - 17**133] /\
eqmod (F**2) (poly X [L0xbefff24c, L0xbefff24e]) [Q, X**2 - 17**69 ] /\
eqmod (F**2) (poly X [L0xbefff250, L0xbefff252]) [Q, X**2 - 17**197] /\
eqmod (F**2) (poly X [L0xbefff254, L0xbefff256]) [Q, X**2 - 17**37 ] /\
eqmod (F**2) (poly X [L0xbefff258, L0xbefff25a]) [Q, X**2 - 17**165] /\
eqmod (F**2) (poly X [L0xbefff25c, L0xbefff25e]) [Q, X**2 - 17**101] /\
eqmod (F**2) (poly X [L0xbefff260, L0xbefff262]) [Q, X**2 - 17**229] /\
eqmod (F**2) (poly X [L0xbefff264, L0xbefff266]) [Q, X**2 - 17**21 ] /\
eqmod (F**2) (poly X [L0xbefff268, L0xbefff26a]) [Q, X**2 - 17**149] /\
eqmod (F**2) (poly X [L0xbefff26c, L0xbefff26e]) [Q, X**2 - 17**85 ] /\
eqmod (F**2) (poly X [L0xbefff270, L0xbefff272]) [Q, X**2 - 17**213] /\
eqmod (F**2) (poly X [L0xbefff274, L0xbefff276]) [Q, X**2 - 17**53 ] /\
eqmod (F**2) (poly X [L0xbefff278, L0xbefff27a]) [Q, X**2 - 17**181] /\
eqmod (F**2) (poly X [L0xbefff27c, L0xbefff27e]) [Q, X**2 - 17**117] /\
eqmod (F**2) (poly X [L0xbefff280, L0xbefff282]) [Q, X**2 - 17**245] /\
eqmod (F**2) (poly X [L0xbefff284, L0xbefff286]) [Q, X**2 - 17**13 ] /\
eqmod (F**2) (poly X [L0xbefff288, L0xbefff28a]) [Q, X**2 - 17**141] /\
eqmod (F**2) (poly X [L0xbefff28c, L0xbefff28e]) [Q, X**2 - 17**77 ] /\
eqmod (F**2) (poly X [L0xbefff290, L0xbefff292]) [Q, X**2 - 17**205] /\
eqmod (F**2) (poly X [L0xbefff294, L0xbefff296]) [Q, X**2 - 17**45 ] /\
eqmod (F**2) (poly X [L0xbefff298, L0xbefff29a]) [Q, X**2 - 17**173] /\
eqmod (F**2) (poly X [L0xbefff29c, L0xbefff29e]) [Q, X**2 - 17**109] /\
eqmod (F**2) (poly X [L0xbefff2a0, L0xbefff2a2]) [Q, X**2 - 17**237] /\
eqmod (F**2) (poly X [L0xbefff2a4, L0xbefff2a6]) [Q, X**2 - 17**29 ] /\
eqmod (F**2) (poly X [L0xbefff2a8, L0xbefff2aa]) [Q, X**2 - 17**157] /\
eqmod (F**2) (poly X [L0xbefff2ac, L0xbefff2ae]) [Q, X**2 - 17**93 ] /\
eqmod (F**2) (poly X [L0xbefff2b0, L0xbefff2b2]) [Q, X**2 - 17**221] /\
eqmod (F**2) (poly X [L0xbefff2b4, L0xbefff2b6]) [Q, X**2 - 17**61 ] /\
eqmod (F**2) (poly X [L0xbefff2b8, L0xbefff2ba]) [Q, X**2 - 17**189] /\
eqmod (F**2) (poly X [L0xbefff2bc, L0xbefff2be]) [Q, X**2 - 17**125] /\
eqmod (F**2) (poly X [L0xbefff2c0, L0xbefff2c2]) [Q, X**2 - 17**253] /\
eqmod (F**2) (poly X [L0xbefff2c4, L0xbefff2c6]) [Q, X**2 - 17**3  ] /\
eqmod (F**2) (poly X [L0xbefff2c8, L0xbefff2ca]) [Q, X**2 - 17**131] /\
eqmod (F**2) (poly X [L0xbefff2cc, L0xbefff2ce]) [Q, X**2 - 17**67 ] /\
eqmod (F**2) (poly X [L0xbefff2d0, L0xbefff2d2]) [Q, X**2 - 17**195] /\
eqmod (F**2) (poly X [L0xbefff2d4, L0xbefff2d6]) [Q, X**2 - 17**35 ] /\
eqmod (F**2) (poly X [L0xbefff2d8, L0xbefff2da]) [Q, X**2 - 17**163] /\
eqmod (F**2) (poly X [L0xbefff2dc, L0xbefff2de]) [Q, X**2 - 17**99 ] /\
eqmod (F**2) (poly X [L0xbefff2e0, L0xbefff2e2]) [Q, X**2 - 17**227] /\
eqmod (F**2) (poly X [L0xbefff2e4, L0xbefff2e6]) [Q, X**2 - 17**19 ] /\
eqmod (F**2) (poly X [L0xbefff2e8, L0xbefff2ea]) [Q, X**2 - 17**147] /\
eqmod (F**2) (poly X [L0xbefff2ec, L0xbefff2ee]) [Q, X**2 - 17**83 ] /\
eqmod (F**2) (poly X [L0xbefff2f0, L0xbefff2f2]) [Q, X**2 - 17**211] /\
eqmod (F**2) (poly X [L0xbefff2f4, L0xbefff2f6]) [Q, X**2 - 17**51 ] /\
eqmod (F**2) (poly X [L0xbefff2f8, L0xbefff2fa]) [Q, X**2 - 17**179] /\
eqmod (F**2) (poly X [L0xbefff2fc, L0xbefff2fe]) [Q, X**2 - 17**115] /\
eqmod (F**2) (poly X [L0xbefff300, L0xbefff302]) [Q, X**2 - 17**243] /\
eqmod (F**2) (poly X [L0xbefff304, L0xbefff306]) [Q, X**2 - 17**11 ] /\
eqmod (F**2) (poly X [L0xbefff308, L0xbefff30a]) [Q, X**2 - 17**139] /\
eqmod (F**2) (poly X [L0xbefff30c, L0xbefff30e]) [Q, X**2 - 17**75 ] /\
eqmod (F**2) (poly X [L0xbefff310, L0xbefff312]) [Q, X**2 - 17**203] /\
eqmod (F**2) (poly X [L0xbefff314, L0xbefff316]) [Q, X**2 - 17**43 ] /\
eqmod (F**2) (poly X [L0xbefff318, L0xbefff31a]) [Q, X**2 - 17**171] /\
eqmod (F**2) (poly X [L0xbefff31c, L0xbefff31e]) [Q, X**2 - 17**107] /\
eqmod (F**2) (poly X [L0xbefff320, L0xbefff322]) [Q, X**2 - 17**235] /\
eqmod (F**2) (poly X [L0xbefff324, L0xbefff326]) [Q, X**2 - 17**27 ] /\
eqmod (F**2) (poly X [L0xbefff328, L0xbefff32a]) [Q, X**2 - 17**155] /\
eqmod (F**2) (poly X [L0xbefff32c, L0xbefff32e]) [Q, X**2 - 17**91 ] /\
eqmod (F**2) (poly X [L0xbefff330, L0xbefff332]) [Q, X**2 - 17**219] /\
eqmod (F**2) (poly X [L0xbefff334, L0xbefff336]) [Q, X**2 - 17**59 ] /\
eqmod (F**2) (poly X [L0xbefff338, L0xbefff33a]) [Q, X**2 - 17**187] /\
eqmod (F**2) (poly X [L0xbefff33c, L0xbefff33e]) [Q, X**2 - 17**123] /\
eqmod (F**2) (poly X [L0xbefff340, L0xbefff342]) [Q, X**2 - 17**251] /\
eqmod (F**2) (poly X [L0xbefff344, L0xbefff346]) [Q, X**2 - 17**7  ] /\
eqmod (F**2) (poly X [L0xbefff348, L0xbefff34a]) [Q, X**2 - 17**135] /\
eqmod (F**2) (poly X [L0xbefff34c, L0xbefff34e]) [Q, X**2 - 17**71 ] /\
eqmod (F**2) (poly X [L0xbefff350, L0xbefff352]) [Q, X**2 - 17**199] /\
eqmod (F**2) (poly X [L0xbefff354, L0xbefff356]) [Q, X**2 - 17**39 ] /\
eqmod (F**2) (poly X [L0xbefff358, L0xbefff35a]) [Q, X**2 - 17**167] /\
eqmod (F**2) (poly X [L0xbefff35c, L0xbefff35e]) [Q, X**2 - 17**103] /\
eqmod (F**2) (poly X [L0xbefff360, L0xbefff362]) [Q, X**2 - 17**231] /\
eqmod (F**2) (poly X [L0xbefff364, L0xbefff366]) [Q, X**2 - 17**23 ] /\
eqmod (F**2) (poly X [L0xbefff368, L0xbefff36a]) [Q, X**2 - 17**151] /\
eqmod (F**2) (poly X [L0xbefff36c, L0xbefff36e]) [Q, X**2 - 17**87 ] /\
eqmod (F**2) (poly X [L0xbefff370, L0xbefff372]) [Q, X**2 - 17**215] /\
eqmod (F**2) (poly X [L0xbefff374, L0xbefff376]) [Q, X**2 - 17**55 ] /\
eqmod (F**2) (poly X [L0xbefff378, L0xbefff37a]) [Q, X**2 - 17**183] /\
eqmod (F**2) (poly X [L0xbefff37c, L0xbefff37e]) [Q, X**2 - 17**119] /\
eqmod (F**2) (poly X [L0xbefff380, L0xbefff382]) [Q, X**2 - 17**247] /\
eqmod (F**2) (poly X [L0xbefff384, L0xbefff386]) [Q, X**2 - 17**15 ] /\
eqmod (F**2) (poly X [L0xbefff388, L0xbefff38a]) [Q, X**2 - 17**143] /\
eqmod (F**2) (poly X [L0xbefff38c, L0xbefff38e]) [Q, X**2 - 17**79 ] /\
eqmod (F**2) (poly X [L0xbefff390, L0xbefff392]) [Q, X**2 - 17**207] /\
eqmod (F**2) (poly X [L0xbefff394, L0xbefff396]) [Q, X**2 - 17**47 ] /\
eqmod (F**2) (poly X [L0xbefff398, L0xbefff39a]) [Q, X**2 - 17**175] /\
eqmod (F**2) (poly X [L0xbefff39c, L0xbefff39e]) [Q, X**2 - 17**111] /\
eqmod (F**2) (poly X [L0xbefff3a0, L0xbefff3a2]) [Q, X**2 - 17**239] /\
eqmod (F**2) (poly X [L0xbefff3a4, L0xbefff3a6]) [Q, X**2 - 17**31 ] /\
eqmod (F**2) (poly X [L0xbefff3a8, L0xbefff3aa]) [Q, X**2 - 17**159] /\
eqmod (F**2) (poly X [L0xbefff3ac, L0xbefff3ae]) [Q, X**2 - 17**95 ] /\
eqmod (F**2) (poly X [L0xbefff3b0, L0xbefff3b2]) [Q, X**2 - 17**223] /\
eqmod (F**2) (poly X [L0xbefff3b4, L0xbefff3b6]) [Q, X**2 - 17**63 ] /\
eqmod (F**2) (poly X [L0xbefff3b8, L0xbefff3ba]) [Q, X**2 - 17**191] /\
eqmod (F**2) (poly X [L0xbefff3bc, L0xbefff3be]) [Q, X**2 - 17**127] /\
eqmod (F**2) (poly X [L0xbefff3c0, L0xbefff3c2]) [Q, X**2 - 17**255] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff1c4,L0xbefff1c6,L0xbefff1c8,L0xbefff1ca] /\
[L0xbefff1c4,L0xbefff1c6,L0xbefff1c8,L0xbefff1ca]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff1cc,L0xbefff1ce,L0xbefff1d0,L0xbefff1d2] /\
[L0xbefff1cc,L0xbefff1ce,L0xbefff1d0,L0xbefff1d2]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff1d4,L0xbefff1d6,L0xbefff1d8,L0xbefff1da] /\
[L0xbefff1d4,L0xbefff1d6,L0xbefff1d8,L0xbefff1da]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff1dc,L0xbefff1de,L0xbefff1e0,L0xbefff1e2] /\
[L0xbefff1dc,L0xbefff1de,L0xbefff1e0,L0xbefff1e2]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff1e4,L0xbefff1e6,L0xbefff1e8,L0xbefff1ea] /\
[L0xbefff1e4,L0xbefff1e6,L0xbefff1e8,L0xbefff1ea]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff1ec,L0xbefff1ee,L0xbefff1f0,L0xbefff1f2] /\
[L0xbefff1ec,L0xbefff1ee,L0xbefff1f0,L0xbefff1f2]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff1f4,L0xbefff1f6,L0xbefff1f8,L0xbefff1fa] /\
[L0xbefff1f4,L0xbefff1f6,L0xbefff1f8,L0xbefff1fa]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff1fc,L0xbefff1fe,L0xbefff200,L0xbefff202] /\
[L0xbefff1fc,L0xbefff1fe,L0xbefff200,L0xbefff202]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff204,L0xbefff206,L0xbefff208,L0xbefff20a] /\
[L0xbefff204,L0xbefff206,L0xbefff208,L0xbefff20a]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff20c,L0xbefff20e,L0xbefff210,L0xbefff212] /\
[L0xbefff20c,L0xbefff20e,L0xbefff210,L0xbefff212]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff214,L0xbefff216,L0xbefff218,L0xbefff21a] /\
[L0xbefff214,L0xbefff216,L0xbefff218,L0xbefff21a]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff21c,L0xbefff21e,L0xbefff220,L0xbefff222] /\
[L0xbefff21c,L0xbefff21e,L0xbefff220,L0xbefff222]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff224,L0xbefff226,L0xbefff228,L0xbefff22a] /\
[L0xbefff224,L0xbefff226,L0xbefff228,L0xbefff22a]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff22c,L0xbefff22e,L0xbefff230,L0xbefff232] /\
[L0xbefff22c,L0xbefff22e,L0xbefff230,L0xbefff232]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff234,L0xbefff236,L0xbefff238,L0xbefff23a] /\
[L0xbefff234,L0xbefff236,L0xbefff238,L0xbefff23a]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff23c,L0xbefff23e,L0xbefff240,L0xbefff242] /\
[L0xbefff23c,L0xbefff23e,L0xbefff240,L0xbefff242]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff244,L0xbefff246,L0xbefff248,L0xbefff24a] /\
[L0xbefff244,L0xbefff246,L0xbefff248,L0xbefff24a]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff24c,L0xbefff24e,L0xbefff250,L0xbefff252] /\
[L0xbefff24c,L0xbefff24e,L0xbefff250,L0xbefff252]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff254,L0xbefff256,L0xbefff258,L0xbefff25a] /\
[L0xbefff254,L0xbefff256,L0xbefff258,L0xbefff25a]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff25c,L0xbefff25e,L0xbefff260,L0xbefff262] /\
[L0xbefff25c,L0xbefff25e,L0xbefff260,L0xbefff262]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff264,L0xbefff266,L0xbefff268,L0xbefff26a] /\
[L0xbefff264,L0xbefff266,L0xbefff268,L0xbefff26a]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff26c,L0xbefff26e,L0xbefff270,L0xbefff272] /\
[L0xbefff26c,L0xbefff26e,L0xbefff270,L0xbefff272]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff274,L0xbefff276,L0xbefff278,L0xbefff27a] /\
[L0xbefff274,L0xbefff276,L0xbefff278,L0xbefff27a]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff27c,L0xbefff27e,L0xbefff280,L0xbefff282] /\
[L0xbefff27c,L0xbefff27e,L0xbefff280,L0xbefff282]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff284,L0xbefff286,L0xbefff288,L0xbefff28a] /\
[L0xbefff284,L0xbefff286,L0xbefff288,L0xbefff28a]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff28c,L0xbefff28e,L0xbefff290,L0xbefff292] /\
[L0xbefff28c,L0xbefff28e,L0xbefff290,L0xbefff292]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff294,L0xbefff296,L0xbefff298,L0xbefff29a] /\
[L0xbefff294,L0xbefff296,L0xbefff298,L0xbefff29a]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff29c,L0xbefff29e,L0xbefff2a0,L0xbefff2a2] /\
[L0xbefff29c,L0xbefff29e,L0xbefff2a0,L0xbefff2a2]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff2a4,L0xbefff2a6,L0xbefff2a8,L0xbefff2aa] /\
[L0xbefff2a4,L0xbefff2a6,L0xbefff2a8,L0xbefff2aa]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff2ac,L0xbefff2ae,L0xbefff2b0,L0xbefff2b2] /\
[L0xbefff2ac,L0xbefff2ae,L0xbefff2b0,L0xbefff2b2]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff2b4,L0xbefff2b6,L0xbefff2b8,L0xbefff2ba] /\
[L0xbefff2b4,L0xbefff2b6,L0xbefff2b8,L0xbefff2ba]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff2bc,L0xbefff2be,L0xbefff2c0,L0xbefff2c2] /\
[L0xbefff2bc,L0xbefff2be,L0xbefff2c0,L0xbefff2c2]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff2c4,L0xbefff2c6,L0xbefff2c8,L0xbefff2ca] /\
[L0xbefff2c4,L0xbefff2c6,L0xbefff2c8,L0xbefff2ca]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff2cc,L0xbefff2ce,L0xbefff2d0,L0xbefff2d2] /\
[L0xbefff2cc,L0xbefff2ce,L0xbefff2d0,L0xbefff2d2]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff2d4,L0xbefff2d6,L0xbefff2d8,L0xbefff2da] /\
[L0xbefff2d4,L0xbefff2d6,L0xbefff2d8,L0xbefff2da]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff2dc,L0xbefff2de,L0xbefff2e0,L0xbefff2e2] /\
[L0xbefff2dc,L0xbefff2de,L0xbefff2e0,L0xbefff2e2]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff2e4,L0xbefff2e6,L0xbefff2e8,L0xbefff2ea] /\
[L0xbefff2e4,L0xbefff2e6,L0xbefff2e8,L0xbefff2ea]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff2ec,L0xbefff2ee,L0xbefff2f0,L0xbefff2f2] /\
[L0xbefff2ec,L0xbefff2ee,L0xbefff2f0,L0xbefff2f2]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff2f4,L0xbefff2f6,L0xbefff2f8,L0xbefff2fa] /\
[L0xbefff2f4,L0xbefff2f6,L0xbefff2f8,L0xbefff2fa]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff2fc,L0xbefff2fe,L0xbefff300,L0xbefff302] /\
[L0xbefff2fc,L0xbefff2fe,L0xbefff300,L0xbefff302]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff304,L0xbefff306,L0xbefff308,L0xbefff30a] /\
[L0xbefff304,L0xbefff306,L0xbefff308,L0xbefff30a]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff30c,L0xbefff30e,L0xbefff310,L0xbefff312] /\
[L0xbefff30c,L0xbefff30e,L0xbefff310,L0xbefff312]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff314,L0xbefff316,L0xbefff318,L0xbefff31a] /\
[L0xbefff314,L0xbefff316,L0xbefff318,L0xbefff31a]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff31c,L0xbefff31e,L0xbefff320,L0xbefff322] /\
[L0xbefff31c,L0xbefff31e,L0xbefff320,L0xbefff322]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff324,L0xbefff326,L0xbefff328,L0xbefff32a] /\
[L0xbefff324,L0xbefff326,L0xbefff328,L0xbefff32a]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff32c,L0xbefff32e,L0xbefff330,L0xbefff332] /\
[L0xbefff32c,L0xbefff32e,L0xbefff330,L0xbefff332]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff334,L0xbefff336,L0xbefff338,L0xbefff33a] /\
[L0xbefff334,L0xbefff336,L0xbefff338,L0xbefff33a]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff33c,L0xbefff33e,L0xbefff340,L0xbefff342] /\
[L0xbefff33c,L0xbefff33e,L0xbefff340,L0xbefff342]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff344,L0xbefff346,L0xbefff348,L0xbefff34a] /\
[L0xbefff344,L0xbefff346,L0xbefff348,L0xbefff34a]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff34c,L0xbefff34e,L0xbefff350,L0xbefff352] /\
[L0xbefff34c,L0xbefff34e,L0xbefff350,L0xbefff352]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff354,L0xbefff356,L0xbefff358,L0xbefff35a] /\
[L0xbefff354,L0xbefff356,L0xbefff358,L0xbefff35a]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff35c,L0xbefff35e,L0xbefff360,L0xbefff362] /\
[L0xbefff35c,L0xbefff35e,L0xbefff360,L0xbefff362]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff364,L0xbefff366,L0xbefff368,L0xbefff36a] /\
[L0xbefff364,L0xbefff366,L0xbefff368,L0xbefff36a]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff36c,L0xbefff36e,L0xbefff370,L0xbefff372] /\
[L0xbefff36c,L0xbefff36e,L0xbefff370,L0xbefff372]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff374,L0xbefff376,L0xbefff378,L0xbefff37a] /\
[L0xbefff374,L0xbefff376,L0xbefff378,L0xbefff37a]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff37c,L0xbefff37e,L0xbefff380,L0xbefff382] /\
[L0xbefff37c,L0xbefff37e,L0xbefff380,L0xbefff382]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff384,L0xbefff386,L0xbefff388,L0xbefff38a] /\
[L0xbefff384,L0xbefff386,L0xbefff388,L0xbefff38a]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff38c,L0xbefff38e,L0xbefff390,L0xbefff392] /\
[L0xbefff38c,L0xbefff38e,L0xbefff390,L0xbefff392]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff394,L0xbefff396,L0xbefff398,L0xbefff39a] /\
[L0xbefff394,L0xbefff396,L0xbefff398,L0xbefff39a]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff39c,L0xbefff39e,L0xbefff3a0,L0xbefff3a2] /\
[L0xbefff39c,L0xbefff39e,L0xbefff3a0,L0xbefff3a2]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff3a4,L0xbefff3a6,L0xbefff3a8,L0xbefff3aa] /\
[L0xbefff3a4,L0xbefff3a6,L0xbefff3a8,L0xbefff3aa]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff3ac,L0xbefff3ae,L0xbefff3b0,L0xbefff3b2] /\
[L0xbefff3ac,L0xbefff3ae,L0xbefff3b0,L0xbefff3b2]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff3b4,L0xbefff3b6,L0xbefff3b8,L0xbefff3ba] /\
[L0xbefff3b4,L0xbefff3b6,L0xbefff3b8,L0xbefff3ba]<[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<[L0xbefff3bc,L0xbefff3be,L0xbefff3c0,L0xbefff3c2] /\
[L0xbefff3bc,L0xbefff3be,L0xbefff3c0,L0xbefff3c2]<[Q2,Q2,Q2,Q2]
&&
Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1c4,L0xbefff1c6,L0xbefff1c8,L0xbefff1ca] /\
[L0xbefff1c4,L0xbefff1c6,L0xbefff1c8,L0xbefff1ca]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1cc,L0xbefff1ce,L0xbefff1d0,L0xbefff1d2] /\
[L0xbefff1cc,L0xbefff1ce,L0xbefff1d0,L0xbefff1d2]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1d4,L0xbefff1d6,L0xbefff1d8,L0xbefff1da] /\
[L0xbefff1d4,L0xbefff1d6,L0xbefff1d8,L0xbefff1da]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1dc,L0xbefff1de,L0xbefff1e0,L0xbefff1e2] /\
[L0xbefff1dc,L0xbefff1de,L0xbefff1e0,L0xbefff1e2]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1e4,L0xbefff1e6,L0xbefff1e8,L0xbefff1ea] /\
[L0xbefff1e4,L0xbefff1e6,L0xbefff1e8,L0xbefff1ea]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1ec,L0xbefff1ee,L0xbefff1f0,L0xbefff1f2] /\
[L0xbefff1ec,L0xbefff1ee,L0xbefff1f0,L0xbefff1f2]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1f4,L0xbefff1f6,L0xbefff1f8,L0xbefff1fa] /\
[L0xbefff1f4,L0xbefff1f6,L0xbefff1f8,L0xbefff1fa]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1fc,L0xbefff1fe,L0xbefff200,L0xbefff202] /\
[L0xbefff1fc,L0xbefff1fe,L0xbefff200,L0xbefff202]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff204,L0xbefff206,L0xbefff208,L0xbefff20a] /\
[L0xbefff204,L0xbefff206,L0xbefff208,L0xbefff20a]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff20c,L0xbefff20e,L0xbefff210,L0xbefff212] /\
[L0xbefff20c,L0xbefff20e,L0xbefff210,L0xbefff212]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff214,L0xbefff216,L0xbefff218,L0xbefff21a] /\
[L0xbefff214,L0xbefff216,L0xbefff218,L0xbefff21a]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff21c,L0xbefff21e,L0xbefff220,L0xbefff222] /\
[L0xbefff21c,L0xbefff21e,L0xbefff220,L0xbefff222]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff224,L0xbefff226,L0xbefff228,L0xbefff22a] /\
[L0xbefff224,L0xbefff226,L0xbefff228,L0xbefff22a]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff22c,L0xbefff22e,L0xbefff230,L0xbefff232] /\
[L0xbefff22c,L0xbefff22e,L0xbefff230,L0xbefff232]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff234,L0xbefff236,L0xbefff238,L0xbefff23a] /\
[L0xbefff234,L0xbefff236,L0xbefff238,L0xbefff23a]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff23c,L0xbefff23e,L0xbefff240,L0xbefff242] /\
[L0xbefff23c,L0xbefff23e,L0xbefff240,L0xbefff242]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff244,L0xbefff246,L0xbefff248,L0xbefff24a] /\
[L0xbefff244,L0xbefff246,L0xbefff248,L0xbefff24a]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff24c,L0xbefff24e,L0xbefff250,L0xbefff252] /\
[L0xbefff24c,L0xbefff24e,L0xbefff250,L0xbefff252]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff254,L0xbefff256,L0xbefff258,L0xbefff25a] /\
[L0xbefff254,L0xbefff256,L0xbefff258,L0xbefff25a]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff25c,L0xbefff25e,L0xbefff260,L0xbefff262] /\
[L0xbefff25c,L0xbefff25e,L0xbefff260,L0xbefff262]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff264,L0xbefff266,L0xbefff268,L0xbefff26a] /\
[L0xbefff264,L0xbefff266,L0xbefff268,L0xbefff26a]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff26c,L0xbefff26e,L0xbefff270,L0xbefff272] /\
[L0xbefff26c,L0xbefff26e,L0xbefff270,L0xbefff272]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff274,L0xbefff276,L0xbefff278,L0xbefff27a] /\
[L0xbefff274,L0xbefff276,L0xbefff278,L0xbefff27a]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff27c,L0xbefff27e,L0xbefff280,L0xbefff282] /\
[L0xbefff27c,L0xbefff27e,L0xbefff280,L0xbefff282]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff284,L0xbefff286,L0xbefff288,L0xbefff28a] /\
[L0xbefff284,L0xbefff286,L0xbefff288,L0xbefff28a]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff28c,L0xbefff28e,L0xbefff290,L0xbefff292] /\
[L0xbefff28c,L0xbefff28e,L0xbefff290,L0xbefff292]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff294,L0xbefff296,L0xbefff298,L0xbefff29a] /\
[L0xbefff294,L0xbefff296,L0xbefff298,L0xbefff29a]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff29c,L0xbefff29e,L0xbefff2a0,L0xbefff2a2] /\
[L0xbefff29c,L0xbefff29e,L0xbefff2a0,L0xbefff2a2]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2a4,L0xbefff2a6,L0xbefff2a8,L0xbefff2aa] /\
[L0xbefff2a4,L0xbefff2a6,L0xbefff2a8,L0xbefff2aa]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2ac,L0xbefff2ae,L0xbefff2b0,L0xbefff2b2] /\
[L0xbefff2ac,L0xbefff2ae,L0xbefff2b0,L0xbefff2b2]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2b4,L0xbefff2b6,L0xbefff2b8,L0xbefff2ba] /\
[L0xbefff2b4,L0xbefff2b6,L0xbefff2b8,L0xbefff2ba]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2bc,L0xbefff2be,L0xbefff2c0,L0xbefff2c2] /\
[L0xbefff2bc,L0xbefff2be,L0xbefff2c0,L0xbefff2c2]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2c4,L0xbefff2c6,L0xbefff2c8,L0xbefff2ca] /\
[L0xbefff2c4,L0xbefff2c6,L0xbefff2c8,L0xbefff2ca]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2cc,L0xbefff2ce,L0xbefff2d0,L0xbefff2d2] /\
[L0xbefff2cc,L0xbefff2ce,L0xbefff2d0,L0xbefff2d2]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2d4,L0xbefff2d6,L0xbefff2d8,L0xbefff2da] /\
[L0xbefff2d4,L0xbefff2d6,L0xbefff2d8,L0xbefff2da]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2dc,L0xbefff2de,L0xbefff2e0,L0xbefff2e2] /\
[L0xbefff2dc,L0xbefff2de,L0xbefff2e0,L0xbefff2e2]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2e4,L0xbefff2e6,L0xbefff2e8,L0xbefff2ea] /\
[L0xbefff2e4,L0xbefff2e6,L0xbefff2e8,L0xbefff2ea]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2ec,L0xbefff2ee,L0xbefff2f0,L0xbefff2f2] /\
[L0xbefff2ec,L0xbefff2ee,L0xbefff2f0,L0xbefff2f2]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2f4,L0xbefff2f6,L0xbefff2f8,L0xbefff2fa] /\
[L0xbefff2f4,L0xbefff2f6,L0xbefff2f8,L0xbefff2fa]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2fc,L0xbefff2fe,L0xbefff300,L0xbefff302] /\
[L0xbefff2fc,L0xbefff2fe,L0xbefff300,L0xbefff302]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff304,L0xbefff306,L0xbefff308,L0xbefff30a] /\
[L0xbefff304,L0xbefff306,L0xbefff308,L0xbefff30a]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff30c,L0xbefff30e,L0xbefff310,L0xbefff312] /\
[L0xbefff30c,L0xbefff30e,L0xbefff310,L0xbefff312]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff314,L0xbefff316,L0xbefff318,L0xbefff31a] /\
[L0xbefff314,L0xbefff316,L0xbefff318,L0xbefff31a]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff31c,L0xbefff31e,L0xbefff320,L0xbefff322] /\
[L0xbefff31c,L0xbefff31e,L0xbefff320,L0xbefff322]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff324,L0xbefff326,L0xbefff328,L0xbefff32a] /\
[L0xbefff324,L0xbefff326,L0xbefff328,L0xbefff32a]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff32c,L0xbefff32e,L0xbefff330,L0xbefff332] /\
[L0xbefff32c,L0xbefff32e,L0xbefff330,L0xbefff332]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff334,L0xbefff336,L0xbefff338,L0xbefff33a] /\
[L0xbefff334,L0xbefff336,L0xbefff338,L0xbefff33a]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff33c,L0xbefff33e,L0xbefff340,L0xbefff342] /\
[L0xbefff33c,L0xbefff33e,L0xbefff340,L0xbefff342]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff344,L0xbefff346,L0xbefff348,L0xbefff34a] /\
[L0xbefff344,L0xbefff346,L0xbefff348,L0xbefff34a]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff34c,L0xbefff34e,L0xbefff350,L0xbefff352] /\
[L0xbefff34c,L0xbefff34e,L0xbefff350,L0xbefff352]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff354,L0xbefff356,L0xbefff358,L0xbefff35a] /\
[L0xbefff354,L0xbefff356,L0xbefff358,L0xbefff35a]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff35c,L0xbefff35e,L0xbefff360,L0xbefff362] /\
[L0xbefff35c,L0xbefff35e,L0xbefff360,L0xbefff362]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff364,L0xbefff366,L0xbefff368,L0xbefff36a] /\
[L0xbefff364,L0xbefff366,L0xbefff368,L0xbefff36a]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff36c,L0xbefff36e,L0xbefff370,L0xbefff372] /\
[L0xbefff36c,L0xbefff36e,L0xbefff370,L0xbefff372]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff374,L0xbefff376,L0xbefff378,L0xbefff37a] /\
[L0xbefff374,L0xbefff376,L0xbefff378,L0xbefff37a]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff37c,L0xbefff37e,L0xbefff380,L0xbefff382] /\
[L0xbefff37c,L0xbefff37e,L0xbefff380,L0xbefff382]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff384,L0xbefff386,L0xbefff388,L0xbefff38a] /\
[L0xbefff384,L0xbefff386,L0xbefff388,L0xbefff38a]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff38c,L0xbefff38e,L0xbefff390,L0xbefff392] /\
[L0xbefff38c,L0xbefff38e,L0xbefff390,L0xbefff392]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff394,L0xbefff396,L0xbefff398,L0xbefff39a] /\
[L0xbefff394,L0xbefff396,L0xbefff398,L0xbefff39a]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff39c,L0xbefff39e,L0xbefff3a0,L0xbefff3a2] /\
[L0xbefff39c,L0xbefff39e,L0xbefff3a0,L0xbefff3a2]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff3a4,L0xbefff3a6,L0xbefff3a8,L0xbefff3aa] /\
[L0xbefff3a4,L0xbefff3a6,L0xbefff3a8,L0xbefff3aa]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff3ac,L0xbefff3ae,L0xbefff3b0,L0xbefff3b2] /\
[L0xbefff3ac,L0xbefff3ae,L0xbefff3b0,L0xbefff3b2]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff3b4,L0xbefff3b6,L0xbefff3b8,L0xbefff3ba] /\
[L0xbefff3b4,L0xbefff3b6,L0xbefff3b8,L0xbefff3ba]<s[Q2,Q2,Q2,Q2] /\
[NQ2,NQ2,NQ2,NQ2]<s[L0xbefff3bc,L0xbefff3be,L0xbefff3c0,L0xbefff3c2] /\
[L0xbefff3bc,L0xbefff3be,L0xbefff3c0,L0xbefff3c2]<s[Q2,Q2,Q2,Q2]
}

(**************** initialization ****************)


mov L0x40187c (    1290168)@int32; (*     1 *)
mov L0x401880 (    1290168)@int32; (*     1 *)
mov L0x401884 ( 2064267851)@int32; (*  1600 *)
mov L0x401888 (    1290168)@int32; (*     1 *)
mov L0x40188c (   51606697)@int32; (*    40 *)
mov L0x401890 ( 2064267851)@int32; (*  1600 *)
mov L0x401894 (  966335388)@int32; (*   749 *)
mov L0x401898 (    1290168)@int32; (*     1 *)
mov L0x40189c (-1094061960)@int32; (*  -848 *)
mov L0x4018a0 (   51606697)@int32; (*    40 *)
mov L0x4018a4 ( -812805466)@int32; (*  -630 *)
mov L0x4018a8 ( 2064267851)@int32; (*  1600 *)
mov L0x4018ac ( 1847519727)@int32; (*  1432 *)
mov L0x4018b0 (  966335388)@int32; (*   749 *)
mov L0x4018b4 (  886345009)@int32; (*   687 *)
mov L0x4018b8 (    1290168)@int32; (*     1 *)
mov L0x4018bc ( 2064267851)@int32; (*  1600 *)
mov L0x4018c0 (    1290168)@int32; (*     1 *)
mov L0x4018c4 (   51606697)@int32; (*    40 *)
mov L0x4018c8 ( 2064267851)@int32; (*  1600 *)
mov L0x4018cc (  966335388)@int32; (*   749 *)
mov L0x4018d0 (-1859131232)@int32; (* -1441 *)
mov L0x4018d4 (  290287667)@int32; (*   225 *)
mov L0x4018d8 (-1350805274)@int32; (* -1047 *)
mov L0x4018dc (-1273395230)@int32; (*  -987 *)
mov L0x4018e0 ( 1802363867)@int32; (*  1397 *)
mov L0x4018e4 (  603798347)@int32; (*   468 *)
mov L0x4018e8 ( -919889360)@int32; (*  -713 *)
mov L0x4018ec (-1617869927)@int32; (* -1254 *)
mov L0x4018f0 ( 2042335005)@int32; (*  1583 *)
mov L0x4018f4 (-1059227440)@int32; (*  -821 *)
mov L0x4018f8 ( 1748176836)@int32; (*  1355 *)
mov L0x4018fc (-1174052339)@int32; (*  -910 *)
mov L0x401900 (  282546663)@int32; (*   219 *)
mov L0x401904 (-1583035407)@int32; (* -1227 *)
mov L0x401908 ( 1103093133)@int32; (*   855 *)
mov L0x40190c ( 1659155285)@int32; (*  1286 *)
mov L0x401910 ( 1785591691)@int32; (*  1384 *)
mov L0x401914 ( 1941701947)@int32; (*  1505 *)
mov L0x401918 (-1590776412)@int32; (* -1233 *)
mov L0x40191c (  358666539)@int32; (*   278 *)
mov L0x401920 (  793452955)@int32; (*   615 *)
mov L0x401924 ( 1461759672)@int32; (*  1133 *)
mov L0x401928 ( 1673347127)@int32; (*  1297 *)
mov L0x40192c (-1094061960)@int32; (*  -848 *)
mov L0x401930 ( 2042335005)@int32; (*  1583 *)
mov L0x401934 ( -734105254)@int32; (*  -569 *)
mov L0x401938 (-1059227440)@int32; (*  -821 *)
mov L0x40193c (  580575333)@int32; (*   450 *)
mov L0x401940 ( 1748176836)@int32; (*  1355 *)
mov L0x401944 ( 1207596693)@int32; (*   936 *)
mov L0x401948 ( -407692900)@int32; (*  -316 *)
mov L0x40194c ( 2126195886)@int32; (*  1648 *)
mov L0x401950 (  872153167)@int32; (*   676 *)
mov L0x401954 ( -851510488)@int32; (*  -660 *)
mov L0x401958 (  526388302)@int32; (*   408 *)
mov L0x40195c (  299318839)@int32; (*   232 *)
mov L0x401960 ( -419304407)@int32; (*  -325 *)
mov L0x401964 ( -912148356)@int32; (*  -707 *)
mov L0x401968 (-1028263422)@int32; (*  -797 *)
mov L0x40196c (-1719793152)@int32; (* -1333 *)
mov L0x401970 ( 1404992306)@int32; (*  1089 *)
mov L0x401974 ( 1824296713)@int32; (*  1414 *)
mov L0x401978 (  -42575524)@int32; (*   -33 *)
mov L0x40197c (-1703020976)@int32; (* -1320 *)
mov L0x401980 (  598637677)@int32; (*   464 *)
mov L0x401984 ( 1997179146)@int32; (*  1548 *)
mov L0x401988 (-1390800464)@int32; (* -1078 *)
mov L0x40198c (-1717212817)@int32; (* -1331 *)
mov L0x401990 (  202556283)@int32; (*   157 *)
mov L0x401994 (   30964018)@int32; (*    24 *)
mov L0x401998 ( -487683279)@int32; (*  -378 *)
mov L0x40199c ( 1238560711)@int32; (*   960 *)
mov L0x4019a0 ( 1967505295)@int32; (*  1525 *)
mov L0x4019a4 (   51606697)@int32; (*    40 *)
mov L0x4019a8 (-1094061960)@int32; (*  -848 *)
mov L0x4019ac ( 1847519727)@int32; (*  1432 *)
mov L0x4019b0 ( 2042335005)@int32; (*  1583 *)
mov L0x4019b4 (   89021552)@int32; (*    69 *)
mov L0x4019b8 ( -734105254)@int32; (*  -569 *)
mov L0x4019bc (  700560902)@int32; (*   543 *)
mov L0x4019c0 ( 1633351937)@int32; (*  1266 *)
mov L0x4019c4 (-2102972872)@int32; (* -1630 *)
mov L0x4019c8 (  909568022)@int32; (*   705 *)
mov L0x4019cc ( 1780431021)@int32; (*  1380 *)
mov L0x4019d0 ( 2022982494)@int32; (*  1568 *)
mov L0x4019d4 (-1797203197)@int32; (* -1393 *)
mov L0x4019d8 ( -685078892)@int32; (*  -531 *)
mov L0x4019dc ( 1126316146)@int32; (*   873 *)
mov L0x4019e0 (   89021552)@int32; (*    69 *)
mov L0x4019e4 (  576704831)@int32; (*   447 *)
mov L0x4019e8 ( -690239562)@int32; (*  -535 *)
mov L0x4019ec ( 1195985186)@int32; (*   927 *)
mov L0x4019f0 (  594767175)@int32; (*   461 *)
mov L0x4019f4 (-1979116801)@int32; (* -1534 *)
mov L0x4019f8 (-1855260730)@int32; (* -1438 *)
mov L0x4019fc ( -661855879)@int32; (*  -513 *)
mov L0x401a00 (-1386929961)@int32; (* -1075 *)
mov L0x401a04 ( -704431403)@int32; (*  -546 *)
mov L0x401a08 (  357376372)@int32; (*   277 *)
mov L0x401a0c ( 1887514916)@int32; (*  1463 *)
mov L0x401a10 ( 1410152976)@int32; (*  1093 *)
mov L0x401a14 (-1808814703)@int32; (* -1402 *)
mov L0x401a18 (  571544162)@int32; (*   443 *)
mov L0x401a1c ( -812805466)@int32; (*  -630 *)
mov L0x401a20 (-1028263422)@int32; (*  -797 *)
mov L0x401a24 ( -249002309)@int32; (*  -193 *)
mov L0x401a28 (-1719793152)@int32; (* -1333 *)
mov L0x401a2c (  -72249374)@int32; (*   -56 *)
mov L0x401a30 ( 1404992306)@int32; (*  1089 *)
mov L0x401a34 (  365117377)@int32; (*   283 *)
mov L0x401a38 ( -291577833)@int32; (*  -226 *)
mov L0x401a3c (-1850100060)@int32; (* -1434 *)
mov L0x401a40 ( 1221788534)@int32; (*   947 *)
mov L0x401a44 ( -989558400)@int32; (*  -767 *)
mov L0x401a48 ( 1626901100)@int32; (*  1261 *)
mov L0x401a4c ( -927630365)@int32; (*  -719 *)
mov L0x401a50 (  651534541)@int32; (*   505 *)
mov L0x401a54 ( 1549491056)@int32; (*  1201 *)
mov L0x401a58 ( 1819136044)@int32; (*  1410 *)
mov L0x401a5c (-1904287091)@int32; (* -1476 *)
mov L0x401a60 (-1727534157)@int32; (* -1339 *)
mov L0x401a64 ( 1643673276)@int32; (*  1274 *)
mov L0x401a68 ( 1322421592)@int32; (*  1025 *)
mov L0x401a6c ( 1357256112)@int32; (*  1052 *)
mov L0x401a70 (-1544330385)@int32; (* -1197 *)
mov L0x401a74 (  993428903)@int32; (*   770 *)
mov L0x401a78 ( -614119685)@int32; (*  -476 *)
mov L0x401a7c ( 1082450454)@int32; (*   839 *)
mov L0x401a80 ( 1205016358)@int32; (*   934 *)
mov L0x401a84 (  348345200)@int32; (*   270 *)
mov L0x401a88 (  956014049)@int32; (*   741 *)
mov L0x401a8c ( 1048906102)@int32; (*   813 *)
mov L0x401a90 ( -414143737)@int32; (*  -321 *)
mov L0x401a94 ( 2064267851)@int32; (*  1600 *)
mov L0x401a98 (   51606697)@int32; (*    40 *)
mov L0x401a9c (  966335388)@int32; (*   749 *)
mov L0x401aa0 (-1094061960)@int32; (*  -848 *)
mov L0x401aa4 ( -812805466)@int32; (*  -630 *)
mov L0x401aa8 ( 1847519727)@int32; (*  1432 *)
mov L0x401aac (  886345009)@int32; (*   687 *)
mov L0x401ab0 ( -952143545)@int32; (*  -738 *)
mov L0x401ab4 (  -36124687)@int32; (*   -28 *)
mov L0x401ab8 (  568963827)@int32; (*   441 *)
mov L0x401abc (-1444987495)@int32; (* -1120 *)
mov L0x401ac0 ( 1283716570)@int32; (*   995 *)
mov L0x401ac4 (-1964924959)@int32; (* -1523 *)
mov L0x401ac8 ( -190944776)@int32; (*  -148 *)
mov L0x401acc (-1287587071)@int32; (*  -998 *)
mov L0x401ad0 ( -734105254)@int32; (*  -569 *)
mov L0x401ad4 (  580575333)@int32; (*   450 *)
mov L0x401ad8 ( 1207596693)@int32; (*   936 *)
mov L0x401adc ( -836028479)@int32; (*  -648 *)
mov L0x401ae0 (  918599194)@int32; (*   712 *)
mov L0x401ae4 (-1910737928)@int32; (* -1481 *)
mov L0x401ae8 (  879894172)@int32; (*   682 *)
mov L0x401aec (-2077169524)@int32; (* -1610 *)
mov L0x401af0 (  503165289)@int32; (*   390 *)
mov L0x401af4 (-1482402349)@int32; (* -1149 *)
mov L0x401af8 (-1348224939)@int32; (* -1045 *)
mov L0x401afc (  833448145)@int32; (*   646 *)
mov L0x401b00 ( 1905577260)@int32; (*  1477 *)
mov L0x401b04 (-1021812585)@int32; (*  -792 *)
mov L0x401b08 (-1086320956)@int32; (*  -842 *)
mov L0x401b0c ( 1847519727)@int32; (*  1432 *)
mov L0x401b10 (   89021552)@int32; (*    69 *)
mov L0x401b14 (  700560902)@int32; (*   543 *)
mov L0x401b18 (  576704831)@int32; (*   447 *)
mov L0x401b1c ( 1593356747)@int32; (*  1235 *)
mov L0x401b20 ( -690239562)@int32; (*  -535 *)
mov L0x401b24 (-1839778721)@int32; (* -1426 *)
mov L0x401b28 (-1132766982)@int32; (*  -878 *)
mov L0x401b2c (-1486272852)@int32; (* -1152 *)
mov L0x401b30 ( 1933960943)@int32; (*  1499 *)
mov L0x401b34 (  678628056)@int32; (*   526 *)
mov L0x401b38 (   49026362)@int32; (*    38 *)
mov L0x401b3c ( 1375318456)@int32; (*  1066 *)
mov L0x401b40 ( 1961054458)@int32; (*  1520 *)
mov L0x401b44 ( -821836637)@int32; (*  -637 *)
mov L0x401b48 ( -249002309)@int32; (*  -193 *)
mov L0x401b4c (  -72249374)@int32; (*   -56 *)
mov L0x401b50 (  365117377)@int32; (*   283 *)
mov L0x401b54 ( -815385800)@int32; (*  -632 *)
mov L0x401b58 ( 1744306334)@int32; (*  1352 *)
mov L0x401b5c ( 1052776604)@int32; (*   816 *)
mov L0x401b60 ( -838608814)@int32; (*  -650 *)
mov L0x401b64 (  438656919)@int32; (*   340 *)
mov L0x401b68 ( 1681088131)@int32; (*  1303 *)
mov L0x401b6c (  366407544)@int32; (*   284 *)
mov L0x401b70 (-1475951512)@int32; (* -1144 *)
mov L0x401b74 ( 1771399850)@int32; (*  1373 *)
mov L0x401b78 ( 1091481626)@int32; (*   846 *)
mov L0x401b7c ( 2136517226)@int32; (*  1656 *)
mov L0x401b80 (  709592074)@int32; (*   550 *)
mov L0x401b84 (  966335388)@int32; (*   749 *)
mov L0x401b88 ( -812805466)@int32; (*  -630 *)
mov L0x401b8c (  886345009)@int32; (*   687 *)
mov L0x401b90 (-1028263422)@int32; (*  -797 *)
mov L0x401b94 ( 1819136044)@int32; (*  1410 *)
mov L0x401b98 ( -249002309)@int32; (*  -193 *)
mov L0x401b9c (-1370157785)@int32; (* -1062 *)
mov L0x401ba0 (   25803349)@int32; (*    20 *)
mov L0x401ba4 ( -406402733)@int32; (*  -315 *)
mov L0x401ba8 ( 1032133926)@int32; (*   800 *)
mov L0x401bac (  923759864)@int32; (*   716 *)
mov L0x401bb0 (-1664315954)@int32; (* -1290 *)
mov L0x401bb4 (-1704311143)@int32; (* -1321 *)
mov L0x401bb8 ( 2146838565)@int32; (*  1664 *)
mov L0x401bbc (  547030981)@int32; (*   424 *)
mov L0x401bc0 (  700560902)@int32; (*   543 *)
mov L0x401bc4 ( 1593356747)@int32; (*  1235 *)
mov L0x401bc8 (-1839778721)@int32; (* -1426 *)
mov L0x401bcc ( -583155667)@int32; (*  -452 *)
mov L0x401bd0 (-1851390228)@int32; (* -1435 *)
mov L0x401bd4 (-1041165096)@int32; (*  -807 *)
mov L0x401bd8 ( 1303069081)@int32; (*  1010 *)
mov L0x401bdc (  254162980)@int32; (*   197 *)
mov L0x401be0 ( -781841448)@int32; (*  -606 *)
mov L0x401be4 ( 1576584571)@int32; (*  1222 *)
mov L0x401be8 (-1208886859)@int32; (*  -937 *)
mov L0x401bec (-1361126613)@int32; (* -1055 *)
mov L0x401bf0 (-1110834136)@int32; (*  -861 *)
mov L0x401bf4 ( 1389510297)@int32; (*  1077 *)
mov L0x401bf8 (-1483692517)@int32; (* -1150 *)
mov L0x401bfc (  886345009)@int32; (*   687 *)
mov L0x401c00 ( 1819136044)@int32; (*  1410 *)
mov L0x401c04 (-1370157785)@int32; (* -1062 *)
mov L0x401c08 (-1904287091)@int32; (* -1476 *)
mov L0x401c0c ( 1137927653)@int32; (*   882 *)
mov L0x401c10 (-1727534157)@int32; (* -1339 *)
mov L0x401c14 ( -381889552)@int32; (*  -296 *)
mov L0x401c18 (-2006210316)@int32; (* -1555 *)
mov L0x401c1c (  459299597)@int32; (*   356 *)
mov L0x401c20 ( 1355965945)@int32; (*  1051 *)
mov L0x401c24 ( 1192114684)@int32; (*   924 *)
mov L0x401c28 (-1595937081)@int32; (* -1237 *)
mov L0x401c2c (  439947086)@int32; (*   341 *)
mov L0x401c30 (  587026170)@int32; (*   455 *)
mov L0x401c34 (  418014240)@int32; (*   324 *)
mov L0x401c38 (-1370157785)@int32; (* -1062 *)
mov L0x401c3c ( 1137927653)@int32; (*   882 *)
mov L0x401c40 ( -381889552)@int32; (*  -296 *)
mov L0x401c44 ( 2029433331)@int32; (*  1573 *)
mov L0x401c48 ( -427045411)@int32; (*  -331 *)
mov L0x401c4c (   98052723)@int32; (*    76 *)
mov L0x401c50 ( -372858380)@int32; (*  -289 *)
mov L0x401c54 (  639923034)@int32; (*   496 *)
mov L0x401c58 (-1488853187)@int32; (* -1154 *)
mov L0x401c5c ( -172882432)@int32; (*  -134 *)
mov L0x401c60 (  575414664)@int32; (*   446 *)
mov L0x401c64 ( 1674637294)@int32; (*  1298 *)
mov L0x401c68 ( 1541750051)@int32; (*  1195 *)
mov L0x401c6c (-1733984994)@int32; (* -1344 *)
mov L0x401c70 ( 1540459884)@int32; (*  1194 *)
mov L0x401c74 (          0)@int32; (*     0 *)

(* #! -> SP = 0xbefff1b0 *)
#! 0xbefff1b0 = 0xbefff1b0;
(* movw r0, #0 ; 0 *)
mov r0_b 0@int16; mov r0_t 0@int16; mov r0 0@int32;
(* movw lr, #0 ; 0 *)
mov lr_b 0@int16; mov lr_t 0@int16; mov lr 0@int32;
(* movt	r12, #3329	; 0xd01                         #! PC = 0x400cb4 *)
mov r12_t 3329@int16;
(* vldmia	r1!, {s8-s22}                            #! EA = L0x40187c; PC = 0x400cb8 *)
mov  s8 L0x40187c; spl  s8_t  s8_b  s8 16; cast  s8_b@int16  s8_b;
mov  s9 L0x401880; spl  s9_t  s9_b  s9 16; cast  s9_b@int16  s9_b;
mov s10 L0x401884; spl s10_t s10_b s10 16; cast s10_b@int16 s10_b;
mov s11 L0x401888; spl s11_t s11_b s11 16; cast s11_b@int16 s11_b;
mov s12 L0x40188c; spl s12_t s12_b s12 16; cast s12_b@int16 s12_b;
mov s13 L0x401890; spl s13_t s13_b s13 16; cast s13_b@int16 s13_b;
mov s14 L0x401894; spl s14_t s14_b s14 16; cast s14_b@int16 s14_b;
mov s15 L0x401898; spl s15_t s15_b s15 16; cast s15_b@int16 s15_b;
mov s16 L0x40189c; spl s16_t s16_b s16 16; cast s16_b@int16 s16_b;
mov s17 L0x4018a0; spl s17_t s17_b s17 16; cast s17_b@int16 s17_b;
mov s18 L0x4018a4; spl s18_t s18_b s18 16; cast s18_b@int16 s18_b;
mov s19 L0x4018a8; spl s19_t s19_b s19 16; cast s19_b@int16 s19_b;
mov s20 L0x4018ac; spl s20_t s20_b s20 16; cast s20_b@int16 s20_b;
mov s21 L0x4018b0; spl s21_t s21_b s21 16; cast s21_b@int16 s21_b;
mov s22 L0x4018b4; spl s22_t s22_b s22 16; cast s22_b@int16 s22_b;
(* add.w	lr, r0, #512	; 0x200                      #! PC = 0x400cbc *)
adds dontcare lr r0 512@int32;
(* vmov	s8, lr                                     #! PC = 0x400cc0 *)
mov [s8_b, s8_t] [lr_b, lr_t]; mov s8 lr;
(* vmov	s23, r0                                    #! PC = 0x400cc4 *)
mov [s23_b, s23_t] [r0_b, r0_t];
(* ldr.w	r2, [r0, #32]                             #! EA = L0xbefff1e4; Value = 0xaf019b4f; PC = 0x400cc8 *)
mov [r2_b, r2_t] [L0xbefff1e4, L0xbefff1e6];
(* ldr.w	r3, [r0, #36]	; 0x24                      #! EA = L0xbefff1e8; Value = 0xbb8b9a6b; PC = 0x400ccc *)
mov [r3_b, r3_t] [L0xbefff1e8, L0xbefff1ea];
(* ldr.w	r4, [r0, #40]	; 0x28                      #! EA = L0xbefff1ec; Value = 0xa44698be; PC = 0x400cd0 *)
mov [r4_b, r4_t] [L0xbefff1ec, L0xbefff1ee];
(* ldr.w	r5, [r0, #44]	; 0x2c                      #! EA = L0xbefff1f0; Value = 0xb01e9f64; PC = 0x400cd4 *)
mov [r5_b, r5_t] [L0xbefff1f0, L0xbefff1f2];
(* ldr.w	r6, [r0, #48]	; 0x30                      #! EA = L0xbefff1f4; Value = 0xbf2da129; PC = 0x400cd8 *)
mov [r6_b, r6_t] [L0xbefff1f4, L0xbefff1f6];
(* ldr.w	r7, [r0, #52]	; 0x34                      #! EA = L0xbefff1f8; Value = 0xb61997dd; PC = 0x400cdc *)
mov [r7_b, r7_t] [L0xbefff1f8, L0xbefff1fa];
(* ldr.w	r8, [r0, #56]	; 0x38                      #! EA = L0xbefff1fc; Value = 0xbcd7a804; PC = 0x400ce0 *)
mov [r8_b, r8_t] [L0xbefff1fc, L0xbefff1fe];
(* ldr.w	r9, [r0, #60]	; 0x3c                      #! EA = L0xbefff200; Value = 0xb5539eca; PC = 0x400ce4 *)
mov [r9_b, r9_t] [L0xbefff200, L0xbefff202];
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400ce8 *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* sadd16	lr, r2, r3                               #! PC = 0x400cec *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* ssub16	r3, r2, r3                               #! PC = 0x400cf0 *)
sub r3_b r2_b r3_b;
sub r3_t r2_t r3_t;
(* sadd16	r11, r4, r5                              #! PC = 0x400cf4 *)
add r11_b r4_b r5_b;
add r11_t r4_t r5_t;
(* ssub16	r5, r4, r5                               #! PC = 0x400cf8 *)
sub r5_b r4_b r5_b;
sub r5_t r4_t r5_t;
(* sadd16	r2, r6, r7                               #! PC = 0x400cfc *)
add r2_b r6_b r7_b;
add r2_t r6_t r7_t;
(* ssub16	r7, r6, r7                               #! PC = 0x400d00 *)
sub r7_b r6_b r7_b;
sub r7_t r6_t r7_t;
(* sadd16	r4, r8, r9                               #! PC = 0x400d04 *)
add r4_b r8_b r9_b;
add r4_t r8_t r9_t;
(* ssub16	r9, r8, r9                               #! PC = 0x400d08 *)
sub r9_b r8_b r9_b;
sub r9_t r8_t r9_t;

ghost Y0@int16, Y1@int16, Y2@int16, Y3@int16:
      17**17*Y0 = X**2 /\ 17**81*Y1 = X**2 /\
      17**49*Y2 = X**2 /\ 17**113*Y3 = X**2
   && true;

(* level *)
(*
assert eqmod (poly Y0 [poly X [lr_b, lr_t],  poly X [r3_b, r3_t]])
             (2*F**2) [Q, Y0**2 - 1] /\
       eqmod (poly Y1 [poly X [r11_b, r11_t], poly X [r5_b, r5_t]])
             (2*F**2) [Q, Y0**2 + 1] /\
       eqmod (poly Y2 [poly X [r2_b, r2_t],  poly X [r7_b, r7_t]])
             (2*F**2) [Q, Y2**2 - 1] /\
       eqmod (poly Y3 [poly X [r4_b, r4_t], poly X [r9_b, r9_t]])
             (2*F**2) [Q, Y2**2 + 1]
       && true;
*)

(* sadd16	r8, lr, r11                              #! PC = 0x400d0c *)
add r8_b lr_b r11_b;
add r8_t lr_t r11_t;
(* ssub16	r11, lr, r11                             #! PC = 0x400d10 *)
sub r11_b lr_b r11_b;
sub r11_t lr_t r11_t;
(* sadd16	r6, r2, r4                               #! PC = 0x400d14 *)
add r6_b r2_b r4_b;
add r6_t r2_t r4_t;
(* ssub16	r4, r2, r4                               #! PC = 0x400d18 *)
sub r4_b r2_b r4_b;
sub r4_t r2_t r4_t;

ghost r5_b0@int16, r5_t0@int16:
      r5_b0 = r5_b /\ r5_t0 = r5_t
   && r5_b0 = r5_b /\ r5_t0 = r5_t;
      
(* vmov	r10, s10                                   #! PC = 0x400d1c *)
mov [r10_b, r10_t] [s10_b, s10_t]; mov r10 s10;
(* smulwb	lr, r10, r5                              #! PC = 0x400d20 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x400d24 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d28 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400d2c *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400d30 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_b0 *  1600) [Q] /\
       eqmod lr_t (r5_t0 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_b0 *  1600) [Q] /\
       eqmod lr_t (r5_t0 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x400d34 *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400d38 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r9_b0@int16, r9_t0@int16:
      r9_b0 = r9_b /\ r9_t0 = r9_t
   && r9_b0 = r9_b /\ r9_t0 = r9_t;
      
(* smulwb	lr, r10, r9                              #! PC = 0x400d3c *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400d40 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d44 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400d48 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400d4c *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b0 *  1600) [Q] /\
       eqmod lr_t (r9_t0 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b0 *  1600) [Q] /\
       eqmod lr_t (r9_t0 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x400d50 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400d54 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;

(* level *)
(* 
assert eqmod (poly Y0 [poly X [r8_b, r8_t],  poly X [r3_b, r3_t],
                       poly X [r11_b, r11_t], poly X [r5_b, r5_t]])
             (4*F**2) [Q, Y0**4 - 1] /\
       eqmod (poly Y2 [poly X [r6_b, r6_t],  poly X [r7_b, r7_t],
                       poly X [r4_b, r4_t], poly X [r9_b, r9_t]])
             (4*F**2) [Q, Y0**4 + 1]
       && true;
*)
   
(* sadd16	r2, r8, r6                               #! PC = 0x400d58 *)
add r2_b r8_b r6_b;
add r2_t r8_t r6_t;
(* ssub16	r6, r8, r6                               #! PC = 0x400d5c *)
sub r6_b r8_b r6_b;
sub r6_t r8_t r6_t;

ghost r7_b0@int16, r7_t0@int16:
      r7_b0 = r7_b /\ r7_t0 = r7_t
   && r7_b0 = r7_b /\ r7_t0 = r7_t;

(* vmov	r10, s12                                   #! PC = 0x400d60 *)
mov [r10_b, r10_t] [s12_b, s12_t]; mov r10 s12;
(* smulwb	lr, r10, r7                              #! PC = 0x400d64 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x400d68 *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d6c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400d70 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400d74 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_b0 *    40) [Q] /\
       eqmod lr_t (r7_t0 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_b0 *    40) [Q] /\
       eqmod lr_t (r7_t0 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x400d78 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400d7c *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r4_b0@int16, r4_t0@int16:
      r4_b0 = r4_b /\ r4_t0 = r4_t
   && r4_b0 = r4_b /\ r4_t0 = r4_t;

(* vmov	r10, s13                                   #! PC = 0x400d80 *)
mov [r10_b, r10_t] [s13_b, s13_t]; mov r10 s13;
(* smulwb	lr, r10, r4                              #! PC = 0x400d84 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x400d88 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d8c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400d90 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400d94 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_b0 *  1600) [Q] /\
       eqmod lr_t (r4_t0 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_b0 *  1600) [Q] /\
       eqmod lr_t (r4_t0 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* ssub16	r8, r11, lr                              #! PC = 0x400d98 *)
sub r8_b r11_b lr_b;
sub r8_t r11_t lr_t;
(* sadd16	r4, r11, lr                              #! PC = 0x400d9c *)
add r4_b r11_b lr_b;
add r4_t r11_t lr_t;

ghost r9_b1@int16, r9_t1@int16:
      r9_b1 = r9_b /\ r9_t1 = r9_t
   && r9_b1 = r9_b /\ r9_t1 = r9_t;

(* vmov	r10, s14                                   #! PC = 0x400da0 *)
mov [r10_b, r10_t] [s14_b, s14_t]; mov r10 s14;
(* smulwb	lr, r10, r9                              #! PC = 0x400da4 *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400da8 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400dac *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400db0 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400db4 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b1 *   749) [Q] /\
       eqmod lr_t (r9_t1 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b1 *   749) [Q] /\
       eqmod lr_t (r9_t1 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x400db8 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400dbc *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;

(* level *)
(*
assert eqmod (poly Y0 [poly X [r2_b, r2_t], poly X [r3_b, r3_t],
                       poly X [r4_b, r4_t], poly X [r5_b, r5_t],
                       poly X [r6_b, r6_t], poly X [r7_b, r7_t],
                       poly X [r8_b, r8_t], poly X [r9_b, r9_t]])
             (8*F**2) [Q, Y0**8 - 1] && true;
*)

ghost r3_b0@int16, r3_t0@int16:
      r3_b0 = r3_b /\ r3_t0 = r3_t
   && r3_b0 = r3_b /\ r3_t0 = r3_t;

(* vmov	r11, s16                                   #! PC = 0x400dc0 *)
mov [r11_b, r11_t] [s16_b, s16_t]; mov r11 s16;
(* smulwb	lr, r11, r3                              #! PC = 0x400dc4 *)
vpc r3_bW@int32 r3_b; mulj tmp r11 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r11, r3                              #! PC = 0x400dc8 *)
vpc r3_tW@int32 r3_t; mulj tmp r11 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400dcc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400dd0 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x400dd4 *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov r3_b tmp_b; mov r3_t tmp_t;

assert eqmod r3_b (r3_b0 *  -848) [Q] /\
       eqmod r3_t (r3_t0 *  -848) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r3_b (r3_b0 *  -848) [Q] /\
       eqmod r3_t (r3_t0 *  -848) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r3_b, r3_t] /\ [r3_b, r3_t] <s[Q2, Q2];

ghost r4_b1@int16, r4_t1@int16:
      r4_b1 = r4_b /\ r4_t1 = r4_t
   && r4_b1 = r4_b /\ r4_t1 = r4_t;

(* vmov	r10, s17                                   #! PC = 0x400dd8 *)
mov [r10_b, r10_t] [s17_b, s17_t]; mov r10 s17;
(* vmov	r11, s18                                   #! PC = 0x400ddc *)
mov [r11_b, r11_t] [s18_b, s18_t]; mov r11 s18;
(* smulwb	lr, r10, r4                              #! PC = 0x400de0 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x400de4 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400de8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400dec *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x400df0 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov r4_b tmp_b; mov r4_t tmp_t;

assert eqmod r4_b (r4_b1 *    40) [Q] /\
       eqmod r4_t (r4_t1 *    40) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r4_b (r4_b1 *    40) [Q] /\
       eqmod r4_t (r4_t1 *    40) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r4_b, r4_t] /\ [r4_b, r4_t] <s[Q2, Q2];

ghost r5_b1@int16, r5_t1@int16:
      r5_b1 = r5_b /\ r5_t1 = r5_t
   && r5_b1 = r5_b /\ r5_t1 = r5_t;

(* smulwb	lr, r11, r5                              #! PC = 0x400df4 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x400df8 *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400dfc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400e00 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x400e04 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov r5_b tmp_b; mov r5_t tmp_t;

assert eqmod r5_b (r5_b1 *  -630) [Q] /\
       eqmod r5_t (r5_t1 *  -630) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r5_b (r5_b1 *  -630) [Q] /\
       eqmod r5_t (r5_t1 *  -630) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r5_b, r5_t] /\ [r5_b, r5_t] <s[Q2, Q2];

ghost r6_b0@int16, r6_t0@int16:
      r6_b0 = r6_b /\ r6_t0 = r6_t
   && r6_b0 = r6_b /\ r6_t0 = r6_t;

(* vmov	r10, s19                                   #! PC = 0x400e08 *)
mov [r10_b, r10_t] [s19_b, s19_t]; mov r10 s19;
(* vmov	r11, s20                                   #! PC = 0x400e0c *)
mov [r11_b, r11_t] [s20_b, s20_t]; mov r11 s20;
(* smulwb	lr, r10, r6                              #! PC = 0x400e10 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x400e14 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e18 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400e1c *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x400e20 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov r6_b tmp_b; mov r6_t tmp_t;

assert eqmod r6_b (r6_b0 *  1600) [Q] /\
       eqmod r6_t (r6_t0 *  1600) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r6_b (r6_b0 *  1600) [Q] /\
       eqmod r6_t (r6_t0 *  1600) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r6_b, r6_t] /\ [r6_b, r6_t] <s[Q2, Q2];

ghost r7_b1@int16, r7_t1@int16:
      r7_b1 = r7_b /\ r7_t1 = r7_t
   && r7_b1 = r7_b /\ r7_t1 = r7_t;

(* smulwb	lr, r11, r7                              #! PC = 0x400e24 *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x400e28 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e2c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400e30 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x400e34 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov r7_b tmp_b; mov r7_t tmp_t;

assert eqmod r7_b (r7_b1 *  1432) [Q] /\
       eqmod r7_t (r7_t1 *  1432) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r7_b (r7_b1 *  1432) [Q] /\
       eqmod r7_t (r7_t1 *  1432) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r7_b, r7_t] /\ [r7_b, r7_t] <s[Q2, Q2];

ghost r8_b0@int16, r8_t0@int16:
      r8_b0 = r8_b /\ r8_t0 = r8_t
   && r8_b0 = r8_b /\ r8_t0 = r8_t;

(* vmov	r10, s21                                   #! PC = 0x400e38 *)
mov [r10_b, r10_t] [s21_b, s21_t]; mov r10 s21;
(* vmov	r11, s22                                   #! PC = 0x400e3c *)
mov [r11_b, r11_t] [s22_b, s22_t]; mov r11 s22;
(* smulwb	lr, r10, r8                              #! PC = 0x400e40 *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x400e44 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e48 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400e4c *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x400e50 *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov r8_b tmp_b; mov r8_t tmp_t;

assert eqmod r8_b (r8_b0 *   749) [Q] /\
       eqmod r8_t (r8_t0 *   749) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r8_b (r8_b0 *   749) [Q] /\
       eqmod r8_t (r8_t0 *   749) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r8_b, r8_t] /\ [r8_b, r8_t] <s[Q2, Q2];

ghost r9_b2@int16, r9_t2@int16:
      r9_b2 = r9_b /\ r9_t2 = r9_t
   && r9_b2 = r9_b /\ r9_t2 = r9_t;

(* smulwb	lr, r11, r9                              #! PC = 0x400e54 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x400e58 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e5c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400e60 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x400e64 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov r9_b tmp_b; mov r9_t tmp_t;

assert eqmod r9_b (r9_b2 *   687) [Q] /\
       eqmod r9_t (r9_t2 *   687) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r9_b (r9_b2 *   687) [Q] /\
       eqmod r9_t (r9_t2 *   687) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r9_b, r9_t] /\ [r9_b, r9_t] <s[Q2, Q2];

(* vmov	s0, r2                                     #! PC = 0x400e68 *)
mov [s0_b, s0_t] [r2_b, r2_t];
(* vmov	s1, r3                                     #! PC = 0x400e6c *)
mov [s1_b, s1_t] [r3_b, r3_t];
(* vmov	s2, r4                                     #! PC = 0x400e70 *)
mov [s2_b, s2_t] [r4_b, r4_t];
(* vmov	s3, r5                                     #! PC = 0x400e74 *)
mov [s3_b, s3_t] [r5_b, r5_t];
(* vmov	s4, r6                                     #! PC = 0x400e78 *)
mov [s4_b, s4_t] [r6_b, r6_t];
(* vmov	s5, r7                                     #! PC = 0x400e7c *)
mov [s5_b, s5_t] [r7_b, r7_t];
(* vmov	s6, r8                                     #! PC = 0x400e80 *)
mov [s6_b, s6_t] [r8_b, r8_t];
(* vmov	s7, r9                                     #! PC = 0x400e84 *)
mov [s7_b, s7_t] [r9_b, r9_t];

assert [8*NQ2,8*NQ2]< [s0_b, s0_t] /\ [s0_b, s0_t]< [8*Q2,8*Q2] /\
       [1*NQ2,1*NQ2]< [s1_b, s1_t] /\ [s1_b, s1_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s2_b, s2_t] /\ [s2_b, s2_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s3_b, s3_t] /\ [s3_b, s3_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s4_b, s4_t] /\ [s4_b, s4_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s5_b, s5_t] /\ [s5_b, s5_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s6_b, s6_t] /\ [s6_b, s6_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s7_b, s7_t] /\ [s7_b, s7_t]< [1*Q2,1*Q2]
       prove with [algebra solver isl, precondition] && true;
assume [8*NQ2,8*NQ2]< [s0_b, s0_t] /\ [s0_b, s0_t]< [8*Q2,8*Q2] /\
       [1*NQ2,1*NQ2]< [s1_b, s1_t] /\ [s1_b, s1_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s2_b, s2_t] /\ [s2_b, s2_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s3_b, s3_t] /\ [s3_b, s3_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s4_b, s4_t] /\ [s4_b, s4_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s5_b, s5_t] /\ [s5_b, s5_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s6_b, s6_t] /\ [s6_b, s6_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s7_b, s7_t] /\ [s7_b, s7_t]< [1*Q2,1*Q2]
    && [8@16*NQ2,8@16*NQ2]<s[s0_b, s0_t] /\ [s0_b, s0_t]<s[8@16*Q2,8@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s1_b, s1_t] /\ [s1_b, s1_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s2_b, s2_t] /\ [s2_b, s2_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s3_b, s3_t] /\ [s3_b, s3_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s4_b, s4_t] /\ [s4_b, s4_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s5_b, s5_t] /\ [s5_b, s5_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s6_b, s6_t] /\ [s6_b, s6_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s7_b, s7_t] /\ [s7_b, s7_t]<s[1@16*Q2,1@16*Q2];

(* CUT 0 *)
ghost Z0@int16: X**2 =  -848*17** 17*Z0 && true;
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod (poly Z0 [poly X [s0_b, s0_t], poly X [s1_b, s1_t],
                    poly X [s2_b, s2_t], poly X [s3_b, s3_t],
                    poly X [s4_b, s4_t], poly X [s5_b, s5_t],
                    poly X [s6_b, s6_t], poly X [s7_b, s7_t]])
          (2**3*F**2) [Q, Z0**8 + 1] /\
    [8*NQ2,8*NQ2]< [s0_b, s0_t] /\ [s0_b, s0_t]< [8*Q2,8*Q2] /\
    [1*NQ2,1*NQ2]< [s1_b, s1_t] /\ [s1_b, s1_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s2_b, s2_t] /\ [s2_b, s2_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s3_b, s3_t] /\ [s3_b, s3_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s4_b, s4_t] /\ [s4_b, s4_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s5_b, s5_t] /\ [s5_b, s5_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s6_b, s6_t] /\ [s6_b, s6_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s7_b, s7_t] /\ [s7_b, s7_t]< [1*Q2,1*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [8@16*NQ2,8@16*NQ2]<s[s0_b, s0_t] /\ [s0_b, s0_t]<s[8@16*Q2,8@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s1_b, s1_t] /\ [s1_b, s1_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s2_b, s2_t] /\ [s2_b, s2_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s3_b, s3_t] /\ [s3_b, s3_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s4_b, s4_t] /\ [s4_b, s4_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s5_b, s5_t] /\ [s5_b, s5_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s6_b, s6_t] /\ [s6_b, s6_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s7_b, s7_t] /\ [s7_b, s7_t]<s[1@16*Q2,1@16*Q2]
    prove with [precondition];


(* vmov	r0, s23                                    #! PC = 0x400e88 *)
mov [r0_b, r0_t] [s23_b, s23_t];
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff1c4; Value = 0xcb3a99aa; PC = 0x400e8c *)
mov [r2_b, r2_t] [L0xbefff1c4, L0xbefff1c6];
(* ldr.w	r3, [r0, #4]                              #! EA = L0xbefff1c8; Value = 0xc0429028; PC = 0x400e90 *)
mov [r3_b, r3_t] [L0xbefff1c8, L0xbefff1ca];
(* ldr.w	r4, [r0, #8]                              #! EA = L0xbefff1cc; Value = 0xbfa49156; PC = 0x400e94 *)
mov [r4_b, r4_t] [L0xbefff1cc, L0xbefff1ce];
(* ldr.w	r5, [r0, #12]                             #! EA = L0xbefff1d0; Value = 0xbe1892a0; PC = 0x400e98 *)
mov [r5_b, r5_t] [L0xbefff1d0, L0xbefff1d2];
(* ldr.w	r6, [r0, #16]                             #! EA = L0xbefff1d4; Value = 0xbd5c953b; PC = 0x400e9c *)
mov [r6_b, r6_t] [L0xbefff1d4, L0xbefff1d6];
(* ldr.w	r7, [r0, #20]                             #! EA = L0xbefff1d8; Value = 0xb11e8d91; PC = 0x400ea0 *)
mov [r7_b, r7_t] [L0xbefff1d8, L0xbefff1da];
(* ldr.w	r8, [r0, #24]                             #! EA = L0xbefff1dc; Value = 0xc4308878; PC = 0x400ea4 *)
mov [r8_b, r8_t] [L0xbefff1dc, L0xbefff1de];
(* ldr.w	r9, [r0, #28]                             #! EA = L0xbefff1e0; Value = 0xbc3e9014; PC = 0x400ea8 *)
mov [r9_b, r9_t] [L0xbefff1e0, L0xbefff1e2];
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400eac *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* sadd16	lr, r2, r3                               #! PC = 0x400eb0 *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* ssub16	r3, r2, r3                               #! PC = 0x400eb4 *)
sub r3_b r2_b r3_b;
sub r3_t r2_t r3_t;
(* sadd16	r11, r4, r5                              #! PC = 0x400eb8 *)
add r11_b r4_b r5_b;
add r11_t r4_t r5_t;
(* ssub16	r5, r4, r5                               #! PC = 0x400ebc *)
sub r5_b r4_b r5_b;
sub r5_t r4_t r5_t;
(* sadd16	r2, r6, r7                               #! PC = 0x400ec0 *)
add r2_b r6_b r7_b;
add r2_t r6_t r7_t;
(* ssub16	r7, r6, r7                               #! PC = 0x400ec4 *)
sub r7_b r6_b r7_b;
sub r7_t r6_t r7_t;
(* sadd16	r4, r8, r9                               #! PC = 0x400ec8 *)
add r4_b r8_b r9_b;
add r4_t r8_t r9_t;
(* ssub16	r9, r8, r9                               #! PC = 0x400ecc *)
sub r9_b r8_b r9_b;
sub r9_t r8_t r9_t;
(* sadd16	r8, lr, r11                              #! PC = 0x400ed0 *)
add r8_b lr_b r11_b;
add r8_t lr_t r11_t;
(* ssub16	r11, lr, r11                             #! PC = 0x400ed4 *)
sub r11_b lr_b r11_b;
sub r11_t lr_t r11_t;
(* sadd16	r6, r2, r4                               #! PC = 0x400ed8 *)
add r6_b r2_b r4_b;
add r6_t r2_t r4_t;
(* ssub16	r4, r2, r4                               #! PC = 0x400edc *)
sub r4_b r2_b r4_b;
sub r4_t r2_t r4_t;

ghost r5_b2@int16, r5_t2@int16:
      r5_b2 = r5_b /\ r5_t2 = r5_t
   && r5_b2 = r5_b /\ r5_t2 = r5_t;

(* vmov	r10, s10                                   #! PC = 0x400ee0 *)
mov [r10_b, r10_t] [s10_b, s10_t]; mov r10 s10;
(* smulwb	lr, r10, r5                              #! PC = 0x400ee4 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x400ee8 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400eec *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400ef0 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400ef4 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_b2 *  1600) [Q] /\
       eqmod lr_t (r5_t2 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_b2 *  1600) [Q] /\
       eqmod lr_t (r5_t2 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x400ef8 *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400efc *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r9_b3@int16, r9_t3@int16:
      r9_b3 = r9_b /\ r9_t3 = r9_t
   && r9_b3 = r9_b /\ r9_t3 = r9_t;

(* smulwb	lr, r10, r9                              #! PC = 0x400f00 *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400f04 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f08 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400f0c *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400f10 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b3 *  1600) [Q] /\
       eqmod lr_t (r9_t3 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b3 *  1600) [Q] /\
       eqmod lr_t (r9_t3 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x400f14 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400f18 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* sadd16	r2, r8, r6                               #! PC = 0x400f1c *)
add r2_b r8_b r6_b;
add r2_t r8_t r6_t;
(* ssub16	r6, r8, r6                               #! PC = 0x400f20 *)
sub r6_b r8_b r6_b;
sub r6_t r8_t r6_t;

ghost r7_b2@int16, r7_t2@int16:
      r7_b2 = r7_b /\ r7_t2 = r7_t
   && r7_b2 = r7_b /\ r7_t2 = r7_t;

(* vmov	r10, s12                                   #! PC = 0x400f24 *)
mov [r10_b, r10_t] [s12_b, s12_t]; mov r10 s12;
(* smulwb	lr, r10, r7                              #! PC = 0x400f28 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x400f2c *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f30 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400f34 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400f38 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_b2 *    40) [Q] /\
       eqmod lr_t (r7_t2 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_b2 *    40) [Q] /\
       eqmod lr_t (r7_t2 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x400f3c *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400f40 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r4_b2@int16, r4_t2@int16:
      r4_b2 = r4_b /\ r4_t2 = r4_t
   && r4_b2 = r4_b /\ r4_t2 = r4_t;

(* vmov	r10, s13                                   #! PC = 0x400f44 *)
mov [r10_b, r10_t] [s13_b, s13_t]; mov r10 s13;
(* smulwb	lr, r10, r4                              #! PC = 0x400f48 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x400f4c *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f50 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400f54 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400f58 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_b2 *  1600) [Q] /\
       eqmod lr_t (r4_t2 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_b2 *  1600) [Q] /\
       eqmod lr_t (r4_t2 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* ssub16	r8, r11, lr                              #! PC = 0x400f5c *)
sub r8_b r11_b lr_b;
sub r8_t r11_t lr_t;
(* sadd16	r4, r11, lr                              #! PC = 0x400f60 *)
add r4_b r11_b lr_b;
add r4_t r11_t lr_t;

ghost r9_b4@int16, r9_t4@int16:
      r9_b4 = r9_b /\ r9_t4 = r9_t
   && r9_b4 = r9_b /\ r9_t4 = r9_t;

(* vmov	r10, s14                                   #! PC = 0x400f64 *)
mov [r10_b, r10_t] [s14_b, s14_t]; mov r10 s14;
(* smulwb	lr, r10, r9                              #! PC = 0x400f68 *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400f6c *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f70 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400f74 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400f78 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b4 *   749) [Q] /\
       eqmod lr_t (r9_t4 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b4 *   749) [Q] /\
       eqmod lr_t (r9_t4 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x400f7c *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400f80 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* vmov	r0, s23                                    #! PC = 0x400f84 *)
mov [r0_b, r0_t] [s23_b, s23_t];
(* vmov	r11, s1                                    #! PC = 0x400f88 *)
mov [r11_b, r11_t] [s1_b, s1_t];
(* uadd16	lr, r3, r11                              #! PC = 0x400f8c *)
add lr_b r3_b r11_b;
add lr_t r3_t r11_t;
(* usub16	r3, r3, r11                              #! PC = 0x400f90 *)
sub r3_b r3_b r11_b;
sub r3_t r3_t r11_t;
(* str.w	lr, [r0, #4]                              #! EA = L0xbefff1c8; PC = 0x400f94 *)
mov [L0xbefff1c8, L0xbefff1ca] [lr_b, lr_t];
(* str.w	r3, [r0, #36]	; 0x24                      #! EA = L0xbefff1e8; PC = 0x400f98 *)
mov [L0xbefff1e8, L0xbefff1ea] [r3_b, r3_t];
(* vmov	r11, s3                                    #! PC = 0x400f9c *)
mov [r11_b, r11_t] [s3_b, s3_t];
(* uadd16	lr, r5, r11                              #! PC = 0x400fa0 *)
add lr_b r5_b r11_b;
add lr_t r5_t r11_t;
(* usub16	r5, r5, r11                              #! PC = 0x400fa4 *)
sub r5_b r5_b r11_b;
sub r5_t r5_t r11_t;
(* str.w	lr, [r0, #12]                             #! EA = L0xbefff1d0; PC = 0x400fa8 *)
mov [L0xbefff1d0, L0xbefff1d2] [lr_b, lr_t];
(* str.w	r5, [r0, #44]	; 0x2c                      #! EA = L0xbefff1f0; PC = 0x400fac *)
mov [L0xbefff1f0, L0xbefff1f2] [r5_b, r5_t];
(* vmov	r11, s5                                    #! PC = 0x400fb0 *)
mov [r11_b, r11_t] [s5_b, s5_t];
(* uadd16	lr, r7, r11                              #! PC = 0x400fb4 *)
add lr_b r7_b r11_b;
add lr_t r7_t r11_t;
(* usub16	r7, r7, r11                              #! PC = 0x400fb8 *)
sub r7_b r7_b r11_b;
sub r7_t r7_t r11_t;
(* str.w	lr, [r0, #20]                             #! EA = L0xbefff1d8; PC = 0x400fbc *)
mov [L0xbefff1d8, L0xbefff1da] [lr_b, lr_t];
(* str.w	r7, [r0, #52]	; 0x34                      #! EA = L0xbefff1f8; PC = 0x400fc0 *)
mov [L0xbefff1f8, L0xbefff1fa] [r7_b, r7_t];
(* vmov	r11, s7                                    #! PC = 0x400fc4 *)
mov [r11_b, r11_t] [s7_b, s7_t];
(* uadd16	lr, r9, r11                              #! PC = 0x400fc8 *)
add lr_b r9_b r11_b;
add lr_t r9_t r11_t;
(* usub16	r9, r9, r11                              #! PC = 0x400fcc *)
sub r9_b r9_b r11_b;
sub r9_t r9_t r11_t;
(* str.w	lr, [r0, #28]                             #! EA = L0xbefff1e0; PC = 0x400fd0 *)
mov [L0xbefff1e0, L0xbefff1e2] [lr_b, lr_t];
(* str.w	r9, [r0, #60]	; 0x3c                      #! EA = L0xbefff200; PC = 0x400fd4 *)
mov [L0xbefff200, L0xbefff202] [r9_b, r9_t];
(* vmov	r5, s2                                     #! PC = 0x400fd8 *)
mov [r5_b, r5_t] [s2_b, s2_t];
(* uadd16	lr, r4, r5                               #! PC = 0x400fdc *)
add lr_b r4_b r5_b;
add lr_t r4_t r5_t;
(* usub16	r11, r4, r5                              #! PC = 0x400fe0 *)
sub r11_b r4_b r5_b;
sub r11_t r4_t r5_t;
(* str.w	lr, [r0, #8]                              #! EA = L0xbefff1cc; PC = 0x400fe4 *)
mov [L0xbefff1cc, L0xbefff1ce] [lr_b, lr_t];
(* str.w	r11, [r0, #40]	; 0x28                     #! EA = L0xbefff1ec; PC = 0x400fe8 *)
mov [L0xbefff1ec, L0xbefff1ee] [r11_b, r11_t];
(* vmov	r7, s4                                     #! PC = 0x400fec *)
mov [r7_b, r7_t] [s4_b, s4_t];
(* uadd16	lr, r6, r7                               #! PC = 0x400ff0 *)
add lr_b r6_b r7_b;
add lr_t r6_t r7_t;
(* usub16	r11, r6, r7                              #! PC = 0x400ff4 *)
sub r11_b r6_b r7_b;
sub r11_t r6_t r7_t;
(* str.w	lr, [r0, #16]                             #! EA = L0xbefff1d4; PC = 0x400ff8 *)
mov [L0xbefff1d4, L0xbefff1d6] [lr_b, lr_t];
(* str.w	r11, [r0, #48]	; 0x30                     #! EA = L0xbefff1f4; PC = 0x400ffc *)
mov [L0xbefff1f4, L0xbefff1f6] [r11_b, r11_t];
(* vmov	r9, s6                                     #! PC = 0x401000 *)
mov [r9_b, r9_t] [s6_b, s6_t];
(* uadd16	lr, r8, r9                               #! PC = 0x401004 *)
add lr_b r8_b r9_b;
add lr_t r8_t r9_t;
(* usub16	r11, r8, r9                              #! PC = 0x401008 *)
sub r11_b r8_b r9_b;
sub r11_t r8_t r9_t;
(* str.w	lr, [r0, #24]                             #! EA = L0xbefff1dc; PC = 0x40100c *)
mov [L0xbefff1dc, L0xbefff1de] [lr_b, lr_t];
(* str.w	r11, [r0, #56]	; 0x38                     #! EA = L0xbefff1fc; PC = 0x401010 *)
mov [L0xbefff1fc, L0xbefff1fe] [r11_b, r11_t];
(* vmov	r3, s0                                     #! PC = 0x401014 *)
mov [r3_b, r3_t] [s0_b, s0_t];
(* uadd16	lr, r2, r3                               #! PC = 0x401018 *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* usub16	r11, r2, r3                              #! PC = 0x40101c *)
sub r11_b r2_b r3_b;
sub r11_t r2_t r3_t;
(* str.w	r11, [r0, #32]                            #! EA = L0xbefff1e4; PC = 0x401020 *)
mov [L0xbefff1e4, L0xbefff1e6] [r11_b, r11_t];
(* str.w	lr, [r0], #64                             #! EA = L0xbefff1c4; PC = 0x401024 *)
mov [L0xbefff1c4, L0xbefff1c6] [lr_b, lr_t];
(* vmov	lr, s8                                     #! PC = 0x401028 *)
mov [lr_b, lr_t] [s8_b, s8_t]; mov lr s8;
(* cmp.w	r0, lr                                    #! PC = 0x40102c *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x400cc4 <invntt_fast+24>                #! PC = 0x401030 *)
#bne.w	0x400cc4 <invntt_fast+24>                #! 0x401030 = 0x401030;

(* CUT 1 *)
assert [16*NQ2,16*NQ2,5*NQ2]< [L0xbefff1c4,L0xbefff1c6,L0xbefff1c8] /\
       [L0xbefff1c4,L0xbefff1c6,L0xbefff1c8]< [16*Q2,16*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff1ca,L0xbefff1cc,L0xbefff1ce] /\
       [L0xbefff1ca,L0xbefff1cc,L0xbefff1ce]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2,9*NQ2]< [L0xbefff1d0,L0xbefff1d2,L0xbefff1d4] /\
       [L0xbefff1d0,L0xbefff1d2,L0xbefff1d4]< [5*Q2,5*Q2,9*Q2] /\
       [9*NQ2,5*NQ2,5*NQ2]< [L0xbefff1d6,L0xbefff1d8,L0xbefff1da] /\
       [L0xbefff1d6,L0xbefff1d8,L0xbefff1da]< [9*Q2,5*Q2,5*Q2] /\
       [6*NQ2,6*NQ2,5*NQ2]< [L0xbefff1dc,L0xbefff1de,L0xbefff1e0] /\
       [L0xbefff1dc,L0xbefff1de,L0xbefff1e0]< [6*Q2,6*Q2,5*Q2] /\
       [5*NQ2,16*NQ2,16*NQ2]< [L0xbefff1e2,L0xbefff1e4,L0xbefff1e6] /\
       [L0xbefff1e2,L0xbefff1e4,L0xbefff1e6]< [5*Q2,16*Q2,16*Q2] /\
       [5*NQ2,5*NQ2,6*NQ2]< [L0xbefff1e8,L0xbefff1ea,L0xbefff1ec] /\
       [L0xbefff1e8,L0xbefff1ea,L0xbefff1ec]< [5*Q2,5*Q2,6*Q2] /\
       [6*NQ2,5*NQ2,5*NQ2]< [L0xbefff1ee,L0xbefff1f0,L0xbefff1f2] /\
       [L0xbefff1ee,L0xbefff1f0,L0xbefff1f2]< [6*Q2,5*Q2,5*Q2] /\
       [9*NQ2,9*NQ2,5*NQ2]< [L0xbefff1f4,L0xbefff1f6,L0xbefff1f8] /\
       [L0xbefff1f4,L0xbefff1f6,L0xbefff1f8]< [9*Q2,9*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff1fa,L0xbefff1fc,L0xbefff1fe] /\
       [L0xbefff1fa,L0xbefff1fc,L0xbefff1fe]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2]< [L0xbefff200,L0xbefff202] /\
       [L0xbefff200,L0xbefff202]< [5*Q2,5*Q2]
       prove with [algebra solver isl, precondition] && true;
assume [16*NQ2,16*NQ2,5*NQ2]< [L0xbefff1c4,L0xbefff1c6,L0xbefff1c8] /\
       [L0xbefff1c4,L0xbefff1c6,L0xbefff1c8]< [16*Q2,16*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff1ca,L0xbefff1cc,L0xbefff1ce] /\
       [L0xbefff1ca,L0xbefff1cc,L0xbefff1ce]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2,9*NQ2]< [L0xbefff1d0,L0xbefff1d2,L0xbefff1d4] /\
       [L0xbefff1d0,L0xbefff1d2,L0xbefff1d4]< [5*Q2,5*Q2,9*Q2] /\
       [9*NQ2,5*NQ2,5*NQ2]< [L0xbefff1d6,L0xbefff1d8,L0xbefff1da] /\
       [L0xbefff1d6,L0xbefff1d8,L0xbefff1da]< [9*Q2,5*Q2,5*Q2] /\
       [6*NQ2,6*NQ2,5*NQ2]< [L0xbefff1dc,L0xbefff1de,L0xbefff1e0] /\
       [L0xbefff1dc,L0xbefff1de,L0xbefff1e0]< [6*Q2,6*Q2,5*Q2] /\
       [5*NQ2,16*NQ2,16*NQ2]< [L0xbefff1e2,L0xbefff1e4,L0xbefff1e6] /\
       [L0xbefff1e2,L0xbefff1e4,L0xbefff1e6]< [5*Q2,16*Q2,16*Q2] /\
       [5*NQ2,5*NQ2,6*NQ2]< [L0xbefff1e8,L0xbefff1ea,L0xbefff1ec] /\
       [L0xbefff1e8,L0xbefff1ea,L0xbefff1ec]< [5*Q2,5*Q2,6*Q2] /\
       [6*NQ2,5*NQ2,5*NQ2]< [L0xbefff1ee,L0xbefff1f0,L0xbefff1f2] /\
       [L0xbefff1ee,L0xbefff1f0,L0xbefff1f2]< [6*Q2,5*Q2,5*Q2] /\
       [9*NQ2,9*NQ2,5*NQ2]< [L0xbefff1f4,L0xbefff1f6,L0xbefff1f8] /\
       [L0xbefff1f4,L0xbefff1f6,L0xbefff1f8]< [9*Q2,9*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff1fa,L0xbefff1fc,L0xbefff1fe] /\
       [L0xbefff1fa,L0xbefff1fc,L0xbefff1fe]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2]< [L0xbefff200,L0xbefff202] /\
       [L0xbefff200,L0xbefff202]< [5*Q2,5*Q2]
    && [16@16*NQ2,16@16*NQ2,5@16*NQ2]< [L0xbefff1c4,L0xbefff1c6,L0xbefff1c8] /\
       [L0xbefff1c4,L0xbefff1c6,L0xbefff1c8]< [16@16*Q2,16@16*Q2,5@16*Q2] /\
       [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff1ca,L0xbefff1cc,L0xbefff1ce] /\
       [L0xbefff1ca,L0xbefff1cc,L0xbefff1ce]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
       [5@16*NQ2,5@16*NQ2,9@16*NQ2]< [L0xbefff1d0,L0xbefff1d2,L0xbefff1d4] /\
       [L0xbefff1d0,L0xbefff1d2,L0xbefff1d4]< [5@16*Q2,5@16*Q2,9@16*Q2] /\
       [9@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff1d6,L0xbefff1d8,L0xbefff1da] /\
       [L0xbefff1d6,L0xbefff1d8,L0xbefff1da]< [9@16*Q2,5@16*Q2,5@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,5@16*NQ2]< [L0xbefff1dc,L0xbefff1de,L0xbefff1e0] /\
       [L0xbefff1dc,L0xbefff1de,L0xbefff1e0]< [6@16*Q2,6@16*Q2,5@16*Q2] /\
       [5@16*NQ2,16@16*NQ2,16@16*NQ2]< [L0xbefff1e2,L0xbefff1e4,L0xbefff1e6] /\
       [L0xbefff1e2,L0xbefff1e4,L0xbefff1e6]< [5@16*Q2,16@16*Q2,16@16*Q2] /\
       [5@16*NQ2,5@16*NQ2,6@16*NQ2]< [L0xbefff1e8,L0xbefff1ea,L0xbefff1ec] /\
       [L0xbefff1e8,L0xbefff1ea,L0xbefff1ec]< [5@16*Q2,5@16*Q2,6@16*Q2] /\
       [6@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff1ee,L0xbefff1f0,L0xbefff1f2] /\
       [L0xbefff1ee,L0xbefff1f0,L0xbefff1f2]< [6@16*Q2,5@16*Q2,5@16*Q2] /\
       [9@16*NQ2,9@16*NQ2,5@16*NQ2]< [L0xbefff1f4,L0xbefff1f6,L0xbefff1f8] /\
       [L0xbefff1f4,L0xbefff1f6,L0xbefff1f8]< [9@16*Q2,9@16*Q2,5@16*Q2] /\
       [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff1fa,L0xbefff1fc,L0xbefff1fe] /\
       [L0xbefff1fa,L0xbefff1fc,L0xbefff1fe]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
       [5@16*NQ2,5@16*NQ2]< [L0xbefff200,L0xbefff202] /\
       [L0xbefff200,L0xbefff202]< [5@16*Q2,5@16*Q2];
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod (poly Z0
           [poly X [L0xbefff1c4,L0xbefff1c6],poly X [L0xbefff1c8,L0xbefff1ca],
            poly X [L0xbefff1cc,L0xbefff1ce],poly X [L0xbefff1d0,L0xbefff1d2],
            poly X [L0xbefff1d4,L0xbefff1d6],poly X [L0xbefff1d8,L0xbefff1da],
            poly X [L0xbefff1dc,L0xbefff1de],poly X [L0xbefff1e0,L0xbefff1e2],
            poly X [L0xbefff1e4,L0xbefff1e6],poly X [L0xbefff1e8,L0xbefff1ea],
            poly X [L0xbefff1ec,L0xbefff1ee],poly X [L0xbefff1f0,L0xbefff1f2],
            poly X [L0xbefff1f4,L0xbefff1f6],poly X [L0xbefff1f8,L0xbefff1fa],
            poly X [L0xbefff1fc,L0xbefff1fe],poly X [L0xbefff200,L0xbefff202]])
          (2**4*F**2) [Q, Z0**16 - 1] /\
    [16*NQ2,16*NQ2,5*NQ2]< [L0xbefff1c4,L0xbefff1c6,L0xbefff1c8] /\
    [L0xbefff1c4,L0xbefff1c6,L0xbefff1c8]< [16*Q2,16*Q2,5*Q2] /\
    [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff1ca,L0xbefff1cc,L0xbefff1ce] /\
    [L0xbefff1ca,L0xbefff1cc,L0xbefff1ce]< [5*Q2,6*Q2,6*Q2] /\
    [5*NQ2,5*NQ2,9*NQ2]< [L0xbefff1d0,L0xbefff1d2,L0xbefff1d4] /\
    [L0xbefff1d0,L0xbefff1d2,L0xbefff1d4]< [5*Q2,5*Q2,9*Q2] /\
    [9*NQ2,5*NQ2,5*NQ2]< [L0xbefff1d6,L0xbefff1d8,L0xbefff1da] /\
    [L0xbefff1d6,L0xbefff1d8,L0xbefff1da]< [9*Q2,5*Q2,5*Q2] /\
    [6*NQ2,6*NQ2,5*NQ2]< [L0xbefff1dc,L0xbefff1de,L0xbefff1e0] /\
    [L0xbefff1dc,L0xbefff1de,L0xbefff1e0]< [6*Q2,6*Q2,5*Q2] /\
    [5*NQ2,16*NQ2,16*NQ2]< [L0xbefff1e2,L0xbefff1e4,L0xbefff1e6] /\
    [L0xbefff1e2,L0xbefff1e4,L0xbefff1e6]< [5*Q2,16*Q2,16*Q2] /\
    [5*NQ2,5*NQ2,6*NQ2]< [L0xbefff1e8,L0xbefff1ea,L0xbefff1ec] /\
    [L0xbefff1e8,L0xbefff1ea,L0xbefff1ec]< [5*Q2,5*Q2,6*Q2] /\
    [6*NQ2,5*NQ2,5*NQ2]< [L0xbefff1ee,L0xbefff1f0,L0xbefff1f2] /\
    [L0xbefff1ee,L0xbefff1f0,L0xbefff1f2]< [6*Q2,5*Q2,5*Q2] /\
    [9*NQ2,9*NQ2,5*NQ2]< [L0xbefff1f4,L0xbefff1f6,L0xbefff1f8] /\
    [L0xbefff1f4,L0xbefff1f6,L0xbefff1f8]< [9*Q2,9*Q2,5*Q2] /\
    [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff1fa,L0xbefff1fc,L0xbefff1fe] /\
    [L0xbefff1fa,L0xbefff1fc,L0xbefff1fe]< [5*Q2,6*Q2,6*Q2] /\
    [5*NQ2,5*NQ2]< [L0xbefff200,L0xbefff202] /\
    [L0xbefff200,L0xbefff202]< [5*Q2,5*Q2]
    prove with [precondition, all ghosts]
 && [16@16*NQ2,16@16*NQ2,5@16*NQ2]< [L0xbefff1c4,L0xbefff1c6,L0xbefff1c8] /\
    [L0xbefff1c4,L0xbefff1c6,L0xbefff1c8]< [16@16*Q2,16@16*Q2,5@16*Q2] /\
    [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff1ca,L0xbefff1cc,L0xbefff1ce] /\
    [L0xbefff1ca,L0xbefff1cc,L0xbefff1ce]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
    [5@16*NQ2,5@16*NQ2,9@16*NQ2]< [L0xbefff1d0,L0xbefff1d2,L0xbefff1d4] /\
    [L0xbefff1d0,L0xbefff1d2,L0xbefff1d4]< [5@16*Q2,5@16*Q2,9@16*Q2] /\
    [9@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff1d6,L0xbefff1d8,L0xbefff1da] /\
    [L0xbefff1d6,L0xbefff1d8,L0xbefff1da]< [9@16*Q2,5@16*Q2,5@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,5@16*NQ2]< [L0xbefff1dc,L0xbefff1de,L0xbefff1e0] /\
    [L0xbefff1dc,L0xbefff1de,L0xbefff1e0]< [6@16*Q2,6@16*Q2,5@16*Q2] /\
    [5@16*NQ2,16@16*NQ2,16@16*NQ2]< [L0xbefff1e2,L0xbefff1e4,L0xbefff1e6] /\
    [L0xbefff1e2,L0xbefff1e4,L0xbefff1e6]< [5@16*Q2,16@16*Q2,16@16*Q2] /\
    [5@16*NQ2,5@16*NQ2,6@16*NQ2]< [L0xbefff1e8,L0xbefff1ea,L0xbefff1ec] /\
    [L0xbefff1e8,L0xbefff1ea,L0xbefff1ec]< [5@16*Q2,5@16*Q2,6@16*Q2] /\
    [6@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff1ee,L0xbefff1f0,L0xbefff1f2] /\
    [L0xbefff1ee,L0xbefff1f0,L0xbefff1f2]< [6@16*Q2,5@16*Q2,5@16*Q2] /\
    [9@16*NQ2,9@16*NQ2,5@16*NQ2]< [L0xbefff1f4,L0xbefff1f6,L0xbefff1f8] /\
    [L0xbefff1f4,L0xbefff1f6,L0xbefff1f8]< [9@16*Q2,9@16*Q2,5@16*Q2] /\
    [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff1fa,L0xbefff1fc,L0xbefff1fe] /\
    [L0xbefff1fa,L0xbefff1fc,L0xbefff1fe]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
    [5@16*NQ2,5@16*NQ2]< [L0xbefff200,L0xbefff202] /\
    [L0xbefff200,L0xbefff202]< [5@16*Q2,5@16*Q2]
    prove with [precondition];

(* vmov	s23, r0                                    #! PC = 0x400cc4 *)
mov [s23_b, s23_t] [r0_b, r0_t];
(* ldr.w	r2, [r0, #32]                             #! EA = L0xbefff224; Value = 0xab6983d7; PC = 0x400cc8 *)
mov [r2_b, r2_t] [L0xbefff224, L0xbefff226];
(* ldr.w	r3, [r0, #36]	; 0x24                      #! EA = L0xbefff228; Value = 0xb3638e45; PC = 0x400ccc *)
mov [r3_b, r3_t] [L0xbefff228, L0xbefff22a];
(* ldr.w	r4, [r0, #40]	; 0x28                      #! EA = L0xbefff22c; Value = 0xb344912f; PC = 0x400cd0 *)
mov [r4_b, r4_t] [L0xbefff22c, L0xbefff22e];
(* ldr.w	r5, [r0, #44]	; 0x2c                      #! EA = L0xbefff230; Value = 0xafb097fd; PC = 0x400cd4 *)
mov [r5_b, r5_t] [L0xbefff230, L0xbefff232];
(* ldr.w	r6, [r0, #48]	; 0x30                      #! EA = L0xbefff234; Value = 0xb2068ea8; PC = 0x400cd8 *)
mov [r6_b, r6_t] [L0xbefff234, L0xbefff236];
(* ldr.w	r7, [r0, #52]	; 0x34                      #! EA = L0xbefff238; Value = 0xadc0947a; PC = 0x400cdc *)
mov [r7_b, r7_t] [L0xbefff238, L0xbefff23a];
(* ldr.w	r8, [r0, #56]	; 0x38                      #! EA = L0xbefff23c; Value = 0xb7109524; PC = 0x400ce0 *)
mov [r8_b, r8_t] [L0xbefff23c, L0xbefff23e];
(* ldr.w	r9, [r0, #60]	; 0x3c                      #! EA = L0xbefff240; Value = 0xba929f42; PC = 0x400ce4 *)
mov [r9_b, r9_t] [L0xbefff240, L0xbefff242];
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400ce8 *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* sadd16	lr, r2, r3                               #! PC = 0x400cec *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* ssub16	r3, r2, r3                               #! PC = 0x400cf0 *)
sub r3_b r2_b r3_b;
sub r3_t r2_t r3_t;
(* sadd16	r11, r4, r5                              #! PC = 0x400cf4 *)
add r11_b r4_b r5_b;
add r11_t r4_t r5_t;
(* ssub16	r5, r4, r5                               #! PC = 0x400cf8 *)
sub r5_b r4_b r5_b;
sub r5_t r4_t r5_t;
(* sadd16	r2, r6, r7                               #! PC = 0x400cfc *)
add r2_b r6_b r7_b;
add r2_t r6_t r7_t;
(* ssub16	r7, r6, r7                               #! PC = 0x400d00 *)
sub r7_b r6_b r7_b;
sub r7_t r6_t r7_t;
(* sadd16	r4, r8, r9                               #! PC = 0x400d04 *)
add r4_b r8_b r9_b;
add r4_t r8_t r9_t;
(* ssub16	r9, r8, r9                               #! PC = 0x400d08 *)
sub r9_b r8_b r9_b;
sub r9_t r8_t r9_t;
(* sadd16	r8, lr, r11                              #! PC = 0x400d0c *)
add r8_b lr_b r11_b;
add r8_t lr_t r11_t;
(* ssub16	r11, lr, r11                             #! PC = 0x400d10 *)
sub r11_b lr_b r11_b;
sub r11_t lr_t r11_t;
(* sadd16	r6, r2, r4                               #! PC = 0x400d14 *)
add r6_b r2_b r4_b;
add r6_t r2_t r4_t;
(* ssub16	r4, r2, r4                               #! PC = 0x400d18 *)
sub r4_b r2_b r4_b;
sub r4_t r2_t r4_t;

ghost r5_b3@int16, r5_t3@int16:
      r5_b3 = r5_b /\ r5_t3 = r5_t
   && r5_b3 = r5_b /\ r5_t3 = r5_t;

(* vmov	r10, s10                                   #! PC = 0x400d1c *)
mov [r10_b, r10_t] [s10_b, s10_t]; mov r10 s10;
(* smulwb	lr, r10, r5                              #! PC = 0x400d20 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x400d24 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d28 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400d2c *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400d30 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_b3 *  1600) [Q] /\
       eqmod lr_t (r5_t3 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_b3 *  1600) [Q] /\
       eqmod lr_t (r5_t3 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x400d34 *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400d38 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r9_b5@int16, r9_t5@int16:
      r9_b5 = r9_b /\ r9_t5 = r9_t
   && r9_b5 = r9_b /\ r9_t5 = r9_t;

(* smulwb	lr, r10, r9                              #! PC = 0x400d3c *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400d40 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d44 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400d48 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400d4c *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b5 *  1600) [Q] /\
       eqmod lr_t (r9_t5 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b5 *  1600) [Q] /\
       eqmod lr_t (r9_t5 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x400d50 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400d54 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* sadd16	r2, r8, r6                               #! PC = 0x400d58 *)
add r2_b r8_b r6_b;
add r2_t r8_t r6_t;
(* ssub16	r6, r8, r6                               #! PC = 0x400d5c *)
sub r6_b r8_b r6_b;
sub r6_t r8_t r6_t;

ghost r7_b3@int16, r7_t3@int16:
      r7_b3 = r7_b /\ r7_t3 = r7_t
   && r7_b3 = r7_b /\ r7_t3 = r7_t;

(* vmov	r10, s12                                   #! PC = 0x400d60 *)
mov [r10_b, r10_t] [s12_b, s12_t]; mov r10 s12;
(* smulwb	lr, r10, r7                              #! PC = 0x400d64 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x400d68 *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d6c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400d70 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400d74 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_b3 *    40) [Q] /\
       eqmod lr_t (r7_t3 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_b3 *    40) [Q] /\
       eqmod lr_t (r7_t3 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x400d78 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400d7c *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r4_b3@int16, r4_t3@int16:
      r4_b3 = r4_b /\ r4_t3 = r4_t
   && r4_b3 = r4_b /\ r4_t3 = r4_t;

(* vmov	r10, s13                                   #! PC = 0x400d80 *)
mov [r10_b, r10_t] [s13_b, s13_t]; mov r10 s13;
(* smulwb	lr, r10, r4                              #! PC = 0x400d84 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x400d88 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d8c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400d90 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400d94 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_b3 *  1600) [Q] /\
       eqmod lr_t (r4_t3 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_b3 *  1600) [Q] /\
       eqmod lr_t (r4_t3 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* ssub16	r8, r11, lr                              #! PC = 0x400d98 *)
sub r8_b r11_b lr_b;
sub r8_t r11_t lr_t;
(* sadd16	r4, r11, lr                              #! PC = 0x400d9c *)
add r4_b r11_b lr_b;
add r4_t r11_t lr_t;

ghost r9_b6@int16, r9_t6@int16:
      r9_b6 = r9_b /\ r9_t6 = r9_t
   && r9_b6 = r9_b /\ r9_t6 = r9_t;

(* vmov	r10, s14                                   #! PC = 0x400da0 *)
mov [r10_b, r10_t] [s14_b, s14_t]; mov r10 s14;
(* smulwb	lr, r10, r9                              #! PC = 0x400da4 *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400da8 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400dac *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400db0 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400db4 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b6 *   749) [Q] /\
       eqmod lr_t (r9_t6 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b6 *   749) [Q] /\
       eqmod lr_t (r9_t6 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x400db8 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400dbc *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;

ghost r3_b1@int16, r3_t1@int16:
      r3_b1 = r3_b /\ r3_t1 = r3_t
   && r3_b1 = r3_b /\ r3_t1 = r3_t;

(* vmov	r11, s16                                   #! PC = 0x400dc0 *)
mov [r11_b, r11_t] [s16_b, s16_t]; mov r11 s16;
(* smulwb	lr, r11, r3                              #! PC = 0x400dc4 *)
vpc r3_bW@int32 r3_b; mulj tmp r11 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r11, r3                              #! PC = 0x400dc8 *)
vpc r3_tW@int32 r3_t; mulj tmp r11 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400dcc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400dd0 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x400dd4 *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov r3_b tmp_b; mov r3_t tmp_t;

assert eqmod r3_b (r3_b1 *  -848) [Q] /\
       eqmod r3_t (r3_t1 *  -848) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r3_b (r3_b1 *  -848) [Q] /\
       eqmod r3_t (r3_t1 *  -848) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r3_b, r3_t] /\ [r3_b, r3_t] <s[Q2, Q2];

ghost r4_b4@int16, r4_t4@int16:
      r4_b4 = r4_b /\ r4_t4 = r4_t
   && r4_b4 = r4_b /\ r4_t4 = r4_t;

(* vmov	r10, s17                                   #! PC = 0x400dd8 *)
mov [r10_b, r10_t] [s17_b, s17_t]; mov r10 s17;
(* vmov	r11, s18                                   #! PC = 0x400ddc *)
mov [r11_b, r11_t] [s18_b, s18_t]; mov r11 s18;
(* smulwb	lr, r10, r4                              #! PC = 0x400de0 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x400de4 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400de8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400dec *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x400df0 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov r4_b tmp_b; mov r4_t tmp_t;

assert eqmod r4_b (r4_b4 *    40) [Q] /\
       eqmod r4_t (r4_t4 *    40) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r4_b (r4_b4 *    40) [Q] /\
       eqmod r4_t (r4_t4 *    40) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r4_b, r4_t] /\ [r4_b, r4_t] <s[Q2, Q2];

ghost r5_b4@int16, r5_t4@int16:
      r5_b4 = r5_b /\ r5_t4 = r5_t
   && r5_b4 = r5_b /\ r5_t4 = r5_t;

(* smulwb	lr, r11, r5                              #! PC = 0x400df4 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x400df8 *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400dfc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400e00 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x400e04 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov r5_b tmp_b; mov r5_t tmp_t;

assert eqmod r5_b (r5_b4 *  -630) [Q] /\
       eqmod r5_t (r5_t4 *  -630) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r5_b (r5_b4 *  -630) [Q] /\
       eqmod r5_t (r5_t4 *  -630) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r5_b, r5_t] /\ [r5_b, r5_t] <s[Q2, Q2];

ghost r6_b1@int16, r6_t1@int16:
      r6_b1 = r6_b /\ r6_t1 = r6_t
   && r6_b1 = r6_b /\ r6_t1 = r6_t;

(* vmov	r10, s19                                   #! PC = 0x400e08 *)
mov [r10_b, r10_t] [s19_b, s19_t]; mov r10 s19;
(* vmov	r11, s20                                   #! PC = 0x400e0c *)
mov [r11_b, r11_t] [s20_b, s20_t]; mov r11 s20;
(* smulwb	lr, r10, r6                              #! PC = 0x400e10 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x400e14 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e18 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400e1c *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x400e20 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov r6_b tmp_b; mov r6_t tmp_t;

assert eqmod r6_b (r6_b1 *  1600) [Q] /\
       eqmod r6_t (r6_t1 *  1600) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r6_b (r6_b1 *  1600) [Q] /\
       eqmod r6_t (r6_t1 *  1600) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r6_b, r6_t] /\ [r6_b, r6_t] <s[Q2, Q2];

ghost r7_b4@int16, r7_t4@int16:
      r7_b4 = r7_b /\ r7_t4 = r7_t
   && r7_b4 = r7_b /\ r7_t4 = r7_t;

(* smulwb	lr, r11, r7                              #! PC = 0x400e24 *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x400e28 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e2c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400e30 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x400e34 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov r7_b tmp_b; mov r7_t tmp_t;

assert eqmod r7_b (r7_b4 *  1432) [Q] /\
       eqmod r7_t (r7_t4 *  1432) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r7_b (r7_b4 *  1432) [Q] /\
       eqmod r7_t (r7_t4 *  1432) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r7_b, r7_t] /\ [r7_b, r7_t] <s[Q2, Q2];

ghost r8_b1@int16, r8_t1@int16:
      r8_b1 = r8_b /\ r8_t1 = r8_t
   && r8_b1 = r8_b /\ r8_t1 = r8_t;

(* vmov	r10, s21                                   #! PC = 0x400e38 *)
mov [r10_b, r10_t] [s21_b, s21_t]; mov r10 s21;
(* vmov	r11, s22                                   #! PC = 0x400e3c *)
mov [r11_b, r11_t] [s22_b, s22_t]; mov r11 s22;
(* smulwb	lr, r10, r8                              #! PC = 0x400e40 *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x400e44 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e48 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400e4c *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x400e50 *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov r8_b tmp_b; mov r8_t tmp_t;

assert eqmod r8_b (r8_b1 *   749) [Q] /\
       eqmod r8_t (r8_t1 *   749) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r8_b (r8_b1 *   749) [Q] /\
       eqmod r8_t (r8_t1 *   749) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r8_b, r8_t] /\ [r8_b, r8_t] <s[Q2, Q2];

ghost r9_b7@int16, r9_t7@int16:
      r9_b7 = r9_b /\ r9_t7 = r9_t
   && r9_b7 = r9_b /\ r9_t7 = r9_t;

(* smulwb	lr, r11, r9                              #! PC = 0x400e54 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x400e58 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e5c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400e60 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x400e64 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov r9_b tmp_b; mov r9_t tmp_t;

assert eqmod r9_b (r9_b7 *   687) [Q] /\
       eqmod r9_t (r9_t7 *   687) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r9_b (r9_b7 *   687) [Q] /\
       eqmod r9_t (r9_t7 *   687) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r9_b, r9_t] /\ [r9_b, r9_t] <s[Q2, Q2];

(* vmov	s0, r2                                     #! PC = 0x400e68 *)
mov [s0_b, s0_t] [r2_b, r2_t];
(* vmov	s1, r3                                     #! PC = 0x400e6c *)
mov [s1_b, s1_t] [r3_b, r3_t];
(* vmov	s2, r4                                     #! PC = 0x400e70 *)
mov [s2_b, s2_t] [r4_b, r4_t];
(* vmov	s3, r5                                     #! PC = 0x400e74 *)
mov [s3_b, s3_t] [r5_b, r5_t];
(* vmov	s4, r6                                     #! PC = 0x400e78 *)
mov [s4_b, s4_t] [r6_b, r6_t];
(* vmov	s5, r7                                     #! PC = 0x400e7c *)
mov [s5_b, s5_t] [r7_b, r7_t];
(* vmov	s6, r8                                     #! PC = 0x400e80 *)
mov [s6_b, s6_t] [r8_b, r8_t];
(* vmov	s7, r9                                     #! PC = 0x400e84 *)
mov [s7_b, s7_t] [r9_b, r9_t];

assert [8*NQ2,8*NQ2]< [s0_b, s0_t] /\ [s0_b, s0_t]< [8*Q2,8*Q2] /\
       [1*NQ2,1*NQ2]< [s1_b, s1_t] /\ [s1_b, s1_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s2_b, s2_t] /\ [s2_b, s2_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s3_b, s3_t] /\ [s3_b, s3_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s4_b, s4_t] /\ [s4_b, s4_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s5_b, s5_t] /\ [s5_b, s5_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s6_b, s6_t] /\ [s6_b, s6_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s7_b, s7_t] /\ [s7_b, s7_t]< [1*Q2,1*Q2]
       prove with [algebra solver isl, precondition] && true;
assume [8*NQ2,8*NQ2]< [s0_b, s0_t] /\ [s0_b, s0_t]< [8*Q2,8*Q2] /\
       [1*NQ2,1*NQ2]< [s1_b, s1_t] /\ [s1_b, s1_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s2_b, s2_t] /\ [s2_b, s2_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s3_b, s3_t] /\ [s3_b, s3_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s4_b, s4_t] /\ [s4_b, s4_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s5_b, s5_t] /\ [s5_b, s5_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s6_b, s6_t] /\ [s6_b, s6_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s7_b, s7_t] /\ [s7_b, s7_t]< [1*Q2,1*Q2]
    && [8@16*NQ2,8@16*NQ2]<s[s0_b, s0_t] /\ [s0_b, s0_t]<s[8@16*Q2,8@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s1_b, s1_t] /\ [s1_b, s1_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s2_b, s2_t] /\ [s2_b, s2_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s3_b, s3_t] /\ [s3_b, s3_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s4_b, s4_t] /\ [s4_b, s4_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s5_b, s5_t] /\ [s5_b, s5_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s6_b, s6_t] /\ [s6_b, s6_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s7_b, s7_t] /\ [s7_b, s7_t]<s[1@16*Q2,1@16*Q2];

(* CUT 2 *)
ghost Z1@int16: X**2 =  -848*17** 25*Z1 && true;
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod (poly Z1 [poly X [s0_b, s0_t], poly X [s1_b, s1_t],
                    poly X [s2_b, s2_t], poly X [s3_b, s3_t],
                    poly X [s4_b, s4_t], poly X [s5_b, s5_t],
                    poly X [s6_b, s6_t], poly X [s7_b, s7_t]])
          (2**3*F**2) [Q, Z1**8 + 1] /\
    [8*NQ2,8*NQ2]< [s0_b, s0_t] /\ [s0_b, s0_t]< [8*Q2,8*Q2] /\
    [1*NQ2,1*NQ2]< [s1_b, s1_t] /\ [s1_b, s1_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s2_b, s2_t] /\ [s2_b, s2_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s3_b, s3_t] /\ [s3_b, s3_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s4_b, s4_t] /\ [s4_b, s4_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s5_b, s5_t] /\ [s5_b, s5_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s6_b, s6_t] /\ [s6_b, s6_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s7_b, s7_t] /\ [s7_b, s7_t]< [1*Q2,1*Q2]
    prove with [precondition]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [8@16*NQ2,8@16*NQ2]<s[s0_b, s0_t] /\ [s0_b, s0_t]<s[8@16*Q2,8@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s1_b, s1_t] /\ [s1_b, s1_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s2_b, s2_t] /\ [s2_b, s2_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s3_b, s3_t] /\ [s3_b, s3_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s4_b, s4_t] /\ [s4_b, s4_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s5_b, s5_t] /\ [s5_b, s5_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s6_b, s6_t] /\ [s6_b, s6_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s7_b, s7_t] /\ [s7_b, s7_t]<s[1@16*Q2,1@16*Q2]
    prove with [precondition];

(* vmov	r0, s23                                    #! PC = 0x400e88 *)
mov [r0_b, r0_t] [s23_b, s23_t];
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff204; Value = 0xb05294da; PC = 0x400e8c *)
mov [r2_b, r2_t] [L0xbefff204, L0xbefff206];
(* ldr.w	r3, [r0, #4]                              #! EA = L0xbefff208; Value = 0xb7928c2c; PC = 0x400e90 *)
mov [r3_b, r3_t] [L0xbefff208, L0xbefff20a];
(* ldr.w	r4, [r0, #8]                              #! EA = L0xbefff20c; Value = 0xb00599c1; PC = 0x400e94 *)
mov [r4_b, r4_t] [L0xbefff20c, L0xbefff20e];
(* ldr.w	r5, [r0, #12]                             #! EA = L0xbefff210; Value = 0xb1d795d1; PC = 0x400e98 *)
mov [r5_b, r5_t] [L0xbefff210, L0xbefff212];
(* ldr.w	r6, [r0, #16]                             #! EA = L0xbefff214; Value = 0xc4228f81; PC = 0x400e9c *)
mov [r6_b, r6_t] [L0xbefff214, L0xbefff216];
(* ldr.w	r7, [r0, #20]                             #! EA = L0xbefff218; Value = 0xb768843f; PC = 0x400ea0 *)
mov [r7_b, r7_t] [L0xbefff218, L0xbefff21a];
(* ldr.w	r8, [r0, #24]                             #! EA = L0xbefff21c; Value = 0xb4698c75; PC = 0x400ea4 *)
mov [r8_b, r8_t] [L0xbefff21c, L0xbefff21e];
(* ldr.w	r9, [r0, #28]                             #! EA = L0xbefff220; Value = 0xb2258693; PC = 0x400ea8 *)
mov [r9_b, r9_t] [L0xbefff220, L0xbefff222];
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400eac *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* sadd16	lr, r2, r3                               #! PC = 0x400eb0 *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* ssub16	r3, r2, r3                               #! PC = 0x400eb4 *)
sub r3_b r2_b r3_b;
sub r3_t r2_t r3_t;
(* sadd16	r11, r4, r5                              #! PC = 0x400eb8 *)
add r11_b r4_b r5_b;
add r11_t r4_t r5_t;
(* ssub16	r5, r4, r5                               #! PC = 0x400ebc *)
sub r5_b r4_b r5_b;
sub r5_t r4_t r5_t;
(* sadd16	r2, r6, r7                               #! PC = 0x400ec0 *)
add r2_b r6_b r7_b;
add r2_t r6_t r7_t;
(* ssub16	r7, r6, r7                               #! PC = 0x400ec4 *)
sub r7_b r6_b r7_b;
sub r7_t r6_t r7_t;
(* sadd16	r4, r8, r9                               #! PC = 0x400ec8 *)
add r4_b r8_b r9_b;
add r4_t r8_t r9_t;
(* ssub16	r9, r8, r9                               #! PC = 0x400ecc *)
sub r9_b r8_b r9_b;
sub r9_t r8_t r9_t;
(* sadd16	r8, lr, r11                              #! PC = 0x400ed0 *)
add r8_b lr_b r11_b;
add r8_t lr_t r11_t;
(* ssub16	r11, lr, r11                             #! PC = 0x400ed4 *)
sub r11_b lr_b r11_b;
sub r11_t lr_t r11_t;
(* sadd16	r6, r2, r4                               #! PC = 0x400ed8 *)
add r6_b r2_b r4_b;
add r6_t r2_t r4_t;
(* ssub16	r4, r2, r4                               #! PC = 0x400edc *)
sub r4_b r2_b r4_b;
sub r4_t r2_t r4_t;

ghost r5_b5@int16, r5_t5@int16:
      r5_b5 = r5_b /\ r5_t5 = r5_t
   && r5_b5 = r5_b /\ r5_t5 = r5_t;

(* vmov	r10, s10                                   #! PC = 0x400ee0 *)
mov [r10_b, r10_t] [s10_b, s10_t]; mov r10 s10;
(* smulwb	lr, r10, r5                              #! PC = 0x400ee4 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x400ee8 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400eec *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400ef0 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400ef4 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_b5 *  1600) [Q] /\
       eqmod lr_t (r5_t5 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_b5 *  1600) [Q] /\
       eqmod lr_t (r5_t5 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x400ef8 *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400efc *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r9_b8@int16, r9_t8@int16:
      r9_b8 = r9_b /\ r9_t8 = r9_t
   && r9_b8 = r9_b /\ r9_t8 = r9_t;

(* smulwb	lr, r10, r9                              #! PC = 0x400f00 *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400f04 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f08 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400f0c *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400f10 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b8 *  1600) [Q] /\
       eqmod lr_t (r9_t8 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b8 *  1600) [Q] /\
       eqmod lr_t (r9_t8 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x400f14 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400f18 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* sadd16	r2, r8, r6                               #! PC = 0x400f1c *)
add r2_b r8_b r6_b;
add r2_t r8_t r6_t;
(* ssub16	r6, r8, r6                               #! PC = 0x400f20 *)
sub r6_b r8_b r6_b;
sub r6_t r8_t r6_t;

ghost r7_b5@int16, r7_t5@int16:
      r7_b5 = r7_b /\ r7_t5 = r7_t
   && r7_b5 = r7_b /\ r7_t5 = r7_t;

(* vmov	r10, s12                                   #! PC = 0x400f24 *)
mov [r10_b, r10_t] [s12_b, s12_t]; mov r10 s12;
(* smulwb	lr, r10, r7                              #! PC = 0x400f28 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x400f2c *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f30 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400f34 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400f38 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_b5 *    40) [Q] /\
       eqmod lr_t (r7_t5 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_b5 *    40) [Q] /\
       eqmod lr_t (r7_t5 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x400f3c *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400f40 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r4_b5@int16, r4_t5@int16:
      r4_b5 = r4_b /\ r4_t5 = r4_t
   && r4_b5 = r4_b /\ r4_t5 = r4_t;

(* vmov	r10, s13                                   #! PC = 0x400f44 *)
mov [r10_b, r10_t] [s13_b, s13_t]; mov r10 s13;
(* smulwb	lr, r10, r4                              #! PC = 0x400f48 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x400f4c *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f50 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400f54 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400f58 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_b5 *  1600) [Q] /\
       eqmod lr_t (r4_t5 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_b5 *  1600) [Q] /\
       eqmod lr_t (r4_t5 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* ssub16	r8, r11, lr                              #! PC = 0x400f5c *)
sub r8_b r11_b lr_b;
sub r8_t r11_t lr_t;
(* sadd16	r4, r11, lr                              #! PC = 0x400f60 *)
add r4_b r11_b lr_b;
add r4_t r11_t lr_t;

ghost r9_b9@int16, r9_t9@int16:
      r9_b9 = r9_b /\ r9_t9 = r9_t
   && r9_b9 = r9_b /\ r9_t9 = r9_t;

(* vmov	r10, s14                                   #! PC = 0x400f64 *)
mov [r10_b, r10_t] [s14_b, s14_t]; mov r10 s14;
(* smulwb	lr, r10, r9                              #! PC = 0x400f68 *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400f6c *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f70 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400f74 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400f78 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b9 *   749) [Q] /\
       eqmod lr_t (r9_t9 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b9 *   749) [Q] /\
       eqmod lr_t (r9_t9 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x400f7c *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400f80 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* vmov	r0, s23                                    #! PC = 0x400f84 *)
mov [r0_b, r0_t] [s23_b, s23_t];
(* vmov	r11, s1                                    #! PC = 0x400f88 *)
mov [r11_b, r11_t] [s1_b, s1_t];
(* uadd16	lr, r3, r11                              #! PC = 0x400f8c *)
add lr_b r3_b r11_b;
add lr_t r3_t r11_t;
(* usub16	r3, r3, r11                              #! PC = 0x400f90 *)
sub r3_b r3_b r11_b;
sub r3_t r3_t r11_t;
(* str.w	lr, [r0, #4]                              #! EA = L0xbefff208; PC = 0x400f94 *)
mov [L0xbefff208, L0xbefff20a] [lr_b, lr_t];
(* str.w	r3, [r0, #36]	; 0x24                      #! EA = L0xbefff228; PC = 0x400f98 *)
mov [L0xbefff228, L0xbefff22a] [r3_b, r3_t];
(* vmov	r11, s3                                    #! PC = 0x400f9c *)
mov [r11_b, r11_t] [s3_b, s3_t];
(* uadd16	lr, r5, r11                              #! PC = 0x400fa0 *)
add lr_b r5_b r11_b;
add lr_t r5_t r11_t;
(* usub16	r5, r5, r11                              #! PC = 0x400fa4 *)
sub r5_b r5_b r11_b;
sub r5_t r5_t r11_t;
(* str.w	lr, [r0, #12]                             #! EA = L0xbefff210; PC = 0x400fa8 *)
mov [L0xbefff210, L0xbefff212] [lr_b, lr_t];
(* str.w	r5, [r0, #44]	; 0x2c                      #! EA = L0xbefff230; PC = 0x400fac *)
mov [L0xbefff230, L0xbefff232] [r5_b, r5_t];
(* vmov	r11, s5                                    #! PC = 0x400fb0 *)
mov [r11_b, r11_t] [s5_b, s5_t];
(* uadd16	lr, r7, r11                              #! PC = 0x400fb4 *)
add lr_b r7_b r11_b;
add lr_t r7_t r11_t;
(* usub16	r7, r7, r11                              #! PC = 0x400fb8 *)
sub r7_b r7_b r11_b;
sub r7_t r7_t r11_t;
(* str.w	lr, [r0, #20]                             #! EA = L0xbefff218; PC = 0x400fbc *)
mov [L0xbefff218, L0xbefff21a] [lr_b, lr_t];
(* str.w	r7, [r0, #52]	; 0x34                      #! EA = L0xbefff238; PC = 0x400fc0 *)
mov [L0xbefff238, L0xbefff23a] [r7_b, r7_t];
(* vmov	r11, s7                                    #! PC = 0x400fc4 *)
mov [r11_b, r11_t] [s7_b, s7_t];
(* uadd16	lr, r9, r11                              #! PC = 0x400fc8 *)
add lr_b r9_b r11_b;
add lr_t r9_t r11_t;
(* usub16	r9, r9, r11                              #! PC = 0x400fcc *)
sub r9_b r9_b r11_b;
sub r9_t r9_t r11_t;
(* str.w	lr, [r0, #28]                             #! EA = L0xbefff220; PC = 0x400fd0 *)
mov [L0xbefff220, L0xbefff222] [lr_b, lr_t];
(* str.w	r9, [r0, #60]	; 0x3c                      #! EA = L0xbefff240; PC = 0x400fd4 *)
mov [L0xbefff240, L0xbefff242] [r9_b, r9_t];
(* vmov	r5, s2                                     #! PC = 0x400fd8 *)
mov [r5_b, r5_t] [s2_b, s2_t];
(* uadd16	lr, r4, r5                               #! PC = 0x400fdc *)
add lr_b r4_b r5_b;
add lr_t r4_t r5_t;
(* usub16	r11, r4, r5                              #! PC = 0x400fe0 *)
sub r11_b r4_b r5_b;
sub r11_t r4_t r5_t;
(* str.w	lr, [r0, #8]                              #! EA = L0xbefff20c; PC = 0x400fe4 *)
mov [L0xbefff20c, L0xbefff20e] [lr_b, lr_t];
(* str.w	r11, [r0, #40]	; 0x28                     #! EA = L0xbefff22c; PC = 0x400fe8 *)
mov [L0xbefff22c, L0xbefff22e] [r11_b, r11_t];
(* vmov	r7, s4                                     #! PC = 0x400fec *)
mov [r7_b, r7_t] [s4_b, s4_t];
(* uadd16	lr, r6, r7                               #! PC = 0x400ff0 *)
add lr_b r6_b r7_b;
add lr_t r6_t r7_t;
(* usub16	r11, r6, r7                              #! PC = 0x400ff4 *)
sub r11_b r6_b r7_b;
sub r11_t r6_t r7_t;
(* str.w	lr, [r0, #16]                             #! EA = L0xbefff214; PC = 0x400ff8 *)
mov [L0xbefff214, L0xbefff216] [lr_b, lr_t];
(* str.w	r11, [r0, #48]	; 0x30                     #! EA = L0xbefff234; PC = 0x400ffc *)
mov [L0xbefff234, L0xbefff236] [r11_b, r11_t];
(* vmov	r9, s6                                     #! PC = 0x401000 *)
mov [r9_b, r9_t] [s6_b, s6_t];
(* uadd16	lr, r8, r9                               #! PC = 0x401004 *)
add lr_b r8_b r9_b;
add lr_t r8_t r9_t;
(* usub16	r11, r8, r9                              #! PC = 0x401008 *)
sub r11_b r8_b r9_b;
sub r11_t r8_t r9_t;
(* str.w	lr, [r0, #24]                             #! EA = L0xbefff21c; PC = 0x40100c *)
mov [L0xbefff21c, L0xbefff21e] [lr_b, lr_t];
(* str.w	r11, [r0, #56]	; 0x38                     #! EA = L0xbefff23c; PC = 0x401010 *)
mov [L0xbefff23c, L0xbefff23e] [r11_b, r11_t];
(* vmov	r3, s0                                     #! PC = 0x401014 *)
mov [r3_b, r3_t] [s0_b, s0_t];
(* uadd16	lr, r2, r3                               #! PC = 0x401018 *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* usub16	r11, r2, r3                              #! PC = 0x40101c *)
sub r11_b r2_b r3_b;
sub r11_t r2_t r3_t;
(* str.w	r11, [r0, #32]                            #! EA = L0xbefff224; PC = 0x401020 *)
mov [L0xbefff224, L0xbefff226] [r11_b, r11_t];
(* str.w	lr, [r0], #64                             #! EA = L0xbefff204; PC = 0x401024 *)
mov [L0xbefff204, L0xbefff206] [lr_b, lr_t];
(* vmov	lr, s8                                     #! PC = 0x401028 *)
mov [lr_b, lr_t] [s8_b, s8_t]; mov lr s8;
(* cmp.w	r0, lr                                    #! PC = 0x40102c *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x400cc4 <invntt_fast+24>                #! PC = 0x401030 *)
#bne.w	0x400cc4 <invntt_fast+24>                #! 0x401030 = 0x401030;

(* CUT 3 *)
assert [16*NQ2,16*NQ2,5*NQ2]< [L0xbefff204,L0xbefff206,L0xbefff208] /\
       [L0xbefff204,L0xbefff206,L0xbefff208]< [16*Q2,16*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff20a,L0xbefff20c,L0xbefff20e] /\
       [L0xbefff20a,L0xbefff20c,L0xbefff20e]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2,9*NQ2]< [L0xbefff210,L0xbefff212,L0xbefff214] /\
       [L0xbefff210,L0xbefff212,L0xbefff214]< [5*Q2,5*Q2,9*Q2] /\
       [9*NQ2,5*NQ2,5*NQ2]< [L0xbefff216,L0xbefff218,L0xbefff21a] /\
       [L0xbefff216,L0xbefff218,L0xbefff21a]< [9*Q2,5*Q2,5*Q2] /\
       [6*NQ2,6*NQ2,5*NQ2]< [L0xbefff21c,L0xbefff21e,L0xbefff220] /\
       [L0xbefff21c,L0xbefff21e,L0xbefff220]< [6*Q2,6*Q2,5*Q2] /\
       [5*NQ2,16*NQ2,16*NQ2]< [L0xbefff222,L0xbefff224,L0xbefff226] /\
       [L0xbefff222,L0xbefff224,L0xbefff226]< [5*Q2,16*Q2,16*Q2] /\
       [5*NQ2,5*NQ2,6*NQ2]< [L0xbefff228,L0xbefff22a,L0xbefff22c] /\
       [L0xbefff228,L0xbefff22a,L0xbefff22c]< [5*Q2,5*Q2,6*Q2] /\
       [6*NQ2,5*NQ2,5*NQ2]< [L0xbefff22e,L0xbefff230,L0xbefff232] /\
       [L0xbefff22e,L0xbefff230,L0xbefff232]< [6*Q2,5*Q2,5*Q2] /\
       [9*NQ2,9*NQ2,5*NQ2]< [L0xbefff234,L0xbefff236,L0xbefff238] /\
       [L0xbefff234,L0xbefff236,L0xbefff238]< [9*Q2,9*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff23a,L0xbefff23c,L0xbefff23e] /\
       [L0xbefff23a,L0xbefff23c,L0xbefff23e]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2]< [L0xbefff240,L0xbefff242] /\
       [L0xbefff240,L0xbefff242]< [5*Q2,5*Q2]
       prove with [algebra solver isl, precondition] && true;
assume [16*NQ2,16*NQ2,5*NQ2]< [L0xbefff204,L0xbefff206,L0xbefff208] /\
       [L0xbefff204,L0xbefff206,L0xbefff208]< [16*Q2,16*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff20a,L0xbefff20c,L0xbefff20e] /\
       [L0xbefff20a,L0xbefff20c,L0xbefff20e]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2,9*NQ2]< [L0xbefff210,L0xbefff212,L0xbefff214] /\
       [L0xbefff210,L0xbefff212,L0xbefff214]< [5*Q2,5*Q2,9*Q2] /\
       [9*NQ2,5*NQ2,5*NQ2]< [L0xbefff216,L0xbefff218,L0xbefff21a] /\
       [L0xbefff216,L0xbefff218,L0xbefff21a]< [9*Q2,5*Q2,5*Q2] /\
       [6*NQ2,6*NQ2,5*NQ2]< [L0xbefff21c,L0xbefff21e,L0xbefff220] /\
       [L0xbefff21c,L0xbefff21e,L0xbefff220]< [6*Q2,6*Q2,5*Q2] /\
       [5*NQ2,16*NQ2,16*NQ2]< [L0xbefff222,L0xbefff224,L0xbefff226] /\
       [L0xbefff222,L0xbefff224,L0xbefff226]< [5*Q2,16*Q2,16*Q2] /\
       [5*NQ2,5*NQ2,6*NQ2]< [L0xbefff228,L0xbefff22a,L0xbefff22c] /\
       [L0xbefff228,L0xbefff22a,L0xbefff22c]< [5*Q2,5*Q2,6*Q2] /\
       [6*NQ2,5*NQ2,5*NQ2]< [L0xbefff22e,L0xbefff230,L0xbefff232] /\
       [L0xbefff22e,L0xbefff230,L0xbefff232]< [6*Q2,5*Q2,5*Q2] /\
       [9*NQ2,9*NQ2,5*NQ2]< [L0xbefff234,L0xbefff236,L0xbefff238] /\
       [L0xbefff234,L0xbefff236,L0xbefff238]< [9*Q2,9*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff23a,L0xbefff23c,L0xbefff23e] /\
       [L0xbefff23a,L0xbefff23c,L0xbefff23e]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2]< [L0xbefff240,L0xbefff242] /\
       [L0xbefff240,L0xbefff242]< [5*Q2,5*Q2]
    && [16@16*NQ2,16@16*NQ2,5@16*NQ2]< [L0xbefff204,L0xbefff206,L0xbefff208] /\
       [L0xbefff204,L0xbefff206,L0xbefff208]< [16@16*Q2,16@16*Q2,5@16*Q2] /\
       [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff20a,L0xbefff20c,L0xbefff20e] /\
       [L0xbefff20a,L0xbefff20c,L0xbefff20e]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
       [5@16*NQ2,5@16*NQ2,9@16*NQ2]< [L0xbefff210,L0xbefff212,L0xbefff214] /\
       [L0xbefff210,L0xbefff212,L0xbefff214]< [5@16*Q2,5@16*Q2,9@16*Q2] /\
       [9@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff216,L0xbefff218,L0xbefff21a] /\
       [L0xbefff216,L0xbefff218,L0xbefff21a]< [9@16*Q2,5@16*Q2,5@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,5@16*NQ2]< [L0xbefff21c,L0xbefff21e,L0xbefff220] /\
       [L0xbefff21c,L0xbefff21e,L0xbefff220]< [6@16*Q2,6@16*Q2,5@16*Q2] /\
       [5@16*NQ2,16@16*NQ2,16@16*NQ2]< [L0xbefff222,L0xbefff224,L0xbefff226] /\
       [L0xbefff222,L0xbefff224,L0xbefff226]< [5@16*Q2,16@16*Q2,16@16*Q2] /\
       [5@16*NQ2,5@16*NQ2,6@16*NQ2]< [L0xbefff228,L0xbefff22a,L0xbefff22c] /\
       [L0xbefff228,L0xbefff22a,L0xbefff22c]< [5@16*Q2,5@16*Q2,6@16*Q2] /\
       [6@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff22e,L0xbefff230,L0xbefff232] /\
       [L0xbefff22e,L0xbefff230,L0xbefff232]< [6@16*Q2,5@16*Q2,5@16*Q2] /\
       [9@16*NQ2,9@16*NQ2,5@16*NQ2]< [L0xbefff234,L0xbefff236,L0xbefff238] /\
       [L0xbefff234,L0xbefff236,L0xbefff238]< [9@16*Q2,9@16*Q2,5@16*Q2] /\
       [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff23a,L0xbefff23c,L0xbefff23e] /\
       [L0xbefff23a,L0xbefff23c,L0xbefff23e]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
       [5@16*NQ2,5@16*NQ2]< [L0xbefff240,L0xbefff242] /\
       [L0xbefff240,L0xbefff242]< [5@16*Q2,5@16*Q2];
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod (poly Z1
           [poly X [L0xbefff204,L0xbefff206],poly X [L0xbefff208,L0xbefff20a],
            poly X [L0xbefff20c,L0xbefff20e],poly X [L0xbefff210,L0xbefff212],
            poly X [L0xbefff214,L0xbefff216],poly X [L0xbefff218,L0xbefff21a],
            poly X [L0xbefff21c,L0xbefff21e],poly X [L0xbefff220,L0xbefff222],
            poly X [L0xbefff224,L0xbefff226],poly X [L0xbefff228,L0xbefff22a],
            poly X [L0xbefff22c,L0xbefff22e],poly X [L0xbefff230,L0xbefff232],
            poly X [L0xbefff234,L0xbefff236],poly X [L0xbefff238,L0xbefff23a],
            poly X [L0xbefff23c,L0xbefff23e],poly X [L0xbefff240,L0xbefff242]])
          (2**4*F**2) [Q, Z1**16 - 1] /\
    [16*NQ2,16*NQ2,5*NQ2]< [L0xbefff204,L0xbefff206,L0xbefff208] /\
    [L0xbefff204,L0xbefff206,L0xbefff208]< [16*Q2,16*Q2,5*Q2] /\
    [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff20a,L0xbefff20c,L0xbefff20e] /\
    [L0xbefff20a,L0xbefff20c,L0xbefff20e]< [5*Q2,6*Q2,6*Q2] /\
    [5*NQ2,5*NQ2,9*NQ2]< [L0xbefff210,L0xbefff212,L0xbefff214] /\
    [L0xbefff210,L0xbefff212,L0xbefff214]< [5*Q2,5*Q2,9*Q2] /\
    [9*NQ2,5*NQ2,5*NQ2]< [L0xbefff216,L0xbefff218,L0xbefff21a] /\
    [L0xbefff216,L0xbefff218,L0xbefff21a]< [9*Q2,5*Q2,5*Q2] /\
    [6*NQ2,6*NQ2,5*NQ2]< [L0xbefff21c,L0xbefff21e,L0xbefff220] /\
    [L0xbefff21c,L0xbefff21e,L0xbefff220]< [6*Q2,6*Q2,5*Q2] /\
    [5*NQ2,16*NQ2,16*NQ2]< [L0xbefff222,L0xbefff224,L0xbefff226] /\
    [L0xbefff222,L0xbefff224,L0xbefff226]< [5*Q2,16*Q2,16*Q2] /\
    [5*NQ2,5*NQ2,6*NQ2]< [L0xbefff228,L0xbefff22a,L0xbefff22c] /\
    [L0xbefff228,L0xbefff22a,L0xbefff22c]< [5*Q2,5*Q2,6*Q2] /\
    [6*NQ2,5*NQ2,5*NQ2]< [L0xbefff22e,L0xbefff230,L0xbefff232] /\
    [L0xbefff22e,L0xbefff230,L0xbefff232]< [6*Q2,5*Q2,5*Q2] /\
    [9*NQ2,9*NQ2,5*NQ2]< [L0xbefff234,L0xbefff236,L0xbefff238] /\
    [L0xbefff234,L0xbefff236,L0xbefff238]< [9*Q2,9*Q2,5*Q2] /\
    [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff23a,L0xbefff23c,L0xbefff23e] /\
    [L0xbefff23a,L0xbefff23c,L0xbefff23e]< [5*Q2,6*Q2,6*Q2] /\
    [5*NQ2,5*NQ2]< [L0xbefff240,L0xbefff242] /\
    [L0xbefff240,L0xbefff242]< [5*Q2,5*Q2]
    prove with [precondition, all ghosts]
 && [16@16*NQ2,16@16*NQ2,5@16*NQ2]< [L0xbefff204,L0xbefff206,L0xbefff208] /\
    [L0xbefff204,L0xbefff206,L0xbefff208]< [16@16*Q2,16@16*Q2,5@16*Q2] /\
    [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff20a,L0xbefff20c,L0xbefff20e] /\
    [L0xbefff20a,L0xbefff20c,L0xbefff20e]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
    [5@16*NQ2,5@16*NQ2,9@16*NQ2]< [L0xbefff210,L0xbefff212,L0xbefff214] /\
    [L0xbefff210,L0xbefff212,L0xbefff214]< [5@16*Q2,5@16*Q2,9@16*Q2] /\
    [9@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff216,L0xbefff218,L0xbefff21a] /\
    [L0xbefff216,L0xbefff218,L0xbefff21a]< [9@16*Q2,5@16*Q2,5@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,5@16*NQ2]< [L0xbefff21c,L0xbefff21e,L0xbefff220] /\
    [L0xbefff21c,L0xbefff21e,L0xbefff220]< [6@16*Q2,6@16*Q2,5@16*Q2] /\
    [5@16*NQ2,16@16*NQ2,16@16*NQ2]< [L0xbefff222,L0xbefff224,L0xbefff226] /\
    [L0xbefff222,L0xbefff224,L0xbefff226]< [5@16*Q2,16@16*Q2,16@16*Q2] /\
    [5@16*NQ2,5@16*NQ2,6@16*NQ2]< [L0xbefff228,L0xbefff22a,L0xbefff22c] /\
    [L0xbefff228,L0xbefff22a,L0xbefff22c]< [5@16*Q2,5@16*Q2,6@16*Q2] /\
    [6@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff22e,L0xbefff230,L0xbefff232] /\
    [L0xbefff22e,L0xbefff230,L0xbefff232]< [6@16*Q2,5@16*Q2,5@16*Q2] /\
    [9@16*NQ2,9@16*NQ2,5@16*NQ2]< [L0xbefff234,L0xbefff236,L0xbefff238] /\
    [L0xbefff234,L0xbefff236,L0xbefff238]< [9@16*Q2,9@16*Q2,5@16*Q2] /\
    [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff23a,L0xbefff23c,L0xbefff23e] /\
    [L0xbefff23a,L0xbefff23c,L0xbefff23e]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
    [5@16*NQ2,5@16*NQ2]< [L0xbefff240,L0xbefff242] /\
    [L0xbefff240,L0xbefff242]< [5@16*Q2,5@16*Q2]
    prove with [precondition];

(* vmov	s23, r0                                    #! PC = 0x400cc4 *)
mov [s23_b, s23_t] [r0_b, r0_t];
(* ldr.w	r2, [r0, #32]                             #! EA = L0xbefff264; Value = 0xb46d98f8; PC = 0x400cc8 *)
mov [r2_b, r2_t] [L0xbefff264, L0xbefff266];
(* ldr.w	r3, [r0, #36]	; 0x24                      #! EA = L0xbefff268; Value = 0xb9ef9b32; PC = 0x400ccc *)
mov [r3_b, r3_t] [L0xbefff268, L0xbefff26a];
(* ldr.w	r4, [r0, #40]	; 0x28                      #! EA = L0xbefff26c; Value = 0xafda99ac; PC = 0x400cd0 *)
mov [r4_b, r4_t] [L0xbefff26c, L0xbefff26e];
(* ldr.w	r5, [r0, #44]	; 0x2c                      #! EA = L0xbefff270; Value = 0xbbd29a02; PC = 0x400cd4 *)
mov [r5_b, r5_t] [L0xbefff270, L0xbefff272];
(* ldr.w	r6, [r0, #48]	; 0x30                      #! EA = L0xbefff274; Value = 0xae5f91cf; PC = 0x400cd8 *)
mov [r6_b, r6_t] [L0xbefff274, L0xbefff276];
(* ldr.w	r7, [r0, #52]	; 0x34                      #! EA = L0xbefff278; Value = 0xb4879df5; PC = 0x400cdc *)
mov [r7_b, r7_t] [L0xbefff278, L0xbefff27a];
(* ldr.w	r8, [r0, #56]	; 0x38                      #! EA = L0xbefff27c; Value = 0xb53f8f00; PC = 0x400ce0 *)
mov [r8_b, r8_t] [L0xbefff27c, L0xbefff27e];
(* ldr.w	r9, [r0, #60]	; 0x3c                      #! EA = L0xbefff280; Value = 0xbb239964; PC = 0x400ce4 *)
mov [r9_b, r9_t] [L0xbefff280, L0xbefff282];
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400ce8 *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* sadd16	lr, r2, r3                               #! PC = 0x400cec *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* ssub16	r3, r2, r3                               #! PC = 0x400cf0 *)
sub r3_b r2_b r3_b;
sub r3_t r2_t r3_t;
(* sadd16	r11, r4, r5                              #! PC = 0x400cf4 *)
add r11_b r4_b r5_b;
add r11_t r4_t r5_t;
(* ssub16	r5, r4, r5                               #! PC = 0x400cf8 *)
sub r5_b r4_b r5_b;
sub r5_t r4_t r5_t;
(* sadd16	r2, r6, r7                               #! PC = 0x400cfc *)
add r2_b r6_b r7_b;
add r2_t r6_t r7_t;
(* ssub16	r7, r6, r7                               #! PC = 0x400d00 *)
sub r7_b r6_b r7_b;
sub r7_t r6_t r7_t;
(* sadd16	r4, r8, r9                               #! PC = 0x400d04 *)
add r4_b r8_b r9_b;
add r4_t r8_t r9_t;
(* ssub16	r9, r8, r9                               #! PC = 0x400d08 *)
sub r9_b r8_b r9_b;
sub r9_t r8_t r9_t;
(* sadd16	r8, lr, r11                              #! PC = 0x400d0c *)
add r8_b lr_b r11_b;
add r8_t lr_t r11_t;
(* ssub16	r11, lr, r11                             #! PC = 0x400d10 *)
sub r11_b lr_b r11_b;
sub r11_t lr_t r11_t;
(* sadd16	r6, r2, r4                               #! PC = 0x400d14 *)
add r6_b r2_b r4_b;
add r6_t r2_t r4_t;
(* ssub16	r4, r2, r4                               #! PC = 0x400d18 *)
sub r4_b r2_b r4_b;
sub r4_t r2_t r4_t;

ghost r5_b6@int16, r5_t6@int16:
      r5_b6 = r5_b /\ r5_t6 = r5_t
   && r5_b6 = r5_b /\ r5_t6 = r5_t;

(* vmov	r10, s10                                   #! PC = 0x400d1c *)
mov [r10_b, r10_t] [s10_b, s10_t]; mov r10 s10;
(* smulwb	lr, r10, r5                              #! PC = 0x400d20 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x400d24 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d28 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400d2c *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400d30 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_b6 *  1600) [Q] /\
       eqmod lr_t (r5_t6 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_b6 *  1600) [Q] /\
       eqmod lr_t (r5_t6 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x400d34 *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400d38 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r9_b10@int16, r9_t10@int16:
      r9_b10 = r9_b /\ r9_t10 = r9_t
   && r9_b10 = r9_b /\ r9_t10 = r9_t;

(* smulwb	lr, r10, r9                              #! PC = 0x400d3c *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400d40 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d44 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400d48 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400d4c *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b10 *  1600) [Q] /\
       eqmod lr_t (r9_t10 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b10 *  1600) [Q] /\
       eqmod lr_t (r9_t10 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x400d50 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400d54 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* sadd16	r2, r8, r6                               #! PC = 0x400d58 *)
add r2_b r8_b r6_b;
add r2_t r8_t r6_t;
(* ssub16	r6, r8, r6                               #! PC = 0x400d5c *)
sub r6_b r8_b r6_b;
sub r6_t r8_t r6_t;

ghost r7_b6@int16, r7_t6@int16:
      r7_b6 = r7_b /\ r7_t6 = r7_t
   && r7_b6 = r7_b /\ r7_t6 = r7_t;

(* vmov	r10, s12                                   #! PC = 0x400d60 *)
mov [r10_b, r10_t] [s12_b, s12_t]; mov r10 s12;
(* smulwb	lr, r10, r7                              #! PC = 0x400d64 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x400d68 *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d6c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400d70 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400d74 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_b6 *    40) [Q] /\
       eqmod lr_t (r7_t6 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_b6 *    40) [Q] /\
       eqmod lr_t (r7_t6 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x400d78 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400d7c *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r4_b6@int16, r4_t6@int16:
      r4_b6 = r4_b /\ r4_t6 = r4_t
   && r4_b6 = r4_b /\ r4_t6 = r4_t;

(* vmov	r10, s13                                   #! PC = 0x400d80 *)
mov [r10_b, r10_t] [s13_b, s13_t]; mov r10 s13;
(* smulwb	lr, r10, r4                              #! PC = 0x400d84 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x400d88 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d8c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400d90 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400d94 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_b6 *  1600) [Q] /\
       eqmod lr_t (r4_t6 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_b6 *  1600) [Q] /\
       eqmod lr_t (r4_t6 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* ssub16	r8, r11, lr                              #! PC = 0x400d98 *)
sub r8_b r11_b lr_b;
sub r8_t r11_t lr_t;
(* sadd16	r4, r11, lr                              #! PC = 0x400d9c *)
add r4_b r11_b lr_b;
add r4_t r11_t lr_t;

ghost r9_b11@int16, r9_t11@int16:
      r9_b11 = r9_b /\ r9_t11 = r9_t
   && r9_b11 = r9_b /\ r9_t11 = r9_t;

(* vmov	r10, s14                                   #! PC = 0x400da0 *)
mov [r10_b, r10_t] [s14_b, s14_t]; mov r10 s14;
(* smulwb	lr, r10, r9                              #! PC = 0x400da4 *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400da8 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400dac *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400db0 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400db4 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b11 *   749) [Q] /\
       eqmod lr_t (r9_t11 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b11 *   749) [Q] /\
       eqmod lr_t (r9_t11 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x400db8 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400dbc *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;

ghost r3_b2@int16, r3_t2@int16:
      r3_b2 = r3_b /\ r3_t2 = r3_t
   && r3_b2 = r3_b /\ r3_t2 = r3_t;

(* vmov	r11, s16                                   #! PC = 0x400dc0 *)
mov [r11_b, r11_t] [s16_b, s16_t]; mov r11 s16;
(* smulwb	lr, r11, r3                              #! PC = 0x400dc4 *)
vpc r3_bW@int32 r3_b; mulj tmp r11 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r11, r3                              #! PC = 0x400dc8 *)
vpc r3_tW@int32 r3_t; mulj tmp r11 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400dcc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400dd0 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x400dd4 *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov r3_b tmp_b; mov r3_t tmp_t;

assert eqmod r3_b (r3_b2 *  -848) [Q] /\
       eqmod r3_t (r3_t2 *  -848) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r3_b (r3_b2 *  -848) [Q] /\
       eqmod r3_t (r3_t2 *  -848) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r3_b, r3_t] /\ [r3_b, r3_t] <s[Q2, Q2];

(* vmov	r10, s17                                   #! PC = 0x400dd8 *)
mov [r10_b, r10_t] [s17_b, s17_t]; mov r10 s17;
(* vmov	r11, s18                                   #! PC = 0x400ddc *)
mov [r11_b, r11_t] [s18_b, s18_t]; mov r11 s18;

ghost r4_b7@int16, r4_t7@int16:
      r4_b7 = r4_b /\ r4_t7 = r4_t
   && r4_b7 = r4_b /\ r4_t7 = r4_t;

(* smulwb	lr, r10, r4                              #! PC = 0x400de0 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x400de4 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400de8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400dec *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x400df0 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov r4_b tmp_b; mov r4_t tmp_t;

assert eqmod r4_b (r4_b7 *    40) [Q] /\
       eqmod r4_t (r4_t7 *    40) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r4_b (r4_b7 *    40) [Q] /\
       eqmod r4_t (r4_t7 *    40) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r4_b, r4_t] /\ [r4_b, r4_t] <s[Q2, Q2];

ghost r5_b7@int16, r5_t7@int16:
      r5_b7 = r5_b /\ r5_t7 = r5_t
   && r5_b7 = r5_b /\ r5_t7 = r5_t;

(* smulwb	lr, r11, r5                              #! PC = 0x400df4 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x400df8 *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400dfc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400e00 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x400e04 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov r5_b tmp_b; mov r5_t tmp_t;

assert eqmod r5_b (r5_b7 *  -630) [Q] /\
       eqmod r5_t (r5_t7 *  -630) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r5_b (r5_b7 *  -630) [Q] /\
       eqmod r5_t (r5_t7 *  -630) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r5_b, r5_t] /\ [r5_b, r5_t] <s[Q2, Q2];

ghost r6_b2@int16, r6_t2@int16:
      r6_b2 = r6_b /\ r6_t2 = r6_t
   && r6_b2 = r6_b /\ r6_t2 = r6_t;

(* vmov	r10, s19                                   #! PC = 0x400e08 *)
mov [r10_b, r10_t] [s19_b, s19_t]; mov r10 s19;
(* vmov	r11, s20                                   #! PC = 0x400e0c *)
mov [r11_b, r11_t] [s20_b, s20_t]; mov r11 s20;
(* smulwb	lr, r10, r6                              #! PC = 0x400e10 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x400e14 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e18 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400e1c *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x400e20 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov r6_b tmp_b; mov r6_t tmp_t;

assert eqmod r6_b (r6_b2 *  1600) [Q] /\
       eqmod r6_t (r6_t2 *  1600) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r6_b (r6_b2 *  1600) [Q] /\
       eqmod r6_t (r6_t2 *  1600) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r6_b, r6_t] /\ [r6_b, r6_t] <s[Q2, Q2];

ghost r7_b7@int16, r7_t7@int16:
      r7_b7 = r7_b /\ r7_t7 = r7_t
   && r7_b7 = r7_b /\ r7_t7 = r7_t;

(* smulwb	lr, r11, r7                              #! PC = 0x400e24 *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x400e28 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e2c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400e30 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x400e34 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov r7_b tmp_b; mov r7_t tmp_t;

assert eqmod r7_b (r7_b7 *  1432) [Q] /\
       eqmod r7_t (r7_t7 *  1432) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r7_b (r7_b7 *  1432) [Q] /\
       eqmod r7_t (r7_t7 *  1432) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r7_b, r7_t] /\ [r7_b, r7_t] <s[Q2, Q2];

ghost r8_b2@int16, r8_t2@int16:
      r8_b2 = r8_b /\ r8_t2 = r8_t
   && r8_b2 = r8_b /\ r8_t2 = r8_t;

(* vmov	r10, s21                                   #! PC = 0x400e38 *)
mov [r10_b, r10_t] [s21_b, s21_t]; mov r10 s21;
(* vmov	r11, s22                                   #! PC = 0x400e3c *)
mov [r11_b, r11_t] [s22_b, s22_t]; mov r11 s22;
(* smulwb	lr, r10, r8                              #! PC = 0x400e40 *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x400e44 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e48 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400e4c *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x400e50 *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov r8_b tmp_b; mov r8_t tmp_t;

assert eqmod r8_b (r8_b2 *   749) [Q] /\
       eqmod r8_t (r8_t2 *   749) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r8_b (r8_b2 *   749) [Q] /\
       eqmod r8_t (r8_t2 *   749) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r8_b, r8_t] /\ [r8_b, r8_t] <s[Q2, Q2];

ghost r9_b12@int16, r9_t12@int16:
      r9_b12 = r9_b /\ r9_t12 = r9_t
   && r9_b12 = r9_b /\ r9_t12 = r9_t;

(* smulwb	lr, r11, r9                              #! PC = 0x400e54 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x400e58 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e5c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400e60 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x400e64 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov r9_b tmp_b; mov r9_t tmp_t;

assert eqmod r9_b (r9_b12 *   687) [Q] /\
       eqmod r9_t (r9_t12 *   687) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r9_b (r9_b12 *   687) [Q] /\
       eqmod r9_t (r9_t12 *   687) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r9_b, r9_t] /\ [r9_b, r9_t] <s[Q2, Q2];

(* vmov	s0, r2                                     #! PC = 0x400e68 *)
mov [s0_b, s0_t] [r2_b, r2_t];
(* vmov	s1, r3                                     #! PC = 0x400e6c *)
mov [s1_b, s1_t] [r3_b, r3_t];
(* vmov	s2, r4                                     #! PC = 0x400e70 *)
mov [s2_b, s2_t] [r4_b, r4_t];
(* vmov	s3, r5                                     #! PC = 0x400e74 *)
mov [s3_b, s3_t] [r5_b, r5_t];
(* vmov	s4, r6                                     #! PC = 0x400e78 *)
mov [s4_b, s4_t] [r6_b, r6_t];
(* vmov	s5, r7                                     #! PC = 0x400e7c *)
mov [s5_b, s5_t] [r7_b, r7_t];
(* vmov	s6, r8                                     #! PC = 0x400e80 *)
mov [s6_b, s6_t] [r8_b, r8_t];
(* vmov	s7, r9                                     #! PC = 0x400e84 *)
mov [s7_b, s7_t] [r9_b, r9_t];

assert [8*NQ2,8*NQ2]< [s0_b, s0_t] /\ [s0_b, s0_t]< [8*Q2,8*Q2] /\
       [1*NQ2,1*NQ2]< [s1_b, s1_t] /\ [s1_b, s1_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s2_b, s2_t] /\ [s2_b, s2_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s3_b, s3_t] /\ [s3_b, s3_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s4_b, s4_t] /\ [s4_b, s4_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s5_b, s5_t] /\ [s5_b, s5_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s6_b, s6_t] /\ [s6_b, s6_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s7_b, s7_t] /\ [s7_b, s7_t]< [1*Q2,1*Q2]
       prove with [algebra solver isl, precondition] && true;
assume [8*NQ2,8*NQ2]< [s0_b, s0_t] /\ [s0_b, s0_t]< [8*Q2,8*Q2] /\
       [1*NQ2,1*NQ2]< [s1_b, s1_t] /\ [s1_b, s1_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s2_b, s2_t] /\ [s2_b, s2_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s3_b, s3_t] /\ [s3_b, s3_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s4_b, s4_t] /\ [s4_b, s4_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s5_b, s5_t] /\ [s5_b, s5_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s6_b, s6_t] /\ [s6_b, s6_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s7_b, s7_t] /\ [s7_b, s7_t]< [1*Q2,1*Q2]
    && [8@16*NQ2,8@16*NQ2]<s[s0_b, s0_t] /\ [s0_b, s0_t]<s[8@16*Q2,8@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s1_b, s1_t] /\ [s1_b, s1_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s2_b, s2_t] /\ [s2_b, s2_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s3_b, s3_t] /\ [s3_b, s3_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s4_b, s4_t] /\ [s4_b, s4_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s5_b, s5_t] /\ [s5_b, s5_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s6_b, s6_t] /\ [s6_b, s6_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s7_b, s7_t] /\ [s7_b, s7_t]<s[1@16*Q2,1@16*Q2];

(* CUT 4 *)
ghost Z2@int16: X**2 =  -848*17** 21*Z2 && true;
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod (poly Z2 [poly X [s0_b, s0_t], poly X [s1_b, s1_t],
                    poly X [s2_b, s2_t], poly X [s3_b, s3_t],
                    poly X [s4_b, s4_t], poly X [s5_b, s5_t],
                    poly X [s6_b, s6_t], poly X [s7_b, s7_t]])
          (2**3*F**2) [Q, Z2**8 + 1] /\
    [8*NQ2,8*NQ2]< [s0_b, s0_t] /\ [s0_b, s0_t]< [8*Q2,8*Q2] /\
    [1*NQ2,1*NQ2]< [s1_b, s1_t] /\ [s1_b, s1_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s2_b, s2_t] /\ [s2_b, s2_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s3_b, s3_t] /\ [s3_b, s3_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s4_b, s4_t] /\ [s4_b, s4_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s5_b, s5_t] /\ [s5_b, s5_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s6_b, s6_t] /\ [s6_b, s6_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s7_b, s7_t] /\ [s7_b, s7_t]< [1*Q2,1*Q2]
    prove with [precondition]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [8@16*NQ2,8@16*NQ2]<s[s0_b, s0_t] /\ [s0_b, s0_t]<s[8@16*Q2,8@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s1_b, s1_t] /\ [s1_b, s1_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s2_b, s2_t] /\ [s2_b, s2_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s3_b, s3_t] /\ [s3_b, s3_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s4_b, s4_t] /\ [s4_b, s4_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s5_b, s5_t] /\ [s5_b, s5_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s6_b, s6_t] /\ [s6_b, s6_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s7_b, s7_t] /\ [s7_b, s7_t]<s[1@16*Q2,1@16*Q2]
    prove with [precondition];

(* vmov	r0, s23                                    #! PC = 0x400e88 *)
mov [r0_b, r0_t] [s23_b, s23_t];
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff244; Value = 0xa5ee908b; PC = 0x400e8c *)
mov [r2_b, r2_t] [L0xbefff244, L0xbefff246];
(* ldr.w	r3, [r0, #4]                              #! EA = L0xbefff248; Value = 0xaec09b9b; PC = 0x400e90 *)
mov [r3_b, r3_t] [L0xbefff248, L0xbefff24a];
(* ldr.w	r4, [r0, #8]                              #! EA = L0xbefff24c; Value = 0xb2c0a358; PC = 0x400e94 *)
mov [r4_b, r4_t] [L0xbefff24c, L0xbefff24e];
(* ldr.w	r5, [r0, #12]                             #! EA = L0xbefff250; Value = 0xb9529742; PC = 0x400e98 *)
mov [r5_b, r5_t] [L0xbefff250, L0xbefff252];
(* ldr.w	r6, [r0, #16]                             #! EA = L0xbefff254; Value = 0xb4ae9ef1; PC = 0x400e9c *)
mov [r6_b, r6_t] [L0xbefff254, L0xbefff256];
(* ldr.w	r7, [r0, #20]                             #! EA = L0xbefff258; Value = 0xbe649bad; PC = 0x400ea0 *)
mov [r7_b, r7_t] [L0xbefff258, L0xbefff25a];
(* ldr.w	r8, [r0, #24]                             #! EA = L0xbefff25c; Value = 0xb0f19625; PC = 0x400ea4 *)
mov [r8_b, r8_t] [L0xbefff25c, L0xbefff25e];
(* ldr.w	r9, [r0, #28]                             #! EA = L0xbefff260; Value = 0xb39d8f1d; PC = 0x400ea8 *)
mov [r9_b, r9_t] [L0xbefff260, L0xbefff262];
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400eac *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* sadd16	lr, r2, r3                               #! PC = 0x400eb0 *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* ssub16	r3, r2, r3                               #! PC = 0x400eb4 *)
sub r3_b r2_b r3_b;
sub r3_t r2_t r3_t;
(* sadd16	r11, r4, r5                              #! PC = 0x400eb8 *)
add r11_b r4_b r5_b;
add r11_t r4_t r5_t;
(* ssub16	r5, r4, r5                               #! PC = 0x400ebc *)
sub r5_b r4_b r5_b;
sub r5_t r4_t r5_t;
(* sadd16	r2, r6, r7                               #! PC = 0x400ec0 *)
add r2_b r6_b r7_b;
add r2_t r6_t r7_t;
(* ssub16	r7, r6, r7                               #! PC = 0x400ec4 *)
sub r7_b r6_b r7_b;
sub r7_t r6_t r7_t;
(* sadd16	r4, r8, r9                               #! PC = 0x400ec8 *)
add r4_b r8_b r9_b;
add r4_t r8_t r9_t;
(* ssub16	r9, r8, r9                               #! PC = 0x400ecc *)
sub r9_b r8_b r9_b;
sub r9_t r8_t r9_t;
(* sadd16	r8, lr, r11                              #! PC = 0x400ed0 *)
add r8_b lr_b r11_b;
add r8_t lr_t r11_t;
(* ssub16	r11, lr, r11                             #! PC = 0x400ed4 *)
sub r11_b lr_b r11_b;
sub r11_t lr_t r11_t;
(* sadd16	r6, r2, r4                               #! PC = 0x400ed8 *)
add r6_b r2_b r4_b;
add r6_t r2_t r4_t;
(* ssub16	r4, r2, r4                               #! PC = 0x400edc *)
sub r4_b r2_b r4_b;
sub r4_t r2_t r4_t;

ghost r5_b8@int16, r5_t8@int16:
      r5_b8 = r5_b /\ r5_t8 = r5_t
   && r5_b8 = r5_b /\ r5_t8 = r5_t;

(* vmov	r10, s10                                   #! PC = 0x400ee0 *)
mov [r10_b, r10_t] [s10_b, s10_t]; mov r10 s10;
(* smulwb	lr, r10, r5                              #! PC = 0x400ee4 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x400ee8 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400eec *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400ef0 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400ef4 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_b8 *  1600) [Q] /\
       eqmod lr_t (r5_t8 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_b8 *  1600) [Q] /\
       eqmod lr_t (r5_t8 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x400ef8 *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400efc *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r9_b13@int16, r9_t13@int16:
      r9_b13 = r9_b /\ r9_t13 = r9_t
   && r9_b13 = r9_b /\ r9_t13 = r9_t;

(* smulwb	lr, r10, r9                              #! PC = 0x400f00 *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400f04 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f08 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400f0c *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400f10 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b13 *  1600) [Q] /\
       eqmod lr_t (r9_t13 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b13 *  1600) [Q] /\
       eqmod lr_t (r9_t13 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x400f14 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400f18 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* sadd16	r2, r8, r6                               #! PC = 0x400f1c *)
add r2_b r8_b r6_b;
add r2_t r8_t r6_t;
(* ssub16	r6, r8, r6                               #! PC = 0x400f20 *)
sub r6_b r8_b r6_b;
sub r6_t r8_t r6_t;

ghost r7_b8@int16, r7_t8@int16:
      r7_b8 = r7_b /\ r7_t8 = r7_t
   && r7_b8 = r7_b /\ r7_t8 = r7_t;

(* vmov	r10, s12                                   #! PC = 0x400f24 *)
mov [r10_b, r10_t] [s12_b, s12_t]; mov r10 s12;
(* smulwb	lr, r10, r7                              #! PC = 0x400f28 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x400f2c *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f30 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400f34 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400f38 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_b8 *    40) [Q] /\
       eqmod lr_t (r7_t8 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_b8 *    40) [Q] /\
       eqmod lr_t (r7_t8 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x400f3c *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400f40 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r4_b8@int16, r4_t8@int16:
      r4_b8 = r4_b /\ r4_t8 = r4_t
   && r4_b8 = r4_b /\ r4_t8 = r4_t;

(* vmov	r10, s13                                   #! PC = 0x400f44 *)
mov [r10_b, r10_t] [s13_b, s13_t]; mov r10 s13;
(* smulwb	lr, r10, r4                              #! PC = 0x400f48 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x400f4c *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f50 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400f54 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400f58 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_b8 *  1600) [Q] /\
       eqmod lr_t (r4_t8 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_b8 *  1600) [Q] /\
       eqmod lr_t (r4_t8 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* ssub16	r8, r11, lr                              #! PC = 0x400f5c *)
sub r8_b r11_b lr_b;
sub r8_t r11_t lr_t;
(* sadd16	r4, r11, lr                              #! PC = 0x400f60 *)
add r4_b r11_b lr_b;
add r4_t r11_t lr_t;

ghost r9_b14@int16, r9_t14@int16:
      r9_b14 = r9_b /\ r9_t14 = r9_t
   && r9_b14 = r9_b /\ r9_t14 = r9_t;

(* vmov	r10, s14                                   #! PC = 0x400f64 *)
mov [r10_b, r10_t] [s14_b, s14_t]; mov r10 s14;
(* smulwb	lr, r10, r9                              #! PC = 0x400f68 *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400f6c *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f70 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400f74 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400f78 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b14 *   749) [Q] /\
       eqmod lr_t (r9_t14 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b14 *   749) [Q] /\
       eqmod lr_t (r9_t14 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x400f7c *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400f80 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* vmov	r0, s23                                    #! PC = 0x400f84 *)
mov [r0_b, r0_t] [s23_b, s23_t];
(* vmov	r11, s1                                    #! PC = 0x400f88 *)
mov [r11_b, r11_t] [s1_b, s1_t];
(* uadd16	lr, r3, r11                              #! PC = 0x400f8c *)
add lr_b r3_b r11_b;
add lr_t r3_t r11_t;
(* usub16	r3, r3, r11                              #! PC = 0x400f90 *)
sub r3_b r3_b r11_b;
sub r3_t r3_t r11_t;
(* str.w	lr, [r0, #4]                              #! EA = L0xbefff248; PC = 0x400f94 *)
mov [L0xbefff248, L0xbefff24a] [lr_b, lr_t];
(* str.w	r3, [r0, #36]	; 0x24                      #! EA = L0xbefff268; PC = 0x400f98 *)
mov [L0xbefff268, L0xbefff26a] [r3_b, r3_t];
(* vmov	r11, s3                                    #! PC = 0x400f9c *)
mov [r11_b, r11_t] [s3_b, s3_t];
(* uadd16	lr, r5, r11                              #! PC = 0x400fa0 *)
add lr_b r5_b r11_b;
add lr_t r5_t r11_t;
(* usub16	r5, r5, r11                              #! PC = 0x400fa4 *)
sub r5_b r5_b r11_b;
sub r5_t r5_t r11_t;
(* str.w	lr, [r0, #12]                             #! EA = L0xbefff250; PC = 0x400fa8 *)
mov [L0xbefff250, L0xbefff252] [lr_b, lr_t];
(* str.w	r5, [r0, #44]	; 0x2c                      #! EA = L0xbefff270; PC = 0x400fac *)
mov [L0xbefff270, L0xbefff272] [r5_b, r5_t];
(* vmov	r11, s5                                    #! PC = 0x400fb0 *)
mov [r11_b, r11_t] [s5_b, s5_t];
(* uadd16	lr, r7, r11                              #! PC = 0x400fb4 *)
add lr_b r7_b r11_b;
add lr_t r7_t r11_t;
(* usub16	r7, r7, r11                              #! PC = 0x400fb8 *)
sub r7_b r7_b r11_b;
sub r7_t r7_t r11_t;
(* str.w	lr, [r0, #20]                             #! EA = L0xbefff258; PC = 0x400fbc *)
mov [L0xbefff258, L0xbefff25a] [lr_b, lr_t];
(* str.w	r7, [r0, #52]	; 0x34                      #! EA = L0xbefff278; PC = 0x400fc0 *)
mov [L0xbefff278, L0xbefff27a] [r7_b, r7_t];
(* vmov	r11, s7                                    #! PC = 0x400fc4 *)
mov [r11_b, r11_t] [s7_b, s7_t];
(* uadd16	lr, r9, r11                              #! PC = 0x400fc8 *)
add lr_b r9_b r11_b;
add lr_t r9_t r11_t;
(* usub16	r9, r9, r11                              #! PC = 0x400fcc *)
sub r9_b r9_b r11_b;
sub r9_t r9_t r11_t;
(* str.w	lr, [r0, #28]                             #! EA = L0xbefff260; PC = 0x400fd0 *)
mov [L0xbefff260, L0xbefff262] [lr_b, lr_t];
(* str.w	r9, [r0, #60]	; 0x3c                      #! EA = L0xbefff280; PC = 0x400fd4 *)
mov [L0xbefff280, L0xbefff282] [r9_b, r9_t];
(* vmov	r5, s2                                     #! PC = 0x400fd8 *)
mov [r5_b, r5_t] [s2_b, s2_t];
(* uadd16	lr, r4, r5                               #! PC = 0x400fdc *)
add lr_b r4_b r5_b;
add lr_t r4_t r5_t;
(* usub16	r11, r4, r5                              #! PC = 0x400fe0 *)
sub r11_b r4_b r5_b;
sub r11_t r4_t r5_t;
(* str.w	lr, [r0, #8]                              #! EA = L0xbefff24c; PC = 0x400fe4 *)
mov [L0xbefff24c, L0xbefff24e] [lr_b, lr_t];
(* str.w	r11, [r0, #40]	; 0x28                     #! EA = L0xbefff26c; PC = 0x400fe8 *)
mov [L0xbefff26c, L0xbefff26e] [r11_b, r11_t];
(* vmov	r7, s4                                     #! PC = 0x400fec *)
mov [r7_b, r7_t] [s4_b, s4_t];
(* uadd16	lr, r6, r7                               #! PC = 0x400ff0 *)
add lr_b r6_b r7_b;
add lr_t r6_t r7_t;
(* usub16	r11, r6, r7                              #! PC = 0x400ff4 *)
sub r11_b r6_b r7_b;
sub r11_t r6_t r7_t;
(* str.w	lr, [r0, #16]                             #! EA = L0xbefff254; PC = 0x400ff8 *)
mov [L0xbefff254, L0xbefff256] [lr_b, lr_t];
(* str.w	r11, [r0, #48]	; 0x30                     #! EA = L0xbefff274; PC = 0x400ffc *)
mov [L0xbefff274, L0xbefff276] [r11_b, r11_t];
(* vmov	r9, s6                                     #! PC = 0x401000 *)
mov [r9_b, r9_t] [s6_b, s6_t];
(* uadd16	lr, r8, r9                               #! PC = 0x401004 *)
add lr_b r8_b r9_b;
add lr_t r8_t r9_t;
(* usub16	r11, r8, r9                              #! PC = 0x401008 *)
sub r11_b r8_b r9_b;
sub r11_t r8_t r9_t;
(* str.w	lr, [r0, #24]                             #! EA = L0xbefff25c; PC = 0x40100c *)
mov [L0xbefff25c, L0xbefff25e] [lr_b, lr_t];
(* str.w	r11, [r0, #56]	; 0x38                     #! EA = L0xbefff27c; PC = 0x401010 *)
mov [L0xbefff27c, L0xbefff27e] [r11_b, r11_t];
(* vmov	r3, s0                                     #! PC = 0x401014 *)
mov [r3_b, r3_t] [s0_b, s0_t];
(* uadd16	lr, r2, r3                               #! PC = 0x401018 *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* usub16	r11, r2, r3                              #! PC = 0x40101c *)
sub r11_b r2_b r3_b;
sub r11_t r2_t r3_t;
(* str.w	r11, [r0, #32]                            #! EA = L0xbefff264; PC = 0x401020 *)
mov [L0xbefff264, L0xbefff266] [r11_b, r11_t];
(* str.w	lr, [r0], #64                             #! EA = L0xbefff244; PC = 0x401024 *)
mov [L0xbefff244, L0xbefff246] [lr_b, lr_t];
(* vmov	lr, s8                                     #! PC = 0x401028 *)
mov [lr_b, lr_t] [s8_b, s8_t]; mov lr s8;
(* cmp.w	r0, lr                                    #! PC = 0x40102c *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x400cc4 <invntt_fast+24>                #! PC = 0x401030 *)
#bne.w	0x400cc4 <invntt_fast+24>                #! 0x401030 = 0x401030;

(* CUT 5 *)
assert [16*NQ2,16*NQ2,5*NQ2]< [L0xbefff244,L0xbefff246,L0xbefff248] /\
       [L0xbefff244,L0xbefff246,L0xbefff248]< [16*Q2,16*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff24a,L0xbefff24c,L0xbefff24e] /\
       [L0xbefff24a,L0xbefff24c,L0xbefff24e]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2,9*NQ2]< [L0xbefff250,L0xbefff252,L0xbefff254] /\
       [L0xbefff250,L0xbefff252,L0xbefff254]< [5*Q2,5*Q2,9*Q2] /\
       [9*NQ2,5*NQ2,5*NQ2]< [L0xbefff256,L0xbefff258,L0xbefff25a] /\
       [L0xbefff256,L0xbefff258,L0xbefff25a]< [9*Q2,5*Q2,5*Q2] /\
       [6*NQ2,6*NQ2,5*NQ2]< [L0xbefff25c,L0xbefff25e,L0xbefff260] /\
       [L0xbefff25c,L0xbefff25e,L0xbefff260]< [6*Q2,6*Q2,5*Q2] /\
       [5*NQ2,16*NQ2,16*NQ2]< [L0xbefff262,L0xbefff264,L0xbefff266] /\
       [L0xbefff262,L0xbefff264,L0xbefff266]< [5*Q2,16*Q2,16*Q2] /\
       [5*NQ2,5*NQ2,6*NQ2]< [L0xbefff268,L0xbefff26a,L0xbefff26c] /\
       [L0xbefff268,L0xbefff26a,L0xbefff26c]< [5*Q2,5*Q2,6*Q2] /\
       [6*NQ2,5*NQ2,5*NQ2]< [L0xbefff26e,L0xbefff270,L0xbefff272] /\
       [L0xbefff26e,L0xbefff270,L0xbefff272]< [6*Q2,5*Q2,5*Q2] /\
       [9*NQ2,9*NQ2,5*NQ2]< [L0xbefff274,L0xbefff276,L0xbefff278] /\
       [L0xbefff274,L0xbefff276,L0xbefff278]< [9*Q2,9*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff27a,L0xbefff27c,L0xbefff27e] /\
       [L0xbefff27a,L0xbefff27c,L0xbefff27e]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2]< [L0xbefff280,L0xbefff282] /\
       [L0xbefff280,L0xbefff282]< [5*Q2,5*Q2]
       prove with [algebra solver isl, precondition] && true;
assume [16*NQ2,16*NQ2,5*NQ2]< [L0xbefff244,L0xbefff246,L0xbefff248] /\
       [L0xbefff244,L0xbefff246,L0xbefff248]< [16*Q2,16*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff24a,L0xbefff24c,L0xbefff24e] /\
       [L0xbefff24a,L0xbefff24c,L0xbefff24e]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2,9*NQ2]< [L0xbefff250,L0xbefff252,L0xbefff254] /\
       [L0xbefff250,L0xbefff252,L0xbefff254]< [5*Q2,5*Q2,9*Q2] /\
       [9*NQ2,5*NQ2,5*NQ2]< [L0xbefff256,L0xbefff258,L0xbefff25a] /\
       [L0xbefff256,L0xbefff258,L0xbefff25a]< [9*Q2,5*Q2,5*Q2] /\
       [6*NQ2,6*NQ2,5*NQ2]< [L0xbefff25c,L0xbefff25e,L0xbefff260] /\
       [L0xbefff25c,L0xbefff25e,L0xbefff260]< [6*Q2,6*Q2,5*Q2] /\
       [5*NQ2,16*NQ2,16*NQ2]< [L0xbefff262,L0xbefff264,L0xbefff266] /\
       [L0xbefff262,L0xbefff264,L0xbefff266]< [5*Q2,16*Q2,16*Q2] /\
       [5*NQ2,5*NQ2,6*NQ2]< [L0xbefff268,L0xbefff26a,L0xbefff26c] /\
       [L0xbefff268,L0xbefff26a,L0xbefff26c]< [5*Q2,5*Q2,6*Q2] /\
       [6*NQ2,5*NQ2,5*NQ2]< [L0xbefff26e,L0xbefff270,L0xbefff272] /\
       [L0xbefff26e,L0xbefff270,L0xbefff272]< [6*Q2,5*Q2,5*Q2] /\
       [9*NQ2,9*NQ2,5*NQ2]< [L0xbefff274,L0xbefff276,L0xbefff278] /\
       [L0xbefff274,L0xbefff276,L0xbefff278]< [9*Q2,9*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff27a,L0xbefff27c,L0xbefff27e] /\
       [L0xbefff27a,L0xbefff27c,L0xbefff27e]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2]< [L0xbefff280,L0xbefff282] /\
       [L0xbefff280,L0xbefff282]< [5*Q2,5*Q2]
    && [16@16*NQ2,16@16*NQ2,5@16*NQ2]< [L0xbefff244,L0xbefff246,L0xbefff248] /\
       [L0xbefff244,L0xbefff246,L0xbefff248]< [16@16*Q2,16@16*Q2,5@16*Q2] /\
       [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff24a,L0xbefff24c,L0xbefff24e] /\
       [L0xbefff24a,L0xbefff24c,L0xbefff24e]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
       [5@16*NQ2,5@16*NQ2,9@16*NQ2]< [L0xbefff250,L0xbefff252,L0xbefff254] /\
       [L0xbefff250,L0xbefff252,L0xbefff254]< [5@16*Q2,5@16*Q2,9@16*Q2] /\
       [9@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff256,L0xbefff258,L0xbefff25a] /\
       [L0xbefff256,L0xbefff258,L0xbefff25a]< [9@16*Q2,5@16*Q2,5@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,5@16*NQ2]< [L0xbefff25c,L0xbefff25e,L0xbefff260] /\
       [L0xbefff25c,L0xbefff25e,L0xbefff260]< [6@16*Q2,6@16*Q2,5@16*Q2] /\
       [5@16*NQ2,16@16*NQ2,16@16*NQ2]< [L0xbefff262,L0xbefff264,L0xbefff266] /\
       [L0xbefff262,L0xbefff264,L0xbefff266]< [5@16*Q2,16@16*Q2,16@16*Q2] /\
       [5@16*NQ2,5@16*NQ2,6@16*NQ2]< [L0xbefff268,L0xbefff26a,L0xbefff26c] /\
       [L0xbefff268,L0xbefff26a,L0xbefff26c]< [5@16*Q2,5@16*Q2,6@16*Q2] /\
       [6@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff26e,L0xbefff270,L0xbefff272] /\
       [L0xbefff26e,L0xbefff270,L0xbefff272]< [6@16*Q2,5@16*Q2,5@16*Q2] /\
       [9@16*NQ2,9@16*NQ2,5@16*NQ2]< [L0xbefff274,L0xbefff276,L0xbefff278] /\
       [L0xbefff274,L0xbefff276,L0xbefff278]< [9@16*Q2,9@16*Q2,5@16*Q2] /\
       [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff27a,L0xbefff27c,L0xbefff27e] /\
       [L0xbefff27a,L0xbefff27c,L0xbefff27e]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
       [5@16*NQ2,5@16*NQ2]< [L0xbefff280,L0xbefff282] /\
       [L0xbefff280,L0xbefff282]< [5@16*Q2,5@16*Q2];
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod (poly Z2
           [poly X [L0xbefff244,L0xbefff246],poly X [L0xbefff248,L0xbefff24a],
            poly X [L0xbefff24c,L0xbefff24e],poly X [L0xbefff250,L0xbefff252],
            poly X [L0xbefff254,L0xbefff256],poly X [L0xbefff258,L0xbefff25a],
            poly X [L0xbefff25c,L0xbefff25e],poly X [L0xbefff260,L0xbefff262],
            poly X [L0xbefff264,L0xbefff266],poly X [L0xbefff268,L0xbefff26a],
            poly X [L0xbefff26c,L0xbefff26e],poly X [L0xbefff270,L0xbefff272],
            poly X [L0xbefff274,L0xbefff276],poly X [L0xbefff278,L0xbefff27a],
            poly X [L0xbefff27c,L0xbefff27e],poly X [L0xbefff280,L0xbefff282]])
          (2**4*F**2) [Q, Z2**16 - 1] /\
    [16*NQ2,16*NQ2,5*NQ2]< [L0xbefff244,L0xbefff246,L0xbefff248] /\
    [L0xbefff244,L0xbefff246,L0xbefff248]< [16*Q2,16*Q2,5*Q2] /\
    [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff24a,L0xbefff24c,L0xbefff24e] /\
    [L0xbefff24a,L0xbefff24c,L0xbefff24e]< [5*Q2,6*Q2,6*Q2] /\
    [5*NQ2,5*NQ2,9*NQ2]< [L0xbefff250,L0xbefff252,L0xbefff254] /\
    [L0xbefff250,L0xbefff252,L0xbefff254]< [5*Q2,5*Q2,9*Q2] /\
    [9*NQ2,5*NQ2,5*NQ2]< [L0xbefff256,L0xbefff258,L0xbefff25a] /\
    [L0xbefff256,L0xbefff258,L0xbefff25a]< [9*Q2,5*Q2,5*Q2] /\
    [6*NQ2,6*NQ2,5*NQ2]< [L0xbefff25c,L0xbefff25e,L0xbefff260] /\
    [L0xbefff25c,L0xbefff25e,L0xbefff260]< [6*Q2,6*Q2,5*Q2] /\
    [5*NQ2,16*NQ2,16*NQ2]< [L0xbefff262,L0xbefff264,L0xbefff266] /\
    [L0xbefff262,L0xbefff264,L0xbefff266]< [5*Q2,16*Q2,16*Q2] /\
    [5*NQ2,5*NQ2,6*NQ2]< [L0xbefff268,L0xbefff26a,L0xbefff26c] /\
    [L0xbefff268,L0xbefff26a,L0xbefff26c]< [5*Q2,5*Q2,6*Q2] /\
    [6*NQ2,5*NQ2,5*NQ2]< [L0xbefff26e,L0xbefff270,L0xbefff272] /\
    [L0xbefff26e,L0xbefff270,L0xbefff272]< [6*Q2,5*Q2,5*Q2] /\
    [9*NQ2,9*NQ2,5*NQ2]< [L0xbefff274,L0xbefff276,L0xbefff278] /\
    [L0xbefff274,L0xbefff276,L0xbefff278]< [9*Q2,9*Q2,5*Q2] /\
    [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff27a,L0xbefff27c,L0xbefff27e] /\
    [L0xbefff27a,L0xbefff27c,L0xbefff27e]< [5*Q2,6*Q2,6*Q2] /\
    [5*NQ2,5*NQ2]< [L0xbefff280,L0xbefff282] /\
    [L0xbefff280,L0xbefff282]< [5*Q2,5*Q2]
    prove with [precondition, all ghosts]
 && [16@16*NQ2,16@16*NQ2,5@16*NQ2]< [L0xbefff244,L0xbefff246,L0xbefff248] /\
    [L0xbefff244,L0xbefff246,L0xbefff248]< [16@16*Q2,16@16*Q2,5@16*Q2] /\
    [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff24a,L0xbefff24c,L0xbefff24e] /\
    [L0xbefff24a,L0xbefff24c,L0xbefff24e]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
    [5@16*NQ2,5@16*NQ2,9@16*NQ2]< [L0xbefff250,L0xbefff252,L0xbefff254] /\
    [L0xbefff250,L0xbefff252,L0xbefff254]< [5@16*Q2,5@16*Q2,9@16*Q2] /\
    [9@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff256,L0xbefff258,L0xbefff25a] /\
    [L0xbefff256,L0xbefff258,L0xbefff25a]< [9@16*Q2,5@16*Q2,5@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,5@16*NQ2]< [L0xbefff25c,L0xbefff25e,L0xbefff260] /\
    [L0xbefff25c,L0xbefff25e,L0xbefff260]< [6@16*Q2,6@16*Q2,5@16*Q2] /\
    [5@16*NQ2,16@16*NQ2,16@16*NQ2]< [L0xbefff262,L0xbefff264,L0xbefff266] /\
    [L0xbefff262,L0xbefff264,L0xbefff266]< [5@16*Q2,16@16*Q2,16@16*Q2] /\
    [5@16*NQ2,5@16*NQ2,6@16*NQ2]< [L0xbefff268,L0xbefff26a,L0xbefff26c] /\
    [L0xbefff268,L0xbefff26a,L0xbefff26c]< [5@16*Q2,5@16*Q2,6@16*Q2] /\
    [6@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff26e,L0xbefff270,L0xbefff272] /\
    [L0xbefff26e,L0xbefff270,L0xbefff272]< [6@16*Q2,5@16*Q2,5@16*Q2] /\
    [9@16*NQ2,9@16*NQ2,5@16*NQ2]< [L0xbefff274,L0xbefff276,L0xbefff278] /\
    [L0xbefff274,L0xbefff276,L0xbefff278]< [9@16*Q2,9@16*Q2,5@16*Q2] /\
    [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff27a,L0xbefff27c,L0xbefff27e] /\
    [L0xbefff27a,L0xbefff27c,L0xbefff27e]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
    [5@16*NQ2,5@16*NQ2]< [L0xbefff280,L0xbefff282] /\
    [L0xbefff280,L0xbefff282]< [5@16*Q2,5@16*Q2]
    prove with [precondition];

(* vmov	s23, r0                                    #! PC = 0x400cc4 *)
mov [s23_b, s23_t] [r0_b, r0_t];
(* ldr.w	r2, [r0, #32]                             #! EA = L0xbefff2a4; Value = 0xc09d98c5; PC = 0x400cc8 *)
mov [r2_b, r2_t] [L0xbefff2a4, L0xbefff2a6];
(* ldr.w	r3, [r0, #36]	; 0x24                      #! EA = L0xbefff2a8; Value = 0xbc018be3; PC = 0x400ccc *)
mov [r3_b, r3_t] [L0xbefff2a8, L0xbefff2aa];
(* ldr.w	r4, [r0, #40]	; 0x28                      #! EA = L0xbefff2ac; Value = 0xc7da9bbb; PC = 0x400cd0 *)
mov [r4_b, r4_t] [L0xbefff2ac, L0xbefff2ae];
(* ldr.w	r5, [r0, #44]	; 0x2c                      #! EA = L0xbefff2b0; Value = 0xc5e0925d; PC = 0x400cd4 *)
mov [r5_b, r5_t] [L0xbefff2b0, L0xbefff2b2];
(* ldr.w	r6, [r0, #48]	; 0x30                      #! EA = L0xbefff2b4; Value = 0xc29593bb; PC = 0x400cd8 *)
mov [r6_b, r6_t] [L0xbefff2b4, L0xbefff2b6];
(* ldr.w	r7, [r0, #52]	; 0x34                      #! EA = L0xbefff2b8; Value = 0xb80594f9; PC = 0x400cdc *)
mov [r7_b, r7_t] [L0xbefff2b8, L0xbefff2ba];
(* ldr.w	r8, [r0, #56]	; 0x38                      #! EA = L0xbefff2bc; Value = 0xbaf1909c; PC = 0x400ce0 *)
mov [r8_b, r8_t] [L0xbefff2bc, L0xbefff2be];
(* ldr.w	r9, [r0, #60]	; 0x3c                      #! EA = L0xbefff2c0; Value = 0xb71d8c80; PC = 0x400ce4 *)
mov [r9_b, r9_t] [L0xbefff2c0, L0xbefff2c2];
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400ce8 *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* sadd16	lr, r2, r3                               #! PC = 0x400cec *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* ssub16	r3, r2, r3                               #! PC = 0x400cf0 *)
sub r3_b r2_b r3_b;
sub r3_t r2_t r3_t;
(* sadd16	r11, r4, r5                              #! PC = 0x400cf4 *)
add r11_b r4_b r5_b;
add r11_t r4_t r5_t;
(* ssub16	r5, r4, r5                               #! PC = 0x400cf8 *)
sub r5_b r4_b r5_b;
sub r5_t r4_t r5_t;
(* sadd16	r2, r6, r7                               #! PC = 0x400cfc *)
add r2_b r6_b r7_b;
add r2_t r6_t r7_t;
(* ssub16	r7, r6, r7                               #! PC = 0x400d00 *)
sub r7_b r6_b r7_b;
sub r7_t r6_t r7_t;
(* sadd16	r4, r8, r9                               #! PC = 0x400d04 *)
add r4_b r8_b r9_b;
add r4_t r8_t r9_t;
(* ssub16	r9, r8, r9                               #! PC = 0x400d08 *)
sub r9_b r8_b r9_b;
sub r9_t r8_t r9_t;
(* sadd16	r8, lr, r11                              #! PC = 0x400d0c *)
add r8_b lr_b r11_b;
add r8_t lr_t r11_t;
(* ssub16	r11, lr, r11                             #! PC = 0x400d10 *)
sub r11_b lr_b r11_b;
sub r11_t lr_t r11_t;
(* sadd16	r6, r2, r4                               #! PC = 0x400d14 *)
add r6_b r2_b r4_b;
add r6_t r2_t r4_t;
(* ssub16	r4, r2, r4                               #! PC = 0x400d18 *)
sub r4_b r2_b r4_b;
sub r4_t r2_t r4_t;

ghost r5_b9@int16, r5_t9@int16:
      r5_b9 = r5_b /\ r5_t9 = r5_t
   && r5_b9 = r5_b /\ r5_t9 = r5_t;

(* vmov	r10, s10                                   #! PC = 0x400d1c *)
mov [r10_b, r10_t] [s10_b, s10_t]; mov r10 s10;
(* smulwb	lr, r10, r5                              #! PC = 0x400d20 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x400d24 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d28 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400d2c *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400d30 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_b9 *  1600) [Q] /\
       eqmod lr_t (r5_t9 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_b9 *  1600) [Q] /\
       eqmod lr_t (r5_t9 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x400d34 *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400d38 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r9_b15@int16, r9_t15@int16:
      r9_b15 = r9_b /\ r9_t15 = r9_t
   && r9_b15 = r9_b /\ r9_t15 = r9_t;

(* smulwb	lr, r10, r9                              #! PC = 0x400d3c *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400d40 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d44 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400d48 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400d4c *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b15 *  1600) [Q] /\
       eqmod lr_t (r9_t15 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b15 *  1600) [Q] /\
       eqmod lr_t (r9_t15 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x400d50 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400d54 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* sadd16	r2, r8, r6                               #! PC = 0x400d58 *)
add r2_b r8_b r6_b;
add r2_t r8_t r6_t;
(* ssub16	r6, r8, r6                               #! PC = 0x400d5c *)
sub r6_b r8_b r6_b;
sub r6_t r8_t r6_t;

ghost r7_b9@int16, r7_t9@int16:
      r7_b9 = r7_b /\ r7_t9 = r7_t
   && r7_b9 = r7_b /\ r7_t9 = r7_t;

(* vmov	r10, s12                                   #! PC = 0x400d60 *)
mov [r10_b, r10_t] [s12_b, s12_t]; mov r10 s12;
(* smulwb	lr, r10, r7                              #! PC = 0x400d64 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x400d68 *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d6c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400d70 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400d74 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_b9 *    40) [Q] /\
       eqmod lr_t (r7_t9 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_b9 *    40) [Q] /\
       eqmod lr_t (r7_t9 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x400d78 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400d7c *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r4_b9@int16, r4_t9@int16:
      r4_b9 = r4_b /\ r4_t9 = r4_t
   && r4_b9 = r4_b /\ r4_t9 = r4_t;

(* vmov	r10, s13                                   #! PC = 0x400d80 *)
mov [r10_b, r10_t] [s13_b, s13_t]; mov r10 s13;
(* smulwb	lr, r10, r4                              #! PC = 0x400d84 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x400d88 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d8c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400d90 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400d94 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_b9 *  1600) [Q] /\
       eqmod lr_t (r4_t9 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_b9 *  1600) [Q] /\
       eqmod lr_t (r4_t9 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* ssub16	r8, r11, lr                              #! PC = 0x400d98 *)
sub r8_b r11_b lr_b;
sub r8_t r11_t lr_t;
(* sadd16	r4, r11, lr                              #! PC = 0x400d9c *)
add r4_b r11_b lr_b;
add r4_t r11_t lr_t;

ghost r9_b16@int16, r9_t16@int16:
      r9_b16 = r9_b /\ r9_t16 = r9_t
   && r9_b16 = r9_b /\ r9_t16 = r9_t;

(* vmov	r10, s14                                   #! PC = 0x400da0 *)
mov [r10_b, r10_t] [s14_b, s14_t]; mov r10 s14;
(* smulwb	lr, r10, r9                              #! PC = 0x400da4 *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400da8 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400dac *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400db0 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400db4 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b16 *   749) [Q] /\
       eqmod lr_t (r9_t16 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b16 *   749) [Q] /\
       eqmod lr_t (r9_t16 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x400db8 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400dbc *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;

ghost r3_b3@int16, r3_t3@int16:
      r3_b3 = r3_b /\ r3_t3 = r3_t
   && r3_b3 = r3_b /\ r3_t3 = r3_t;

(* vmov	r11, s16                                   #! PC = 0x400dc0 *)
mov [r11_b, r11_t] [s16_b, s16_t]; mov r11 s16;
(* smulwb	lr, r11, r3                              #! PC = 0x400dc4 *)
vpc r3_bW@int32 r3_b; mulj tmp r11 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r11, r3                              #! PC = 0x400dc8 *)
vpc r3_tW@int32 r3_t; mulj tmp r11 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400dcc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400dd0 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x400dd4 *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov r3_b tmp_b; mov r3_t tmp_t;

assert eqmod r3_b (r3_b3 *  -848) [Q] /\
       eqmod r3_t (r3_t3 *  -848) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r3_b (r3_b3 *  -848) [Q] /\
       eqmod r3_t (r3_t3 *  -848) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r3_b, r3_t] /\ [r3_b, r3_t] <s[Q2, Q2];

ghost r4_b10@int16, r4_t10@int16:
      r4_b10 = r4_b /\ r4_t10 = r4_t
   && r4_b10 = r4_b /\ r4_t10 = r4_t;

(* vmov	r10, s17                                   #! PC = 0x400dd8 *)
mov [r10_b, r10_t] [s17_b, s17_t]; mov r10 s17;
(* vmov	r11, s18                                   #! PC = 0x400ddc *)
mov [r11_b, r11_t] [s18_b, s18_t]; mov r11 s18;
(* smulwb	lr, r10, r4                              #! PC = 0x400de0 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x400de4 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400de8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400dec *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x400df0 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov r4_b tmp_b; mov r4_t tmp_t;

assert eqmod r4_b (r4_b10 *    40) [Q] /\
       eqmod r4_t (r4_t10 *    40) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r4_b (r4_b10 *    40) [Q] /\
       eqmod r4_t (r4_t10 *    40) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r4_b, r4_t] /\ [r4_b, r4_t] <s[Q2, Q2];

ghost r5_b10@int16, r5_t10@int16:
      r5_b10 = r5_b /\ r5_t10 = r5_t
   && r5_b10 = r5_b /\ r5_t10 = r5_t;

(* smulwb	lr, r11, r5                              #! PC = 0x400df4 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x400df8 *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400dfc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400e00 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x400e04 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov r5_b tmp_b; mov r5_t tmp_t;

assert eqmod r5_b (r5_b10 *  -630) [Q] /\
       eqmod r5_t (r5_t10 *  -630) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r5_b (r5_b10 *  -630) [Q] /\
       eqmod r5_t (r5_t10 *  -630) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r5_b, r5_t] /\ [r5_b, r5_t] <s[Q2, Q2];

ghost r6_b3@int16, r6_t3@int16:
      r6_b3 = r6_b /\ r6_t3 = r6_t
   && r6_b3 = r6_b /\ r6_t3 = r6_t;

(* vmov	r10, s19                                   #! PC = 0x400e08 *)
mov [r10_b, r10_t] [s19_b, s19_t]; mov r10 s19;
(* vmov	r11, s20                                   #! PC = 0x400e0c *)
mov [r11_b, r11_t] [s20_b, s20_t]; mov r11 s20;
(* smulwb	lr, r10, r6                              #! PC = 0x400e10 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x400e14 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e18 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400e1c *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x400e20 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov r6_b tmp_b; mov r6_t tmp_t;

assert eqmod r6_b (r6_b3 *  1600) [Q] /\
       eqmod r6_t (r6_t3 *  1600) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r6_b (r6_b3 *  1600) [Q] /\
       eqmod r6_t (r6_t3 *  1600) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r6_b, r6_t] /\ [r6_b, r6_t] <s[Q2, Q2];

ghost r7_b10@int16, r7_t10@int16:
      r7_b10 = r7_b /\ r7_t10 = r7_t
   && r7_b10 = r7_b /\ r7_t10 = r7_t;

(* smulwb	lr, r11, r7                              #! PC = 0x400e24 *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x400e28 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e2c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400e30 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x400e34 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov r7_b tmp_b; mov r7_t tmp_t;

assert eqmod r7_b (r7_b10 *  1432) [Q] /\
       eqmod r7_t (r7_t10 *  1432) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r7_b (r7_b10 *  1432) [Q] /\
       eqmod r7_t (r7_t10 *  1432) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r7_b, r7_t] /\ [r7_b, r7_t] <s[Q2, Q2];

ghost r8_b3@int16, r8_t3@int16:
      r8_b3 = r8_b /\ r8_t3 = r8_t
   && r8_b3 = r8_b /\ r8_t3 = r8_t;

(* vmov	r10, s21                                   #! PC = 0x400e38 *)
mov [r10_b, r10_t] [s21_b, s21_t]; mov r10 s21;
(* vmov	r11, s22                                   #! PC = 0x400e3c *)
mov [r11_b, r11_t] [s22_b, s22_t]; mov r11 s22;
(* smulwb	lr, r10, r8                              #! PC = 0x400e40 *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x400e44 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e48 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400e4c *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x400e50 *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov r8_b tmp_b; mov r8_t tmp_t;

assert eqmod r8_b (r8_b3 *   749) [Q] /\
       eqmod r8_t (r8_t3 *   749) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r8_b (r8_b3 *   749) [Q] /\
       eqmod r8_t (r8_t3 *   749) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r8_b, r8_t] /\ [r8_b, r8_t] <s[Q2, Q2];

ghost r9_b17@int16, r9_t17@int16:
      r9_b17 = r9_b /\ r9_t17 = r9_t
   && r9_b17 = r9_b /\ r9_t17 = r9_t;

(* smulwb	lr, r11, r9                              #! PC = 0x400e54 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x400e58 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e5c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400e60 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x400e64 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov r9_b tmp_b; mov r9_t tmp_t;

assert eqmod r9_b (r9_b17 *   687) [Q] /\
       eqmod r9_t (r9_t17 *   687) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r9_b (r9_b17 *   687) [Q] /\
       eqmod r9_t (r9_t17 *   687) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r9_b, r9_t] /\ [r9_b, r9_t] <s[Q2, Q2];

(* vmov	s0, r2                                     #! PC = 0x400e68 *)
mov [s0_b, s0_t] [r2_b, r2_t];
(* vmov	s1, r3                                     #! PC = 0x400e6c *)
mov [s1_b, s1_t] [r3_b, r3_t];
(* vmov	s2, r4                                     #! PC = 0x400e70 *)
mov [s2_b, s2_t] [r4_b, r4_t];
(* vmov	s3, r5                                     #! PC = 0x400e74 *)
mov [s3_b, s3_t] [r5_b, r5_t];
(* vmov	s4, r6                                     #! PC = 0x400e78 *)
mov [s4_b, s4_t] [r6_b, r6_t];
(* vmov	s5, r7                                     #! PC = 0x400e7c *)
mov [s5_b, s5_t] [r7_b, r7_t];
(* vmov	s6, r8                                     #! PC = 0x400e80 *)
mov [s6_b, s6_t] [r8_b, r8_t];
(* vmov	s7, r9                                     #! PC = 0x400e84 *)
mov [s7_b, s7_t] [r9_b, r9_t];

assert [8*NQ2,8*NQ2]< [s0_b, s0_t] /\ [s0_b, s0_t]< [8*Q2,8*Q2] /\
       [1*NQ2,1*NQ2]< [s1_b, s1_t] /\ [s1_b, s1_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s2_b, s2_t] /\ [s2_b, s2_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s3_b, s3_t] /\ [s3_b, s3_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s4_b, s4_t] /\ [s4_b, s4_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s5_b, s5_t] /\ [s5_b, s5_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s6_b, s6_t] /\ [s6_b, s6_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s7_b, s7_t] /\ [s7_b, s7_t]< [1*Q2,1*Q2]
       prove with [algebra solver isl, precondition] && true;
assume [8*NQ2,8*NQ2]< [s0_b, s0_t] /\ [s0_b, s0_t]< [8*Q2,8*Q2] /\
       [1*NQ2,1*NQ2]< [s1_b, s1_t] /\ [s1_b, s1_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s2_b, s2_t] /\ [s2_b, s2_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s3_b, s3_t] /\ [s3_b, s3_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s4_b, s4_t] /\ [s4_b, s4_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s5_b, s5_t] /\ [s5_b, s5_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s6_b, s6_t] /\ [s6_b, s6_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s7_b, s7_t] /\ [s7_b, s7_t]< [1*Q2,1*Q2]
    && [8@16*NQ2,8@16*NQ2]<s[s0_b, s0_t] /\ [s0_b, s0_t]<s[8@16*Q2,8@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s1_b, s1_t] /\ [s1_b, s1_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s2_b, s2_t] /\ [s2_b, s2_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s3_b, s3_t] /\ [s3_b, s3_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s4_b, s4_t] /\ [s4_b, s4_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s5_b, s5_t] /\ [s5_b, s5_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s6_b, s6_t] /\ [s6_b, s6_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s7_b, s7_t] /\ [s7_b, s7_t]<s[1@16*Q2,1@16*Q2];

(* CUT 6 *)
ghost Z3@int16: X**2 =  -848*17** 29*Z3 && true;
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod (poly Z3 [poly X [s0_b, s0_t], poly X [s1_b, s1_t],
                    poly X [s2_b, s2_t], poly X [s3_b, s3_t],
                    poly X [s4_b, s4_t], poly X [s5_b, s5_t],
                    poly X [s6_b, s6_t], poly X [s7_b, s7_t]])
          (2**3*F**2) [Q, Z3**8 + 1] /\
    [8*NQ2,8*NQ2]< [s0_b, s0_t] /\ [s0_b, s0_t]< [8*Q2,8*Q2] /\
    [1*NQ2,1*NQ2]< [s1_b, s1_t] /\ [s1_b, s1_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s2_b, s2_t] /\ [s2_b, s2_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s3_b, s3_t] /\ [s3_b, s3_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s4_b, s4_t] /\ [s4_b, s4_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s5_b, s5_t] /\ [s5_b, s5_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s6_b, s6_t] /\ [s6_b, s6_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s7_b, s7_t] /\ [s7_b, s7_t]< [1*Q2,1*Q2]
    prove with [precondition]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [8@16*NQ2,8@16*NQ2]<s[s0_b, s0_t] /\ [s0_b, s0_t]<s[8@16*Q2,8@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s1_b, s1_t] /\ [s1_b, s1_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s2_b, s2_t] /\ [s2_b, s2_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s3_b, s3_t] /\ [s3_b, s3_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s4_b, s4_t] /\ [s4_b, s4_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s5_b, s5_t] /\ [s5_b, s5_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s6_b, s6_t] /\ [s6_b, s6_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s7_b, s7_t] /\ [s7_b, s7_t]<s[1@16*Q2,1@16*Q2]
    prove with [precondition];

(* vmov	r0, s23                                    #! PC = 0x400e88 *)
mov [r0_b, r0_t] [s23_b, s23_t];
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff284; Value = 0xac4c973a; PC = 0x400e8c *)
mov [r2_b, r2_t] [L0xbefff284, L0xbefff286];
(* ldr.w	r3, [r0, #4]                              #! EA = L0xbefff288; Value = 0xb21c9452; PC = 0x400e90 *)
mov [r3_b, r3_t] [L0xbefff288, L0xbefff28a];
(* ldr.w	r4, [r0, #8]                              #! EA = L0xbefff28c; Value = 0xae138bf6; PC = 0x400e94 *)
mov [r4_b, r4_t] [L0xbefff28c, L0xbefff28e];
(* ldr.w	r5, [r0, #12]                             #! EA = L0xbefff290; Value = 0xb43d8f3a; PC = 0x400e98 *)
mov [r5_b, r5_t] [L0xbefff290, L0xbefff292];
(* ldr.w	r6, [r0, #16]                             #! EA = L0xbefff294; Value = 0xb8e68880; PC = 0x400e9c *)
mov [r6_b, r6_t] [L0xbefff294, L0xbefff296];
(* ldr.w	r7, [r0, #20]                             #! EA = L0xbefff298; Value = 0xb0327da8; PC = 0x400ea0 *)
mov [r7_b, r7_t] [L0xbefff298, L0xbefff29a];
(* ldr.w	r8, [r0, #24]                             #! EA = L0xbefff29c; Value = 0xb7408437; PC = 0x400ea4 *)
mov [r8_b, r8_t] [L0xbefff29c, L0xbefff29e];
(* ldr.w	r9, [r0, #28]                             #! EA = L0xbefff2a0; Value = 0xbfc090b5; PC = 0x400ea8 *)
mov [r9_b, r9_t] [L0xbefff2a0, L0xbefff2a2];
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400eac *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* sadd16	lr, r2, r3                               #! PC = 0x400eb0 *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* ssub16	r3, r2, r3                               #! PC = 0x400eb4 *)
sub r3_b r2_b r3_b;
sub r3_t r2_t r3_t;
(* sadd16	r11, r4, r5                              #! PC = 0x400eb8 *)
add r11_b r4_b r5_b;
add r11_t r4_t r5_t;
(* ssub16	r5, r4, r5                               #! PC = 0x400ebc *)
sub r5_b r4_b r5_b;
sub r5_t r4_t r5_t;
(* sadd16	r2, r6, r7                               #! PC = 0x400ec0 *)
add r2_b r6_b r7_b;
add r2_t r6_t r7_t;
(* ssub16	r7, r6, r7                               #! PC = 0x400ec4 *)
sub r7_b r6_b r7_b;
sub r7_t r6_t r7_t;
(* sadd16	r4, r8, r9                               #! PC = 0x400ec8 *)
add r4_b r8_b r9_b;
add r4_t r8_t r9_t;
(* ssub16	r9, r8, r9                               #! PC = 0x400ecc *)
sub r9_b r8_b r9_b;
sub r9_t r8_t r9_t;
(* sadd16	r8, lr, r11                              #! PC = 0x400ed0 *)
add r8_b lr_b r11_b;
add r8_t lr_t r11_t;
(* ssub16	r11, lr, r11                             #! PC = 0x400ed4 *)
sub r11_b lr_b r11_b;
sub r11_t lr_t r11_t;
(* sadd16	r6, r2, r4                               #! PC = 0x400ed8 *)
add r6_b r2_b r4_b;
add r6_t r2_t r4_t;
(* ssub16	r4, r2, r4                               #! PC = 0x400edc *)
sub r4_b r2_b r4_b;
sub r4_t r2_t r4_t;

ghost r5_b11@int16, r5_t11@int16:
      r5_b11 = r5_b /\ r5_t11 = r5_t
   && r5_b11 = r5_b /\ r5_t11 = r5_t;

(* vmov	r10, s10                                   #! PC = 0x400ee0 *)
mov [r10_b, r10_t] [s10_b, s10_t]; mov r10 s10;
(* smulwb	lr, r10, r5                              #! PC = 0x400ee4 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x400ee8 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400eec *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400ef0 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400ef4 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_b11 *  1600) [Q] /\
       eqmod lr_t (r5_t11 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_b11 *  1600) [Q] /\
       eqmod lr_t (r5_t11 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x400ef8 *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400efc *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r9_b18@int16, r9_t18@int16:
      r9_b18 = r9_b /\ r9_t18 = r9_t
   && r9_b18 = r9_b /\ r9_t18 = r9_t;

(* smulwb	lr, r10, r9                              #! PC = 0x400f00 *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400f04 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f08 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400f0c *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400f10 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b18 *  1600) [Q] /\
       eqmod lr_t (r9_t18 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b18 *  1600) [Q] /\
       eqmod lr_t (r9_t18 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x400f14 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400f18 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* sadd16	r2, r8, r6                               #! PC = 0x400f1c *)
add r2_b r8_b r6_b;
add r2_t r8_t r6_t;
(* ssub16	r6, r8, r6                               #! PC = 0x400f20 *)
sub r6_b r8_b r6_b;
sub r6_t r8_t r6_t;

ghost r7_b11@int16, r7_t11@int16:
      r7_b11 = r7_b /\ r7_t11 = r7_t
   && r7_b11 = r7_b /\ r7_t11 = r7_t;

(* vmov	r10, s12                                   #! PC = 0x400f24 *)
mov [r10_b, r10_t] [s12_b, s12_t]; mov r10 s12;
(* smulwb	lr, r10, r7                              #! PC = 0x400f28 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x400f2c *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f30 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400f34 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400f38 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_b11 *    40) [Q] /\
       eqmod lr_t (r7_t11 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_b11 *    40) [Q] /\
       eqmod lr_t (r7_t11 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x400f3c *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400f40 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r4_b11@int16, r4_t11@int16:
      r4_b11 = r4_b /\ r4_t11 = r4_t
   && r4_b11 = r4_b /\ r4_t11 = r4_t;

(* vmov	r10, s13                                   #! PC = 0x400f44 *)
mov [r10_b, r10_t] [s13_b, s13_t]; mov r10 s13;
(* smulwb	lr, r10, r4                              #! PC = 0x400f48 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x400f4c *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f50 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400f54 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400f58 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_b11 *  1600) [Q] /\
       eqmod lr_t (r4_t11 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_b11 *  1600) [Q] /\
       eqmod lr_t (r4_t11 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* ssub16	r8, r11, lr                              #! PC = 0x400f5c *)
sub r8_b r11_b lr_b;
sub r8_t r11_t lr_t;
(* sadd16	r4, r11, lr                              #! PC = 0x400f60 *)
add r4_b r11_b lr_b;
add r4_t r11_t lr_t;

ghost r9_b19@int16, r9_t19@int16:
      r9_b19 = r9_b /\ r9_t19 = r9_t
   && r9_b19 = r9_b /\ r9_t19 = r9_t;

(* vmov	r10, s14                                   #! PC = 0x400f64 *)
mov [r10_b, r10_t] [s14_b, s14_t]; mov r10 s14;
(* smulwb	lr, r10, r9                              #! PC = 0x400f68 *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400f6c *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f70 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400f74 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400f78 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;
assert eqmod lr_b (r9_b19 *   749) [Q] /\
       eqmod lr_t (r9_t19 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b19 *   749) [Q] /\
       eqmod lr_t (r9_t19 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x400f7c *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400f80 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* vmov	r0, s23                                    #! PC = 0x400f84 *)
mov [r0_b, r0_t] [s23_b, s23_t];
(* vmov	r11, s1                                    #! PC = 0x400f88 *)
mov [r11_b, r11_t] [s1_b, s1_t];
(* uadd16	lr, r3, r11                              #! PC = 0x400f8c *)
add lr_b r3_b r11_b;
add lr_t r3_t r11_t;
(* usub16	r3, r3, r11                              #! PC = 0x400f90 *)
sub r3_b r3_b r11_b;
sub r3_t r3_t r11_t;
(* str.w	lr, [r0, #4]                              #! EA = L0xbefff288; PC = 0x400f94 *)
mov [L0xbefff288, L0xbefff28a] [lr_b, lr_t];
(* str.w	r3, [r0, #36]	; 0x24                      #! EA = L0xbefff2a8; PC = 0x400f98 *)
mov [L0xbefff2a8, L0xbefff2aa] [r3_b, r3_t];
(* vmov	r11, s3                                    #! PC = 0x400f9c *)
mov [r11_b, r11_t] [s3_b, s3_t];
(* uadd16	lr, r5, r11                              #! PC = 0x400fa0 *)
add lr_b r5_b r11_b;
add lr_t r5_t r11_t;
(* usub16	r5, r5, r11                              #! PC = 0x400fa4 *)
sub r5_b r5_b r11_b;
sub r5_t r5_t r11_t;
(* str.w	lr, [r0, #12]                             #! EA = L0xbefff290; PC = 0x400fa8 *)
mov [L0xbefff290, L0xbefff292] [lr_b, lr_t];
(* str.w	r5, [r0, #44]	; 0x2c                      #! EA = L0xbefff2b0; PC = 0x400fac *)
mov [L0xbefff2b0, L0xbefff2b2] [r5_b, r5_t];
(* vmov	r11, s5                                    #! PC = 0x400fb0 *)
mov [r11_b, r11_t] [s5_b, s5_t];
(* uadd16	lr, r7, r11                              #! PC = 0x400fb4 *)
add lr_b r7_b r11_b;
add lr_t r7_t r11_t;
(* usub16	r7, r7, r11                              #! PC = 0x400fb8 *)
sub r7_b r7_b r11_b;
sub r7_t r7_t r11_t;
(* str.w	lr, [r0, #20]                             #! EA = L0xbefff298; PC = 0x400fbc *)
mov [L0xbefff298, L0xbefff29a] [lr_b, lr_t];
(* str.w	r7, [r0, #52]	; 0x34                      #! EA = L0xbefff2b8; PC = 0x400fc0 *)
mov [L0xbefff2b8, L0xbefff2ba] [r7_b, r7_t];
(* vmov	r11, s7                                    #! PC = 0x400fc4 *)
mov [r11_b, r11_t] [s7_b, s7_t];
(* uadd16	lr, r9, r11                              #! PC = 0x400fc8 *)
add lr_b r9_b r11_b;
add lr_t r9_t r11_t;
(* usub16	r9, r9, r11                              #! PC = 0x400fcc *)
sub r9_b r9_b r11_b;
sub r9_t r9_t r11_t;
(* str.w	lr, [r0, #28]                             #! EA = L0xbefff2a0; PC = 0x400fd0 *)
mov [L0xbefff2a0, L0xbefff2a2] [lr_b, lr_t];
(* str.w	r9, [r0, #60]	; 0x3c                      #! EA = L0xbefff2c0; PC = 0x400fd4 *)
mov [L0xbefff2c0, L0xbefff2c2] [r9_b, r9_t];
(* vmov	r5, s2                                     #! PC = 0x400fd8 *)
mov [r5_b, r5_t] [s2_b, s2_t];
(* uadd16	lr, r4, r5                               #! PC = 0x400fdc *)
add lr_b r4_b r5_b;
add lr_t r4_t r5_t;
(* usub16	r11, r4, r5                              #! PC = 0x400fe0 *)
sub r11_b r4_b r5_b;
sub r11_t r4_t r5_t;
(* str.w	lr, [r0, #8]                              #! EA = L0xbefff28c; PC = 0x400fe4 *)
mov [L0xbefff28c, L0xbefff28e] [lr_b, lr_t];
(* str.w	r11, [r0, #40]	; 0x28                     #! EA = L0xbefff2ac; PC = 0x400fe8 *)
mov [L0xbefff2ac, L0xbefff2ae] [r11_b, r11_t];
(* vmov	r7, s4                                     #! PC = 0x400fec *)
mov [r7_b, r7_t] [s4_b, s4_t];
(* uadd16	lr, r6, r7                               #! PC = 0x400ff0 *)
add lr_b r6_b r7_b;
add lr_t r6_t r7_t;
(* usub16	r11, r6, r7                              #! PC = 0x400ff4 *)
sub r11_b r6_b r7_b;
sub r11_t r6_t r7_t;
(* str.w	lr, [r0, #16]                             #! EA = L0xbefff294; PC = 0x400ff8 *)
mov [L0xbefff294, L0xbefff296] [lr_b, lr_t];
(* str.w	r11, [r0, #48]	; 0x30                     #! EA = L0xbefff2b4; PC = 0x400ffc *)
mov [L0xbefff2b4, L0xbefff2b6] [r11_b, r11_t];
(* vmov	r9, s6                                     #! PC = 0x401000 *)
mov [r9_b, r9_t] [s6_b, s6_t];
(* uadd16	lr, r8, r9                               #! PC = 0x401004 *)
add lr_b r8_b r9_b;
add lr_t r8_t r9_t;
(* usub16	r11, r8, r9                              #! PC = 0x401008 *)
sub r11_b r8_b r9_b;
sub r11_t r8_t r9_t;
(* str.w	lr, [r0, #24]                             #! EA = L0xbefff29c; PC = 0x40100c *)
mov [L0xbefff29c, L0xbefff29e] [lr_b, lr_t];
(* str.w	r11, [r0, #56]	; 0x38                     #! EA = L0xbefff2bc; PC = 0x401010 *)
mov [L0xbefff2bc, L0xbefff2be] [r11_b, r11_t];
(* vmov	r3, s0                                     #! PC = 0x401014 *)
mov [r3_b, r3_t] [s0_b, s0_t];
(* uadd16	lr, r2, r3                               #! PC = 0x401018 *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* usub16	r11, r2, r3                              #! PC = 0x40101c *)
sub r11_b r2_b r3_b;
sub r11_t r2_t r3_t;
(* str.w	r11, [r0, #32]                            #! EA = L0xbefff2a4; PC = 0x401020 *)
mov [L0xbefff2a4, L0xbefff2a6] [r11_b, r11_t];
(* str.w	lr, [r0], #64                             #! EA = L0xbefff284; PC = 0x401024 *)
mov [L0xbefff284, L0xbefff286] [lr_b, lr_t];
(* vmov	lr, s8                                     #! PC = 0x401028 *)
mov [lr_b, lr_t] [s8_b, s8_t]; mov lr s8;
(* cmp.w	r0, lr                                    #! PC = 0x40102c *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x400cc4 <invntt_fast+24>                #! PC = 0x401030 *)
#bne.w	0x400cc4 <invntt_fast+24>                #! 0x401030 = 0x401030;

(* CUT 7 *)
assert [16*NQ2,16*NQ2,5*NQ2]< [L0xbefff284,L0xbefff286,L0xbefff288] /\
       [L0xbefff284,L0xbefff286,L0xbefff288]< [16*Q2,16*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff28a,L0xbefff28c,L0xbefff28e] /\
       [L0xbefff28a,L0xbefff28c,L0xbefff28e]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2,9*NQ2]< [L0xbefff290,L0xbefff292,L0xbefff294] /\
       [L0xbefff290,L0xbefff292,L0xbefff294]< [5*Q2,5*Q2,9*Q2] /\
       [9*NQ2,5*NQ2,5*NQ2]< [L0xbefff296,L0xbefff298,L0xbefff29a] /\
       [L0xbefff296,L0xbefff298,L0xbefff29a]< [9*Q2,5*Q2,5*Q2] /\
       [6*NQ2,6*NQ2,5*NQ2]< [L0xbefff29c,L0xbefff29e,L0xbefff2a0] /\
       [L0xbefff29c,L0xbefff29e,L0xbefff2a0]< [6*Q2,6*Q2,5*Q2] /\
       [5*NQ2,16*NQ2,16*NQ2]< [L0xbefff2a2,L0xbefff2a4,L0xbefff2a6] /\
       [L0xbefff2a2,L0xbefff2a4,L0xbefff2a6]< [5*Q2,16*Q2,16*Q2] /\
       [5*NQ2,5*NQ2,6*NQ2]< [L0xbefff2a8,L0xbefff2aa,L0xbefff2ac] /\
       [L0xbefff2a8,L0xbefff2aa,L0xbefff2ac]< [5*Q2,5*Q2,6*Q2] /\
       [6*NQ2,5*NQ2,5*NQ2]< [L0xbefff2ae,L0xbefff2b0,L0xbefff2b2] /\
       [L0xbefff2ae,L0xbefff2b0,L0xbefff2b2]< [6*Q2,5*Q2,5*Q2] /\
       [9*NQ2,9*NQ2,5*NQ2]< [L0xbefff2b4,L0xbefff2b6,L0xbefff2b8] /\
       [L0xbefff2b4,L0xbefff2b6,L0xbefff2b8]< [9*Q2,9*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff2ba,L0xbefff2bc,L0xbefff2be] /\
       [L0xbefff2ba,L0xbefff2bc,L0xbefff2be]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2]< [L0xbefff2c0,L0xbefff2c2] /\
       [L0xbefff2c0,L0xbefff2c2]< [5*Q2,5*Q2]
       prove with [algebra solver isl, precondition] && true;
assume [16*NQ2,16*NQ2,5*NQ2]< [L0xbefff284,L0xbefff286,L0xbefff288] /\
       [L0xbefff284,L0xbefff286,L0xbefff288]< [16*Q2,16*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff28a,L0xbefff28c,L0xbefff28e] /\
       [L0xbefff28a,L0xbefff28c,L0xbefff28e]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2,9*NQ2]< [L0xbefff290,L0xbefff292,L0xbefff294] /\
       [L0xbefff290,L0xbefff292,L0xbefff294]< [5*Q2,5*Q2,9*Q2] /\
       [9*NQ2,5*NQ2,5*NQ2]< [L0xbefff296,L0xbefff298,L0xbefff29a] /\
       [L0xbefff296,L0xbefff298,L0xbefff29a]< [9*Q2,5*Q2,5*Q2] /\
       [6*NQ2,6*NQ2,5*NQ2]< [L0xbefff29c,L0xbefff29e,L0xbefff2a0] /\
       [L0xbefff29c,L0xbefff29e,L0xbefff2a0]< [6*Q2,6*Q2,5*Q2] /\
       [5*NQ2,16*NQ2,16*NQ2]< [L0xbefff2a2,L0xbefff2a4,L0xbefff2a6] /\
       [L0xbefff2a2,L0xbefff2a4,L0xbefff2a6]< [5*Q2,16*Q2,16*Q2] /\
       [5*NQ2,5*NQ2,6*NQ2]< [L0xbefff2a8,L0xbefff2aa,L0xbefff2ac] /\
       [L0xbefff2a8,L0xbefff2aa,L0xbefff2ac]< [5*Q2,5*Q2,6*Q2] /\
       [6*NQ2,5*NQ2,5*NQ2]< [L0xbefff2ae,L0xbefff2b0,L0xbefff2b2] /\
       [L0xbefff2ae,L0xbefff2b0,L0xbefff2b2]< [6*Q2,5*Q2,5*Q2] /\
       [9*NQ2,9*NQ2,5*NQ2]< [L0xbefff2b4,L0xbefff2b6,L0xbefff2b8] /\
       [L0xbefff2b4,L0xbefff2b6,L0xbefff2b8]< [9*Q2,9*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff2ba,L0xbefff2bc,L0xbefff2be] /\
       [L0xbefff2ba,L0xbefff2bc,L0xbefff2be]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2]< [L0xbefff2c0,L0xbefff2c2] /\
       [L0xbefff2c0,L0xbefff2c2]< [5*Q2,5*Q2]
    && [16@16*NQ2,16@16*NQ2,5@16*NQ2]< [L0xbefff284,L0xbefff286,L0xbefff288] /\
       [L0xbefff284,L0xbefff286,L0xbefff288]< [16@16*Q2,16@16*Q2,5@16*Q2] /\
       [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff28a,L0xbefff28c,L0xbefff28e] /\
       [L0xbefff28a,L0xbefff28c,L0xbefff28e]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
       [5@16*NQ2,5@16*NQ2,9@16*NQ2]< [L0xbefff290,L0xbefff292,L0xbefff294] /\
       [L0xbefff290,L0xbefff292,L0xbefff294]< [5@16*Q2,5@16*Q2,9@16*Q2] /\
       [9@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff296,L0xbefff298,L0xbefff29a] /\
       [L0xbefff296,L0xbefff298,L0xbefff29a]< [9@16*Q2,5@16*Q2,5@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,5@16*NQ2]< [L0xbefff29c,L0xbefff29e,L0xbefff2a0] /\
       [L0xbefff29c,L0xbefff29e,L0xbefff2a0]< [6@16*Q2,6@16*Q2,5@16*Q2] /\
       [5@16*NQ2,16@16*NQ2,16@16*NQ2]< [L0xbefff2a2,L0xbefff2a4,L0xbefff2a6] /\
       [L0xbefff2a2,L0xbefff2a4,L0xbefff2a6]< [5@16*Q2,16@16*Q2,16@16*Q2] /\
       [5@16*NQ2,5@16*NQ2,6@16*NQ2]< [L0xbefff2a8,L0xbefff2aa,L0xbefff2ac] /\
       [L0xbefff2a8,L0xbefff2aa,L0xbefff2ac]< [5@16*Q2,5@16*Q2,6@16*Q2] /\
       [6@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff2ae,L0xbefff2b0,L0xbefff2b2] /\
       [L0xbefff2ae,L0xbefff2b0,L0xbefff2b2]< [6@16*Q2,5@16*Q2,5@16*Q2] /\
       [9@16*NQ2,9@16*NQ2,5@16*NQ2]< [L0xbefff2b4,L0xbefff2b6,L0xbefff2b8] /\
       [L0xbefff2b4,L0xbefff2b6,L0xbefff2b8]< [9@16*Q2,9@16*Q2,5@16*Q2] /\
       [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff2ba,L0xbefff2bc,L0xbefff2be] /\
       [L0xbefff2ba,L0xbefff2bc,L0xbefff2be]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
       [5@16*NQ2,5@16*NQ2]< [L0xbefff2c0,L0xbefff2c2] /\
       [L0xbefff2c0,L0xbefff2c2]< [5@16*Q2,5@16*Q2];
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod (poly Z3
           [poly X [L0xbefff284,L0xbefff286],poly X [L0xbefff288,L0xbefff28a],
            poly X [L0xbefff28c,L0xbefff28e],poly X [L0xbefff290,L0xbefff292],
            poly X [L0xbefff294,L0xbefff296],poly X [L0xbefff298,L0xbefff29a],
            poly X [L0xbefff29c,L0xbefff29e],poly X [L0xbefff2a0,L0xbefff2a2],
            poly X [L0xbefff2a4,L0xbefff2a6],poly X [L0xbefff2a8,L0xbefff2aa],
            poly X [L0xbefff2ac,L0xbefff2ae],poly X [L0xbefff2b0,L0xbefff2b2],
            poly X [L0xbefff2b4,L0xbefff2b6],poly X [L0xbefff2b8,L0xbefff2ba],
            poly X [L0xbefff2bc,L0xbefff2be],poly X [L0xbefff2c0,L0xbefff2c2]])
          (2**4*F**2) [Q, Z3**16 - 1] /\
    [16*NQ2,16*NQ2,5*NQ2]< [L0xbefff284,L0xbefff286,L0xbefff288] /\
    [L0xbefff284,L0xbefff286,L0xbefff288]< [16*Q2,16*Q2,5*Q2] /\
    [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff28a,L0xbefff28c,L0xbefff28e] /\
    [L0xbefff28a,L0xbefff28c,L0xbefff28e]< [5*Q2,6*Q2,6*Q2] /\
    [5*NQ2,5*NQ2,9*NQ2]< [L0xbefff290,L0xbefff292,L0xbefff294] /\
    [L0xbefff290,L0xbefff292,L0xbefff294]< [5*Q2,5*Q2,9*Q2] /\
    [9*NQ2,5*NQ2,5*NQ2]< [L0xbefff296,L0xbefff298,L0xbefff29a] /\
    [L0xbefff296,L0xbefff298,L0xbefff29a]< [9*Q2,5*Q2,5*Q2] /\
    [6*NQ2,6*NQ2,5*NQ2]< [L0xbefff29c,L0xbefff29e,L0xbefff2a0] /\
    [L0xbefff29c,L0xbefff29e,L0xbefff2a0]< [6*Q2,6*Q2,5*Q2] /\
    [5*NQ2,16*NQ2,16*NQ2]< [L0xbefff2a2,L0xbefff2a4,L0xbefff2a6] /\
    [L0xbefff2a2,L0xbefff2a4,L0xbefff2a6]< [5*Q2,16*Q2,16*Q2] /\
    [5*NQ2,5*NQ2,6*NQ2]< [L0xbefff2a8,L0xbefff2aa,L0xbefff2ac] /\
    [L0xbefff2a8,L0xbefff2aa,L0xbefff2ac]< [5*Q2,5*Q2,6*Q2] /\
    [6*NQ2,5*NQ2,5*NQ2]< [L0xbefff2ae,L0xbefff2b0,L0xbefff2b2] /\
    [L0xbefff2ae,L0xbefff2b0,L0xbefff2b2]< [6*Q2,5*Q2,5*Q2] /\
    [9*NQ2,9*NQ2,5*NQ2]< [L0xbefff2b4,L0xbefff2b6,L0xbefff2b8] /\
    [L0xbefff2b4,L0xbefff2b6,L0xbefff2b8]< [9*Q2,9*Q2,5*Q2] /\
    [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff2ba,L0xbefff2bc,L0xbefff2be] /\
    [L0xbefff2ba,L0xbefff2bc,L0xbefff2be]< [5*Q2,6*Q2,6*Q2] /\
    [5*NQ2,5*NQ2]< [L0xbefff2c0,L0xbefff2c2] /\
    [L0xbefff2c0,L0xbefff2c2]< [5*Q2,5*Q2]
    prove with [precondition, all ghosts]
 && [16@16*NQ2,16@16*NQ2,5@16*NQ2]< [L0xbefff284,L0xbefff286,L0xbefff288] /\
    [L0xbefff284,L0xbefff286,L0xbefff288]< [16@16*Q2,16@16*Q2,5@16*Q2] /\
    [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff28a,L0xbefff28c,L0xbefff28e] /\
    [L0xbefff28a,L0xbefff28c,L0xbefff28e]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
    [5@16*NQ2,5@16*NQ2,9@16*NQ2]< [L0xbefff290,L0xbefff292,L0xbefff294] /\
    [L0xbefff290,L0xbefff292,L0xbefff294]< [5@16*Q2,5@16*Q2,9@16*Q2] /\
    [9@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff296,L0xbefff298,L0xbefff29a] /\
    [L0xbefff296,L0xbefff298,L0xbefff29a]< [9@16*Q2,5@16*Q2,5@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,5@16*NQ2]< [L0xbefff29c,L0xbefff29e,L0xbefff2a0] /\
    [L0xbefff29c,L0xbefff29e,L0xbefff2a0]< [6@16*Q2,6@16*Q2,5@16*Q2] /\
    [5@16*NQ2,16@16*NQ2,16@16*NQ2]< [L0xbefff2a2,L0xbefff2a4,L0xbefff2a6] /\
    [L0xbefff2a2,L0xbefff2a4,L0xbefff2a6]< [5@16*Q2,16@16*Q2,16@16*Q2] /\
    [5@16*NQ2,5@16*NQ2,6@16*NQ2]< [L0xbefff2a8,L0xbefff2aa,L0xbefff2ac] /\
    [L0xbefff2a8,L0xbefff2aa,L0xbefff2ac]< [5@16*Q2,5@16*Q2,6@16*Q2] /\
    [6@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff2ae,L0xbefff2b0,L0xbefff2b2] /\
    [L0xbefff2ae,L0xbefff2b0,L0xbefff2b2]< [6@16*Q2,5@16*Q2,5@16*Q2] /\
    [9@16*NQ2,9@16*NQ2,5@16*NQ2]< [L0xbefff2b4,L0xbefff2b6,L0xbefff2b8] /\
    [L0xbefff2b4,L0xbefff2b6,L0xbefff2b8]< [9@16*Q2,9@16*Q2,5@16*Q2] /\
    [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff2ba,L0xbefff2bc,L0xbefff2be] /\
    [L0xbefff2ba,L0xbefff2bc,L0xbefff2be]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
    [5@16*NQ2,5@16*NQ2]< [L0xbefff2c0,L0xbefff2c2] /\
    [L0xbefff2c0,L0xbefff2c2]< [5@16*Q2,5@16*Q2]
    prove with [precondition];

(* vmov	s23, r0                                    #! PC = 0x400cc4 *)
mov [s23_b, s23_t] [r0_b, r0_t];
(* ldr.w	r2, [r0, #32]                             #! EA = L0xbefff2e4; Value = 0xb5c69308; PC = 0x400cc8 *)
mov [r2_b, r2_t] [L0xbefff2e4, L0xbefff2e6];
(* ldr.w	r3, [r0, #36]	; 0x24                      #! EA = L0xbefff2e8; Value = 0xc2409dc8; PC = 0x400ccc *)
mov [r3_b, r3_t] [L0xbefff2e8, L0xbefff2ea];
(* ldr.w	r4, [r0, #40]	; 0x28                      #! EA = L0xbefff2ec; Value = 0xb73a95e4; PC = 0x400cd0 *)
mov [r4_b, r4_t] [L0xbefff2ec, L0xbefff2ee];
(* ldr.w	r5, [r0, #44]	; 0x2c                      #! EA = L0xbefff2f0; Value = 0xad349b18; PC = 0x400cd4 *)
mov [r5_b, r5_t] [L0xbefff2f0, L0xbefff2f2];
(* ldr.w	r6, [r0, #48]	; 0x30                      #! EA = L0xbefff2f4; Value = 0xc4a09585; PC = 0x400cd8 *)
mov [r6_b, r6_t] [L0xbefff2f4, L0xbefff2f6];
(* ldr.w	r7, [r0, #52]	; 0x34                      #! EA = L0xbefff2f8; Value = 0xbd749383; PC = 0x400cdc *)
mov [r7_b, r7_t] [L0xbefff2f8, L0xbefff2fa];
(* ldr.w	r8, [r0, #56]	; 0x38                      #! EA = L0xbefff2fc; Value = 0xb79a8f32; PC = 0x400ce0 *)
mov [r8_b, r8_t] [L0xbefff2fc, L0xbefff2fe];
(* ldr.w	r9, [r0, #60]	; 0x3c                      #! EA = L0xbefff300; Value = 0xb5468a22; PC = 0x400ce4 *)
mov [r9_b, r9_t] [L0xbefff300, L0xbefff302];
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400ce8 *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* sadd16	lr, r2, r3                               #! PC = 0x400cec *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* ssub16	r3, r2, r3                               #! PC = 0x400cf0 *)
sub r3_b r2_b r3_b;
sub r3_t r2_t r3_t;
(* sadd16	r11, r4, r5                              #! PC = 0x400cf4 *)
add r11_b r4_b r5_b;
add r11_t r4_t r5_t;
(* ssub16	r5, r4, r5                               #! PC = 0x400cf8 *)
sub r5_b r4_b r5_b;
sub r5_t r4_t r5_t;
(* sadd16	r2, r6, r7                               #! PC = 0x400cfc *)
add r2_b r6_b r7_b;
add r2_t r6_t r7_t;
(* ssub16	r7, r6, r7                               #! PC = 0x400d00 *)
sub r7_b r6_b r7_b;
sub r7_t r6_t r7_t;
(* sadd16	r4, r8, r9                               #! PC = 0x400d04 *)
add r4_b r8_b r9_b;
add r4_t r8_t r9_t;
(* ssub16	r9, r8, r9                               #! PC = 0x400d08 *)
sub r9_b r8_b r9_b;
sub r9_t r8_t r9_t;
(* sadd16	r8, lr, r11                              #! PC = 0x400d0c *)
add r8_b lr_b r11_b;
add r8_t lr_t r11_t;
(* ssub16	r11, lr, r11                             #! PC = 0x400d10 *)
sub r11_b lr_b r11_b;
sub r11_t lr_t r11_t;
(* sadd16	r6, r2, r4                               #! PC = 0x400d14 *)
add r6_b r2_b r4_b;
add r6_t r2_t r4_t;
(* ssub16	r4, r2, r4                               #! PC = 0x400d18 *)
sub r4_b r2_b r4_b;
sub r4_t r2_t r4_t;

ghost r5_b12@int16, r5_t12@int16:
      r5_b12 = r5_b /\ r5_t12 = r5_t
   && r5_b12 = r5_b /\ r5_t12 = r5_t;

(* vmov	r10, s10                                   #! PC = 0x400d1c *)
mov [r10_b, r10_t] [s10_b, s10_t]; mov r10 s10;
(* smulwb	lr, r10, r5                              #! PC = 0x400d20 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x400d24 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d28 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400d2c *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400d30 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_b12 *  1600) [Q] /\
       eqmod lr_t (r5_t12 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_b12 *  1600) [Q] /\
       eqmod lr_t (r5_t12 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x400d34 *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400d38 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r9_b20@int16, r9_t20@int16:
      r9_b20 = r9_b /\ r9_t20 = r9_t
   && r9_b20 = r9_b /\ r9_t20 = r9_t;

(* smulwb	lr, r10, r9                              #! PC = 0x400d3c *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400d40 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d44 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400d48 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400d4c *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b20 *  1600) [Q] /\
       eqmod lr_t (r9_t20 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b20 *  1600) [Q] /\
       eqmod lr_t (r9_t20 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x400d50 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400d54 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* sadd16	r2, r8, r6                               #! PC = 0x400d58 *)
add r2_b r8_b r6_b;
add r2_t r8_t r6_t;
(* ssub16	r6, r8, r6                               #! PC = 0x400d5c *)
sub r6_b r8_b r6_b;
sub r6_t r8_t r6_t;

ghost r7_b12@int16, r7_t12@int16:
      r7_b12 = r7_b /\ r7_t12 = r7_t
   && r7_b12 = r7_b /\ r7_t12 = r7_t;

(* vmov	r10, s12                                   #! PC = 0x400d60 *)
mov [r10_b, r10_t] [s12_b, s12_t]; mov r10 s12;
(* smulwb	lr, r10, r7                              #! PC = 0x400d64 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x400d68 *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d6c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400d70 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400d74 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_b12 *    40) [Q] /\
       eqmod lr_t (r7_t12 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_b12 *    40) [Q] /\
       eqmod lr_t (r7_t12 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x400d78 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400d7c *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r4_b12@int16, r4_t12@int16:
      r4_b12 = r4_b /\ r4_t12 = r4_t
   && r4_b12 = r4_b /\ r4_t12 = r4_t;

(* vmov	r10, s13                                   #! PC = 0x400d80 *)
mov [r10_b, r10_t] [s13_b, s13_t]; mov r10 s13;
(* smulwb	lr, r10, r4                              #! PC = 0x400d84 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x400d88 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d8c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400d90 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400d94 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_b12 *  1600) [Q] /\
       eqmod lr_t (r4_t12 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_b12 *  1600) [Q] /\
       eqmod lr_t (r4_t12 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* ssub16	r8, r11, lr                              #! PC = 0x400d98 *)
sub r8_b r11_b lr_b;
sub r8_t r11_t lr_t;
(* sadd16	r4, r11, lr                              #! PC = 0x400d9c *)
add r4_b r11_b lr_b;
add r4_t r11_t lr_t;

ghost r9_b21@int16, r9_t21@int16:
      r9_b21 = r9_b /\ r9_t21 = r9_t
   && r9_b21 = r9_b /\ r9_t21 = r9_t;

(* vmov	r10, s14                                   #! PC = 0x400da0 *)
mov [r10_b, r10_t] [s14_b, s14_t]; mov r10 s14;
(* smulwb	lr, r10, r9                              #! PC = 0x400da4 *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400da8 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400dac *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400db0 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400db4 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b21 *   749) [Q] /\
       eqmod lr_t (r9_t21 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b21 *   749) [Q] /\
       eqmod lr_t (r9_t21 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x400db8 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400dbc *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;

ghost r3_b4@int16, r3_t4@int16:
      r3_b4 = r3_b /\ r3_t4 = r3_t
   && r3_b4 = r3_b /\ r3_t4 = r3_t;

(* vmov	r11, s16                                   #! PC = 0x400dc0 *)
mov [r11_b, r11_t] [s16_b, s16_t]; mov r11 s16;
(* smulwb	lr, r11, r3                              #! PC = 0x400dc4 *)
vpc r3_bW@int32 r3_b; mulj tmp r11 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r11, r3                              #! PC = 0x400dc8 *)
vpc r3_tW@int32 r3_t; mulj tmp r11 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400dcc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400dd0 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x400dd4 *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov r3_b tmp_b; mov r3_t tmp_t;

assert eqmod r3_b (r3_b4 *  -848) [Q] /\
       eqmod r3_t (r3_t4 *  -848) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r3_b (r3_b4 *  -848) [Q] /\
       eqmod r3_t (r3_t4 *  -848) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r3_b, r3_t] /\ [r3_b, r3_t] <s[Q2, Q2];

ghost r4_b13@int16, r4_t13@int16:
      r4_b13 = r4_b /\ r4_t13 = r4_t
   && r4_b13 = r4_b /\ r4_t13 = r4_t;

(* vmov	r10, s17                                   #! PC = 0x400dd8 *)
mov [r10_b, r10_t] [s17_b, s17_t]; mov r10 s17;
(* vmov	r11, s18                                   #! PC = 0x400ddc *)
mov [r11_b, r11_t] [s18_b, s18_t]; mov r11 s18;
(* smulwb	lr, r10, r4                              #! PC = 0x400de0 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x400de4 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400de8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400dec *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x400df0 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov r4_b tmp_b; mov r4_t tmp_t;

assert eqmod r4_b (r4_b13 *    40) [Q] /\
       eqmod r4_t (r4_t13 *    40) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r4_b (r4_b13 *    40) [Q] /\
       eqmod r4_t (r4_t13 *    40) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r4_b, r4_t] /\ [r4_b, r4_t] <s[Q2, Q2];

ghost r5_b13@int16, r5_t13@int16:
      r5_b13 = r5_b /\ r5_t13 = r5_t
   && r5_b13 = r5_b /\ r5_t13 = r5_t;

(* smulwb	lr, r11, r5                              #! PC = 0x400df4 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x400df8 *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400dfc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400e00 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x400e04 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov r5_b tmp_b; mov r5_t tmp_t;

assert eqmod r5_b (r5_b13 *  -630) [Q] /\
       eqmod r5_t (r5_t13 *  -630) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r5_b (r5_b13 *  -630) [Q] /\
       eqmod r5_t (r5_t13 *  -630) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r5_b, r5_t] /\ [r5_b, r5_t] <s[Q2, Q2];

ghost r6_b4@int16, r6_t4@int16:
      r6_b4 = r6_b /\ r6_t4 = r6_t
   && r6_b4 = r6_b /\ r6_t4 = r6_t;

(* vmov	r10, s19                                   #! PC = 0x400e08 *)
mov [r10_b, r10_t] [s19_b, s19_t]; mov r10 s19;
(* vmov	r11, s20                                   #! PC = 0x400e0c *)
mov [r11_b, r11_t] [s20_b, s20_t]; mov r11 s20;
(* smulwb	lr, r10, r6                              #! PC = 0x400e10 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x400e14 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e18 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400e1c *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x400e20 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov r6_b tmp_b; mov r6_t tmp_t;

assert eqmod r6_b (r6_b4 *  1600) [Q] /\
       eqmod r6_t (r6_t4 *  1600) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r6_b (r6_b4 *  1600) [Q] /\
       eqmod r6_t (r6_t4 *  1600) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r6_b, r6_t] /\ [r6_b, r6_t] <s[Q2, Q2];

ghost r7_b13@int16, r7_t13@int16:
      r7_b13 = r7_b /\ r7_t13 = r7_t
   && r7_b13 = r7_b /\ r7_t13 = r7_t;

(* smulwb	lr, r11, r7                              #! PC = 0x400e24 *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x400e28 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e2c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400e30 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x400e34 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov r7_b tmp_b; mov r7_t tmp_t;

assert eqmod r7_b (r7_b13 *  1432) [Q] /\
       eqmod r7_t (r7_t13 *  1432) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r7_b (r7_b13 *  1432) [Q] /\
       eqmod r7_t (r7_t13 *  1432) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r7_b, r7_t] /\ [r7_b, r7_t] <s[Q2, Q2];

ghost r8_b4@int16, r8_t4@int16:
      r8_b4 = r8_b /\ r8_t4 = r8_t
   && r8_b4 = r8_b /\ r8_t4 = r8_t;

(* vmov	r10, s21                                   #! PC = 0x400e38 *)
mov [r10_b, r10_t] [s21_b, s21_t]; mov r10 s21;
(* vmov	r11, s22                                   #! PC = 0x400e3c *)
mov [r11_b, r11_t] [s22_b, s22_t]; mov r11 s22;
(* smulwb	lr, r10, r8                              #! PC = 0x400e40 *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x400e44 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e48 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400e4c *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x400e50 *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov r8_b tmp_b; mov r8_t tmp_t;

assert eqmod r8_b (r8_b4 *   749) [Q] /\
       eqmod r8_t (r8_t4 *   749) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r8_b (r8_b4 *   749) [Q] /\
       eqmod r8_t (r8_t4 *   749) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r8_b, r8_t] /\ [r8_b, r8_t] <s[Q2, Q2];

ghost r9_b22@int16, r9_t22@int16:
      r9_b22 = r9_b /\ r9_t22 = r9_t
   && r9_b22 = r9_b /\ r9_t22 = r9_t;

(* smulwb	lr, r11, r9                              #! PC = 0x400e54 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x400e58 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e5c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400e60 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x400e64 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov r9_b tmp_b; mov r9_t tmp_t;

assert eqmod r9_b (r9_b22 *   687) [Q] /\
       eqmod r9_t (r9_t22 *   687) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r9_b (r9_b22 *   687) [Q] /\
       eqmod r9_t (r9_t22 *   687) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r9_b, r9_t] /\ [r9_b, r9_t] <s[Q2, Q2];

(* vmov	s0, r2                                     #! PC = 0x400e68 *)
mov [s0_b, s0_t] [r2_b, r2_t];
(* vmov	s1, r3                                     #! PC = 0x400e6c *)
mov [s1_b, s1_t] [r3_b, r3_t];
(* vmov	s2, r4                                     #! PC = 0x400e70 *)
mov [s2_b, s2_t] [r4_b, r4_t];
(* vmov	s3, r5                                     #! PC = 0x400e74 *)
mov [s3_b, s3_t] [r5_b, r5_t];
(* vmov	s4, r6                                     #! PC = 0x400e78 *)
mov [s4_b, s4_t] [r6_b, r6_t];
(* vmov	s5, r7                                     #! PC = 0x400e7c *)
mov [s5_b, s5_t] [r7_b, r7_t];
(* vmov	s6, r8                                     #! PC = 0x400e80 *)
mov [s6_b, s6_t] [r8_b, r8_t];
(* vmov	s7, r9                                     #! PC = 0x400e84 *)
mov [s7_b, s7_t] [r9_b, r9_t];

assert [8*NQ2,8*NQ2]< [s0_b, s0_t] /\ [s0_b, s0_t]< [8*Q2,8*Q2] /\
       [1*NQ2,1*NQ2]< [s1_b, s1_t] /\ [s1_b, s1_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s2_b, s2_t] /\ [s2_b, s2_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s3_b, s3_t] /\ [s3_b, s3_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s4_b, s4_t] /\ [s4_b, s4_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s5_b, s5_t] /\ [s5_b, s5_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s6_b, s6_t] /\ [s6_b, s6_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s7_b, s7_t] /\ [s7_b, s7_t]< [1*Q2,1*Q2]
       prove with [algebra solver isl, precondition] && true;
assume [8*NQ2,8*NQ2]< [s0_b, s0_t] /\ [s0_b, s0_t]< [8*Q2,8*Q2] /\
       [1*NQ2,1*NQ2]< [s1_b, s1_t] /\ [s1_b, s1_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s2_b, s2_t] /\ [s2_b, s2_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s3_b, s3_t] /\ [s3_b, s3_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s4_b, s4_t] /\ [s4_b, s4_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s5_b, s5_t] /\ [s5_b, s5_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s6_b, s6_t] /\ [s6_b, s6_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s7_b, s7_t] /\ [s7_b, s7_t]< [1*Q2,1*Q2]
    && [8@16*NQ2,8@16*NQ2]<s[s0_b, s0_t] /\ [s0_b, s0_t]<s[8@16*Q2,8@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s1_b, s1_t] /\ [s1_b, s1_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s2_b, s2_t] /\ [s2_b, s2_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s3_b, s3_t] /\ [s3_b, s3_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s4_b, s4_t] /\ [s4_b, s4_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s5_b, s5_t] /\ [s5_b, s5_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s6_b, s6_t] /\ [s6_b, s6_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s7_b, s7_t] /\ [s7_b, s7_t]<s[1@16*Q2,1@16*Q2];

(* CUT 8 *)
ghost Z4@int16: X**2 =  -848*17** 19*Z4 && true;
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod (poly Z4 [poly X [s0_b, s0_t], poly X [s1_b, s1_t],
                    poly X [s2_b, s2_t], poly X [s3_b, s3_t],
                    poly X [s4_b, s4_t], poly X [s5_b, s5_t],
                    poly X [s6_b, s6_t], poly X [s7_b, s7_t]])
          (2**3*F**2) [Q, Z4**8 + 1] /\
    [8*NQ2,8*NQ2]< [s0_b, s0_t] /\ [s0_b, s0_t]< [8*Q2,8*Q2] /\
    [1*NQ2,1*NQ2]< [s1_b, s1_t] /\ [s1_b, s1_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s2_b, s2_t] /\ [s2_b, s2_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s3_b, s3_t] /\ [s3_b, s3_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s4_b, s4_t] /\ [s4_b, s4_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s5_b, s5_t] /\ [s5_b, s5_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s6_b, s6_t] /\ [s6_b, s6_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s7_b, s7_t] /\ [s7_b, s7_t]< [1*Q2,1*Q2]
    prove with [precondition]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [8@16*NQ2,8@16*NQ2]<s[s0_b, s0_t] /\ [s0_b, s0_t]<s[8@16*Q2,8@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s1_b, s1_t] /\ [s1_b, s1_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s2_b, s2_t] /\ [s2_b, s2_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s3_b, s3_t] /\ [s3_b, s3_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s4_b, s4_t] /\ [s4_b, s4_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s5_b, s5_t] /\ [s5_b, s5_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s6_b, s6_t] /\ [s6_b, s6_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s7_b, s7_t] /\ [s7_b, s7_t]<s[1@16*Q2,1@16*Q2]
    prove with [precondition];

(* vmov	r0, s23                                    #! PC = 0x400e88 *)
mov [r0_b, r0_t] [s23_b, s23_t];
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff2c4; Value = 0xbc14a0c7; PC = 0x400e8c *)
mov [r2_b, r2_t] [L0xbefff2c4, L0xbefff2c6];
(* ldr.w	r3, [r0, #4]                              #! EA = L0xbefff2c8; Value = 0xb548a16b; PC = 0x400e90 *)
mov [r3_b, r3_t] [L0xbefff2c8, L0xbefff2ca];
(* ldr.w	r4, [r0, #8]                              #! EA = L0xbefff2cc; Value = 0xab1ea48b; PC = 0x400e94 *)
mov [r4_b, r4_t] [L0xbefff2cc, L0xbefff2ce];
(* ldr.w	r5, [r0, #12]                             #! EA = L0xbefff2d0; Value = 0xb552adf3; PC = 0x400e98 *)
mov [r5_b, r5_t] [L0xbefff2d0, L0xbefff2d2];
(* ldr.w	r6, [r0, #16]                             #! EA = L0xbefff2d4; Value = 0xb745a0aa; PC = 0x400e9c *)
mov [r6_b, r6_t] [L0xbefff2d4, L0xbefff2d6];
(* ldr.w	r7, [r0, #20]                             #! EA = L0xbefff2d8; Value = 0xc3b99b0e; PC = 0x400ea0 *)
mov [r7_b, r7_t] [L0xbefff2d8, L0xbefff2da];
(* ldr.w	r8, [r0, #24]                             #! EA = L0xbefff2dc; Value = 0xb2429d5a; PC = 0x400ea4 *)
mov [r8_b, r8_t] [L0xbefff2dc, L0xbefff2de];
(* ldr.w	r9, [r0, #28]                             #! EA = L0xbefff2e0; Value = 0xba4c92c6; PC = 0x400ea8 *)
mov [r9_b, r9_t] [L0xbefff2e0, L0xbefff2e2];
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400eac *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* sadd16	lr, r2, r3                               #! PC = 0x400eb0 *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* ssub16	r3, r2, r3                               #! PC = 0x400eb4 *)
sub r3_b r2_b r3_b;
sub r3_t r2_t r3_t;
(* sadd16	r11, r4, r5                              #! PC = 0x400eb8 *)
add r11_b r4_b r5_b;
add r11_t r4_t r5_t;
(* ssub16	r5, r4, r5                               #! PC = 0x400ebc *)
sub r5_b r4_b r5_b;
sub r5_t r4_t r5_t;
(* sadd16	r2, r6, r7                               #! PC = 0x400ec0 *)
add r2_b r6_b r7_b;
add r2_t r6_t r7_t;
(* ssub16	r7, r6, r7                               #! PC = 0x400ec4 *)
sub r7_b r6_b r7_b;
sub r7_t r6_t r7_t;
(* sadd16	r4, r8, r9                               #! PC = 0x400ec8 *)
add r4_b r8_b r9_b;
add r4_t r8_t r9_t;
(* ssub16	r9, r8, r9                               #! PC = 0x400ecc *)
sub r9_b r8_b r9_b;
sub r9_t r8_t r9_t;
(* sadd16	r8, lr, r11                              #! PC = 0x400ed0 *)
add r8_b lr_b r11_b;
add r8_t lr_t r11_t;
(* ssub16	r11, lr, r11                             #! PC = 0x400ed4 *)
sub r11_b lr_b r11_b;
sub r11_t lr_t r11_t;
(* sadd16	r6, r2, r4                               #! PC = 0x400ed8 *)
add r6_b r2_b r4_b;
add r6_t r2_t r4_t;
(* ssub16	r4, r2, r4                               #! PC = 0x400edc *)
sub r4_b r2_b r4_b;
sub r4_t r2_t r4_t;

ghost r5_b14@int16, r5_t14@int16:
      r5_b14 = r5_b /\ r5_t14 = r5_t
   && r5_b14 = r5_b /\ r5_t14 = r5_t;

(* vmov	r10, s10                                   #! PC = 0x400ee0 *)
mov [r10_b, r10_t] [s10_b, s10_t]; mov r10 s10;
(* smulwb	lr, r10, r5                              #! PC = 0x400ee4 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x400ee8 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400eec *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400ef0 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400ef4 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_b14 *  1600) [Q] /\
       eqmod lr_t (r5_t14 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_b14 *  1600) [Q] /\
       eqmod lr_t (r5_t14 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x400ef8 *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400efc *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r9_b23@int16, r9_t23@int16:
      r9_b23 = r9_b /\ r9_t23 = r9_t
   && r9_b23 = r9_b /\ r9_t23 = r9_t;

(* smulwb	lr, r10, r9                              #! PC = 0x400f00 *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400f04 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f08 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400f0c *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400f10 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b23 *  1600) [Q] /\
       eqmod lr_t (r9_t23 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b23 *  1600) [Q] /\
       eqmod lr_t (r9_t23 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x400f14 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400f18 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* sadd16	r2, r8, r6                               #! PC = 0x400f1c *)
add r2_b r8_b r6_b;
add r2_t r8_t r6_t;
(* ssub16	r6, r8, r6                               #! PC = 0x400f20 *)
sub r6_b r8_b r6_b;
sub r6_t r8_t r6_t;

ghost r7_b14@int16, r7_t14@int16:
      r7_b14 = r7_b /\ r7_t14 = r7_t
   && r7_b14 = r7_b /\ r7_t14 = r7_t;

(* vmov	r10, s12                                   #! PC = 0x400f24 *)
mov [r10_b, r10_t] [s12_b, s12_t]; mov r10 s12;
(* smulwb	lr, r10, r7                              #! PC = 0x400f28 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x400f2c *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f30 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400f34 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400f38 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_b14 *    40) [Q] /\
       eqmod lr_t (r7_t14 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_b14 *    40) [Q] /\
       eqmod lr_t (r7_t14 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x400f3c *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400f40 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r4_b14@int16, r4_t14@int16:
      r4_b14 = r4_b /\ r4_t14 = r4_t
   && r4_b14 = r4_b /\ r4_t14 = r4_t;

(* vmov	r10, s13                                   #! PC = 0x400f44 *)
mov [r10_b, r10_t] [s13_b, s13_t]; mov r10 s13;
(* smulwb	lr, r10, r4                              #! PC = 0x400f48 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x400f4c *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f50 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400f54 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400f58 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_b14 *  1600) [Q] /\
       eqmod lr_t (r4_t14 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_b14 *  1600) [Q] /\
       eqmod lr_t (r4_t14 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* ssub16	r8, r11, lr                              #! PC = 0x400f5c *)
sub r8_b r11_b lr_b;
sub r8_t r11_t lr_t;
(* sadd16	r4, r11, lr                              #! PC = 0x400f60 *)
add r4_b r11_b lr_b;
add r4_t r11_t lr_t;

ghost r9_b24@int16, r9_t24@int16:
      r9_b24 = r9_b /\ r9_t24 = r9_t
   && r9_b24 = r9_b /\ r9_t24 = r9_t;

(* vmov	r10, s14                                   #! PC = 0x400f64 *)
mov [r10_b, r10_t] [s14_b, s14_t]; mov r10 s14;
(* smulwb	lr, r10, r9                              #! PC = 0x400f68 *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400f6c *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f70 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400f74 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400f78 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b24 *   749) [Q] /\
       eqmod lr_t (r9_t24 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b24 *   749) [Q] /\
       eqmod lr_t (r9_t24 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x400f7c *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400f80 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* vmov	r0, s23                                    #! PC = 0x400f84 *)
mov [r0_b, r0_t] [s23_b, s23_t];
(* vmov	r11, s1                                    #! PC = 0x400f88 *)
mov [r11_b, r11_t] [s1_b, s1_t];
(* uadd16	lr, r3, r11                              #! PC = 0x400f8c *)
add lr_b r3_b r11_b;
add lr_t r3_t r11_t;
(* usub16	r3, r3, r11                              #! PC = 0x400f90 *)
sub r3_b r3_b r11_b;
sub r3_t r3_t r11_t;
(* str.w	lr, [r0, #4]                              #! EA = L0xbefff2c8; PC = 0x400f94 *)
mov [L0xbefff2c8, L0xbefff2ca] [lr_b, lr_t];
(* str.w	r3, [r0, #36]	; 0x24                      #! EA = L0xbefff2e8; PC = 0x400f98 *)
mov [L0xbefff2e8, L0xbefff2ea] [r3_b, r3_t];
(* vmov	r11, s3                                    #! PC = 0x400f9c *)
mov [r11_b, r11_t] [s3_b, s3_t];
(* uadd16	lr, r5, r11                              #! PC = 0x400fa0 *)
add lr_b r5_b r11_b;
add lr_t r5_t r11_t;
(* usub16	r5, r5, r11                              #! PC = 0x400fa4 *)
sub r5_b r5_b r11_b;
sub r5_t r5_t r11_t;
(* str.w	lr, [r0, #12]                             #! EA = L0xbefff2d0; PC = 0x400fa8 *)
mov [L0xbefff2d0, L0xbefff2d2] [lr_b, lr_t];
(* str.w	r5, [r0, #44]	; 0x2c                      #! EA = L0xbefff2f0; PC = 0x400fac *)
mov [L0xbefff2f0, L0xbefff2f2] [r5_b, r5_t];
(* vmov	r11, s5                                    #! PC = 0x400fb0 *)
mov [r11_b, r11_t] [s5_b, s5_t];
(* uadd16	lr, r7, r11                              #! PC = 0x400fb4 *)
add lr_b r7_b r11_b;
add lr_t r7_t r11_t;
(* usub16	r7, r7, r11                              #! PC = 0x400fb8 *)
sub r7_b r7_b r11_b;
sub r7_t r7_t r11_t;
(* str.w	lr, [r0, #20]                             #! EA = L0xbefff2d8; PC = 0x400fbc *)
mov [L0xbefff2d8, L0xbefff2da] [lr_b, lr_t];
(* str.w	r7, [r0, #52]	; 0x34                      #! EA = L0xbefff2f8; PC = 0x400fc0 *)
mov [L0xbefff2f8, L0xbefff2fa] [r7_b, r7_t];
(* vmov	r11, s7                                    #! PC = 0x400fc4 *)
mov [r11_b, r11_t] [s7_b, s7_t];
(* uadd16	lr, r9, r11                              #! PC = 0x400fc8 *)
add lr_b r9_b r11_b;
add lr_t r9_t r11_t;
(* usub16	r9, r9, r11                              #! PC = 0x400fcc *)
sub r9_b r9_b r11_b;
sub r9_t r9_t r11_t;
(* str.w	lr, [r0, #28]                             #! EA = L0xbefff2e0; PC = 0x400fd0 *)
mov [L0xbefff2e0, L0xbefff2e2] [lr_b, lr_t];
(* str.w	r9, [r0, #60]	; 0x3c                      #! EA = L0xbefff300; PC = 0x400fd4 *)
mov [L0xbefff300, L0xbefff302] [r9_b, r9_t];
(* vmov	r5, s2                                     #! PC = 0x400fd8 *)
mov [r5_b, r5_t] [s2_b, s2_t];
(* uadd16	lr, r4, r5                               #! PC = 0x400fdc *)
add lr_b r4_b r5_b;
add lr_t r4_t r5_t;
(* usub16	r11, r4, r5                              #! PC = 0x400fe0 *)
sub r11_b r4_b r5_b;
sub r11_t r4_t r5_t;
(* str.w	lr, [r0, #8]                              #! EA = L0xbefff2cc; PC = 0x400fe4 *)
mov [L0xbefff2cc, L0xbefff2ce] [lr_b, lr_t];
(* str.w	r11, [r0, #40]	; 0x28                     #! EA = L0xbefff2ec; PC = 0x400fe8 *)
mov [L0xbefff2ec, L0xbefff2ee] [r11_b, r11_t];
(* vmov	r7, s4                                     #! PC = 0x400fec *)
mov [r7_b, r7_t] [s4_b, s4_t];
(* uadd16	lr, r6, r7                               #! PC = 0x400ff0 *)
add lr_b r6_b r7_b;
add lr_t r6_t r7_t;
(* usub16	r11, r6, r7                              #! PC = 0x400ff4 *)
sub r11_b r6_b r7_b;
sub r11_t r6_t r7_t;
(* str.w	lr, [r0, #16]                             #! EA = L0xbefff2d4; PC = 0x400ff8 *)
mov [L0xbefff2d4, L0xbefff2d6] [lr_b, lr_t];
(* str.w	r11, [r0, #48]	; 0x30                     #! EA = L0xbefff2f4; PC = 0x400ffc *)
mov [L0xbefff2f4, L0xbefff2f6] [r11_b, r11_t];
(* vmov	r9, s6                                     #! PC = 0x401000 *)
mov [r9_b, r9_t] [s6_b, s6_t];
(* uadd16	lr, r8, r9                               #! PC = 0x401004 *)
add lr_b r8_b r9_b;
add lr_t r8_t r9_t;
(* usub16	r11, r8, r9                              #! PC = 0x401008 *)
sub r11_b r8_b r9_b;
sub r11_t r8_t r9_t;
(* str.w	lr, [r0, #24]                             #! EA = L0xbefff2dc; PC = 0x40100c *)
mov [L0xbefff2dc, L0xbefff2de] [lr_b, lr_t];
(* str.w	r11, [r0, #56]	; 0x38                     #! EA = L0xbefff2fc; PC = 0x401010 *)
mov [L0xbefff2fc, L0xbefff2fe] [r11_b, r11_t];
(* vmov	r3, s0                                     #! PC = 0x401014 *)
mov [r3_b, r3_t] [s0_b, s0_t];
(* uadd16	lr, r2, r3                               #! PC = 0x401018 *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* usub16	r11, r2, r3                              #! PC = 0x40101c *)
sub r11_b r2_b r3_b;
sub r11_t r2_t r3_t;
(* str.w	r11, [r0, #32]                            #! EA = L0xbefff2e4; PC = 0x401020 *)
mov [L0xbefff2e4, L0xbefff2e6] [r11_b, r11_t];
(* str.w	lr, [r0], #64                             #! EA = L0xbefff2c4; PC = 0x401024 *)
mov [L0xbefff2c4, L0xbefff2c6] [lr_b, lr_t];
(* vmov	lr, s8                                     #! PC = 0x401028 *)
mov [lr_b, lr_t] [s8_b, s8_t]; mov lr s8;
(* cmp.w	r0, lr                                    #! PC = 0x40102c *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x400cc4 <invntt_fast+24>                #! PC = 0x401030 *)
#bne.w	0x400cc4 <invntt_fast+24>                #! 0x401030 = 0x401030;

(* CUT 9 *)
assert [16*NQ2,16*NQ2,5*NQ2]< [L0xbefff2c4,L0xbefff2c6,L0xbefff2c8] /\
       [L0xbefff2c4,L0xbefff2c6,L0xbefff2c8]< [16*Q2,16*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff2ca,L0xbefff2cc,L0xbefff2ce] /\
       [L0xbefff2ca,L0xbefff2cc,L0xbefff2ce]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2,9*NQ2]< [L0xbefff2d0,L0xbefff2d2,L0xbefff2d4] /\
       [L0xbefff2d0,L0xbefff2d2,L0xbefff2d4]< [5*Q2,5*Q2,9*Q2] /\
       [9*NQ2,5*NQ2,5*NQ2]< [L0xbefff2d6,L0xbefff2d8,L0xbefff2da] /\
       [L0xbefff2d6,L0xbefff2d8,L0xbefff2da]< [9*Q2,5*Q2,5*Q2] /\
       [6*NQ2,6*NQ2,5*NQ2]< [L0xbefff2dc,L0xbefff2de,L0xbefff2e0] /\
       [L0xbefff2dc,L0xbefff2de,L0xbefff2e0]< [6*Q2,6*Q2,5*Q2] /\
       [5*NQ2,16*NQ2,16*NQ2]< [L0xbefff2e2,L0xbefff2e4,L0xbefff2e6] /\
       [L0xbefff2e2,L0xbefff2e4,L0xbefff2e6]< [5*Q2,16*Q2,16*Q2] /\
       [5*NQ2,5*NQ2,6*NQ2]< [L0xbefff2e8,L0xbefff2ea,L0xbefff2ec] /\
       [L0xbefff2e8,L0xbefff2ea,L0xbefff2ec]< [5*Q2,5*Q2,6*Q2] /\
       [6*NQ2,5*NQ2,5*NQ2]< [L0xbefff2ee,L0xbefff2f0,L0xbefff2f2] /\
       [L0xbefff2ee,L0xbefff2f0,L0xbefff2f2]< [6*Q2,5*Q2,5*Q2] /\
       [9*NQ2,9*NQ2,5*NQ2]< [L0xbefff2f4,L0xbefff2f6,L0xbefff2f8] /\
       [L0xbefff2f4,L0xbefff2f6,L0xbefff2f8]< [9*Q2,9*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff2fa,L0xbefff2fc,L0xbefff2fe] /\
       [L0xbefff2fa,L0xbefff2fc,L0xbefff2fe]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2]< [L0xbefff300,L0xbefff302] /\
       [L0xbefff300,L0xbefff302]< [5*Q2,5*Q2]
       prove with [algebra solver isl, precondition] && true;
assume [16*NQ2,16*NQ2,5*NQ2]< [L0xbefff2c4,L0xbefff2c6,L0xbefff2c8] /\
       [L0xbefff2c4,L0xbefff2c6,L0xbefff2c8]< [16*Q2,16*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff2ca,L0xbefff2cc,L0xbefff2ce] /\
       [L0xbefff2ca,L0xbefff2cc,L0xbefff2ce]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2,9*NQ2]< [L0xbefff2d0,L0xbefff2d2,L0xbefff2d4] /\
       [L0xbefff2d0,L0xbefff2d2,L0xbefff2d4]< [5*Q2,5*Q2,9*Q2] /\
       [9*NQ2,5*NQ2,5*NQ2]< [L0xbefff2d6,L0xbefff2d8,L0xbefff2da] /\
       [L0xbefff2d6,L0xbefff2d8,L0xbefff2da]< [9*Q2,5*Q2,5*Q2] /\
       [6*NQ2,6*NQ2,5*NQ2]< [L0xbefff2dc,L0xbefff2de,L0xbefff2e0] /\
       [L0xbefff2dc,L0xbefff2de,L0xbefff2e0]< [6*Q2,6*Q2,5*Q2] /\
       [5*NQ2,16*NQ2,16*NQ2]< [L0xbefff2e2,L0xbefff2e4,L0xbefff2e6] /\
       [L0xbefff2e2,L0xbefff2e4,L0xbefff2e6]< [5*Q2,16*Q2,16*Q2] /\
       [5*NQ2,5*NQ2,6*NQ2]< [L0xbefff2e8,L0xbefff2ea,L0xbefff2ec] /\
       [L0xbefff2e8,L0xbefff2ea,L0xbefff2ec]< [5*Q2,5*Q2,6*Q2] /\
       [6*NQ2,5*NQ2,5*NQ2]< [L0xbefff2ee,L0xbefff2f0,L0xbefff2f2] /\
       [L0xbefff2ee,L0xbefff2f0,L0xbefff2f2]< [6*Q2,5*Q2,5*Q2] /\
       [9*NQ2,9*NQ2,5*NQ2]< [L0xbefff2f4,L0xbefff2f6,L0xbefff2f8] /\
       [L0xbefff2f4,L0xbefff2f6,L0xbefff2f8]< [9*Q2,9*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff2fa,L0xbefff2fc,L0xbefff2fe] /\
       [L0xbefff2fa,L0xbefff2fc,L0xbefff2fe]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2]< [L0xbefff300,L0xbefff302] /\
       [L0xbefff300,L0xbefff302]< [5*Q2,5*Q2]
    && [16@16*NQ2,16@16*NQ2,5@16*NQ2]< [L0xbefff2c4,L0xbefff2c6,L0xbefff2c8] /\
       [L0xbefff2c4,L0xbefff2c6,L0xbefff2c8]< [16@16*Q2,16@16*Q2,5@16*Q2] /\
       [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff2ca,L0xbefff2cc,L0xbefff2ce] /\
       [L0xbefff2ca,L0xbefff2cc,L0xbefff2ce]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
       [5@16*NQ2,5@16*NQ2,9@16*NQ2]< [L0xbefff2d0,L0xbefff2d2,L0xbefff2d4] /\
       [L0xbefff2d0,L0xbefff2d2,L0xbefff2d4]< [5@16*Q2,5@16*Q2,9@16*Q2] /\
       [9@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff2d6,L0xbefff2d8,L0xbefff2da] /\
       [L0xbefff2d6,L0xbefff2d8,L0xbefff2da]< [9@16*Q2,5@16*Q2,5@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,5@16*NQ2]< [L0xbefff2dc,L0xbefff2de,L0xbefff2e0] /\
       [L0xbefff2dc,L0xbefff2de,L0xbefff2e0]< [6@16*Q2,6@16*Q2,5@16*Q2] /\
       [5@16*NQ2,16@16*NQ2,16@16*NQ2]< [L0xbefff2e2,L0xbefff2e4,L0xbefff2e6] /\
       [L0xbefff2e2,L0xbefff2e4,L0xbefff2e6]< [5@16*Q2,16@16*Q2,16@16*Q2] /\
       [5@16*NQ2,5@16*NQ2,6@16*NQ2]< [L0xbefff2e8,L0xbefff2ea,L0xbefff2ec] /\
       [L0xbefff2e8,L0xbefff2ea,L0xbefff2ec]< [5@16*Q2,5@16*Q2,6@16*Q2] /\
       [6@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff2ee,L0xbefff2f0,L0xbefff2f2] /\
       [L0xbefff2ee,L0xbefff2f0,L0xbefff2f2]< [6@16*Q2,5@16*Q2,5@16*Q2] /\
       [9@16*NQ2,9@16*NQ2,5@16*NQ2]< [L0xbefff2f4,L0xbefff2f6,L0xbefff2f8] /\
       [L0xbefff2f4,L0xbefff2f6,L0xbefff2f8]< [9@16*Q2,9@16*Q2,5@16*Q2] /\
       [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff2fa,L0xbefff2fc,L0xbefff2fe] /\
       [L0xbefff2fa,L0xbefff2fc,L0xbefff2fe]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
       [5@16*NQ2,5@16*NQ2]< [L0xbefff300,L0xbefff302] /\
       [L0xbefff300,L0xbefff302]< [5@16*Q2,5@16*Q2];
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod (poly Z4
           [poly X [L0xbefff2c4,L0xbefff2c6],poly X [L0xbefff2c8,L0xbefff2ca],
            poly X [L0xbefff2cc,L0xbefff2ce],poly X [L0xbefff2d0,L0xbefff2d2],
            poly X [L0xbefff2d4,L0xbefff2d6],poly X [L0xbefff2d8,L0xbefff2da],
            poly X [L0xbefff2dc,L0xbefff2de],poly X [L0xbefff2e0,L0xbefff2e2],
            poly X [L0xbefff2e4,L0xbefff2e6],poly X [L0xbefff2e8,L0xbefff2ea],
            poly X [L0xbefff2ec,L0xbefff2ee],poly X [L0xbefff2f0,L0xbefff2f2],
            poly X [L0xbefff2f4,L0xbefff2f6],poly X [L0xbefff2f8,L0xbefff2fa],
            poly X [L0xbefff2fc,L0xbefff2fe],poly X [L0xbefff300,L0xbefff302]])
          (2**4*F**2) [Q, Z4**16 - 1] /\
    [16*NQ2,16*NQ2,5*NQ2]< [L0xbefff2c4,L0xbefff2c6,L0xbefff2c8] /\
    [L0xbefff2c4,L0xbefff2c6,L0xbefff2c8]< [16*Q2,16*Q2,5*Q2] /\
    [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff2ca,L0xbefff2cc,L0xbefff2ce] /\
    [L0xbefff2ca,L0xbefff2cc,L0xbefff2ce]< [5*Q2,6*Q2,6*Q2] /\
    [5*NQ2,5*NQ2,9*NQ2]< [L0xbefff2d0,L0xbefff2d2,L0xbefff2d4] /\
    [L0xbefff2d0,L0xbefff2d2,L0xbefff2d4]< [5*Q2,5*Q2,9*Q2] /\
    [9*NQ2,5*NQ2,5*NQ2]< [L0xbefff2d6,L0xbefff2d8,L0xbefff2da] /\
    [L0xbefff2d6,L0xbefff2d8,L0xbefff2da]< [9*Q2,5*Q2,5*Q2] /\
    [6*NQ2,6*NQ2,5*NQ2]< [L0xbefff2dc,L0xbefff2de,L0xbefff2e0] /\
    [L0xbefff2dc,L0xbefff2de,L0xbefff2e0]< [6*Q2,6*Q2,5*Q2] /\
    [5*NQ2,16*NQ2,16*NQ2]< [L0xbefff2e2,L0xbefff2e4,L0xbefff2e6] /\
    [L0xbefff2e2,L0xbefff2e4,L0xbefff2e6]< [5*Q2,16*Q2,16*Q2] /\
    [5*NQ2,5*NQ2,6*NQ2]< [L0xbefff2e8,L0xbefff2ea,L0xbefff2ec] /\
    [L0xbefff2e8,L0xbefff2ea,L0xbefff2ec]< [5*Q2,5*Q2,6*Q2] /\
    [6*NQ2,5*NQ2,5*NQ2]< [L0xbefff2ee,L0xbefff2f0,L0xbefff2f2] /\
    [L0xbefff2ee,L0xbefff2f0,L0xbefff2f2]< [6*Q2,5*Q2,5*Q2] /\
    [9*NQ2,9*NQ2,5*NQ2]< [L0xbefff2f4,L0xbefff2f6,L0xbefff2f8] /\
    [L0xbefff2f4,L0xbefff2f6,L0xbefff2f8]< [9*Q2,9*Q2,5*Q2] /\
    [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff2fa,L0xbefff2fc,L0xbefff2fe] /\
    [L0xbefff2fa,L0xbefff2fc,L0xbefff2fe]< [5*Q2,6*Q2,6*Q2] /\
    [5*NQ2,5*NQ2]< [L0xbefff300,L0xbefff302] /\
    [L0xbefff300,L0xbefff302]< [5*Q2,5*Q2]
    prove with [precondition, all ghosts]
 && [16@16*NQ2,16@16*NQ2,5@16*NQ2]< [L0xbefff2c4,L0xbefff2c6,L0xbefff2c8] /\
    [L0xbefff2c4,L0xbefff2c6,L0xbefff2c8]< [16@16*Q2,16@16*Q2,5@16*Q2] /\
    [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff2ca,L0xbefff2cc,L0xbefff2ce] /\
    [L0xbefff2ca,L0xbefff2cc,L0xbefff2ce]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
    [5@16*NQ2,5@16*NQ2,9@16*NQ2]< [L0xbefff2d0,L0xbefff2d2,L0xbefff2d4] /\
    [L0xbefff2d0,L0xbefff2d2,L0xbefff2d4]< [5@16*Q2,5@16*Q2,9@16*Q2] /\
    [9@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff2d6,L0xbefff2d8,L0xbefff2da] /\
    [L0xbefff2d6,L0xbefff2d8,L0xbefff2da]< [9@16*Q2,5@16*Q2,5@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,5@16*NQ2]< [L0xbefff2dc,L0xbefff2de,L0xbefff2e0] /\
    [L0xbefff2dc,L0xbefff2de,L0xbefff2e0]< [6@16*Q2,6@16*Q2,5@16*Q2] /\
    [5@16*NQ2,16@16*NQ2,16@16*NQ2]< [L0xbefff2e2,L0xbefff2e4,L0xbefff2e6] /\
    [L0xbefff2e2,L0xbefff2e4,L0xbefff2e6]< [5@16*Q2,16@16*Q2,16@16*Q2] /\
    [5@16*NQ2,5@16*NQ2,6@16*NQ2]< [L0xbefff2e8,L0xbefff2ea,L0xbefff2ec] /\
    [L0xbefff2e8,L0xbefff2ea,L0xbefff2ec]< [5@16*Q2,5@16*Q2,6@16*Q2] /\
    [6@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff2ee,L0xbefff2f0,L0xbefff2f2] /\
    [L0xbefff2ee,L0xbefff2f0,L0xbefff2f2]< [6@16*Q2,5@16*Q2,5@16*Q2] /\
    [9@16*NQ2,9@16*NQ2,5@16*NQ2]< [L0xbefff2f4,L0xbefff2f6,L0xbefff2f8] /\
    [L0xbefff2f4,L0xbefff2f6,L0xbefff2f8]< [9@16*Q2,9@16*Q2,5@16*Q2] /\
    [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff2fa,L0xbefff2fc,L0xbefff2fe] /\
    [L0xbefff2fa,L0xbefff2fc,L0xbefff2fe]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
    [5@16*NQ2,5@16*NQ2]< [L0xbefff300,L0xbefff302] /\
    [L0xbefff300,L0xbefff302]< [5@16*Q2,5@16*Q2]
    prove with [precondition];

(* vmov	s23, r0                                    #! PC = 0x400cc4 *)
mov [s23_b, s23_t] [r0_b, r0_t];
(* ldr.w	r2, [r0, #32]                             #! EA = L0xbefff324; Value = 0xaaee890b; PC = 0x400cc8 *)
mov [r2_b, r2_t] [L0xbefff324, L0xbefff326];
(* ldr.w	r3, [r0, #36]	; 0x24                      #! EA = L0xbefff328; Value = 0xaebe94df; PC = 0x400ccc *)
mov [r3_b, r3_t] [L0xbefff328, L0xbefff32a];
(* ldr.w	r4, [r0, #40]	; 0x28                      #! EA = L0xbefff32c; Value = 0xb77794ba; PC = 0x400cd0 *)
mov [r4_b, r4_t] [L0xbefff32c, L0xbefff32e];
(* ldr.w	r5, [r0, #44]	; 0x2c                      #! EA = L0xbefff330; Value = 0xae3d972c; PC = 0x400cd4 *)
mov [r5_b, r5_t] [L0xbefff330, L0xbefff332];
(* ldr.w	r6, [r0, #48]	; 0x30                      #! EA = L0xbefff334; Value = 0xa9068f00; PC = 0x400cd8 *)
mov [r6_b, r6_t] [L0xbefff334, L0xbefff336];
(* ldr.w	r7, [r0, #52]	; 0x34                      #! EA = L0xbefff338; Value = 0xb032851e; PC = 0x400cdc *)
mov [r7_b, r7_t] [L0xbefff338, L0xbefff33a];
(* ldr.w	r8, [r0, #56]	; 0x38                      #! EA = L0xbefff33c; Value = 0xbed68334; PC = 0x400ce0 *)
mov [r8_b, r8_t] [L0xbefff33c, L0xbefff33e];
(* ldr.w	r9, [r0, #60]	; 0x3c                      #! EA = L0xbefff340; Value = 0xb42a860e; PC = 0x400ce4 *)
mov [r9_b, r9_t] [L0xbefff340, L0xbefff342];
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400ce8 *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* sadd16	lr, r2, r3                               #! PC = 0x400cec *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* ssub16	r3, r2, r3                               #! PC = 0x400cf0 *)
sub r3_b r2_b r3_b;
sub r3_t r2_t r3_t;
(* sadd16	r11, r4, r5                              #! PC = 0x400cf4 *)
add r11_b r4_b r5_b;
add r11_t r4_t r5_t;
(* ssub16	r5, r4, r5                               #! PC = 0x400cf8 *)
sub r5_b r4_b r5_b;
sub r5_t r4_t r5_t;
(* sadd16	r2, r6, r7                               #! PC = 0x400cfc *)
add r2_b r6_b r7_b;
add r2_t r6_t r7_t;
(* ssub16	r7, r6, r7                               #! PC = 0x400d00 *)
sub r7_b r6_b r7_b;
sub r7_t r6_t r7_t;
(* sadd16	r4, r8, r9                               #! PC = 0x400d04 *)
add r4_b r8_b r9_b;
add r4_t r8_t r9_t;
(* ssub16	r9, r8, r9                               #! PC = 0x400d08 *)
sub r9_b r8_b r9_b;
sub r9_t r8_t r9_t;
(* sadd16	r8, lr, r11                              #! PC = 0x400d0c *)
add r8_b lr_b r11_b;
add r8_t lr_t r11_t;
(* ssub16	r11, lr, r11                             #! PC = 0x400d10 *)
sub r11_b lr_b r11_b;
sub r11_t lr_t r11_t;
(* sadd16	r6, r2, r4                               #! PC = 0x400d14 *)
add r6_b r2_b r4_b;
add r6_t r2_t r4_t;
(* ssub16	r4, r2, r4                               #! PC = 0x400d18 *)
sub r4_b r2_b r4_b;
sub r4_t r2_t r4_t;

ghost r5_b15@int16, r5_t15@int16:
      r5_b15 = r5_b /\ r5_t15 = r5_t
   && r5_b15 = r5_b /\ r5_t15 = r5_t;

(* vmov	r10, s10                                   #! PC = 0x400d1c *)
mov [r10_b, r10_t] [s10_b, s10_t]; mov r10 s10;
(* smulwb	lr, r10, r5                              #! PC = 0x400d20 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x400d24 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d28 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400d2c *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400d30 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_b15 *  1600) [Q] /\
       eqmod lr_t (r5_t15 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_b15 *  1600) [Q] /\
       eqmod lr_t (r5_t15 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x400d34 *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400d38 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r9_b25@int16, r9_t25@int16:
      r9_b25 = r9_b /\ r9_t25 = r9_t
   && r9_b25 = r9_b /\ r9_t25 = r9_t;

(* smulwb	lr, r10, r9                              #! PC = 0x400d3c *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400d40 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d44 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400d48 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400d4c *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b25 *  1600) [Q] /\
       eqmod lr_t (r9_t25 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b25 *  1600) [Q] /\
       eqmod lr_t (r9_t25 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x400d50 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400d54 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* sadd16	r2, r8, r6                               #! PC = 0x400d58 *)
add r2_b r8_b r6_b;
add r2_t r8_t r6_t;
(* ssub16	r6, r8, r6                               #! PC = 0x400d5c *)
sub r6_b r8_b r6_b;
sub r6_t r8_t r6_t;

ghost r7_b15@int16, r7_t15@int16:
      r7_b15 = r7_b /\ r7_t15 = r7_t
   && r7_b15 = r7_b /\ r7_t15 = r7_t;

(* vmov	r10, s12                                   #! PC = 0x400d60 *)
mov [r10_b, r10_t] [s12_b, s12_t]; mov r10 s12;
(* smulwb	lr, r10, r7                              #! PC = 0x400d64 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x400d68 *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d6c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400d70 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400d74 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_b15 *    40) [Q] /\
       eqmod lr_t (r7_t15 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_b15 *    40) [Q] /\
       eqmod lr_t (r7_t15 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x400d78 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400d7c *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r4_b15@int16, r4_t15@int16:
      r4_b15 = r4_b /\ r4_t15 = r4_t
   && r4_b15 = r4_b /\ r4_t15 = r4_t;

(* vmov	r10, s13                                   #! PC = 0x400d80 *)
mov [r10_b, r10_t] [s13_b, s13_t]; mov r10 s13;
(* smulwb	lr, r10, r4                              #! PC = 0x400d84 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x400d88 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d8c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400d90 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400d94 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_b15 *  1600) [Q] /\
       eqmod lr_t (r4_t15 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_b15 *  1600) [Q] /\
       eqmod lr_t (r4_t15 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* ssub16	r8, r11, lr                              #! PC = 0x400d98 *)
sub r8_b r11_b lr_b;
sub r8_t r11_t lr_t;
(* sadd16	r4, r11, lr                              #! PC = 0x400d9c *)
add r4_b r11_b lr_b;
add r4_t r11_t lr_t;

ghost r9_b26@int16, r9_t26@int16:
      r9_b26 = r9_b /\ r9_t26 = r9_t
   && r9_b26 = r9_b /\ r9_t26 = r9_t;

(* vmov	r10, s14                                   #! PC = 0x400da0 *)
mov [r10_b, r10_t] [s14_b, s14_t]; mov r10 s14;
(* smulwb	lr, r10, r9                              #! PC = 0x400da4 *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400da8 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400dac *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400db0 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400db4 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b26 *   749) [Q] /\
       eqmod lr_t (r9_t26 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b26 *   749) [Q] /\
       eqmod lr_t (r9_t26 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x400db8 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400dbc *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;

ghost r3_b5@int16, r3_t5@int16:
      r3_b5 = r3_b /\ r3_t5 = r3_t
   && r3_b5 = r3_b /\ r3_t5 = r3_t;

(* vmov	r11, s16                                   #! PC = 0x400dc0 *)
mov [r11_b, r11_t] [s16_b, s16_t]; mov r11 s16;
(* smulwb	lr, r11, r3                              #! PC = 0x400dc4 *)
vpc r3_bW@int32 r3_b; mulj tmp r11 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r11, r3                              #! PC = 0x400dc8 *)
vpc r3_tW@int32 r3_t; mulj tmp r11 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400dcc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400dd0 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x400dd4 *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov r3_b tmp_b; mov r3_t tmp_t;

assert eqmod r3_b (r3_b5 *  -848) [Q] /\
       eqmod r3_t (r3_t5 *  -848) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r3_b (r3_b5 *  -848) [Q] /\
       eqmod r3_t (r3_t5 *  -848) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r3_b, r3_t] /\ [r3_b, r3_t] <s[Q2, Q2];

ghost r4_b16@int16, r4_t16@int16:
      r4_b16 = r4_b /\ r4_t16 = r4_t
   && r4_b16 = r4_b /\ r4_t16 = r4_t;

(* vmov	r10, s17                                   #! PC = 0x400dd8 *)
mov [r10_b, r10_t] [s17_b, s17_t]; mov r10 s17;
(* vmov	r11, s18                                   #! PC = 0x400ddc *)
mov [r11_b, r11_t] [s18_b, s18_t]; mov r11 s18;
(* smulwb	lr, r10, r4                              #! PC = 0x400de0 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x400de4 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400de8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400dec *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x400df0 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov r4_b tmp_b; mov r4_t tmp_t;

assert eqmod r4_b (r4_b16 *    40) [Q] /\
       eqmod r4_t (r4_t16 *    40) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r4_b (r4_b16 *    40) [Q] /\
       eqmod r4_t (r4_t16 *    40) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r4_b, r4_t] /\ [r4_b, r4_t] <s[Q2, Q2];

ghost r5_b16@int16, r5_t16@int16:
      r5_b16 = r5_b /\ r5_t16 = r5_t
   && r5_b16 = r5_b /\ r5_t16 = r5_t;

(* smulwb	lr, r11, r5                              #! PC = 0x400df4 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x400df8 *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400dfc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400e00 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x400e04 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov r5_b tmp_b; mov r5_t tmp_t;

assert eqmod r5_b (r5_b16 *  -630) [Q] /\
       eqmod r5_t (r5_t16 *  -630) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r5_b (r5_b16 *  -630) [Q] /\
       eqmod r5_t (r5_t16 *  -630) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r5_b, r5_t] /\ [r5_b, r5_t] <s[Q2, Q2];

ghost r6_b5@int16, r6_t5@int16:
      r6_b5 = r6_b /\ r6_t5 = r6_t
   && r6_b5 = r6_b /\ r6_t5 = r6_t;

(* vmov	r10, s19                                   #! PC = 0x400e08 *)
mov [r10_b, r10_t] [s19_b, s19_t]; mov r10 s19;
(* vmov	r11, s20                                   #! PC = 0x400e0c *)
mov [r11_b, r11_t] [s20_b, s20_t]; mov r11 s20;
(* smulwb	lr, r10, r6                              #! PC = 0x400e10 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x400e14 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e18 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400e1c *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x400e20 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov r6_b tmp_b; mov r6_t tmp_t;

assert eqmod r6_b (r6_b5 *  1600) [Q] /\
       eqmod r6_t (r6_t5 *  1600) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r6_b (r6_b5 *  1600) [Q] /\
       eqmod r6_t (r6_t5 *  1600) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r6_b, r6_t] /\ [r6_b, r6_t] <s[Q2, Q2];

ghost r7_b16@int16, r7_t16@int16:
      r7_b16 = r7_b /\ r7_t16 = r7_t
   && r7_b16 = r7_b /\ r7_t16 = r7_t;

(* smulwb	lr, r11, r7                              #! PC = 0x400e24 *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x400e28 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e2c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400e30 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x400e34 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov r7_b tmp_b; mov r7_t tmp_t;

assert eqmod r7_b (r7_b16 *  1432) [Q] /\
       eqmod r7_t (r7_t16 *  1432) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r7_b (r7_b16 *  1432) [Q] /\
       eqmod r7_t (r7_t16 *  1432) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r7_b, r7_t] /\ [r7_b, r7_t] <s[Q2, Q2];

ghost r8_b5@int16, r8_t5@int16:
      r8_b5 = r8_b /\ r8_t5 = r8_t
   && r8_b5 = r8_b /\ r8_t5 = r8_t;

(* vmov	r10, s21                                   #! PC = 0x400e38 *)
mov [r10_b, r10_t] [s21_b, s21_t]; mov r10 s21;
(* vmov	r11, s22                                   #! PC = 0x400e3c *)
mov [r11_b, r11_t] [s22_b, s22_t]; mov r11 s22;
(* smulwb	lr, r10, r8                              #! PC = 0x400e40 *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x400e44 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e48 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400e4c *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x400e50 *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov r8_b tmp_b; mov r8_t tmp_t;

assert eqmod r8_b (r8_b5 *   749) [Q] /\
       eqmod r8_t (r8_t5 *   749) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r8_b (r8_b5 *   749) [Q] /\
       eqmod r8_t (r8_t5 *   749) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r8_b, r8_t] /\ [r8_b, r8_t] <s[Q2, Q2];

ghost r9_b27@int16, r9_t27@int16:
      r9_b27 = r9_b /\ r9_t27 = r9_t
   && r9_b27 = r9_b /\ r9_t27 = r9_t;

(* smulwb	lr, r11, r9                              #! PC = 0x400e54 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x400e58 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e5c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400e60 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x400e64 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov r9_b tmp_b; mov r9_t tmp_t;

assert eqmod r9_b (r9_b27 *   687) [Q] /\
       eqmod r9_t (r9_t27 *   687) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r9_b (r9_b27 *   687) [Q] /\
       eqmod r9_t (r9_t27 *   687) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r9_b, r9_t] /\ [r9_b, r9_t] <s[Q2, Q2];

(* vmov	s0, r2                                     #! PC = 0x400e68 *)
mov [s0_b, s0_t] [r2_b, r2_t];
(* vmov	s1, r3                                     #! PC = 0x400e6c *)
mov [s1_b, s1_t] [r3_b, r3_t];
(* vmov	s2, r4                                     #! PC = 0x400e70 *)
mov [s2_b, s2_t] [r4_b, r4_t];
(* vmov	s3, r5                                     #! PC = 0x400e74 *)
mov [s3_b, s3_t] [r5_b, r5_t];
(* vmov	s4, r6                                     #! PC = 0x400e78 *)
mov [s4_b, s4_t] [r6_b, r6_t];
(* vmov	s5, r7                                     #! PC = 0x400e7c *)
mov [s5_b, s5_t] [r7_b, r7_t];
(* vmov	s6, r8                                     #! PC = 0x400e80 *)
mov [s6_b, s6_t] [r8_b, r8_t];
(* vmov	s7, r9                                     #! PC = 0x400e84 *)
mov [s7_b, s7_t] [r9_b, r9_t];

assert [8*NQ2,8*NQ2]< [s0_b, s0_t] /\ [s0_b, s0_t]< [8*Q2,8*Q2] /\
       [1*NQ2,1*NQ2]< [s1_b, s1_t] /\ [s1_b, s1_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s2_b, s2_t] /\ [s2_b, s2_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s3_b, s3_t] /\ [s3_b, s3_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s4_b, s4_t] /\ [s4_b, s4_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s5_b, s5_t] /\ [s5_b, s5_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s6_b, s6_t] /\ [s6_b, s6_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s7_b, s7_t] /\ [s7_b, s7_t]< [1*Q2,1*Q2]
       prove with [algebra solver isl, precondition] && true;
assume [8*NQ2,8*NQ2]< [s0_b, s0_t] /\ [s0_b, s0_t]< [8*Q2,8*Q2] /\
       [1*NQ2,1*NQ2]< [s1_b, s1_t] /\ [s1_b, s1_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s2_b, s2_t] /\ [s2_b, s2_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s3_b, s3_t] /\ [s3_b, s3_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s4_b, s4_t] /\ [s4_b, s4_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s5_b, s5_t] /\ [s5_b, s5_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s6_b, s6_t] /\ [s6_b, s6_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s7_b, s7_t] /\ [s7_b, s7_t]< [1*Q2,1*Q2]
    && [8@16*NQ2,8@16*NQ2]<s[s0_b, s0_t] /\ [s0_b, s0_t]<s[8@16*Q2,8@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s1_b, s1_t] /\ [s1_b, s1_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s2_b, s2_t] /\ [s2_b, s2_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s3_b, s3_t] /\ [s3_b, s3_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s4_b, s4_t] /\ [s4_b, s4_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s5_b, s5_t] /\ [s5_b, s5_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s6_b, s6_t] /\ [s6_b, s6_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s7_b, s7_t] /\ [s7_b, s7_t]<s[1@16*Q2,1@16*Q2];

(* CUT 10 *)
ghost Z5@int16: X**2 =  -848*17** 27*Z5 && true;
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod (poly Z5 [poly X [s0_b, s0_t], poly X [s1_b, s1_t],
                    poly X [s2_b, s2_t], poly X [s3_b, s3_t],
                    poly X [s4_b, s4_t], poly X [s5_b, s5_t],
                    poly X [s6_b, s6_t], poly X [s7_b, s7_t]])
          (2**3*F**2) [Q, Z5**8 + 1] /\
    [8*NQ2,8*NQ2]< [s0_b, s0_t] /\ [s0_b, s0_t]< [8*Q2,8*Q2] /\
    [1*NQ2,1*NQ2]< [s1_b, s1_t] /\ [s1_b, s1_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s2_b, s2_t] /\ [s2_b, s2_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s3_b, s3_t] /\ [s3_b, s3_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s4_b, s4_t] /\ [s4_b, s4_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s5_b, s5_t] /\ [s5_b, s5_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s6_b, s6_t] /\ [s6_b, s6_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s7_b, s7_t] /\ [s7_b, s7_t]< [1*Q2,1*Q2]
    prove with [precondition]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [8@16*NQ2,8@16*NQ2]<s[s0_b, s0_t] /\ [s0_b, s0_t]<s[8@16*Q2,8@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s1_b, s1_t] /\ [s1_b, s1_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s2_b, s2_t] /\ [s2_b, s2_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s3_b, s3_t] /\ [s3_b, s3_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s4_b, s4_t] /\ [s4_b, s4_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s5_b, s5_t] /\ [s5_b, s5_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s6_b, s6_t] /\ [s6_b, s6_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s7_b, s7_t] /\ [s7_b, s7_t]<s[1@16*Q2,1@16*Q2]
    prove with [precondition];

(* vmov	r0, s23                                    #! PC = 0x400e88 *)
mov [r0_b, r0_t] [s23_b, s23_t];
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff304; Value = 0xb3c1854e; PC = 0x400e8c *)
mov [r2_b, r2_t] [L0xbefff304, L0xbefff306];
(* ldr.w	r3, [r0, #4]                              #! EA = L0xbefff308; Value = 0xb95388ee; PC = 0x400e90 *)
mov [r3_b, r3_t] [L0xbefff308, L0xbefff30a];
(* ldr.w	r4, [r0, #8]                              #! EA = L0xbefff30c; Value = 0xb9029770; PC = 0x400e94 *)
mov [r4_b, r4_t] [L0xbefff30c, L0xbefff30e];
(* ldr.w	r5, [r0, #12]                             #! EA = L0xbefff310; Value = 0xb9e69024; PC = 0x400e98 *)
mov [r5_b, r5_t] [L0xbefff310, L0xbefff312];
(* ldr.w	r6, [r0, #16]                             #! EA = L0xbefff314; Value = 0xbc59957a; PC = 0x400e9c *)
mov [r6_b, r6_t] [L0xbefff314, L0xbefff316];
(* ldr.w	r7, [r0, #20]                             #! EA = L0xbefff318; Value = 0xc6ed9724; PC = 0x400ea0 *)
mov [r7_b, r7_t] [L0xbefff318, L0xbefff31a];
(* ldr.w	r8, [r0, #24]                             #! EA = L0xbefff31c; Value = 0xaeec8534; PC = 0x400ea4 *)
mov [r8_b, r8_t] [L0xbefff31c, L0xbefff31e];
(* ldr.w	r9, [r0, #28]                             #! EA = L0xbefff320; Value = 0xbafa8d7e; PC = 0x400ea8 *)
mov [r9_b, r9_t] [L0xbefff320, L0xbefff322];
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400eac *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* sadd16	lr, r2, r3                               #! PC = 0x400eb0 *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* ssub16	r3, r2, r3                               #! PC = 0x400eb4 *)
sub r3_b r2_b r3_b;
sub r3_t r2_t r3_t;
(* sadd16	r11, r4, r5                              #! PC = 0x400eb8 *)
add r11_b r4_b r5_b;
add r11_t r4_t r5_t;
(* ssub16	r5, r4, r5                               #! PC = 0x400ebc *)
sub r5_b r4_b r5_b;
sub r5_t r4_t r5_t;
(* sadd16	r2, r6, r7                               #! PC = 0x400ec0 *)
add r2_b r6_b r7_b;
add r2_t r6_t r7_t;
(* ssub16	r7, r6, r7                               #! PC = 0x400ec4 *)
sub r7_b r6_b r7_b;
sub r7_t r6_t r7_t;
(* sadd16	r4, r8, r9                               #! PC = 0x400ec8 *)
add r4_b r8_b r9_b;
add r4_t r8_t r9_t;
(* ssub16	r9, r8, r9                               #! PC = 0x400ecc *)
sub r9_b r8_b r9_b;
sub r9_t r8_t r9_t;
(* sadd16	r8, lr, r11                              #! PC = 0x400ed0 *)
add r8_b lr_b r11_b;
add r8_t lr_t r11_t;
(* ssub16	r11, lr, r11                             #! PC = 0x400ed4 *)
sub r11_b lr_b r11_b;
sub r11_t lr_t r11_t;
(* sadd16	r6, r2, r4                               #! PC = 0x400ed8 *)
add r6_b r2_b r4_b;
add r6_t r2_t r4_t;
(* ssub16	r4, r2, r4                               #! PC = 0x400edc *)
sub r4_b r2_b r4_b;
sub r4_t r2_t r4_t;

ghost r5_b17@int16, r5_t17@int16:
      r5_b17 = r5_b /\ r5_t17 = r5_t
   && r5_b17 = r5_b /\ r5_t17 = r5_t;

(* vmov	r10, s10                                   #! PC = 0x400ee0 *)
mov [r10_b, r10_t] [s10_b, s10_t]; mov r10 s10;
(* smulwb	lr, r10, r5                              #! PC = 0x400ee4 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x400ee8 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400eec *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400ef0 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400ef4 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_b17 *  1600) [Q] /\
       eqmod lr_t (r5_t17 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_b17 *  1600) [Q] /\
       eqmod lr_t (r5_t17 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x400ef8 *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400efc *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r9_b28@int16, r9_t28@int16:
      r9_b28 = r9_b /\ r9_t28 = r9_t
   && r9_b28 = r9_b /\ r9_t28 = r9_t;

(* smulwb	lr, r10, r9                              #! PC = 0x400f00 *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400f04 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f08 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400f0c *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400f10 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b28 *  1600) [Q] /\
       eqmod lr_t (r9_t28 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b28 *  1600) [Q] /\
       eqmod lr_t (r9_t28 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x400f14 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400f18 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* sadd16	r2, r8, r6                               #! PC = 0x400f1c *)
add r2_b r8_b r6_b;
add r2_t r8_t r6_t;
(* ssub16	r6, r8, r6                               #! PC = 0x400f20 *)
sub r6_b r8_b r6_b;
sub r6_t r8_t r6_t;

ghost r7_b17@int16, r7_t17@int16:
      r7_b17 = r7_b /\ r7_t17 = r7_t
   && r7_b17 = r7_b /\ r7_t17 = r7_t;

(* vmov	r10, s12                                   #! PC = 0x400f24 *)
mov [r10_b, r10_t] [s12_b, s12_t]; mov r10 s12;
(* smulwb	lr, r10, r7                              #! PC = 0x400f28 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x400f2c *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f30 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400f34 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400f38 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_b17 *    40) [Q] /\
       eqmod lr_t (r7_t17 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_b17 *    40) [Q] /\
       eqmod lr_t (r7_t17 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x400f3c *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400f40 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r4_b17@int16, r4_t17@int16:
      r4_b17 = r4_b /\ r4_t17 = r4_t
   && r4_b17 = r4_b /\ r4_t17 = r4_t;

(* vmov	r10, s13                                   #! PC = 0x400f44 *)
mov [r10_b, r10_t] [s13_b, s13_t]; mov r10 s13;
(* smulwb	lr, r10, r4                              #! PC = 0x400f48 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x400f4c *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f50 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400f54 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400f58 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_b17 *  1600) [Q] /\
       eqmod lr_t (r4_t17 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_b17 *  1600) [Q] /\
       eqmod lr_t (r4_t17 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* ssub16	r8, r11, lr                              #! PC = 0x400f5c *)
sub r8_b r11_b lr_b;
sub r8_t r11_t lr_t;
(* sadd16	r4, r11, lr                              #! PC = 0x400f60 *)
add r4_b r11_b lr_b;
add r4_t r11_t lr_t;

ghost r9_b29@int16, r9_t29@int16:
      r9_b29 = r9_b /\ r9_t29 = r9_t
   && r9_b29 = r9_b /\ r9_t29 = r9_t;

(* vmov	r10, s14                                   #! PC = 0x400f64 *)
mov [r10_b, r10_t] [s14_b, s14_t]; mov r10 s14;
(* smulwb	lr, r10, r9                              #! PC = 0x400f68 *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400f6c *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f70 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400f74 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400f78 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b29 *   749) [Q] /\
       eqmod lr_t (r9_t29 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b29 *   749) [Q] /\
       eqmod lr_t (r9_t29 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x400f7c *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400f80 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* vmov	r0, s23                                    #! PC = 0x400f84 *)
mov [r0_b, r0_t] [s23_b, s23_t];
(* vmov	r11, s1                                    #! PC = 0x400f88 *)
mov [r11_b, r11_t] [s1_b, s1_t];
(* uadd16	lr, r3, r11                              #! PC = 0x400f8c *)
add lr_b r3_b r11_b;
add lr_t r3_t r11_t;
(* usub16	r3, r3, r11                              #! PC = 0x400f90 *)
sub r3_b r3_b r11_b;
sub r3_t r3_t r11_t;
(* str.w	lr, [r0, #4]                              #! EA = L0xbefff308; PC = 0x400f94 *)
mov [L0xbefff308, L0xbefff30a] [lr_b, lr_t];
(* str.w	r3, [r0, #36]	; 0x24                      #! EA = L0xbefff328; PC = 0x400f98 *)
mov [L0xbefff328, L0xbefff32a] [r3_b, r3_t];
(* vmov	r11, s3                                    #! PC = 0x400f9c *)
mov [r11_b, r11_t] [s3_b, s3_t];
(* uadd16	lr, r5, r11                              #! PC = 0x400fa0 *)
add lr_b r5_b r11_b;
add lr_t r5_t r11_t;
(* usub16	r5, r5, r11                              #! PC = 0x400fa4 *)
sub r5_b r5_b r11_b;
sub r5_t r5_t r11_t;
(* str.w	lr, [r0, #12]                             #! EA = L0xbefff310; PC = 0x400fa8 *)
mov [L0xbefff310, L0xbefff312] [lr_b, lr_t];
(* str.w	r5, [r0, #44]	; 0x2c                      #! EA = L0xbefff330; PC = 0x400fac *)
mov [L0xbefff330, L0xbefff332] [r5_b, r5_t];
(* vmov	r11, s5                                    #! PC = 0x400fb0 *)
mov [r11_b, r11_t] [s5_b, s5_t];
(* uadd16	lr, r7, r11                              #! PC = 0x400fb4 *)
add lr_b r7_b r11_b;
add lr_t r7_t r11_t;
(* usub16	r7, r7, r11                              #! PC = 0x400fb8 *)
sub r7_b r7_b r11_b;
sub r7_t r7_t r11_t;
(* str.w	lr, [r0, #20]                             #! EA = L0xbefff318; PC = 0x400fbc *)
mov [L0xbefff318, L0xbefff31a] [lr_b, lr_t];
(* str.w	r7, [r0, #52]	; 0x34                      #! EA = L0xbefff338; PC = 0x400fc0 *)
mov [L0xbefff338, L0xbefff33a] [r7_b, r7_t];
(* vmov	r11, s7                                    #! PC = 0x400fc4 *)
mov [r11_b, r11_t] [s7_b, s7_t];
(* uadd16	lr, r9, r11                              #! PC = 0x400fc8 *)
add lr_b r9_b r11_b;
add lr_t r9_t r11_t;
(* usub16	r9, r9, r11                              #! PC = 0x400fcc *)
sub r9_b r9_b r11_b;
sub r9_t r9_t r11_t;
(* str.w	lr, [r0, #28]                             #! EA = L0xbefff320; PC = 0x400fd0 *)
mov [L0xbefff320, L0xbefff322] [lr_b, lr_t];
(* str.w	r9, [r0, #60]	; 0x3c                      #! EA = L0xbefff340; PC = 0x400fd4 *)
mov [L0xbefff340, L0xbefff342] [r9_b, r9_t];
(* vmov	r5, s2                                     #! PC = 0x400fd8 *)
mov [r5_b, r5_t] [s2_b, s2_t];
(* uadd16	lr, r4, r5                               #! PC = 0x400fdc *)
add lr_b r4_b r5_b;
add lr_t r4_t r5_t;
(* usub16	r11, r4, r5                              #! PC = 0x400fe0 *)
sub r11_b r4_b r5_b;
sub r11_t r4_t r5_t;
(* str.w	lr, [r0, #8]                              #! EA = L0xbefff30c; PC = 0x400fe4 *)
mov [L0xbefff30c, L0xbefff30e] [lr_b, lr_t];
(* str.w	r11, [r0, #40]	; 0x28                     #! EA = L0xbefff32c; PC = 0x400fe8 *)
mov [L0xbefff32c, L0xbefff32e] [r11_b, r11_t];
(* vmov	r7, s4                                     #! PC = 0x400fec *)
mov [r7_b, r7_t] [s4_b, s4_t];
(* uadd16	lr, r6, r7                               #! PC = 0x400ff0 *)
add lr_b r6_b r7_b;
add lr_t r6_t r7_t;
(* usub16	r11, r6, r7                              #! PC = 0x400ff4 *)
sub r11_b r6_b r7_b;
sub r11_t r6_t r7_t;
(* str.w	lr, [r0, #16]                             #! EA = L0xbefff314; PC = 0x400ff8 *)
mov [L0xbefff314, L0xbefff316] [lr_b, lr_t];
(* str.w	r11, [r0, #48]	; 0x30                     #! EA = L0xbefff334; PC = 0x400ffc *)
mov [L0xbefff334, L0xbefff336] [r11_b, r11_t];
(* vmov	r9, s6                                     #! PC = 0x401000 *)
mov [r9_b, r9_t] [s6_b, s6_t];
(* uadd16	lr, r8, r9                               #! PC = 0x401004 *)
add lr_b r8_b r9_b;
add lr_t r8_t r9_t;
(* usub16	r11, r8, r9                              #! PC = 0x401008 *)
sub r11_b r8_b r9_b;
sub r11_t r8_t r9_t;
(* str.w	lr, [r0, #24]                             #! EA = L0xbefff31c; PC = 0x40100c *)
mov [L0xbefff31c, L0xbefff31e] [lr_b, lr_t];
(* str.w	r11, [r0, #56]	; 0x38                     #! EA = L0xbefff33c; PC = 0x401010 *)
mov [L0xbefff33c, L0xbefff33e] [r11_b, r11_t];
(* vmov	r3, s0                                     #! PC = 0x401014 *)
mov [r3_b, r3_t] [s0_b, s0_t];
(* uadd16	lr, r2, r3                               #! PC = 0x401018 *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* usub16	r11, r2, r3                              #! PC = 0x40101c *)
sub r11_b r2_b r3_b;
sub r11_t r2_t r3_t;
(* str.w	r11, [r0, #32]                            #! EA = L0xbefff324; PC = 0x401020 *)
mov [L0xbefff324, L0xbefff326] [r11_b, r11_t];
(* str.w	lr, [r0], #64                             #! EA = L0xbefff304; PC = 0x401024 *)
mov [L0xbefff304, L0xbefff306] [lr_b, lr_t];
(* vmov	lr, s8                                     #! PC = 0x401028 *)
mov [lr_b, lr_t] [s8_b, s8_t]; mov lr s8;
(* cmp.w	r0, lr                                    #! PC = 0x40102c *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x400cc4 <invntt_fast+24>                #! PC = 0x401030 *)
#bne.w	0x400cc4 <invntt_fast+24>                #! 0x401030 = 0x401030;

(* CUT 11 *)
assert [16*NQ2,16*NQ2,5*NQ2]< [L0xbefff304,L0xbefff306,L0xbefff308] /\
       [L0xbefff304,L0xbefff306,L0xbefff308]< [16*Q2,16*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff30a,L0xbefff30c,L0xbefff30e] /\
       [L0xbefff30a,L0xbefff30c,L0xbefff30e]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2,9*NQ2]< [L0xbefff310,L0xbefff312,L0xbefff314] /\
       [L0xbefff310,L0xbefff312,L0xbefff314]< [5*Q2,5*Q2,9*Q2] /\
       [9*NQ2,5*NQ2,5*NQ2]< [L0xbefff316,L0xbefff318,L0xbefff31a] /\
       [L0xbefff316,L0xbefff318,L0xbefff31a]< [9*Q2,5*Q2,5*Q2] /\
       [6*NQ2,6*NQ2,5*NQ2]< [L0xbefff31c,L0xbefff31e,L0xbefff320] /\
       [L0xbefff31c,L0xbefff31e,L0xbefff320]< [6*Q2,6*Q2,5*Q2] /\
       [5*NQ2,16*NQ2,16*NQ2]< [L0xbefff322,L0xbefff324,L0xbefff326] /\
       [L0xbefff322,L0xbefff324,L0xbefff326]< [5*Q2,16*Q2,16*Q2] /\
       [5*NQ2,5*NQ2,6*NQ2]< [L0xbefff328,L0xbefff32a,L0xbefff32c] /\
       [L0xbefff328,L0xbefff32a,L0xbefff32c]< [5*Q2,5*Q2,6*Q2] /\
       [6*NQ2,5*NQ2,5*NQ2]< [L0xbefff32e,L0xbefff330,L0xbefff332] /\
       [L0xbefff32e,L0xbefff330,L0xbefff332]< [6*Q2,5*Q2,5*Q2] /\
       [9*NQ2,9*NQ2,5*NQ2]< [L0xbefff334,L0xbefff336,L0xbefff338] /\
       [L0xbefff334,L0xbefff336,L0xbefff338]< [9*Q2,9*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff33a,L0xbefff33c,L0xbefff33e] /\
       [L0xbefff33a,L0xbefff33c,L0xbefff33e]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2]< [L0xbefff340,L0xbefff342] /\
       [L0xbefff340,L0xbefff342]< [5*Q2,5*Q2]
       prove with [algebra solver isl, precondition] && true;
assume [16*NQ2,16*NQ2,5*NQ2]< [L0xbefff304,L0xbefff306,L0xbefff308] /\
       [L0xbefff304,L0xbefff306,L0xbefff308]< [16*Q2,16*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff30a,L0xbefff30c,L0xbefff30e] /\
       [L0xbefff30a,L0xbefff30c,L0xbefff30e]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2,9*NQ2]< [L0xbefff310,L0xbefff312,L0xbefff314] /\
       [L0xbefff310,L0xbefff312,L0xbefff314]< [5*Q2,5*Q2,9*Q2] /\
       [9*NQ2,5*NQ2,5*NQ2]< [L0xbefff316,L0xbefff318,L0xbefff31a] /\
       [L0xbefff316,L0xbefff318,L0xbefff31a]< [9*Q2,5*Q2,5*Q2] /\
       [6*NQ2,6*NQ2,5*NQ2]< [L0xbefff31c,L0xbefff31e,L0xbefff320] /\
       [L0xbefff31c,L0xbefff31e,L0xbefff320]< [6*Q2,6*Q2,5*Q2] /\
       [5*NQ2,16*NQ2,16*NQ2]< [L0xbefff322,L0xbefff324,L0xbefff326] /\
       [L0xbefff322,L0xbefff324,L0xbefff326]< [5*Q2,16*Q2,16*Q2] /\
       [5*NQ2,5*NQ2,6*NQ2]< [L0xbefff328,L0xbefff32a,L0xbefff32c] /\
       [L0xbefff328,L0xbefff32a,L0xbefff32c]< [5*Q2,5*Q2,6*Q2] /\
       [6*NQ2,5*NQ2,5*NQ2]< [L0xbefff32e,L0xbefff330,L0xbefff332] /\
       [L0xbefff32e,L0xbefff330,L0xbefff332]< [6*Q2,5*Q2,5*Q2] /\
       [9*NQ2,9*NQ2,5*NQ2]< [L0xbefff334,L0xbefff336,L0xbefff338] /\
       [L0xbefff334,L0xbefff336,L0xbefff338]< [9*Q2,9*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff33a,L0xbefff33c,L0xbefff33e] /\
       [L0xbefff33a,L0xbefff33c,L0xbefff33e]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2]< [L0xbefff340,L0xbefff342] /\
       [L0xbefff340,L0xbefff342]< [5*Q2,5*Q2]
    && [16@16*NQ2,16@16*NQ2,5@16*NQ2]< [L0xbefff304,L0xbefff306,L0xbefff308] /\
       [L0xbefff304,L0xbefff306,L0xbefff308]< [16@16*Q2,16@16*Q2,5@16*Q2] /\
       [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff30a,L0xbefff30c,L0xbefff30e] /\
       [L0xbefff30a,L0xbefff30c,L0xbefff30e]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
       [5@16*NQ2,5@16*NQ2,9@16*NQ2]< [L0xbefff310,L0xbefff312,L0xbefff314] /\
       [L0xbefff310,L0xbefff312,L0xbefff314]< [5@16*Q2,5@16*Q2,9@16*Q2] /\
       [9@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff316,L0xbefff318,L0xbefff31a] /\
       [L0xbefff316,L0xbefff318,L0xbefff31a]< [9@16*Q2,5@16*Q2,5@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,5@16*NQ2]< [L0xbefff31c,L0xbefff31e,L0xbefff320] /\
       [L0xbefff31c,L0xbefff31e,L0xbefff320]< [6@16*Q2,6@16*Q2,5@16*Q2] /\
       [5@16*NQ2,16@16*NQ2,16@16*NQ2]< [L0xbefff322,L0xbefff324,L0xbefff326] /\
       [L0xbefff322,L0xbefff324,L0xbefff326]< [5@16*Q2,16@16*Q2,16@16*Q2] /\
       [5@16*NQ2,5@16*NQ2,6@16*NQ2]< [L0xbefff328,L0xbefff32a,L0xbefff32c] /\
       [L0xbefff328,L0xbefff32a,L0xbefff32c]< [5@16*Q2,5@16*Q2,6@16*Q2] /\
       [6@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff32e,L0xbefff330,L0xbefff332] /\
       [L0xbefff32e,L0xbefff330,L0xbefff332]< [6@16*Q2,5@16*Q2,5@16*Q2] /\
       [9@16*NQ2,9@16*NQ2,5@16*NQ2]< [L0xbefff334,L0xbefff336,L0xbefff338] /\
       [L0xbefff334,L0xbefff336,L0xbefff338]< [9@16*Q2,9@16*Q2,5@16*Q2] /\
       [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff33a,L0xbefff33c,L0xbefff33e] /\
       [L0xbefff33a,L0xbefff33c,L0xbefff33e]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
       [5@16*NQ2,5@16*NQ2]< [L0xbefff340,L0xbefff342] /\
       [L0xbefff340,L0xbefff342]< [5@16*Q2,5@16*Q2];
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod (poly Z5
           [poly X [L0xbefff304,L0xbefff306],poly X [L0xbefff308,L0xbefff30a],
            poly X [L0xbefff30c,L0xbefff30e],poly X [L0xbefff310,L0xbefff312],
            poly X [L0xbefff314,L0xbefff316],poly X [L0xbefff318,L0xbefff31a],
            poly X [L0xbefff31c,L0xbefff31e],poly X [L0xbefff320,L0xbefff322],
            poly X [L0xbefff324,L0xbefff326],poly X [L0xbefff328,L0xbefff32a],
            poly X [L0xbefff32c,L0xbefff32e],poly X [L0xbefff330,L0xbefff332],
            poly X [L0xbefff334,L0xbefff336],poly X [L0xbefff338,L0xbefff33a],
            poly X [L0xbefff33c,L0xbefff33e],poly X [L0xbefff340,L0xbefff342]])
          (2**4*F**2) [Q, Z5**16 - 1] /\
    [16*NQ2,16*NQ2,5*NQ2]< [L0xbefff304,L0xbefff306,L0xbefff308] /\
    [L0xbefff304,L0xbefff306,L0xbefff308]< [16*Q2,16*Q2,5*Q2] /\
    [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff30a,L0xbefff30c,L0xbefff30e] /\
    [L0xbefff30a,L0xbefff30c,L0xbefff30e]< [5*Q2,6*Q2,6*Q2] /\
    [5*NQ2,5*NQ2,9*NQ2]< [L0xbefff310,L0xbefff312,L0xbefff314] /\
    [L0xbefff310,L0xbefff312,L0xbefff314]< [5*Q2,5*Q2,9*Q2] /\
    [9*NQ2,5*NQ2,5*NQ2]< [L0xbefff316,L0xbefff318,L0xbefff31a] /\
    [L0xbefff316,L0xbefff318,L0xbefff31a]< [9*Q2,5*Q2,5*Q2] /\
    [6*NQ2,6*NQ2,5*NQ2]< [L0xbefff31c,L0xbefff31e,L0xbefff320] /\
    [L0xbefff31c,L0xbefff31e,L0xbefff320]< [6*Q2,6*Q2,5*Q2] /\
    [5*NQ2,16*NQ2,16*NQ2]< [L0xbefff322,L0xbefff324,L0xbefff326] /\
    [L0xbefff322,L0xbefff324,L0xbefff326]< [5*Q2,16*Q2,16*Q2] /\
    [5*NQ2,5*NQ2,6*NQ2]< [L0xbefff328,L0xbefff32a,L0xbefff32c] /\
    [L0xbefff328,L0xbefff32a,L0xbefff32c]< [5*Q2,5*Q2,6*Q2] /\
    [6*NQ2,5*NQ2,5*NQ2]< [L0xbefff32e,L0xbefff330,L0xbefff332] /\
    [L0xbefff32e,L0xbefff330,L0xbefff332]< [6*Q2,5*Q2,5*Q2] /\
    [9*NQ2,9*NQ2,5*NQ2]< [L0xbefff334,L0xbefff336,L0xbefff338] /\
    [L0xbefff334,L0xbefff336,L0xbefff338]< [9*Q2,9*Q2,5*Q2] /\
    [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff33a,L0xbefff33c,L0xbefff33e] /\
    [L0xbefff33a,L0xbefff33c,L0xbefff33e]< [5*Q2,6*Q2,6*Q2] /\
    [5*NQ2,5*NQ2]< [L0xbefff340,L0xbefff342] /\
    [L0xbefff340,L0xbefff342]< [5*Q2,5*Q2]
    prove with [precondition, all ghosts]
 && [16@16*NQ2,16@16*NQ2,5@16*NQ2]< [L0xbefff304,L0xbefff306,L0xbefff308] /\
    [L0xbefff304,L0xbefff306,L0xbefff308]< [16@16*Q2,16@16*Q2,5@16*Q2] /\
    [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff30a,L0xbefff30c,L0xbefff30e] /\
    [L0xbefff30a,L0xbefff30c,L0xbefff30e]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
    [5@16*NQ2,5@16*NQ2,9@16*NQ2]< [L0xbefff310,L0xbefff312,L0xbefff314] /\
    [L0xbefff310,L0xbefff312,L0xbefff314]< [5@16*Q2,5@16*Q2,9@16*Q2] /\
    [9@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff316,L0xbefff318,L0xbefff31a] /\
    [L0xbefff316,L0xbefff318,L0xbefff31a]< [9@16*Q2,5@16*Q2,5@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,5@16*NQ2]< [L0xbefff31c,L0xbefff31e,L0xbefff320] /\
    [L0xbefff31c,L0xbefff31e,L0xbefff320]< [6@16*Q2,6@16*Q2,5@16*Q2] /\
    [5@16*NQ2,16@16*NQ2,16@16*NQ2]< [L0xbefff322,L0xbefff324,L0xbefff326] /\
    [L0xbefff322,L0xbefff324,L0xbefff326]< [5@16*Q2,16@16*Q2,16@16*Q2] /\
    [5@16*NQ2,5@16*NQ2,6@16*NQ2]< [L0xbefff328,L0xbefff32a,L0xbefff32c] /\
    [L0xbefff328,L0xbefff32a,L0xbefff32c]< [5@16*Q2,5@16*Q2,6@16*Q2] /\
    [6@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff32e,L0xbefff330,L0xbefff332] /\
    [L0xbefff32e,L0xbefff330,L0xbefff332]< [6@16*Q2,5@16*Q2,5@16*Q2] /\
    [9@16*NQ2,9@16*NQ2,5@16*NQ2]< [L0xbefff334,L0xbefff336,L0xbefff338] /\
    [L0xbefff334,L0xbefff336,L0xbefff338]< [9@16*Q2,9@16*Q2,5@16*Q2] /\
    [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff33a,L0xbefff33c,L0xbefff33e] /\
    [L0xbefff33a,L0xbefff33c,L0xbefff33e]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
    [5@16*NQ2,5@16*NQ2]< [L0xbefff340,L0xbefff342] /\
    [L0xbefff340,L0xbefff342]< [5@16*Q2,5@16*Q2]
    prove with [precondition];

(* vmov	s23, r0                                    #! PC = 0x400cc4 *)
mov [s23_b, s23_t] [r0_b, r0_t];
(* ldr.w	r2, [r0, #32]                             #! EA = L0xbefff364; Value = 0xbfbe8e76; PC = 0x400cc8 *)
mov [r2_b, r2_t] [L0xbefff364, L0xbefff366];
(* ldr.w	r3, [r0, #36]	; 0x24                      #! EA = L0xbefff368; Value = 0xb3888f16; PC = 0x400ccc *)
mov [r3_b, r3_t] [L0xbefff368, L0xbefff36a];
(* ldr.w	r4, [r0, #40]	; 0x28                      #! EA = L0xbefff36c; Value = 0xc1b49832; PC = 0x400cd0 *)
mov [r4_b, r4_t] [L0xbefff36c, L0xbefff36e];
(* ldr.w	r5, [r0, #44]	; 0x2c                      #! EA = L0xbefff370; Value = 0xc3be9bfe; PC = 0x400cd4 *)
mov [r5_b, r5_t] [L0xbefff370, L0xbefff372];
(* ldr.w	r6, [r0, #48]	; 0x30                      #! EA = L0xbefff374; Value = 0xbd0b8ab5; PC = 0x400cd8 *)
mov [r6_b, r6_t] [L0xbefff374, L0xbefff376];
(* ldr.w	r7, [r0, #52]	; 0x34                      #! EA = L0xbefff378; Value = 0xb68581fb; PC = 0x400cdc *)
mov [r7_b, r7_t] [L0xbefff378, L0xbefff37a];
(* ldr.w	r8, [r0, #56]	; 0x38                      #! EA = L0xbefff37c; Value = 0xb32e8a8a; PC = 0x400ce0 *)
mov [r8_b, r8_t] [L0xbefff37c, L0xbefff37e];
(* ldr.w	r9, [r0, #60]	; 0x3c                      #! EA = L0xbefff380; Value = 0xb6728faa; PC = 0x400ce4 *)
mov [r9_b, r9_t] [L0xbefff380, L0xbefff382];
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400ce8 *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* sadd16	lr, r2, r3                               #! PC = 0x400cec *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* ssub16	r3, r2, r3                               #! PC = 0x400cf0 *)
sub r3_b r2_b r3_b;
sub r3_t r2_t r3_t;
(* sadd16	r11, r4, r5                              #! PC = 0x400cf4 *)
add r11_b r4_b r5_b;
add r11_t r4_t r5_t;
(* ssub16	r5, r4, r5                               #! PC = 0x400cf8 *)
sub r5_b r4_b r5_b;
sub r5_t r4_t r5_t;
(* sadd16	r2, r6, r7                               #! PC = 0x400cfc *)
add r2_b r6_b r7_b;
add r2_t r6_t r7_t;
(* ssub16	r7, r6, r7                               #! PC = 0x400d00 *)
sub r7_b r6_b r7_b;
sub r7_t r6_t r7_t;
(* sadd16	r4, r8, r9                               #! PC = 0x400d04 *)
add r4_b r8_b r9_b;
add r4_t r8_t r9_t;
(* ssub16	r9, r8, r9                               #! PC = 0x400d08 *)
sub r9_b r8_b r9_b;
sub r9_t r8_t r9_t;
(* sadd16	r8, lr, r11                              #! PC = 0x400d0c *)
add r8_b lr_b r11_b;
add r8_t lr_t r11_t;
(* ssub16	r11, lr, r11                             #! PC = 0x400d10 *)
sub r11_b lr_b r11_b;
sub r11_t lr_t r11_t;
(* sadd16	r6, r2, r4                               #! PC = 0x400d14 *)
add r6_b r2_b r4_b;
add r6_t r2_t r4_t;
(* ssub16	r4, r2, r4                               #! PC = 0x400d18 *)
sub r4_b r2_b r4_b;
sub r4_t r2_t r4_t;

ghost r5_b18@int16, r5_t18@int16:
      r5_b18 = r5_b /\ r5_t18 = r5_t
   && r5_b18 = r5_b /\ r5_t18 = r5_t;

(* vmov	r10, s10                                   #! PC = 0x400d1c *)
mov [r10_b, r10_t] [s10_b, s10_t]; mov r10 s10;
(* smulwb	lr, r10, r5                              #! PC = 0x400d20 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x400d24 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d28 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400d2c *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400d30 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_b18 *  1600) [Q] /\
       eqmod lr_t (r5_t18 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_b18 *  1600) [Q] /\
       eqmod lr_t (r5_t18 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x400d34 *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400d38 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r9_b30@int16, r9_t30@int16:
      r9_b30 = r9_b /\ r9_t30 = r9_t
   && r9_b30 = r9_b /\ r9_t30 = r9_t;

(* smulwb	lr, r10, r9                              #! PC = 0x400d3c *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400d40 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d44 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400d48 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400d4c *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b30 *  1600) [Q] /\
       eqmod lr_t (r9_t30 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b30 *  1600) [Q] /\
       eqmod lr_t (r9_t30 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x400d50 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400d54 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* sadd16	r2, r8, r6                               #! PC = 0x400d58 *)
add r2_b r8_b r6_b;
add r2_t r8_t r6_t;
(* ssub16	r6, r8, r6                               #! PC = 0x400d5c *)
sub r6_b r8_b r6_b;
sub r6_t r8_t r6_t;

ghost r7_b18@int16, r7_t18@int16:
      r7_b18 = r7_b /\ r7_t18 = r7_t
   && r7_b18 = r7_b /\ r7_t18 = r7_t;

(* vmov	r10, s12                                   #! PC = 0x400d60 *)
mov [r10_b, r10_t] [s12_b, s12_t]; mov r10 s12;
(* smulwb	lr, r10, r7                              #! PC = 0x400d64 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x400d68 *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d6c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400d70 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400d74 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_b18 *    40) [Q] /\
       eqmod lr_t (r7_t18 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_b18 *    40) [Q] /\
       eqmod lr_t (r7_t18 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x400d78 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400d7c *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r4_b18@int16, r4_t18@int16:
      r4_b18 = r4_b /\ r4_t18 = r4_t
   && r4_b18 = r4_b /\ r4_t18 = r4_t;

(* vmov	r10, s13                                   #! PC = 0x400d80 *)
mov [r10_b, r10_t] [s13_b, s13_t]; mov r10 s13;
(* smulwb	lr, r10, r4                              #! PC = 0x400d84 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x400d88 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d8c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400d90 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400d94 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_b18 *  1600) [Q] /\
       eqmod lr_t (r4_t18 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_b18 *  1600) [Q] /\
       eqmod lr_t (r4_t18 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* ssub16	r8, r11, lr                              #! PC = 0x400d98 *)
sub r8_b r11_b lr_b;
sub r8_t r11_t lr_t;
(* sadd16	r4, r11, lr                              #! PC = 0x400d9c *)
add r4_b r11_b lr_b;
add r4_t r11_t lr_t;

ghost r9_b31@int16, r9_t31@int16:
      r9_b31 = r9_b /\ r9_t31 = r9_t
   && r9_b31 = r9_b /\ r9_t31 = r9_t;

(* vmov	r10, s14                                   #! PC = 0x400da0 *)
mov [r10_b, r10_t] [s14_b, s14_t]; mov r10 s14;
(* smulwb	lr, r10, r9                              #! PC = 0x400da4 *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400da8 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400dac *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400db0 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400db4 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b31 *   749) [Q] /\
       eqmod lr_t (r9_t31 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b31 *   749) [Q] /\
       eqmod lr_t (r9_t31 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x400db8 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400dbc *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;

ghost r3_b6@int16, r3_t6@int16:
      r3_b6 = r3_b /\ r3_t6 = r3_t
   && r3_b6 = r3_b /\ r3_t6 = r3_t;

(* vmov	r11, s16                                   #! PC = 0x400dc0 *)
mov [r11_b, r11_t] [s16_b, s16_t]; mov r11 s16;
(* smulwb	lr, r11, r3                              #! PC = 0x400dc4 *)
vpc r3_bW@int32 r3_b; mulj tmp r11 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r11, r3                              #! PC = 0x400dc8 *)
vpc r3_tW@int32 r3_t; mulj tmp r11 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400dcc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400dd0 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x400dd4 *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov r3_b tmp_b; mov r3_t tmp_t;

assert eqmod r3_b (r3_b6 *  -848) [Q] /\
       eqmod r3_t (r3_t6 *  -848) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r3_b (r3_b6 *  -848) [Q] /\
       eqmod r3_t (r3_t6 *  -848) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r3_b, r3_t] /\ [r3_b, r3_t] <s[Q2, Q2];


ghost r4_b19@int16, r4_t19@int16:
      r4_b19 = r4_b /\ r4_t19 = r4_t
   && r4_b19 = r4_b /\ r4_t19 = r4_t;

(* vmov	r10, s17                                   #! PC = 0x400dd8 *)
mov [r10_b, r10_t] [s17_b, s17_t]; mov r10 s17;
(* vmov	r11, s18                                   #! PC = 0x400ddc *)
mov [r11_b, r11_t] [s18_b, s18_t]; mov r11 s18;
(* smulwb	lr, r10, r4                              #! PC = 0x400de0 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x400de4 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400de8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400dec *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x400df0 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov r4_b tmp_b; mov r4_t tmp_t;

assert eqmod r4_b (r4_b19 *    40) [Q] /\
       eqmod r4_t (r4_t19 *    40) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r4_b (r4_b19 *    40) [Q] /\
       eqmod r4_t (r4_t19 *    40) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r4_b, r4_t] /\ [r4_b, r4_t] <s[Q2, Q2];


ghost r5_b19@int16, r5_t19@int16:
      r5_b19 = r5_b /\ r5_t19 = r5_t
   && r5_b19 = r5_b /\ r5_t19 = r5_t;

(* smulwb	lr, r11, r5                              #! PC = 0x400df4 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x400df8 *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400dfc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400e00 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x400e04 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov r5_b tmp_b; mov r5_t tmp_t;

assert eqmod r5_b (r5_b19 *  -630) [Q] /\
       eqmod r5_t (r5_t19 *  -630) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r5_b (r5_b19 *  -630) [Q] /\
       eqmod r5_t (r5_t19 *  -630) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r5_b, r5_t] /\ [r5_b, r5_t] <s[Q2, Q2];


ghost r6_b6@int16, r6_t6@int16:
      r6_b6 = r6_b /\ r6_t6 = r6_t
   && r6_b6 = r6_b /\ r6_t6 = r6_t;

(* vmov	r10, s19                                   #! PC = 0x400e08 *)
mov [r10_b, r10_t] [s19_b, s19_t]; mov r10 s19;
(* vmov	r11, s20                                   #! PC = 0x400e0c *)
mov [r11_b, r11_t] [s20_b, s20_t]; mov r11 s20;
(* smulwb	lr, r10, r6                              #! PC = 0x400e10 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x400e14 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e18 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400e1c *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x400e20 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov r6_b tmp_b; mov r6_t tmp_t;

assert eqmod r6_b (r6_b6 *  1600) [Q] /\
       eqmod r6_t (r6_t6 *  1600) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r6_b (r6_b6 *  1600) [Q] /\
       eqmod r6_t (r6_t6 *  1600) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r6_b, r6_t] /\ [r6_b, r6_t] <s[Q2, Q2];


ghost r7_b19@int16, r7_t19@int16:
      r7_b19 = r7_b /\ r7_t19 = r7_t
   && r7_b19 = r7_b /\ r7_t19 = r7_t;

(* smulwb	lr, r11, r7                              #! PC = 0x400e24 *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x400e28 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e2c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400e30 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x400e34 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov r7_b tmp_b; mov r7_t tmp_t;

assert eqmod r7_b (r7_b19 *  1432) [Q] /\
       eqmod r7_t (r7_t19 *  1432) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r7_b (r7_b19 *  1432) [Q] /\
       eqmod r7_t (r7_t19 *  1432) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r7_b, r7_t] /\ [r7_b, r7_t] <s[Q2, Q2];


ghost r8_b6@int16, r8_t6@int16:
      r8_b6 = r8_b /\ r8_t6 = r8_t
   && r8_b6 = r8_b /\ r8_t6 = r8_t;

(* vmov	r10, s21                                   #! PC = 0x400e38 *)
mov [r10_b, r10_t] [s21_b, s21_t]; mov r10 s21;
(* vmov	r11, s22                                   #! PC = 0x400e3c *)
mov [r11_b, r11_t] [s22_b, s22_t]; mov r11 s22;
(* smulwb	lr, r10, r8                              #! PC = 0x400e40 *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x400e44 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e48 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400e4c *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x400e50 *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov r8_b tmp_b; mov r8_t tmp_t;

assert eqmod r8_b (r8_b6 *   749) [Q] /\
       eqmod r8_t (r8_t6 *   749) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r8_b (r8_b6 *   749) [Q] /\
       eqmod r8_t (r8_t6 *   749) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r8_b, r8_t] /\ [r8_b, r8_t] <s[Q2, Q2];


ghost r9_b32@int16, r9_t32@int16:
      r9_b32 = r9_b /\ r9_t32 = r9_t
   && r9_b32 = r9_b /\ r9_t32 = r9_t;

(* smulwb	lr, r11, r9                              #! PC = 0x400e54 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x400e58 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e5c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400e60 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x400e64 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov r9_b tmp_b; mov r9_t tmp_t;

assert eqmod r9_b (r9_b32 *   687) [Q] /\
       eqmod r9_t (r9_t32 *   687) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r9_b (r9_b32 *   687) [Q] /\
       eqmod r9_t (r9_t32 *   687) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r9_b, r9_t] /\ [r9_b, r9_t] <s[Q2, Q2];

(* vmov	s0, r2                                     #! PC = 0x400e68 *)
mov [s0_b, s0_t] [r2_b, r2_t];
(* vmov	s1, r3                                     #! PC = 0x400e6c *)
mov [s1_b, s1_t] [r3_b, r3_t];
(* vmov	s2, r4                                     #! PC = 0x400e70 *)
mov [s2_b, s2_t] [r4_b, r4_t];
(* vmov	s3, r5                                     #! PC = 0x400e74 *)
mov [s3_b, s3_t] [r5_b, r5_t];
(* vmov	s4, r6                                     #! PC = 0x400e78 *)
mov [s4_b, s4_t] [r6_b, r6_t];
(* vmov	s5, r7                                     #! PC = 0x400e7c *)
mov [s5_b, s5_t] [r7_b, r7_t];
(* vmov	s6, r8                                     #! PC = 0x400e80 *)
mov [s6_b, s6_t] [r8_b, r8_t];
(* vmov	s7, r9                                     #! PC = 0x400e84 *)
mov [s7_b, s7_t] [r9_b, r9_t];

assert [8*NQ2,8*NQ2]< [s0_b, s0_t] /\ [s0_b, s0_t]< [8*Q2,8*Q2] /\
       [1*NQ2,1*NQ2]< [s1_b, s1_t] /\ [s1_b, s1_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s2_b, s2_t] /\ [s2_b, s2_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s3_b, s3_t] /\ [s3_b, s3_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s4_b, s4_t] /\ [s4_b, s4_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s5_b, s5_t] /\ [s5_b, s5_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s6_b, s6_t] /\ [s6_b, s6_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s7_b, s7_t] /\ [s7_b, s7_t]< [1*Q2,1*Q2]
       prove with [algebra solver isl, precondition] && true;
assume [8*NQ2,8*NQ2]< [s0_b, s0_t] /\ [s0_b, s0_t]< [8*Q2,8*Q2] /\
       [1*NQ2,1*NQ2]< [s1_b, s1_t] /\ [s1_b, s1_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s2_b, s2_t] /\ [s2_b, s2_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s3_b, s3_t] /\ [s3_b, s3_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s4_b, s4_t] /\ [s4_b, s4_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s5_b, s5_t] /\ [s5_b, s5_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s6_b, s6_t] /\ [s6_b, s6_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s7_b, s7_t] /\ [s7_b, s7_t]< [1*Q2,1*Q2]
    && [8@16*NQ2,8@16*NQ2]<s[s0_b, s0_t] /\ [s0_b, s0_t]<s[8@16*Q2,8@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s1_b, s1_t] /\ [s1_b, s1_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s2_b, s2_t] /\ [s2_b, s2_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s3_b, s3_t] /\ [s3_b, s3_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s4_b, s4_t] /\ [s4_b, s4_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s5_b, s5_t] /\ [s5_b, s5_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s6_b, s6_t] /\ [s6_b, s6_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s7_b, s7_t] /\ [s7_b, s7_t]<s[1@16*Q2,1@16*Q2];

(* CUT 12 *)
ghost Z6@int16: X**2 =  -848*17** 23*Z6 && true;
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod (poly Z6 [poly X [s0_b, s0_t], poly X [s1_b, s1_t],
                    poly X [s2_b, s2_t], poly X [s3_b, s3_t],
                    poly X [s4_b, s4_t], poly X [s5_b, s5_t],
                    poly X [s6_b, s6_t], poly X [s7_b, s7_t]])
          (2**3*F**2) [Q, Z6**8 + 1] /\
    [8*NQ2,8*NQ2]< [s0_b, s0_t] /\ [s0_b, s0_t]< [8*Q2,8*Q2] /\
    [1*NQ2,1*NQ2]< [s1_b, s1_t] /\ [s1_b, s1_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s2_b, s2_t] /\ [s2_b, s2_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s3_b, s3_t] /\ [s3_b, s3_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s4_b, s4_t] /\ [s4_b, s4_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s5_b, s5_t] /\ [s5_b, s5_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s6_b, s6_t] /\ [s6_b, s6_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s7_b, s7_t] /\ [s7_b, s7_t]< [1*Q2,1*Q2]
    prove with [precondition]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [8@16*NQ2,8@16*NQ2]<s[s0_b, s0_t] /\ [s0_b, s0_t]<s[8@16*Q2,8@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s1_b, s1_t] /\ [s1_b, s1_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s2_b, s2_t] /\ [s2_b, s2_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s3_b, s3_t] /\ [s3_b, s3_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s4_b, s4_t] /\ [s4_b, s4_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s5_b, s5_t] /\ [s5_b, s5_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s6_b, s6_t] /\ [s6_b, s6_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s7_b, s7_t] /\ [s7_b, s7_t]<s[1@16*Q2,1@16*Q2]
    prove with [precondition];

(* vmov	r0, s23                                    #! PC = 0x400e88 *)
mov [r0_b, r0_t] [s23_b, s23_t];
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff344; Value = 0xa896909a; PC = 0x400e8c *)
mov [r2_b, r2_t] [L0xbefff344, L0xbefff346];
(* ldr.w	r3, [r0, #4]                              #! EA = L0xbefff348; Value = 0xae988504; PC = 0x400e90 *)
mov [r3_b, r3_t] [L0xbefff348, L0xbefff34a];
(* ldr.w	r4, [r0, #8]                              #! EA = L0xbefff34c; Value = 0xac9c8c12; PC = 0x400e94 *)
mov [r4_b, r4_t] [L0xbefff34c, L0xbefff34e];
(* ldr.w	r5, [r0, #12]                             #! EA = L0xbefff350; Value = 0xb64284c8; PC = 0x400e98 *)
mov [r5_b, r5_t] [L0xbefff350, L0xbefff352];
(* ldr.w	r6, [r0, #16]                             #! EA = L0xbefff354; Value = 0xaf4a9247; PC = 0x400e9c *)
mov [r6_b, r6_t] [L0xbefff354, L0xbefff356];
(* ldr.w	r7, [r0, #20]                             #! EA = L0xbefff358; Value = 0xb5329399; PC = 0x400ea0 *)
mov [r7_b, r7_t] [L0xbefff358, L0xbefff35a];
(* ldr.w	r8, [r0, #24]                             #! EA = L0xbefff35c; Value = 0xa6169586; PC = 0x400ea4 *)
mov [r8_b, r8_t] [L0xbefff35c, L0xbefff35e];
(* ldr.w	r9, [r0, #28]                             #! EA = L0xbefff360; Value = 0xaeca8a22; PC = 0x400ea8 *)
mov [r9_b, r9_t] [L0xbefff360, L0xbefff362];
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400eac *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* sadd16	lr, r2, r3                               #! PC = 0x400eb0 *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* ssub16	r3, r2, r3                               #! PC = 0x400eb4 *)
sub r3_b r2_b r3_b;
sub r3_t r2_t r3_t;
(* sadd16	r11, r4, r5                              #! PC = 0x400eb8 *)
add r11_b r4_b r5_b;
add r11_t r4_t r5_t;
(* ssub16	r5, r4, r5                               #! PC = 0x400ebc *)
sub r5_b r4_b r5_b;
sub r5_t r4_t r5_t;
(* sadd16	r2, r6, r7                               #! PC = 0x400ec0 *)
add r2_b r6_b r7_b;
add r2_t r6_t r7_t;
(* ssub16	r7, r6, r7                               #! PC = 0x400ec4 *)
sub r7_b r6_b r7_b;
sub r7_t r6_t r7_t;
(* sadd16	r4, r8, r9                               #! PC = 0x400ec8 *)
add r4_b r8_b r9_b;
add r4_t r8_t r9_t;
(* ssub16	r9, r8, r9                               #! PC = 0x400ecc *)
sub r9_b r8_b r9_b;
sub r9_t r8_t r9_t;
(* sadd16	r8, lr, r11                              #! PC = 0x400ed0 *)
add r8_b lr_b r11_b;
add r8_t lr_t r11_t;
(* ssub16	r11, lr, r11                             #! PC = 0x400ed4 *)
sub r11_b lr_b r11_b;
sub r11_t lr_t r11_t;
(* sadd16	r6, r2, r4                               #! PC = 0x400ed8 *)
add r6_b r2_b r4_b;
add r6_t r2_t r4_t;
(* ssub16	r4, r2, r4                               #! PC = 0x400edc *)
sub r4_b r2_b r4_b;
sub r4_t r2_t r4_t;

ghost r5_b20@int16, r5_t20@int16:
      r5_b20 = r5_b /\ r5_t20 = r5_t
   && r5_b20 = r5_b /\ r5_t20 = r5_t;

(* vmov	r10, s10                                   #! PC = 0x400ee0 *)
mov [r10_b, r10_t] [s10_b, s10_t]; mov r10 s10;
(* smulwb	lr, r10, r5                              #! PC = 0x400ee4 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x400ee8 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400eec *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400ef0 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400ef4 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_b20 *  1600) [Q] /\
       eqmod lr_t (r5_t20 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_b20 *  1600) [Q] /\
       eqmod lr_t (r5_t20 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x400ef8 *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400efc *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r9_b33@int16, r9_t33@int16:
      r9_b33 = r9_b /\ r9_t33 = r9_t
   && r9_b33 = r9_b /\ r9_t33 = r9_t;

(* smulwb	lr, r10, r9                              #! PC = 0x400f00 *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400f04 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f08 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400f0c *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400f10 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b33 *  1600) [Q] /\
       eqmod lr_t (r9_t33 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b33 *  1600) [Q] /\
       eqmod lr_t (r9_t33 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x400f14 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400f18 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* sadd16	r2, r8, r6                               #! PC = 0x400f1c *)
add r2_b r8_b r6_b;
add r2_t r8_t r6_t;
(* ssub16	r6, r8, r6                               #! PC = 0x400f20 *)
sub r6_b r8_b r6_b;
sub r6_t r8_t r6_t;

ghost r7_b20@int16, r7_t20@int16:
      r7_b20 = r7_b /\ r7_t20 = r7_t
   && r7_b20 = r7_b /\ r7_t20 = r7_t;

(* vmov	r10, s12                                   #! PC = 0x400f24 *)
mov [r10_b, r10_t] [s12_b, s12_t]; mov r10 s12;
(* smulwb	lr, r10, r7                              #! PC = 0x400f28 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x400f2c *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f30 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400f34 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400f38 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_b20 *    40) [Q] /\
       eqmod lr_t (r7_t20 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_b20 *    40) [Q] /\
       eqmod lr_t (r7_t20 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x400f3c *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400f40 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r4_b20@int16, r4_t20@int16:
      r4_b20 = r4_b /\ r4_t20 = r4_t
   && r4_b20 = r4_b /\ r4_t20 = r4_t;

(* vmov	r10, s13                                   #! PC = 0x400f44 *)
mov [r10_b, r10_t] [s13_b, s13_t]; mov r10 s13;
(* smulwb	lr, r10, r4                              #! PC = 0x400f48 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x400f4c *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f50 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400f54 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400f58 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_b20 *  1600) [Q] /\
       eqmod lr_t (r4_t20 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_b20 *  1600) [Q] /\
       eqmod lr_t (r4_t20 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* ssub16	r8, r11, lr                              #! PC = 0x400f5c *)
sub r8_b r11_b lr_b;
sub r8_t r11_t lr_t;
(* sadd16	r4, r11, lr                              #! PC = 0x400f60 *)
add r4_b r11_b lr_b;
add r4_t r11_t lr_t;

ghost r9_b34@int16, r9_t34@int16:
      r9_b34 = r9_b /\ r9_t34 = r9_t
   && r9_b34 = r9_b /\ r9_t34 = r9_t;

(* vmov	r10, s14                                   #! PC = 0x400f64 *)
mov [r10_b, r10_t] [s14_b, s14_t]; mov r10 s14;
(* smulwb	lr, r10, r9                              #! PC = 0x400f68 *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400f6c *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f70 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400f74 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400f78 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b34 *   749) [Q] /\
       eqmod lr_t (r9_t34 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b34 *   749) [Q] /\
       eqmod lr_t (r9_t34 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x400f7c *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400f80 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* vmov	r0, s23                                    #! PC = 0x400f84 *)
mov [r0_b, r0_t] [s23_b, s23_t];
(* vmov	r11, s1                                    #! PC = 0x400f88 *)
mov [r11_b, r11_t] [s1_b, s1_t];
(* uadd16	lr, r3, r11                              #! PC = 0x400f8c *)
add lr_b r3_b r11_b;
add lr_t r3_t r11_t;
(* usub16	r3, r3, r11                              #! PC = 0x400f90 *)
sub r3_b r3_b r11_b;
sub r3_t r3_t r11_t;
(* str.w	lr, [r0, #4]                              #! EA = L0xbefff348; PC = 0x400f94 *)
mov [L0xbefff348, L0xbefff34a] [lr_b, lr_t];
(* str.w	r3, [r0, #36]	; 0x24                      #! EA = L0xbefff368; PC = 0x400f98 *)
mov [L0xbefff368, L0xbefff36a] [r3_b, r3_t];
(* vmov	r11, s3                                    #! PC = 0x400f9c *)
mov [r11_b, r11_t] [s3_b, s3_t];
(* uadd16	lr, r5, r11                              #! PC = 0x400fa0 *)
add lr_b r5_b r11_b;
add lr_t r5_t r11_t;
(* usub16	r5, r5, r11                              #! PC = 0x400fa4 *)
sub r5_b r5_b r11_b;
sub r5_t r5_t r11_t;
(* str.w	lr, [r0, #12]                             #! EA = L0xbefff350; PC = 0x400fa8 *)
mov [L0xbefff350, L0xbefff352] [lr_b, lr_t];
(* str.w	r5, [r0, #44]	; 0x2c                      #! EA = L0xbefff370; PC = 0x400fac *)
mov [L0xbefff370, L0xbefff372] [r5_b, r5_t];
(* vmov	r11, s5                                    #! PC = 0x400fb0 *)
mov [r11_b, r11_t] [s5_b, s5_t];
(* uadd16	lr, r7, r11                              #! PC = 0x400fb4 *)
add lr_b r7_b r11_b;
add lr_t r7_t r11_t;
(* usub16	r7, r7, r11                              #! PC = 0x400fb8 *)
sub r7_b r7_b r11_b;
sub r7_t r7_t r11_t;
(* str.w	lr, [r0, #20]                             #! EA = L0xbefff358; PC = 0x400fbc *)
mov [L0xbefff358, L0xbefff35a] [lr_b, lr_t];
(* str.w	r7, [r0, #52]	; 0x34                      #! EA = L0xbefff378; PC = 0x400fc0 *)
mov [L0xbefff378, L0xbefff37a] [r7_b, r7_t];
(* vmov	r11, s7                                    #! PC = 0x400fc4 *)
mov [r11_b, r11_t] [s7_b, s7_t];
(* uadd16	lr, r9, r11                              #! PC = 0x400fc8 *)
add lr_b r9_b r11_b;
add lr_t r9_t r11_t;
(* usub16	r9, r9, r11                              #! PC = 0x400fcc *)
sub r9_b r9_b r11_b;
sub r9_t r9_t r11_t;
(* str.w	lr, [r0, #28]                             #! EA = L0xbefff360; PC = 0x400fd0 *)
mov [L0xbefff360, L0xbefff362] [lr_b, lr_t];
(* str.w	r9, [r0, #60]	; 0x3c                      #! EA = L0xbefff380; PC = 0x400fd4 *)
mov [L0xbefff380, L0xbefff382] [r9_b, r9_t];
(* vmov	r5, s2                                     #! PC = 0x400fd8 *)
mov [r5_b, r5_t] [s2_b, s2_t];
(* uadd16	lr, r4, r5                               #! PC = 0x400fdc *)
add lr_b r4_b r5_b;
add lr_t r4_t r5_t;
(* usub16	r11, r4, r5                              #! PC = 0x400fe0 *)
sub r11_b r4_b r5_b;
sub r11_t r4_t r5_t;
(* str.w	lr, [r0, #8]                              #! EA = L0xbefff34c; PC = 0x400fe4 *)
mov [L0xbefff34c, L0xbefff34e] [lr_b, lr_t];
(* str.w	r11, [r0, #40]	; 0x28                     #! EA = L0xbefff36c; PC = 0x400fe8 *)
mov [L0xbefff36c, L0xbefff36e] [r11_b, r11_t];
(* vmov	r7, s4                                     #! PC = 0x400fec *)
mov [r7_b, r7_t] [s4_b, s4_t];
(* uadd16	lr, r6, r7                               #! PC = 0x400ff0 *)
add lr_b r6_b r7_b;
add lr_t r6_t r7_t;
(* usub16	r11, r6, r7                              #! PC = 0x400ff4 *)
sub r11_b r6_b r7_b;
sub r11_t r6_t r7_t;
(* str.w	lr, [r0, #16]                             #! EA = L0xbefff354; PC = 0x400ff8 *)
mov [L0xbefff354, L0xbefff356] [lr_b, lr_t];
(* str.w	r11, [r0, #48]	; 0x30                     #! EA = L0xbefff374; PC = 0x400ffc *)
mov [L0xbefff374, L0xbefff376] [r11_b, r11_t];
(* vmov	r9, s6                                     #! PC = 0x401000 *)
mov [r9_b, r9_t] [s6_b, s6_t];
(* uadd16	lr, r8, r9                               #! PC = 0x401004 *)
add lr_b r8_b r9_b;
add lr_t r8_t r9_t;
(* usub16	r11, r8, r9                              #! PC = 0x401008 *)
sub r11_b r8_b r9_b;
sub r11_t r8_t r9_t;
(* str.w	lr, [r0, #24]                             #! EA = L0xbefff35c; PC = 0x40100c *)
mov [L0xbefff35c, L0xbefff35e] [lr_b, lr_t];
(* str.w	r11, [r0, #56]	; 0x38                     #! EA = L0xbefff37c; PC = 0x401010 *)
mov [L0xbefff37c, L0xbefff37e] [r11_b, r11_t];
(* vmov	r3, s0                                     #! PC = 0x401014 *)
mov [r3_b, r3_t] [s0_b, s0_t];
(* uadd16	lr, r2, r3                               #! PC = 0x401018 *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* usub16	r11, r2, r3                              #! PC = 0x40101c *)
sub r11_b r2_b r3_b;
sub r11_t r2_t r3_t;
(* str.w	r11, [r0, #32]                            #! EA = L0xbefff364; PC = 0x401020 *)
mov [L0xbefff364, L0xbefff366] [r11_b, r11_t];
(* str.w	lr, [r0], #64                             #! EA = L0xbefff344; PC = 0x401024 *)
mov [L0xbefff344, L0xbefff346] [lr_b, lr_t];
(* vmov	lr, s8                                     #! PC = 0x401028 *)
mov [lr_b, lr_t] [s8_b, s8_t]; mov lr s8;
(* cmp.w	r0, lr                                    #! PC = 0x40102c *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x400cc4 <invntt_fast+24>                #! PC = 0x401030 *)
#bne.w	0x400cc4 <invntt_fast+24>                #! 0x401030 = 0x401030;

(* CUT 13 *)
assert [16*NQ2,16*NQ2,5*NQ2]< [L0xbefff344,L0xbefff346,L0xbefff348] /\
       [L0xbefff344,L0xbefff346,L0xbefff348]< [16*Q2,16*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff34a,L0xbefff34c,L0xbefff34e] /\
       [L0xbefff34a,L0xbefff34c,L0xbefff34e]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2,9*NQ2]< [L0xbefff350,L0xbefff352,L0xbefff354] /\
       [L0xbefff350,L0xbefff352,L0xbefff354]< [5*Q2,5*Q2,9*Q2] /\
       [9*NQ2,5*NQ2,5*NQ2]< [L0xbefff356,L0xbefff358,L0xbefff35a] /\
       [L0xbefff356,L0xbefff358,L0xbefff35a]< [9*Q2,5*Q2,5*Q2] /\
       [6*NQ2,6*NQ2,5*NQ2]< [L0xbefff35c,L0xbefff35e,L0xbefff360] /\
       [L0xbefff35c,L0xbefff35e,L0xbefff360]< [6*Q2,6*Q2,5*Q2] /\
       [5*NQ2,16*NQ2,16*NQ2]< [L0xbefff362,L0xbefff364,L0xbefff366] /\
       [L0xbefff362,L0xbefff364,L0xbefff366]< [5*Q2,16*Q2,16*Q2] /\
       [5*NQ2,5*NQ2,6*NQ2]< [L0xbefff368,L0xbefff36a,L0xbefff36c] /\
       [L0xbefff368,L0xbefff36a,L0xbefff36c]< [5*Q2,5*Q2,6*Q2] /\
       [6*NQ2,5*NQ2,5*NQ2]< [L0xbefff36e,L0xbefff370,L0xbefff372] /\
       [L0xbefff36e,L0xbefff370,L0xbefff372]< [6*Q2,5*Q2,5*Q2] /\
       [9*NQ2,9*NQ2,5*NQ2]< [L0xbefff374,L0xbefff376,L0xbefff378] /\
       [L0xbefff374,L0xbefff376,L0xbefff378]< [9*Q2,9*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff37a,L0xbefff37c,L0xbefff37e] /\
       [L0xbefff37a,L0xbefff37c,L0xbefff37e]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2]< [L0xbefff380,L0xbefff382] /\
       [L0xbefff380,L0xbefff382]< [5*Q2,5*Q2]
       prove with [algebra solver isl, precondition] && true;
assume [16*NQ2,16*NQ2,5*NQ2]< [L0xbefff344,L0xbefff346,L0xbefff348] /\
       [L0xbefff344,L0xbefff346,L0xbefff348]< [16*Q2,16*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff34a,L0xbefff34c,L0xbefff34e] /\
       [L0xbefff34a,L0xbefff34c,L0xbefff34e]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2,9*NQ2]< [L0xbefff350,L0xbefff352,L0xbefff354] /\
       [L0xbefff350,L0xbefff352,L0xbefff354]< [5*Q2,5*Q2,9*Q2] /\
       [9*NQ2,5*NQ2,5*NQ2]< [L0xbefff356,L0xbefff358,L0xbefff35a] /\
       [L0xbefff356,L0xbefff358,L0xbefff35a]< [9*Q2,5*Q2,5*Q2] /\
       [6*NQ2,6*NQ2,5*NQ2]< [L0xbefff35c,L0xbefff35e,L0xbefff360] /\
       [L0xbefff35c,L0xbefff35e,L0xbefff360]< [6*Q2,6*Q2,5*Q2] /\
       [5*NQ2,16*NQ2,16*NQ2]< [L0xbefff362,L0xbefff364,L0xbefff366] /\
       [L0xbefff362,L0xbefff364,L0xbefff366]< [5*Q2,16*Q2,16*Q2] /\
       [5*NQ2,5*NQ2,6*NQ2]< [L0xbefff368,L0xbefff36a,L0xbefff36c] /\
       [L0xbefff368,L0xbefff36a,L0xbefff36c]< [5*Q2,5*Q2,6*Q2] /\
       [6*NQ2,5*NQ2,5*NQ2]< [L0xbefff36e,L0xbefff370,L0xbefff372] /\
       [L0xbefff36e,L0xbefff370,L0xbefff372]< [6*Q2,5*Q2,5*Q2] /\
       [9*NQ2,9*NQ2,5*NQ2]< [L0xbefff374,L0xbefff376,L0xbefff378] /\
       [L0xbefff374,L0xbefff376,L0xbefff378]< [9*Q2,9*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff37a,L0xbefff37c,L0xbefff37e] /\
       [L0xbefff37a,L0xbefff37c,L0xbefff37e]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2]< [L0xbefff380,L0xbefff382] /\
       [L0xbefff380,L0xbefff382]< [5*Q2,5*Q2]
    && [16@16*NQ2,16@16*NQ2,5@16*NQ2]< [L0xbefff344,L0xbefff346,L0xbefff348] /\
       [L0xbefff344,L0xbefff346,L0xbefff348]< [16@16*Q2,16@16*Q2,5@16*Q2] /\
       [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff34a,L0xbefff34c,L0xbefff34e] /\
       [L0xbefff34a,L0xbefff34c,L0xbefff34e]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
       [5@16*NQ2,5@16*NQ2,9@16*NQ2]< [L0xbefff350,L0xbefff352,L0xbefff354] /\
       [L0xbefff350,L0xbefff352,L0xbefff354]< [5@16*Q2,5@16*Q2,9@16*Q2] /\
       [9@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff356,L0xbefff358,L0xbefff35a] /\
       [L0xbefff356,L0xbefff358,L0xbefff35a]< [9@16*Q2,5@16*Q2,5@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,5@16*NQ2]< [L0xbefff35c,L0xbefff35e,L0xbefff360] /\
       [L0xbefff35c,L0xbefff35e,L0xbefff360]< [6@16*Q2,6@16*Q2,5@16*Q2] /\
       [5@16*NQ2,16@16*NQ2,16@16*NQ2]< [L0xbefff362,L0xbefff364,L0xbefff366] /\
       [L0xbefff362,L0xbefff364,L0xbefff366]< [5@16*Q2,16@16*Q2,16@16*Q2] /\
       [5@16*NQ2,5@16*NQ2,6@16*NQ2]< [L0xbefff368,L0xbefff36a,L0xbefff36c] /\
       [L0xbefff368,L0xbefff36a,L0xbefff36c]< [5@16*Q2,5@16*Q2,6@16*Q2] /\
       [6@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff36e,L0xbefff370,L0xbefff372] /\
       [L0xbefff36e,L0xbefff370,L0xbefff372]< [6@16*Q2,5@16*Q2,5@16*Q2] /\
       [9@16*NQ2,9@16*NQ2,5@16*NQ2]< [L0xbefff374,L0xbefff376,L0xbefff378] /\
       [L0xbefff374,L0xbefff376,L0xbefff378]< [9@16*Q2,9@16*Q2,5@16*Q2] /\
       [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff37a,L0xbefff37c,L0xbefff37e] /\
       [L0xbefff37a,L0xbefff37c,L0xbefff37e]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
       [5@16*NQ2,5@16*NQ2]< [L0xbefff380,L0xbefff382] /\
       [L0xbefff380,L0xbefff382]< [5@16*Q2,5@16*Q2];
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod (poly Z6
           [poly X [L0xbefff344,L0xbefff346],poly X [L0xbefff348,L0xbefff34a],
            poly X [L0xbefff34c,L0xbefff34e],poly X [L0xbefff350,L0xbefff352],
            poly X [L0xbefff354,L0xbefff356],poly X [L0xbefff358,L0xbefff35a],
            poly X [L0xbefff35c,L0xbefff35e],poly X [L0xbefff360,L0xbefff362],
            poly X [L0xbefff364,L0xbefff366],poly X [L0xbefff368,L0xbefff36a],
            poly X [L0xbefff36c,L0xbefff36e],poly X [L0xbefff370,L0xbefff372],
            poly X [L0xbefff374,L0xbefff376],poly X [L0xbefff378,L0xbefff37a],
            poly X [L0xbefff37c,L0xbefff37e],poly X [L0xbefff380,L0xbefff382]])
          (2**4*F**2) [Q, Z6**16 - 1] /\
    [16*NQ2,16*NQ2,5*NQ2]< [L0xbefff344,L0xbefff346,L0xbefff348] /\
    [L0xbefff344,L0xbefff346,L0xbefff348]< [16*Q2,16*Q2,5*Q2] /\
    [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff34a,L0xbefff34c,L0xbefff34e] /\
    [L0xbefff34a,L0xbefff34c,L0xbefff34e]< [5*Q2,6*Q2,6*Q2] /\
    [5*NQ2,5*NQ2,9*NQ2]< [L0xbefff350,L0xbefff352,L0xbefff354] /\
    [L0xbefff350,L0xbefff352,L0xbefff354]< [5*Q2,5*Q2,9*Q2] /\
    [9*NQ2,5*NQ2,5*NQ2]< [L0xbefff356,L0xbefff358,L0xbefff35a] /\
    [L0xbefff356,L0xbefff358,L0xbefff35a]< [9*Q2,5*Q2,5*Q2] /\
    [6*NQ2,6*NQ2,5*NQ2]< [L0xbefff35c,L0xbefff35e,L0xbefff360] /\
    [L0xbefff35c,L0xbefff35e,L0xbefff360]< [6*Q2,6*Q2,5*Q2] /\
    [5*NQ2,16*NQ2,16*NQ2]< [L0xbefff362,L0xbefff364,L0xbefff366] /\
    [L0xbefff362,L0xbefff364,L0xbefff366]< [5*Q2,16*Q2,16*Q2] /\
    [5*NQ2,5*NQ2,6*NQ2]< [L0xbefff368,L0xbefff36a,L0xbefff36c] /\
    [L0xbefff368,L0xbefff36a,L0xbefff36c]< [5*Q2,5*Q2,6*Q2] /\
    [6*NQ2,5*NQ2,5*NQ2]< [L0xbefff36e,L0xbefff370,L0xbefff372] /\
    [L0xbefff36e,L0xbefff370,L0xbefff372]< [6*Q2,5*Q2,5*Q2] /\
    [9*NQ2,9*NQ2,5*NQ2]< [L0xbefff374,L0xbefff376,L0xbefff378] /\
    [L0xbefff374,L0xbefff376,L0xbefff378]< [9*Q2,9*Q2,5*Q2] /\
    [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff37a,L0xbefff37c,L0xbefff37e] /\
    [L0xbefff37a,L0xbefff37c,L0xbefff37e]< [5*Q2,6*Q2,6*Q2] /\
    [5*NQ2,5*NQ2]< [L0xbefff380,L0xbefff382] /\
    [L0xbefff380,L0xbefff382]< [5*Q2,5*Q2]
    prove with [precondition, all ghosts]
 && [16@16*NQ2,16@16*NQ2,5@16*NQ2]< [L0xbefff344,L0xbefff346,L0xbefff348] /\
    [L0xbefff344,L0xbefff346,L0xbefff348]< [16@16*Q2,16@16*Q2,5@16*Q2] /\
    [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff34a,L0xbefff34c,L0xbefff34e] /\
    [L0xbefff34a,L0xbefff34c,L0xbefff34e]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
    [5@16*NQ2,5@16*NQ2,9@16*NQ2]< [L0xbefff350,L0xbefff352,L0xbefff354] /\
    [L0xbefff350,L0xbefff352,L0xbefff354]< [5@16*Q2,5@16*Q2,9@16*Q2] /\
    [9@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff356,L0xbefff358,L0xbefff35a] /\
    [L0xbefff356,L0xbefff358,L0xbefff35a]< [9@16*Q2,5@16*Q2,5@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,5@16*NQ2]< [L0xbefff35c,L0xbefff35e,L0xbefff360] /\
    [L0xbefff35c,L0xbefff35e,L0xbefff360]< [6@16*Q2,6@16*Q2,5@16*Q2] /\
    [5@16*NQ2,16@16*NQ2,16@16*NQ2]< [L0xbefff362,L0xbefff364,L0xbefff366] /\
    [L0xbefff362,L0xbefff364,L0xbefff366]< [5@16*Q2,16@16*Q2,16@16*Q2] /\
    [5@16*NQ2,5@16*NQ2,6@16*NQ2]< [L0xbefff368,L0xbefff36a,L0xbefff36c] /\
    [L0xbefff368,L0xbefff36a,L0xbefff36c]< [5@16*Q2,5@16*Q2,6@16*Q2] /\
    [6@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff36e,L0xbefff370,L0xbefff372] /\
    [L0xbefff36e,L0xbefff370,L0xbefff372]< [6@16*Q2,5@16*Q2,5@16*Q2] /\
    [9@16*NQ2,9@16*NQ2,5@16*NQ2]< [L0xbefff374,L0xbefff376,L0xbefff378] /\
    [L0xbefff374,L0xbefff376,L0xbefff378]< [9@16*Q2,9@16*Q2,5@16*Q2] /\
    [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff37a,L0xbefff37c,L0xbefff37e] /\
    [L0xbefff37a,L0xbefff37c,L0xbefff37e]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
    [5@16*NQ2,5@16*NQ2]< [L0xbefff380,L0xbefff382] /\
    [L0xbefff380,L0xbefff382]< [5@16*Q2,5@16*Q2]
    prove with [precondition];

(* vmov	s23, r0                                    #! PC = 0x400cc4 *)
mov [s23_b, s23_t] [r0_b, r0_t];
(* ldr.w	r2, [r0, #32]                             #! EA = L0xbefff3a4; Value = 0xb3849ce4; PC = 0x400cc8 *)
mov [r2_b, r2_t] [L0xbefff3a4, L0xbefff3a6];
(* ldr.w	r3, [r0, #36]	; 0x24                      #! EA = L0xbefff3a8; Value = 0xba5a9d5e; PC = 0x400ccc *)
mov [r3_b, r3_t] [L0xbefff3a8, L0xbefff3aa];
(* ldr.w	r4, [r0, #40]	; 0x28                      #! EA = L0xbefff3ac; Value = 0xbea89602; PC = 0x400cd0 *)
mov [r4_b, r4_t] [L0xbefff3ac, L0xbefff3ae];
(* ldr.w	r5, [r0, #44]	; 0x2c                      #! EA = L0xbefff3b0; Value = 0xb5ce9858; PC = 0x400cd4 *)
mov [r5_b, r5_t] [L0xbefff3b0, L0xbefff3b2];
(* ldr.w	r6, [r0, #48]	; 0x30                      #! EA = L0xbefff3b4; Value = 0xb9729df2; PC = 0x400cd8 *)
mov [r6_b, r6_t] [L0xbefff3b4, L0xbefff3b6];
(* ldr.w	r7, [r0, #52]	; 0x34                      #! EA = L0xbefff3b8; Value = 0xc2ce97c4; PC = 0x400cdc *)
mov [r7_b, r7_t] [L0xbefff3b8, L0xbefff3ba];
(* ldr.w	r8, [r0, #56]	; 0x38                      #! EA = L0xbefff3bc; Value = 0xce559908; PC = 0x400ce0 *)
mov [r8_b, r8_t] [L0xbefff3bc, L0xbefff3be];
(* ldr.w	r9, [r0, #60]	; 0x3c                      #! EA = L0xbefff3c0; Value = 0xc4b793b6; PC = 0x400ce4 *)
mov [r9_b, r9_t] [L0xbefff3c0, L0xbefff3c2];
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400ce8 *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* sadd16	lr, r2, r3                               #! PC = 0x400cec *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* ssub16	r3, r2, r3                               #! PC = 0x400cf0 *)
sub r3_b r2_b r3_b;
sub r3_t r2_t r3_t;
(* sadd16	r11, r4, r5                              #! PC = 0x400cf4 *)
add r11_b r4_b r5_b;
add r11_t r4_t r5_t;
(* ssub16	r5, r4, r5                               #! PC = 0x400cf8 *)
sub r5_b r4_b r5_b;
sub r5_t r4_t r5_t;
(* sadd16	r2, r6, r7                               #! PC = 0x400cfc *)
add r2_b r6_b r7_b;
add r2_t r6_t r7_t;
(* ssub16	r7, r6, r7                               #! PC = 0x400d00 *)
sub r7_b r6_b r7_b;
sub r7_t r6_t r7_t;
(* sadd16	r4, r8, r9                               #! PC = 0x400d04 *)
add r4_b r8_b r9_b;
add r4_t r8_t r9_t;
(* ssub16	r9, r8, r9                               #! PC = 0x400d08 *)
sub r9_b r8_b r9_b;
sub r9_t r8_t r9_t;
(* sadd16	r8, lr, r11                              #! PC = 0x400d0c *)
add r8_b lr_b r11_b;
add r8_t lr_t r11_t;
(* ssub16	r11, lr, r11                             #! PC = 0x400d10 *)
sub r11_b lr_b r11_b;
sub r11_t lr_t r11_t;
(* sadd16	r6, r2, r4                               #! PC = 0x400d14 *)
add r6_b r2_b r4_b;
add r6_t r2_t r4_t;
(* ssub16	r4, r2, r4                               #! PC = 0x400d18 *)
sub r4_b r2_b r4_b;
sub r4_t r2_t r4_t;

ghost r5_b21@int16, r5_t21@int16:
      r5_b21 = r5_b /\ r5_t21 = r5_t
   && r5_b21 = r5_b /\ r5_t21 = r5_t;

(* vmov	r10, s10                                   #! PC = 0x400d1c *)
mov [r10_b, r10_t] [s10_b, s10_t]; mov r10 s10;
(* smulwb	lr, r10, r5                              #! PC = 0x400d20 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x400d24 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d28 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400d2c *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400d30 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_b21 *  1600) [Q] /\
       eqmod lr_t (r5_t21 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_b21 *  1600) [Q] /\
       eqmod lr_t (r5_t21 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x400d34 *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400d38 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r9_b35@int16, r9_t35@int16:
      r9_b35 = r9_b /\ r9_t35 = r9_t
   && r9_b35 = r9_b /\ r9_t35 = r9_t;

(* smulwb	lr, r10, r9                              #! PC = 0x400d3c *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400d40 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d44 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400d48 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400d4c *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b35 *  1600) [Q] /\
       eqmod lr_t (r9_t35 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b35 *  1600) [Q] /\
       eqmod lr_t (r9_t35 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x400d50 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400d54 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* sadd16	r2, r8, r6                               #! PC = 0x400d58 *)
add r2_b r8_b r6_b;
add r2_t r8_t r6_t;
(* ssub16	r6, r8, r6                               #! PC = 0x400d5c *)
sub r6_b r8_b r6_b;
sub r6_t r8_t r6_t;

ghost r7_b21@int16, r7_t21@int16:
      r7_b21 = r7_b /\ r7_t21 = r7_t
   && r7_b21 = r7_b /\ r7_t21 = r7_t;

(* vmov	r10, s12                                   #! PC = 0x400d60 *)
mov [r10_b, r10_t] [s12_b, s12_t]; mov r10 s12;
(* smulwb	lr, r10, r7                              #! PC = 0x400d64 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x400d68 *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d6c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400d70 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400d74 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_b21 *    40) [Q] /\
       eqmod lr_t (r7_t21 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_b21 *    40) [Q] /\
       eqmod lr_t (r7_t21 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x400d78 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400d7c *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r4_b21@int16, r4_t21@int16:
      r4_b21 = r4_b /\ r4_t21 = r4_t
   && r4_b21 = r4_b /\ r4_t21 = r4_t;

(* vmov	r10, s13                                   #! PC = 0x400d80 *)
mov [r10_b, r10_t] [s13_b, s13_t]; mov r10 s13;
(* smulwb	lr, r10, r4                              #! PC = 0x400d84 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x400d88 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400d8c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400d90 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400d94 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_b21 *  1600) [Q] /\
       eqmod lr_t (r4_t21 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_b21 *  1600) [Q] /\
       eqmod lr_t (r4_t21 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* ssub16	r8, r11, lr                              #! PC = 0x400d98 *)
sub r8_b r11_b lr_b;
sub r8_t r11_t lr_t;
(* sadd16	r4, r11, lr                              #! PC = 0x400d9c *)
add r4_b r11_b lr_b;
add r4_t r11_t lr_t;

ghost r9_b36@int16, r9_t36@int16:
      r9_b36 = r9_b /\ r9_t36 = r9_t
   && r9_b36 = r9_b /\ r9_t36 = r9_t;

(* vmov	r10, s14                                   #! PC = 0x400da0 *)
mov [r10_b, r10_t] [s14_b, s14_t]; mov r10 s14;
(* smulwb	lr, r10, r9                              #! PC = 0x400da4 *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400da8 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400dac *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400db0 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400db4 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b36 *   749) [Q] /\
       eqmod lr_t (r9_t36 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b36 *   749) [Q] /\
       eqmod lr_t (r9_t36 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x400db8 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400dbc *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;

ghost r3_b7@int16, r3_t7@int16:
      r3_b7 = r3_b /\ r3_t7 = r3_t
   && r3_b7 = r3_b /\ r3_t7 = r3_t;

(* vmov	r11, s16                                   #! PC = 0x400dc0 *)
mov [r11_b, r11_t] [s16_b, s16_t]; mov r11 s16;
(* smulwb	lr, r11, r3                              #! PC = 0x400dc4 *)
vpc r3_bW@int32 r3_b; mulj tmp r11 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r11, r3                              #! PC = 0x400dc8 *)
vpc r3_tW@int32 r3_t; mulj tmp r11 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400dcc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400dd0 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x400dd4 *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov r3_b tmp_b; mov r3_t tmp_t;

assert eqmod r3_b (r3_b7 *  -848) [Q] /\
       eqmod r3_t (r3_t7 *  -848) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r3_b (r3_b7 *  -848) [Q] /\
       eqmod r3_t (r3_t7 *  -848) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r3_b, r3_t] /\ [r3_b, r3_t] <s[Q2, Q2];


ghost r4_b22@int16, r4_t22@int16:
      r4_b22 = r4_b /\ r4_t22 = r4_t
   && r4_b22 = r4_b /\ r4_t22 = r4_t;

(* vmov	r10, s17                                   #! PC = 0x400dd8 *)
mov [r10_b, r10_t] [s17_b, s17_t]; mov r10 s17;
(* vmov	r11, s18                                   #! PC = 0x400ddc *)
mov [r11_b, r11_t] [s18_b, s18_t]; mov r11 s18;
(* smulwb	lr, r10, r4                              #! PC = 0x400de0 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x400de4 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400de8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400dec *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x400df0 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov r4_b tmp_b; mov r4_t tmp_t;

assert eqmod r4_b (r4_b22 *    40) [Q] /\
       eqmod r4_t (r4_t22 *    40) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r4_b (r4_b22 *    40) [Q] /\
       eqmod r4_t (r4_t22 *    40) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r4_b, r4_t] /\ [r4_b, r4_t] <s[Q2, Q2];


ghost r5_b22@int16, r5_t22@int16:
      r5_b22 = r5_b /\ r5_t22 = r5_t
   && r5_b22 = r5_b /\ r5_t22 = r5_t;

(* smulwb	lr, r11, r5                              #! PC = 0x400df4 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x400df8 *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400dfc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400e00 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x400e04 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov r5_b tmp_b; mov r5_t tmp_t;

assert eqmod r5_b (r5_b22 *  -630) [Q] /\
       eqmod r5_t (r5_t22 *  -630) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r5_b (r5_b22 *  -630) [Q] /\
       eqmod r5_t (r5_t22 *  -630) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r5_b, r5_t] /\ [r5_b, r5_t] <s[Q2, Q2];


ghost r6_b7@int16, r6_t7@int16:
      r6_b7 = r6_b /\ r6_t7 = r6_t
   && r6_b7 = r6_b /\ r6_t7 = r6_t;

(* vmov	r10, s19                                   #! PC = 0x400e08 *)
mov [r10_b, r10_t] [s19_b, s19_t]; mov r10 s19;
(* vmov	r11, s20                                   #! PC = 0x400e0c *)
mov [r11_b, r11_t] [s20_b, s20_t]; mov r11 s20;
(* smulwb	lr, r10, r6                              #! PC = 0x400e10 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x400e14 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e18 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400e1c *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x400e20 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov r6_b tmp_b; mov r6_t tmp_t;

assert eqmod r6_b (r6_b7 *  1600) [Q] /\
       eqmod r6_t (r6_t7 *  1600) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r6_b (r6_b7 *  1600) [Q] /\
       eqmod r6_t (r6_t7 *  1600) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r6_b, r6_t] /\ [r6_b, r6_t] <s[Q2, Q2];


ghost r7_b22@int16, r7_t22@int16:
      r7_b22 = r7_b /\ r7_t22 = r7_t
   && r7_b22 = r7_b /\ r7_t22 = r7_t;

(* smulwb	lr, r11, r7                              #! PC = 0x400e24 *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x400e28 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e2c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400e30 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x400e34 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov r7_b tmp_b; mov r7_t tmp_t;

assert eqmod r7_b (r7_b22 *  1432) [Q] /\
       eqmod r7_t (r7_t22 *  1432) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r7_b (r7_b22 *  1432) [Q] /\
       eqmod r7_t (r7_t22 *  1432) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r7_b, r7_t] /\ [r7_b, r7_t] <s[Q2, Q2];


ghost r8_b7@int16, r8_t7@int16:
      r8_b7 = r8_b /\ r8_t7 = r8_t
   && r8_b7 = r8_b /\ r8_t7 = r8_t;

(* vmov	r10, s21                                   #! PC = 0x400e38 *)
mov [r10_b, r10_t] [s21_b, s21_t]; mov r10 s21;
(* vmov	r11, s22                                   #! PC = 0x400e3c *)
mov [r11_b, r11_t] [s22_b, s22_t]; mov r11 s22;
(* smulwb	lr, r10, r8                              #! PC = 0x400e40 *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x400e44 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e48 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400e4c *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x400e50 *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov r8_b tmp_b; mov r8_t tmp_t;

assert eqmod r8_b (r8_b7 *   749) [Q] /\
       eqmod r8_t (r8_t7 *   749) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r8_b (r8_b7 *   749) [Q] /\
       eqmod r8_t (r8_t7 *   749) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r8_b, r8_t] /\ [r8_b, r8_t] <s[Q2, Q2];


ghost r9_b37@int16, r9_t37@int16:
      r9_b37 = r9_b /\ r9_t37 = r9_t
   && r9_b37 = r9_b /\ r9_t37 = r9_t;

(* smulwb	lr, r11, r9                              #! PC = 0x400e54 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x400e58 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400e5c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400e60 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x400e64 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov r9_b tmp_b; mov r9_t tmp_t;

assert eqmod r9_b (r9_b37 *   687) [Q] /\
       eqmod r9_t (r9_t37 *   687) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r9_b (r9_b37 *   687) [Q] /\
       eqmod r9_t (r9_t37 *   687) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r9_b, r9_t] /\ [r9_b, r9_t] <s[Q2, Q2];

(* vmov	s0, r2                                     #! PC = 0x400e68 *)
mov [s0_b, s0_t] [r2_b, r2_t];
(* vmov	s1, r3                                     #! PC = 0x400e6c *)
mov [s1_b, s1_t] [r3_b, r3_t];
(* vmov	s2, r4                                     #! PC = 0x400e70 *)
mov [s2_b, s2_t] [r4_b, r4_t];
(* vmov	s3, r5                                     #! PC = 0x400e74 *)
mov [s3_b, s3_t] [r5_b, r5_t];
(* vmov	s4, r6                                     #! PC = 0x400e78 *)
mov [s4_b, s4_t] [r6_b, r6_t];
(* vmov	s5, r7                                     #! PC = 0x400e7c *)
mov [s5_b, s5_t] [r7_b, r7_t];
(* vmov	s6, r8                                     #! PC = 0x400e80 *)
mov [s6_b, s6_t] [r8_b, r8_t];
(* vmov	s7, r9                                     #! PC = 0x400e84 *)
mov [s7_b, s7_t] [r9_b, r9_t];

assert [8*NQ2,8*NQ2]< [s0_b, s0_t] /\ [s0_b, s0_t]< [8*Q2,8*Q2] /\
       [1*NQ2,1*NQ2]< [s1_b, s1_t] /\ [s1_b, s1_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s2_b, s2_t] /\ [s2_b, s2_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s3_b, s3_t] /\ [s3_b, s3_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s4_b, s4_t] /\ [s4_b, s4_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s5_b, s5_t] /\ [s5_b, s5_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s6_b, s6_t] /\ [s6_b, s6_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s7_b, s7_t] /\ [s7_b, s7_t]< [1*Q2,1*Q2]
       prove with [algebra solver isl, precondition] && true;
assume [8*NQ2,8*NQ2]< [s0_b, s0_t] /\ [s0_b, s0_t]< [8*Q2,8*Q2] /\
       [1*NQ2,1*NQ2]< [s1_b, s1_t] /\ [s1_b, s1_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s2_b, s2_t] /\ [s2_b, s2_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s3_b, s3_t] /\ [s3_b, s3_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s4_b, s4_t] /\ [s4_b, s4_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s5_b, s5_t] /\ [s5_b, s5_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s6_b, s6_t] /\ [s6_b, s6_t]< [1*Q2,1*Q2] /\
       [1*NQ2,1*NQ2]< [s7_b, s7_t] /\ [s7_b, s7_t]< [1*Q2,1*Q2]
    && [8@16*NQ2,8@16*NQ2]<s[s0_b, s0_t] /\ [s0_b, s0_t]<s[8@16*Q2,8@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s1_b, s1_t] /\ [s1_b, s1_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s2_b, s2_t] /\ [s2_b, s2_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s3_b, s3_t] /\ [s3_b, s3_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s4_b, s4_t] /\ [s4_b, s4_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s5_b, s5_t] /\ [s5_b, s5_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s6_b, s6_t] /\ [s6_b, s6_t]<s[1@16*Q2,1@16*Q2] /\
       [1@16*NQ2,1@16*NQ2]<s[s7_b, s7_t] /\ [s7_b, s7_t]<s[1@16*Q2,1@16*Q2];

(* CUT 14 *)
ghost Z7@int16: X**2 =  -848*17** 31*Z7 && true;
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod (poly Z7 [poly X [s0_b, s0_t], poly X [s1_b, s1_t],
                    poly X [s2_b, s2_t], poly X [s3_b, s3_t],
                    poly X [s4_b, s4_t], poly X [s5_b, s5_t],
                    poly X [s6_b, s6_t], poly X [s7_b, s7_t]])
          (2**3*F**2) [Q, Z7**8 + 1] /\
    [8*NQ2,8*NQ2]< [s0_b, s0_t] /\ [s0_b, s0_t]< [8*Q2,8*Q2] /\
    [1*NQ2,1*NQ2]< [s1_b, s1_t] /\ [s1_b, s1_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s2_b, s2_t] /\ [s2_b, s2_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s3_b, s3_t] /\ [s3_b, s3_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s4_b, s4_t] /\ [s4_b, s4_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s5_b, s5_t] /\ [s5_b, s5_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s6_b, s6_t] /\ [s6_b, s6_t]< [1*Q2,1*Q2] /\
    [1*NQ2,1*NQ2]< [s7_b, s7_t] /\ [s7_b, s7_t]< [1*Q2,1*Q2]
    prove with [precondition]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [8@16*NQ2,8@16*NQ2]<s[s0_b, s0_t] /\ [s0_b, s0_t]<s[8@16*Q2,8@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s1_b, s1_t] /\ [s1_b, s1_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s2_b, s2_t] /\ [s2_b, s2_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s3_b, s3_t] /\ [s3_b, s3_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s4_b, s4_t] /\ [s4_b, s4_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s5_b, s5_t] /\ [s5_b, s5_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s6_b, s6_t] /\ [s6_b, s6_t]<s[1@16*Q2,1@16*Q2] /\
    [1@16*NQ2,1@16*NQ2]<s[s7_b, s7_t] /\ [s7_b, s7_t]<s[1@16*Q2,1@16*Q2]
    prove with [precondition];

(* vmov	r0, s23                                    #! PC = 0x400e88 *)
mov [r0_b, r0_t] [s23_b, s23_t];
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff384; Value = 0xaa9a91d0; PC = 0x400e8c *)
mov [r2_b, r2_t] [L0xbefff384, L0xbefff386];
(* ldr.w	r3, [r0, #4]                              #! EA = L0xbefff388; Value = 0xb5ae9a82; PC = 0x400e90 *)
mov [r3_b, r3_t] [L0xbefff388, L0xbefff38a];
(* ldr.w	r4, [r0, #8]                              #! EA = L0xbefff38c; Value = 0xabad9185; PC = 0x400e94 *)
mov [r4_b, r4_t] [L0xbefff38c, L0xbefff38e];
(* ldr.w	r5, [r0, #12]                             #! EA = L0xbefff390; Value = 0xb3af96ad; PC = 0x400e98 *)
mov [r5_b, r5_t] [L0xbefff390, L0xbefff392];
(* ldr.w	r6, [r0, #16]                             #! EA = L0xbefff394; Value = 0xbfc49825; PC = 0x400e9c *)
mov [r6_b, r6_t] [L0xbefff394, L0xbefff396];
(* ldr.w	r7, [r0, #20]                             #! EA = L0xbefff398; Value = 0xbe6aa4f3; PC = 0x400ea0 *)
mov [r7_b, r7_t] [L0xbefff398, L0xbefff39a];
(* ldr.w	r8, [r0, #24]                             #! EA = L0xbefff39c; Value = 0xb2c59c24; PC = 0x400ea4 *)
mov [r8_b, r8_t] [L0xbefff39c, L0xbefff39e];
(* ldr.w	r9, [r0, #28]                             #! EA = L0xbefff3a0; Value = 0xb1f9a390; PC = 0x400ea8 *)
mov [r9_b, r9_t] [L0xbefff3a0, L0xbefff3a2];
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400eac *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* sadd16	lr, r2, r3                               #! PC = 0x400eb0 *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* ssub16	r3, r2, r3                               #! PC = 0x400eb4 *)
sub r3_b r2_b r3_b;
sub r3_t r2_t r3_t;
(* sadd16	r11, r4, r5                              #! PC = 0x400eb8 *)
add r11_b r4_b r5_b;
add r11_t r4_t r5_t;
(* ssub16	r5, r4, r5                               #! PC = 0x400ebc *)
sub r5_b r4_b r5_b;
sub r5_t r4_t r5_t;
(* sadd16	r2, r6, r7                               #! PC = 0x400ec0 *)
add r2_b r6_b r7_b;
add r2_t r6_t r7_t;
(* ssub16	r7, r6, r7                               #! PC = 0x400ec4 *)
sub r7_b r6_b r7_b;
sub r7_t r6_t r7_t;
(* sadd16	r4, r8, r9                               #! PC = 0x400ec8 *)
add r4_b r8_b r9_b;
add r4_t r8_t r9_t;
(* ssub16	r9, r8, r9                               #! PC = 0x400ecc *)
sub r9_b r8_b r9_b;
sub r9_t r8_t r9_t;
(* sadd16	r8, lr, r11                              #! PC = 0x400ed0 *)
add r8_b lr_b r11_b;
add r8_t lr_t r11_t;
(* ssub16	r11, lr, r11                             #! PC = 0x400ed4 *)
sub r11_b lr_b r11_b;
sub r11_t lr_t r11_t;
(* sadd16	r6, r2, r4                               #! PC = 0x400ed8 *)
add r6_b r2_b r4_b;
add r6_t r2_t r4_t;
(* ssub16	r4, r2, r4                               #! PC = 0x400edc *)
sub r4_b r2_b r4_b;
sub r4_t r2_t r4_t;

ghost r5_b23@int16, r5_t23@int16:
      r5_b23 = r5_b /\ r5_t23 = r5_t
   && r5_b23 = r5_b /\ r5_t23 = r5_t;

(* vmov	r10, s10                                   #! PC = 0x400ee0 *)
mov [r10_b, r10_t] [s10_b, s10_t]; mov r10 s10;
(* smulwb	lr, r10, r5                              #! PC = 0x400ee4 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x400ee8 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400eec *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400ef0 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400ef4 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_b23 *  1600) [Q] /\
       eqmod lr_t (r5_t23 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_b23 *  1600) [Q] /\
       eqmod lr_t (r5_t23 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x400ef8 *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400efc *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r9_b38@int16, r9_t38@int16:
      r9_b38 = r9_b /\ r9_t38 = r9_t
   && r9_b38 = r9_b /\ r9_t38 = r9_t;

(* smulwb	lr, r10, r9                              #! PC = 0x400f00 *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400f04 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f08 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400f0c *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400f10 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b38 *  1600) [Q] /\
       eqmod lr_t (r9_t38 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b38 *  1600) [Q] /\
       eqmod lr_t (r9_t38 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x400f14 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400f18 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* sadd16	r2, r8, r6                               #! PC = 0x400f1c *)
add r2_b r8_b r6_b;
add r2_t r8_t r6_t;
(* ssub16	r6, r8, r6                               #! PC = 0x400f20 *)
sub r6_b r8_b r6_b;
sub r6_t r8_t r6_t;

ghost r7_b23@int16, r7_t23@int16:
      r7_b23 = r7_b /\ r7_t23 = r7_t
   && r7_b23 = r7_b /\ r7_t23 = r7_t;

(* vmov	r10, s12                                   #! PC = 0x400f24 *)
mov [r10_b, r10_t] [s12_b, s12_t]; mov r10 s12;
(* smulwb	lr, r10, r7                              #! PC = 0x400f28 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x400f2c *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f30 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400f34 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400f38 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_b23 *    40) [Q] /\
       eqmod lr_t (r7_t23 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_b23 *    40) [Q] /\
       eqmod lr_t (r7_t23 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x400f3c *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400f40 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

ghost r4_b23@int16, r4_t23@int16:
      r4_b23 = r4_b /\ r4_t23 = r4_t
   && r4_b23 = r4_b /\ r4_t23 = r4_t;

(* vmov	r10, s13                                   #! PC = 0x400f44 *)
mov [r10_b, r10_t] [s13_b, s13_t]; mov r10 s13;
(* smulwb	lr, r10, r4                              #! PC = 0x400f48 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x400f4c *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f50 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400f54 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400f58 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_b23 *  1600) [Q] /\
       eqmod lr_t (r4_t23 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_b23 *  1600) [Q] /\
       eqmod lr_t (r4_t23 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* ssub16	r8, r11, lr                              #! PC = 0x400f5c *)
sub r8_b r11_b lr_b;
sub r8_t r11_t lr_t;
(* sadd16	r4, r11, lr                              #! PC = 0x400f60 *)
add r4_b r11_b lr_b;
add r4_t r11_t lr_t;

ghost r9_b39@int16, r9_t39@int16:
      r9_b39 = r9_b /\ r9_t39 = r9_t
   && r9_b39 = r9_b /\ r9_t39 = r9_t;

(* vmov	r10, s14                                   #! PC = 0x400f64 *)
mov [r10_b, r10_t] [s14_b, s14_t]; mov r10 s14;
(* smulwb	lr, r10, r9                              #! PC = 0x400f68 *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x400f6c *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400f70 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400f74 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400f78 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_b39 *   749) [Q] /\
       eqmod lr_t (r9_t39 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_b39 *   749) [Q] /\
       eqmod lr_t (r9_t39 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x400f7c *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400f80 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* vmov	r0, s23                                    #! PC = 0x400f84 *)
mov [r0_b, r0_t] [s23_b, s23_t];
(* vmov	r11, s1                                    #! PC = 0x400f88 *)
mov [r11_b, r11_t] [s1_b, s1_t];
(* uadd16	lr, r3, r11                              #! PC = 0x400f8c *)
add lr_b r3_b r11_b;
add lr_t r3_t r11_t;
(* usub16	r3, r3, r11                              #! PC = 0x400f90 *)
sub r3_b r3_b r11_b;
sub r3_t r3_t r11_t;
(* str.w	lr, [r0, #4]                              #! EA = L0xbefff388; PC = 0x400f94 *)
mov [L0xbefff388, L0xbefff38a] [lr_b, lr_t];
(* str.w	r3, [r0, #36]	; 0x24                      #! EA = L0xbefff3a8; PC = 0x400f98 *)
mov [L0xbefff3a8, L0xbefff3aa] [r3_b, r3_t];
(* vmov	r11, s3                                    #! PC = 0x400f9c *)
mov [r11_b, r11_t] [s3_b, s3_t];
(* uadd16	lr, r5, r11                              #! PC = 0x400fa0 *)
add lr_b r5_b r11_b;
add lr_t r5_t r11_t;
(* usub16	r5, r5, r11                              #! PC = 0x400fa4 *)
sub r5_b r5_b r11_b;
sub r5_t r5_t r11_t;
(* str.w	lr, [r0, #12]                             #! EA = L0xbefff390; PC = 0x400fa8 *)
mov [L0xbefff390, L0xbefff392] [lr_b, lr_t];
(* str.w	r5, [r0, #44]	; 0x2c                      #! EA = L0xbefff3b0; PC = 0x400fac *)
mov [L0xbefff3b0, L0xbefff3b2] [r5_b, r5_t];
(* vmov	r11, s5                                    #! PC = 0x400fb0 *)
mov [r11_b, r11_t] [s5_b, s5_t];
(* uadd16	lr, r7, r11                              #! PC = 0x400fb4 *)
add lr_b r7_b r11_b;
add lr_t r7_t r11_t;
(* usub16	r7, r7, r11                              #! PC = 0x400fb8 *)
sub r7_b r7_b r11_b;
sub r7_t r7_t r11_t;
(* str.w	lr, [r0, #20]                             #! EA = L0xbefff398; PC = 0x400fbc *)
mov [L0xbefff398, L0xbefff39a] [lr_b, lr_t];
(* str.w	r7, [r0, #52]	; 0x34                      #! EA = L0xbefff3b8; PC = 0x400fc0 *)
mov [L0xbefff3b8, L0xbefff3ba] [r7_b, r7_t];
(* vmov	r11, s7                                    #! PC = 0x400fc4 *)
mov [r11_b, r11_t] [s7_b, s7_t];
(* uadd16	lr, r9, r11                              #! PC = 0x400fc8 *)
add lr_b r9_b r11_b;
add lr_t r9_t r11_t;
(* usub16	r9, r9, r11                              #! PC = 0x400fcc *)
sub r9_b r9_b r11_b;
sub r9_t r9_t r11_t;
(* str.w	lr, [r0, #28]                             #! EA = L0xbefff3a0; PC = 0x400fd0 *)
mov [L0xbefff3a0, L0xbefff3a2] [lr_b, lr_t];
(* str.w	r9, [r0, #60]	; 0x3c                      #! EA = L0xbefff3c0; PC = 0x400fd4 *)
mov [L0xbefff3c0, L0xbefff3c2] [r9_b, r9_t];
(* vmov	r5, s2                                     #! PC = 0x400fd8 *)
mov [r5_b, r5_t] [s2_b, s2_t];
(* uadd16	lr, r4, r5                               #! PC = 0x400fdc *)
add lr_b r4_b r5_b;
add lr_t r4_t r5_t;
(* usub16	r11, r4, r5                              #! PC = 0x400fe0 *)
sub r11_b r4_b r5_b;
sub r11_t r4_t r5_t;
(* str.w	lr, [r0, #8]                              #! EA = L0xbefff38c; PC = 0x400fe4 *)
mov [L0xbefff38c, L0xbefff38e] [lr_b, lr_t];
(* str.w	r11, [r0, #40]	; 0x28                     #! EA = L0xbefff3ac; PC = 0x400fe8 *)
mov [L0xbefff3ac, L0xbefff3ae] [r11_b, r11_t];
(* vmov	r7, s4                                     #! PC = 0x400fec *)
mov [r7_b, r7_t] [s4_b, s4_t];
(* uadd16	lr, r6, r7                               #! PC = 0x400ff0 *)
add lr_b r6_b r7_b;
add lr_t r6_t r7_t;
(* usub16	r11, r6, r7                              #! PC = 0x400ff4 *)
sub r11_b r6_b r7_b;
sub r11_t r6_t r7_t;
(* str.w	lr, [r0, #16]                             #! EA = L0xbefff394; PC = 0x400ff8 *)
mov [L0xbefff394, L0xbefff396] [lr_b, lr_t];
(* str.w	r11, [r0, #48]	; 0x30                     #! EA = L0xbefff3b4; PC = 0x400ffc *)
mov [L0xbefff3b4, L0xbefff3b6] [r11_b, r11_t];
(* vmov	r9, s6                                     #! PC = 0x401000 *)
mov [r9_b, r9_t] [s6_b, s6_t];
(* uadd16	lr, r8, r9                               #! PC = 0x401004 *)
add lr_b r8_b r9_b;
add lr_t r8_t r9_t;
(* usub16	r11, r8, r9                              #! PC = 0x401008 *)
sub r11_b r8_b r9_b;
sub r11_t r8_t r9_t;
(* str.w	lr, [r0, #24]                             #! EA = L0xbefff39c; PC = 0x40100c *)
mov [L0xbefff39c, L0xbefff39e] [lr_b, lr_t];
(* str.w	r11, [r0, #56]	; 0x38                     #! EA = L0xbefff3bc; PC = 0x401010 *)
mov [L0xbefff3bc, L0xbefff3be] [r11_b, r11_t];
(* vmov	r3, s0                                     #! PC = 0x401014 *)
mov [r3_b, r3_t] [s0_b, s0_t];
(* uadd16	lr, r2, r3                               #! PC = 0x401018 *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* usub16	r11, r2, r3                              #! PC = 0x40101c *)
sub r11_b r2_b r3_b;
sub r11_t r2_t r3_t;
(* str.w	r11, [r0, #32]                            #! EA = L0xbefff3a4; PC = 0x401020 *)
mov [L0xbefff3a4, L0xbefff3a6] [r11_b, r11_t];
(* str.w	lr, [r0], #64                             #! EA = L0xbefff384; PC = 0x401024 *)
mov [L0xbefff384, L0xbefff386] [lr_b, lr_t];
(* vmov	lr, s8                                     #! PC = 0x401028 *)
mov [lr_b, lr_t] [s8_b, s8_t]; mov lr s8;
(* cmp.w	r0, lr                                    #! PC = 0x40102c *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x400cc4 <invntt_fast+24>                #! PC = 0x401030 *)
#bne.w	0x400cc4 <invntt_fast+24>                #! 0x401030 = 0x401030;

(* CUT 15 *)
assert [16*NQ2,16*NQ2,5*NQ2]< [L0xbefff384,L0xbefff386,L0xbefff388] /\
       [L0xbefff384,L0xbefff386,L0xbefff388]< [16*Q2,16*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff38a,L0xbefff38c,L0xbefff38e] /\
       [L0xbefff38a,L0xbefff38c,L0xbefff38e]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2,9*NQ2]< [L0xbefff390,L0xbefff392,L0xbefff394] /\
       [L0xbefff390,L0xbefff392,L0xbefff394]< [5*Q2,5*Q2,9*Q2] /\
       [9*NQ2,5*NQ2,5*NQ2]< [L0xbefff396,L0xbefff398,L0xbefff39a] /\
       [L0xbefff396,L0xbefff398,L0xbefff39a]< [9*Q2,5*Q2,5*Q2] /\
       [6*NQ2,6*NQ2,5*NQ2]< [L0xbefff39c,L0xbefff39e,L0xbefff3a0] /\
       [L0xbefff39c,L0xbefff39e,L0xbefff3a0]< [6*Q2,6*Q2,5*Q2] /\
       [5*NQ2,16*NQ2,16*NQ2]< [L0xbefff3a2,L0xbefff3a4,L0xbefff3a6] /\
       [L0xbefff3a2,L0xbefff3a4,L0xbefff3a6]< [5*Q2,16*Q2,16*Q2] /\
       [5*NQ2,5*NQ2,6*NQ2]< [L0xbefff3a8,L0xbefff3aa,L0xbefff3ac] /\
       [L0xbefff3a8,L0xbefff3aa,L0xbefff3ac]< [5*Q2,5*Q2,6*Q2] /\
       [6*NQ2,5*NQ2,5*NQ2]< [L0xbefff3ae,L0xbefff3b0,L0xbefff3b2] /\
       [L0xbefff3ae,L0xbefff3b0,L0xbefff3b2]< [6*Q2,5*Q2,5*Q2] /\
       [9*NQ2,9*NQ2,5*NQ2]< [L0xbefff3b4,L0xbefff3b6,L0xbefff3b8] /\
       [L0xbefff3b4,L0xbefff3b6,L0xbefff3b8]< [9*Q2,9*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff3ba,L0xbefff3bc,L0xbefff3be] /\
       [L0xbefff3ba,L0xbefff3bc,L0xbefff3be]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2]< [L0xbefff3c0,L0xbefff3c2] /\
       [L0xbefff3c0,L0xbefff3c2]< [5*Q2,5*Q2]
       prove with [algebra solver isl, precondition] && true;
assume [16*NQ2,16*NQ2,5*NQ2]< [L0xbefff384,L0xbefff386,L0xbefff388] /\
       [L0xbefff384,L0xbefff386,L0xbefff388]< [16*Q2,16*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff38a,L0xbefff38c,L0xbefff38e] /\
       [L0xbefff38a,L0xbefff38c,L0xbefff38e]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2,9*NQ2]< [L0xbefff390,L0xbefff392,L0xbefff394] /\
       [L0xbefff390,L0xbefff392,L0xbefff394]< [5*Q2,5*Q2,9*Q2] /\
       [9*NQ2,5*NQ2,5*NQ2]< [L0xbefff396,L0xbefff398,L0xbefff39a] /\
       [L0xbefff396,L0xbefff398,L0xbefff39a]< [9*Q2,5*Q2,5*Q2] /\
       [6*NQ2,6*NQ2,5*NQ2]< [L0xbefff39c,L0xbefff39e,L0xbefff3a0] /\
       [L0xbefff39c,L0xbefff39e,L0xbefff3a0]< [6*Q2,6*Q2,5*Q2] /\
       [5*NQ2,16*NQ2,16*NQ2]< [L0xbefff3a2,L0xbefff3a4,L0xbefff3a6] /\
       [L0xbefff3a2,L0xbefff3a4,L0xbefff3a6]< [5*Q2,16*Q2,16*Q2] /\
       [5*NQ2,5*NQ2,6*NQ2]< [L0xbefff3a8,L0xbefff3aa,L0xbefff3ac] /\
       [L0xbefff3a8,L0xbefff3aa,L0xbefff3ac]< [5*Q2,5*Q2,6*Q2] /\
       [6*NQ2,5*NQ2,5*NQ2]< [L0xbefff3ae,L0xbefff3b0,L0xbefff3b2] /\
       [L0xbefff3ae,L0xbefff3b0,L0xbefff3b2]< [6*Q2,5*Q2,5*Q2] /\
       [9*NQ2,9*NQ2,5*NQ2]< [L0xbefff3b4,L0xbefff3b6,L0xbefff3b8] /\
       [L0xbefff3b4,L0xbefff3b6,L0xbefff3b8]< [9*Q2,9*Q2,5*Q2] /\
       [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff3ba,L0xbefff3bc,L0xbefff3be] /\
       [L0xbefff3ba,L0xbefff3bc,L0xbefff3be]< [5*Q2,6*Q2,6*Q2] /\
       [5*NQ2,5*NQ2]< [L0xbefff3c0,L0xbefff3c2] /\
       [L0xbefff3c0,L0xbefff3c2]< [5*Q2,5*Q2]
    && [16@16*NQ2,16@16*NQ2,5@16*NQ2]< [L0xbefff384,L0xbefff386,L0xbefff388] /\
       [L0xbefff384,L0xbefff386,L0xbefff388]< [16@16*Q2,16@16*Q2,5@16*Q2] /\
       [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff38a,L0xbefff38c,L0xbefff38e] /\
       [L0xbefff38a,L0xbefff38c,L0xbefff38e]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
       [5@16*NQ2,5@16*NQ2,9@16*NQ2]< [L0xbefff390,L0xbefff392,L0xbefff394] /\
       [L0xbefff390,L0xbefff392,L0xbefff394]< [5@16*Q2,5@16*Q2,9@16*Q2] /\
       [9@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff396,L0xbefff398,L0xbefff39a] /\
       [L0xbefff396,L0xbefff398,L0xbefff39a]< [9@16*Q2,5@16*Q2,5@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,5@16*NQ2]< [L0xbefff39c,L0xbefff39e,L0xbefff3a0] /\
       [L0xbefff39c,L0xbefff39e,L0xbefff3a0]< [6@16*Q2,6@16*Q2,5@16*Q2] /\
       [5@16*NQ2,16@16*NQ2,16@16*NQ2]< [L0xbefff3a2,L0xbefff3a4,L0xbefff3a6] /\
       [L0xbefff3a2,L0xbefff3a4,L0xbefff3a6]< [5@16*Q2,16@16*Q2,16@16*Q2] /\
       [5@16*NQ2,5@16*NQ2,6@16*NQ2]< [L0xbefff3a8,L0xbefff3aa,L0xbefff3ac] /\
       [L0xbefff3a8,L0xbefff3aa,L0xbefff3ac]< [5@16*Q2,5@16*Q2,6@16*Q2] /\
       [6@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff3ae,L0xbefff3b0,L0xbefff3b2] /\
       [L0xbefff3ae,L0xbefff3b0,L0xbefff3b2]< [6@16*Q2,5@16*Q2,5@16*Q2] /\
       [9@16*NQ2,9@16*NQ2,5@16*NQ2]< [L0xbefff3b4,L0xbefff3b6,L0xbefff3b8] /\
       [L0xbefff3b4,L0xbefff3b6,L0xbefff3b8]< [9@16*Q2,9@16*Q2,5@16*Q2] /\
       [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff3ba,L0xbefff3bc,L0xbefff3be] /\
       [L0xbefff3ba,L0xbefff3bc,L0xbefff3be]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
       [5@16*NQ2,5@16*NQ2]< [L0xbefff3c0,L0xbefff3c2] /\
       [L0xbefff3c0,L0xbefff3c2]< [5@16*Q2,5@16*Q2];
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod (poly Z7
           [poly X [L0xbefff384,L0xbefff386],poly X [L0xbefff388,L0xbefff38a],
            poly X [L0xbefff38c,L0xbefff38e],poly X [L0xbefff390,L0xbefff392],
            poly X [L0xbefff394,L0xbefff396],poly X [L0xbefff398,L0xbefff39a],
            poly X [L0xbefff39c,L0xbefff39e],poly X [L0xbefff3a0,L0xbefff3a2],
            poly X [L0xbefff3a4,L0xbefff3a6],poly X [L0xbefff3a8,L0xbefff3aa],
            poly X [L0xbefff3ac,L0xbefff3ae],poly X [L0xbefff3b0,L0xbefff3b2],
            poly X [L0xbefff3b4,L0xbefff3b6],poly X [L0xbefff3b8,L0xbefff3ba],
            poly X [L0xbefff3bc,L0xbefff3be],poly X [L0xbefff3c0,L0xbefff3c2]])
          (2**4*F**2) [Q, Z7**16 - 1] /\
    [16*NQ2,16*NQ2,5*NQ2]< [L0xbefff384,L0xbefff386,L0xbefff388] /\
    [L0xbefff384,L0xbefff386,L0xbefff388]< [16*Q2,16*Q2,5*Q2] /\
    [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff38a,L0xbefff38c,L0xbefff38e] /\
    [L0xbefff38a,L0xbefff38c,L0xbefff38e]< [5*Q2,6*Q2,6*Q2] /\
    [5*NQ2,5*NQ2,9*NQ2]< [L0xbefff390,L0xbefff392,L0xbefff394] /\
    [L0xbefff390,L0xbefff392,L0xbefff394]< [5*Q2,5*Q2,9*Q2] /\
    [9*NQ2,5*NQ2,5*NQ2]< [L0xbefff396,L0xbefff398,L0xbefff39a] /\
    [L0xbefff396,L0xbefff398,L0xbefff39a]< [9*Q2,5*Q2,5*Q2] /\
    [6*NQ2,6*NQ2,5*NQ2]< [L0xbefff39c,L0xbefff39e,L0xbefff3a0] /\
    [L0xbefff39c,L0xbefff39e,L0xbefff3a0]< [6*Q2,6*Q2,5*Q2] /\
    [5*NQ2,16*NQ2,16*NQ2]< [L0xbefff3a2,L0xbefff3a4,L0xbefff3a6] /\
    [L0xbefff3a2,L0xbefff3a4,L0xbefff3a6]< [5*Q2,16*Q2,16*Q2] /\
    [5*NQ2,5*NQ2,6*NQ2]< [L0xbefff3a8,L0xbefff3aa,L0xbefff3ac] /\
    [L0xbefff3a8,L0xbefff3aa,L0xbefff3ac]< [5*Q2,5*Q2,6*Q2] /\
    [6*NQ2,5*NQ2,5*NQ2]< [L0xbefff3ae,L0xbefff3b0,L0xbefff3b2] /\
    [L0xbefff3ae,L0xbefff3b0,L0xbefff3b2]< [6*Q2,5*Q2,5*Q2] /\
    [9*NQ2,9*NQ2,5*NQ2]< [L0xbefff3b4,L0xbefff3b6,L0xbefff3b8] /\
    [L0xbefff3b4,L0xbefff3b6,L0xbefff3b8]< [9*Q2,9*Q2,5*Q2] /\
    [5*NQ2,6*NQ2,6*NQ2]< [L0xbefff3ba,L0xbefff3bc,L0xbefff3be] /\
    [L0xbefff3ba,L0xbefff3bc,L0xbefff3be]< [5*Q2,6*Q2,6*Q2] /\
    [5*NQ2,5*NQ2]< [L0xbefff3c0,L0xbefff3c2] /\
    [L0xbefff3c0,L0xbefff3c2]< [5*Q2,5*Q2]
    prove with [precondition, all ghosts]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [16@16*NQ2,16@16*NQ2,5@16*NQ2]< [L0xbefff384,L0xbefff386,L0xbefff388] /\
    [L0xbefff384,L0xbefff386,L0xbefff388]< [16@16*Q2,16@16*Q2,5@16*Q2] /\
    [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff38a,L0xbefff38c,L0xbefff38e] /\
    [L0xbefff38a,L0xbefff38c,L0xbefff38e]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
    [5@16*NQ2,5@16*NQ2,9@16*NQ2]< [L0xbefff390,L0xbefff392,L0xbefff394] /\
    [L0xbefff390,L0xbefff392,L0xbefff394]< [5@16*Q2,5@16*Q2,9@16*Q2] /\
    [9@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff396,L0xbefff398,L0xbefff39a] /\
    [L0xbefff396,L0xbefff398,L0xbefff39a]< [9@16*Q2,5@16*Q2,5@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,5@16*NQ2]< [L0xbefff39c,L0xbefff39e,L0xbefff3a0] /\
    [L0xbefff39c,L0xbefff39e,L0xbefff3a0]< [6@16*Q2,6@16*Q2,5@16*Q2] /\
    [5@16*NQ2,16@16*NQ2,16@16*NQ2]< [L0xbefff3a2,L0xbefff3a4,L0xbefff3a6] /\
    [L0xbefff3a2,L0xbefff3a4,L0xbefff3a6]< [5@16*Q2,16@16*Q2,16@16*Q2] /\
    [5@16*NQ2,5@16*NQ2,6@16*NQ2]< [L0xbefff3a8,L0xbefff3aa,L0xbefff3ac] /\
    [L0xbefff3a8,L0xbefff3aa,L0xbefff3ac]< [5@16*Q2,5@16*Q2,6@16*Q2] /\
    [6@16*NQ2,5@16*NQ2,5@16*NQ2]< [L0xbefff3ae,L0xbefff3b0,L0xbefff3b2] /\
    [L0xbefff3ae,L0xbefff3b0,L0xbefff3b2]< [6@16*Q2,5@16*Q2,5@16*Q2] /\
    [9@16*NQ2,9@16*NQ2,5@16*NQ2]< [L0xbefff3b4,L0xbefff3b6,L0xbefff3b8] /\
    [L0xbefff3b4,L0xbefff3b6,L0xbefff3b8]< [9@16*Q2,9@16*Q2,5@16*Q2] /\
    [5@16*NQ2,6@16*NQ2,6@16*NQ2]< [L0xbefff3ba,L0xbefff3bc,L0xbefff3be] /\
    [L0xbefff3ba,L0xbefff3bc,L0xbefff3be]< [5@16*Q2,6@16*Q2,6@16*Q2] /\
    [5@16*NQ2,5@16*NQ2]< [L0xbefff3c0,L0xbefff3c2] /\
    [L0xbefff3c0,L0xbefff3c2]< [5@16*Q2,5@16*Q2]
    prove with [precondition];

(* sub.w	r0, r0, #512	; 0x200                      #! PC = 0x401034 *)
subs dontcare r0 r0 512@int32;
(* vmov	s6, r0                                     #! PC = 0x401038 *)
mov [s6_b, s6_t] [r0_b, r0_t];
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff1c4; Value = 0x9e8076d0; PC = 0x40103c *)
mov [r2_b, r2_t] [L0xbefff1c4, L0xbefff1c6];
(* ldr.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff204; Value = 0x3f000a30; PC = 0x401040 *)
mov [r3_b, r3_t] [L0xbefff204, L0xbefff206];
(* ldr.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff244; Value = 0x45b086a0; PC = 0x401044 *)
mov [r4_b, r4_t] [L0xbefff244, L0xbefff246];
(* ldr.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff284; Value = 0x97d0fa60; PC = 0x401048 *)
mov [r5_b, r5_t] [L0xbefff284, L0xbefff286];
(* ldr.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2c4; Value = 0x84c0a4b0; PC = 0x40104c *)
mov [r6_b, r6_t] [L0xbefff2c4, L0xbefff2c6];
(* ldr.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff304; Value = 0x58c0dc50; PC = 0x401050 *)
mov [r7_b, r7_t] [L0xbefff304, L0xbefff306];
(* ldr.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff344; Value = 0x4950e4a0; PC = 0x401054 *)
mov [r8_b, r8_t] [L0xbefff344, L0xbefff346];
(* ldr.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff384; Value = 0x94309c60; PC = 0x401058 *)
mov [r9_b, r9_t] [L0xbefff384, L0xbefff386];
(* vldmia	r1!, {s0-s5}                             #! EA = L0x4018b8; PC = 0x40105c *)
mov s0 L0x4018b8; spl s0_t s0_b s0 16; cast s0_b@int16 s0_b;
mov s1 L0x4018bc; spl s1_t s1_b s1 16; cast s1_b@int16 s1_b;
mov s2 L0x4018c0; spl s2_t s2_b s2 16; cast s2_b@int16 s2_b;
mov s3 L0x4018c4; spl s3_t s3_b s3 16; cast s3_b@int16 s3_b;
mov s4 L0x4018c8; spl s4_t s4_b s4 16; cast s4_b@int16 s4_b;
mov s5 L0x4018cc; spl s5_t s5_b s5 16; cast s5_b@int16 s5_b;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x401060 *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* movw	r10, #44984	; 0xafb8                       #! PC = 0x401064 *)
mov r10_b (-20552)@int16; mov r10_t 0@int16;
(* movt	r10, #19                                   #! PC = 0x401068 *)
mov r10_t 19@int16;

mov r10 1290168@int32; # = 19*2**16 + 44984

ghost r2_b0@int16, r2_t0@int16:
      r2_b0 = r2_b /\ r2_t0 = r2_t
   && r2_b0 = r2_b /\ r2_t0 = r2_t;

(* smulwb	lr, r10, r2                              #! PC = 0x40106c *)
vpc r2_bW@int32 r2_b; mulj tmp r10 r2_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r2, r10, r2                              #! PC = 0x401070 *)
vpc r2_tW@int32 r2_t; mulj tmp r10 r2_tW; spl r2_w dc tmp 16;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b; vpc r2_t@int16 r2_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401074 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r2, r2, r12, r0                          #! PC = 0x401078 *)
mulj prod r2_b r12_t; add r2_w prod r0;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b;
(* pkhtb	r2, r2, lr, asr #16                       #! PC = 0x40107c *)
mov tmp_b lr_t; mov tmp_t r2_t;
mov r2_b tmp_b; mov r2_t tmp_t;

assert eqmod r2_b (r2_b0 *   1) [Q] /\
       eqmod r2_t (r2_t0 *   1) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r2_b (r2_b0 *   1) [Q] /\
       eqmod r2_t (r2_t0 *   1) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r2_b, r2_t] /\ [r2_b, r2_t] <s[Q2, Q2];

ghost r3_b8@int16, r3_t8@int16:
      r3_b8 = r3_b /\ r3_t8 = r3_t
   && r3_b8 = r3_b /\ r3_t8 = r3_t;

(* smulwb	lr, r10, r3                              #! PC = 0x401080 *)
vpc r3_bW@int32 r3_b; mulj tmp r10 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r10, r3                              #! PC = 0x401084 *)
vpc r3_tW@int32 r3_t; mulj tmp r10 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401088 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x40108c *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x401090 *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov r3_b tmp_b; mov r3_t tmp_t;

assert eqmod r3_b (r3_b8 *   1) [Q] /\
       eqmod r3_t (r3_t8 *   1) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r3_b (r3_b8 *   1) [Q] /\
       eqmod r3_t (r3_t8 *   1) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r3_b, r3_t] /\ [r3_b, r3_t] <s[Q2, Q2];

ghost r4_b24@int16, r4_t24@int16:
      r4_b24 = r4_b /\ r4_t24 = r4_t
   && r4_b24 = r4_b /\ r4_t24 = r4_t;

(* smulwb	lr, r10, r4                              #! PC = 0x401094 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401098 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40109c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x4010a0 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x4010a4 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov r4_b tmp_b; mov r4_t tmp_t;

assert eqmod r4_b (r4_b24 *   1) [Q] /\
       eqmod r4_t (r4_t24 *   1) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r4_b (r4_b24 *   1) [Q] /\
       eqmod r4_t (r4_t24 *   1) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r4_b, r4_t] /\ [r4_b, r4_t] <s[Q2, Q2];

ghost r5_b24@int16, r5_t24@int16:
      r5_b24 = r5_b /\ r5_t24 = r5_t
   && r5_b24 = r5_b /\ r5_t24 = r5_t;

(* smulwb	lr, r10, r5                              #! PC = 0x4010a8 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x4010ac *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4010b0 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x4010b4 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x4010b8 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov r5_b tmp_b; mov r5_t tmp_t;

assert eqmod r5_b (r5_b24 *   1) [Q] /\
       eqmod r5_t (r5_t24 *   1) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r5_b (r5_b24 *   1) [Q] /\
       eqmod r5_t (r5_t24 *   1) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r5_b, r5_t] /\ [r5_b, r5_t] <s[Q2, Q2];

ghost r6_b8@int16, r6_t8@int16:
      r6_b8 = r6_b /\ r6_t8 = r6_t
   && r6_b8 = r6_b /\ r6_t8 = r6_t;

(* smulwb	lr, r10, r6                              #! PC = 0x4010bc *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4010c0 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4010c4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4010c8 *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x4010cc *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov r6_b tmp_b; mov r6_t tmp_t;

assert eqmod r6_b (r6_b8 *   1) [Q] /\
       eqmod r6_t (r6_t8 *   1) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r6_b (r6_b8 *   1) [Q] /\
       eqmod r6_t (r6_t8 *   1) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r6_b, r6_t] /\ [r6_b, r6_t] <s[Q2, Q2];

ghost r7_b24@int16, r7_t24@int16:
      r7_b24 = r7_b /\ r7_t24 = r7_t
   && r7_b24 = r7_b /\ r7_t24 = r7_t;

(* smulwb	lr, r10, r7                              #! PC = 0x4010d0 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x4010d4 *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4010d8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4010dc *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x4010e0 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov r7_b tmp_b; mov r7_t tmp_t;

assert eqmod r7_b (r7_b24 *   1) [Q] /\
       eqmod r7_t (r7_t24 *   1) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r7_b (r7_b24 *   1) [Q] /\
       eqmod r7_t (r7_t24 *   1) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r7_b, r7_t] /\ [r7_b, r7_t] <s[Q2, Q2];

ghost r8_b8@int16, r8_t8@int16:
      r8_b8 = r8_b /\ r8_t8 = r8_t
   && r8_b8 = r8_b /\ r8_t8 = r8_t;

(* smulwb	lr, r10, r8                              #! PC = 0x4010e4 *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x4010e8 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4010ec *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4010f0 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x4010f4 *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov r8_b tmp_b; mov r8_t tmp_t;

assert eqmod r8_b (r8_b8 *   1) [Q] /\
       eqmod r8_t (r8_t8 *   1) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r8_b (r8_b8 *   1) [Q] /\
       eqmod r8_t (r8_t8 *   1) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r8_b, r8_t] /\ [r8_b, r8_t] <s[Q2, Q2];

ghost r9_b40@int16, r9_t40@int16:
      r9_b40 = r9_b /\ r9_t40 = r9_t
   && r9_b40 = r9_b /\ r9_t40 = r9_t;

(* smulwb	lr, r10, r9                              #! PC = 0x4010f8 *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x4010fc *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401100 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401104 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x401108 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov r9_b tmp_b; mov r9_t tmp_t;

assert eqmod r9_b (r9_b40 *   1) [Q] /\
       eqmod r9_t (r9_t40 *   1) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r9_b (r9_b40 *   1) [Q] /\
       eqmod r9_t (r9_t40 *   1) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r9_b, r9_t] /\ [r9_b, r9_t] <s[Q2, Q2];

(* CUT 16 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod r2_b (r2_b0 *   1) [Q] /\
    eqmod r2_t (r2_t0 *   1) [Q] /\
    eqmod r3_b (r3_b8 *   1) [Q] /\
    eqmod r3_t (r3_t8 *   1) [Q] /\
    eqmod r4_b (r4_b24 *   1) [Q] /\
    eqmod r4_t (r4_t24 *   1) [Q] /\
    eqmod r5_b (r5_b24 *   1) [Q] /\
    eqmod r5_t (r5_t24 *   1) [Q] /\
    eqmod r6_b (r6_b8 *   1) [Q] /\
    eqmod r6_t (r6_t8 *   1) [Q] /\
    eqmod r7_b (r7_b24 *   1) [Q] /\
    eqmod r7_t (r7_t24 *   1) [Q] /\
    eqmod r8_b (r8_b8 *   1) [Q] /\
    eqmod r8_t (r8_t8 *   1) [Q] /\
    eqmod r9_b (r9_b40 *   1) [Q] /\
    eqmod r9_t (r9_t40 *   1) [Q] /\
    [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2] /\
    [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2] /\
    [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2] /\
    [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2] /\
    [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2] /\
    [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2] /\
    [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2] /\
    [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [NQ2, NQ2] <s[r2_b, r2_t] /\ [r2_b, r2_t] <s[Q2, Q2] /\
    [NQ2, NQ2] <s[r3_b, r3_t] /\ [r3_b, r3_t] <s[Q2, Q2] /\
    [NQ2, NQ2] <s[r4_b, r4_t] /\ [r4_b, r4_t] <s[Q2, Q2] /\
    [NQ2, NQ2] <s[r5_b, r5_t] /\ [r5_b, r5_t] <s[Q2, Q2] /\
    [NQ2, NQ2] <s[r6_b, r6_t] /\ [r6_b, r6_t] <s[Q2, Q2] /\
    [NQ2, NQ2] <s[r7_b, r7_t] /\ [r7_b, r7_t] <s[Q2, Q2] /\
    [NQ2, NQ2] <s[r8_b, r8_t] /\ [r8_b, r8_t] <s[Q2, Q2] /\
    [NQ2, NQ2] <s[r9_b, r9_t] /\ [r9_b, r9_t] <s[Q2, Q2];

ghost r2_bo0@int16,r2_to0@int16,r3_bo0@int16,r3_to0@int16,
      r4_bo0@int16,r4_to0@int16,r5_bo0@int16,r5_to0@int16,
      r6_bo0@int16,r6_to0@int16,r7_bo0@int16,r7_to0@int16,
      r8_bo0@int16,r8_to0@int16,r9_bo0@int16,r9_to0@int16:
      r2_bo0 = r2_b /\ r2_to0 = r2_t /\ r3_bo0 = r3_b /\ r3_to0 = r3_t /\
      r4_bo0 = r4_b /\ r4_to0 = r4_t /\ r5_bo0 = r5_b /\ r5_to0 = r5_t /\
      r6_bo0 = r6_b /\ r6_to0 = r6_t /\ r7_bo0 = r7_b /\ r7_to0 = r7_t /\
      r8_bo0 = r8_b /\ r8_to0 = r8_t /\ r9_bo0 = r9_b /\ r9_to0 = r9_t
   && r2_bo0 = r2_b /\ r2_to0 = r2_t /\ r3_bo0 = r3_b /\ r3_to0 = r3_t /\
      r4_bo0 = r4_b /\ r4_to0 = r4_t /\ r5_bo0 = r5_b /\ r5_to0 = r5_t /\
      r6_bo0 = r6_b /\ r6_to0 = r6_t /\ r7_bo0 = r7_b /\ r7_to0 = r7_t /\
      r8_bo0 = r8_b /\ r8_to0 = r8_t /\ r9_bo0 = r9_b /\ r9_to0 = r9_t;

(* sadd16	lr, r2, r3                               #! PC = 0x40110c *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* ssub16	r3, r2, r3                               #! PC = 0x401110 *)
sub r3_b r2_b r3_b;
sub r3_t r2_t r3_t;
(* sadd16	r11, r4, r5                              #! PC = 0x401114 *)
add r11_b r4_b r5_b;
add r11_t r4_t r5_t;
(* ssub16	r5, r4, r5                               #! PC = 0x401118 *)
sub r5_b r4_b r5_b;
sub r5_t r4_t r5_t;
(* sadd16	r2, r6, r7                               #! PC = 0x40111c *)
add r2_b r6_b r7_b;
add r2_t r6_t r7_t;
(* ssub16	r7, r6, r7                               #! PC = 0x401120 *)
sub r7_b r6_b r7_b;
sub r7_t r6_t r7_t;
(* sadd16	r4, r8, r9                               #! PC = 0x401124 *)
add r4_b r8_b r9_b;
add r4_t r8_t r9_t;
(* ssub16	r9, r8, r9                               #! PC = 0x401128 *)
sub r9_b r8_b r9_b;
sub r9_t r8_t r9_t;

assert [2*NQ2, 2*NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [2*Q2, 2*Q2] /\
       [2*NQ2, 2*NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [2*Q2, 2*Q2] /\
       [2*NQ2, 2*NQ2] < [r11_b, r11_t] /\ [r11_b, r11_t] < [2*Q2, 2*Q2] /\
       [2*NQ2, 2*NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [2*Q2, 2*Q2] /\
       [2*NQ2, 2*NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [2*Q2, 2*Q2] /\
       [2*NQ2, 2*NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [2*Q2, 2*Q2] /\
       [2*NQ2, 2*NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [2*Q2, 2*Q2] /\
       [2*NQ2, 2*NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [2*Q2, 2*Q2]
       prove with [algebra solver isl] && true;
assume [2*NQ2, 2*NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [2*Q2, 2*Q2] /\
       [2*NQ2, 2*NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [2*Q2, 2*Q2] /\
       [2*NQ2, 2*NQ2] < [r11_b, r11_t] /\ [r11_b, r11_t] < [2*Q2, 2*Q2] /\
       [2*NQ2, 2*NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [2*Q2, 2*Q2] /\
       [2*NQ2, 2*NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [2*Q2, 2*Q2] /\
       [2*NQ2, 2*NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [2*Q2, 2*Q2] /\
       [2*NQ2, 2*NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [2*Q2, 2*Q2] /\
       [2*NQ2, 2*NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [2*Q2, 2*Q2]
    && [2@16*NQ2,2@16*NQ2]<s[lr_b,lr_t]/\[lr_b,lr_t]<s[2@16*Q2,2@16*Q2] /\
       [2@16*NQ2,2@16*NQ2]<s[r3_b,r3_t]/\[r3_b,r3_t]<s[2@16*Q2,2@16*Q2] /\
       [2@16*NQ2,2@16*NQ2]<s[r11_b,r11_t]/\[r11_b,r11_t]<s[2@16*Q2,2@16*Q2] /\
       [2@16*NQ2,2@16*NQ2]<s[r5_b,r5_t]/\[r5_b,r5_t]<s[2@16*Q2,2@16*Q2] /\
       [2@16*NQ2,2@16*NQ2]<s[r2_b,r2_t]/\[r2_b,r2_t]<s[2@16*Q2,2@16*Q2] /\
       [2@16*NQ2,2@16*NQ2]<s[r7_b,r7_t]/\[r7_b,r7_t]<s[2@16*Q2,2@16*Q2] /\
       [2@16*NQ2,2@16*NQ2]<s[r4_b,r4_t]/\[r4_b,r4_t]<s[2@16*Q2,2@16*Q2] /\
       [2@16*NQ2, 2@16*NQ2]<s[r9_b,r9_t]/\[r9_b,r9_t]<s[2@16*Q2,2@16*Q2];

(* CUT 17 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    [lr_b, lr_t] = [r2_bo0, r2_to0] + [r3_bo0, r3_to0] /\
    [r3_b, r3_t] = [r2_bo0, r2_to0] - [r3_bo0, r3_to0] /\
    [r11_b, r11_t] = [r4_bo0, r4_to0] + [r5_bo0, r5_to0] /\
    [r5_b, r5_t] = [r4_bo0, r4_to0] - [r5_bo0, r5_to0] /\
    [r2_b, r2_t] = [r6_bo0, r6_to0] + [r7_bo0, r7_to0] /\
    [r7_b, r7_t] = [r6_bo0, r6_to0] - [r7_bo0, r7_to0] /\
    [r4_b, r4_t] = [r8_bo0, r8_to0] + [r9_bo0, r9_to0] /\
    [r9_b, r9_t] = [r8_bo0, r8_to0] - [r9_bo0, r9_to0] /\
    [2*NQ2, 2*NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [2*Q2, 2*Q2] /\
    [2*NQ2, 2*NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [2*Q2, 2*Q2] /\
    [2*NQ2, 2*NQ2] < [r11_b, r11_t] /\ [r11_b, r11_t] < [2*Q2, 2*Q2] /\
    [2*NQ2, 2*NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [2*Q2, 2*Q2] /\
    [2*NQ2, 2*NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [2*Q2, 2*Q2] /\
    [2*NQ2, 2*NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [2*Q2, 2*Q2] /\
    [2*NQ2, 2*NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [2*Q2, 2*Q2] /\
    [2*NQ2, 2*NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [2*Q2, 2*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [2@16*NQ2,2@16*NQ2]<s[lr_b,lr_t]/\[lr_b,lr_t]<s[2@16*Q2,2@16*Q2] /\
    [2@16*NQ2,2@16*NQ2]<s[r3_b,r3_t]/\[r3_b,r3_t]<s[2@16*Q2,2@16*Q2] /\
    [2@16*NQ2,2@16*NQ2]<s[r11_b,r11_t]/\[r11_b,r11_t]<s[2@16*Q2,2@16*Q2] /\
    [2@16*NQ2,2@16*NQ2]<s[r5_b,r5_t]/\[r5_b,r5_t]<s[2@16*Q2,2@16*Q2] /\
    [2@16*NQ2,2@16*NQ2]<s[r2_b,r2_t]/\[r2_b,r2_t]<s[2@16*Q2,2@16*Q2] /\
    [2@16*NQ2,2@16*NQ2]<s[r7_b,r7_t]/\[r7_b,r7_t]<s[2@16*Q2,2@16*Q2] /\
    [2@16*NQ2,2@16*NQ2]<s[r4_b,r4_t]/\[r4_b,r4_t]<s[2@16*Q2,2@16*Q2] /\
    [2@16*NQ2, 2@16*NQ2]<s[r9_b,r9_t]/\[r9_b,r9_t]<s[2@16*Q2,2@16*Q2];

ghost r3_bo1@int16,r3_to1@int16,r4_bo1@int16,r4_to1@int16,
      r5_bo1@int16,r5_to1@int16,r7_bo1@int16,r7_to1@int16,
      r9_bo1@int16,r9_to1@int16,r11_bo1@int16,r11_to1@int16,
      lr_bo1@int16,lr_to1@int16:
      r3_bo1 = r3_b /\ r3_to1 = r3_t /\ r4_bo1 = r4_b /\ r4_to1 = r4_t /\
      r5_bo1 = r5_b /\ r5_to1 = r5_t /\ r7_bo1 = r7_b /\ r7_to1 = r7_t /\
      r9_bo1 = r9_b /\ r9_to1 = r9_t /\ r11_bo1 = r11_b /\ r11_to1 = r11_t /\
      lr_bo1 = lr_b /\ lr_to1 = lr_t
   && r3_bo1 = r3_b /\ r3_to1 = r3_t /\ r4_bo1 = r4_b /\ r4_to1 = r4_t /\
      r5_bo1 = r5_b /\ r5_to1 = r5_t /\ r7_bo1 = r7_b /\ r7_to1 = r7_t /\
      r9_bo1 = r9_b /\ r9_to1 = r9_t /\ r11_bo1 = r11_b /\ r11_to1 = r11_t /\
      lr_bo1 = lr_b /\ lr_to1 = lr_t;

(* sadd16	r8, lr, r11                              #! PC = 0x40112c *)
add r8_b lr_b r11_b;
add r8_t lr_t r11_t;
(* ssub16	r11, lr, r11                             #! PC = 0x401130 *)
sub r11_b lr_b r11_b;
sub r11_t lr_t r11_t;
(* sadd16	r6, r2, r4                               #! PC = 0x401134 *)
add r6_b r2_b r4_b;
add r6_t r2_t r4_t;
(* ssub16	r4, r2, r4                               #! PC = 0x401138 *)
sub r4_b r2_b r4_b;
sub r4_t r2_t r4_t;

(* vmov	r10, s1                                    #! PC = 0x40113c *)
mov [r10_b, r10_t] [s1_b, s1_t]; mov r10 s1;
(* smulwb	lr, r10, r5                              #! PC = 0x401140 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x401144 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401148 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x40114c *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401150 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo1 * 1600) [Q] /\
       eqmod lr_t (r5_to1 * 1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo1 * 1600) [Q] /\
       eqmod lr_t (r5_to1 * 1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x401154 *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401158 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

(* smulwb	lr, r10, r9                              #! PC = 0x40115c *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x401160 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401164 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401168 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x40116c *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo1 * 1600) [Q] /\
       eqmod lr_t (r9_to1 * 1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo1 * 1600) [Q] /\
       eqmod lr_t (r9_to1 * 1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x401170 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x401174 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;

assert [4*NQ,4*NQ]<[r8_b,r8_t]/\[r8_b,r8_t]<[4*Q,4*Q] /\
       [4*NQ,4*NQ]<[r11_b,r11_t]/\[r11_b,r11_t]<[4*Q,4*Q] /\
       [4*NQ,4*NQ]<[r6_b,r6_t]/\[r6_b,r6_t]<[4*Q,4*Q] /\
       [4*NQ,4*NQ]<[r4_b,r4_t]/\[r4_b,r4_t]<[4*Q,4*Q] /\
       [3*NQ,3*NQ]<[r3_b,r3_t]/\[r3_b,r3_t]<[3*Q,3*Q] /\
       [3*NQ,3*NQ]<[r5_b,r5_t]/\[r5_b,r5_t]<[3*Q,3*Q] /\
       [3*NQ,3*NQ]<[r7_b,r7_t]/\[r7_b,r7_t]<[3*Q,3*Q] /\
       [3*NQ,3*NQ]<[r9_b,r9_t]/\[r9_b,r9_t]<[3*Q,3*Q]
       prove with [algebra solver isl] && true;
assume [4*NQ,4*NQ]<[r8_b,r8_t]/\[r8_b,r8_t]<[4*Q,4*Q] /\
       [4*NQ,4*NQ]<[r11_b,r11_t]/\[r11_b,r11_t]<[4*Q,4*Q] /\
       [4*NQ,4*NQ]<[r6_b,r6_t]/\[r6_b,r6_t]<[4*Q,4*Q] /\
       [4*NQ,4*NQ]<[r4_b,r4_t]/\[r4_b,r4_t]<[4*Q,4*Q] /\
       [3*NQ,3*NQ]<[r3_b,r3_t]/\[r3_b,r3_t]<[3*Q,3*Q] /\
       [3*NQ,3*NQ]<[r5_b,r5_t]/\[r5_b,r5_t]<[3*Q,3*Q] /\
       [3*NQ,3*NQ]<[r7_b,r7_t]/\[r7_b,r7_t]<[3*Q,3*Q] /\
       [3*NQ,3*NQ]<[r9_b,r9_t]/\[r9_b,r9_t]<[3*Q,3*Q]
   &&  [4@16*NQ,4@16*NQ]<[r8_b,r8_t]/\[r8_b,r8_t]<[4@16*Q,4@16*Q] /\
       [4@16*NQ,4@16*NQ]<[r11_b,r11_t]/\[r11_b,r11_t]<[4@16*Q,4@16*Q] /\
       [4@16*NQ,4@16*NQ]<[r6_b,r6_t]/\[r6_b,r6_t]<[4@16*Q,4@16*Q] /\
       [4@16*NQ,4@16*NQ]<[r4_b,r4_t]/\[r4_b,r4_t]<[4@16*Q,4@16*Q] /\
       [3@16*NQ,3@16*NQ]<[r3_b,r3_t]/\[r3_b,r3_t]<[3@16*Q,3@16*Q] /\
       [3@16*NQ,3@16*NQ]<[r5_b,r5_t]/\[r5_b,r5_t]<[3@16*Q,3@16*Q] /\
       [3@16*NQ,3@16*NQ]<[r7_b,r7_t]/\[r7_b,r7_t]<[3@16*Q,3@16*Q] /\
       [3@16*NQ,3@16*NQ]<[r9_b,r9_t]/\[r9_b,r9_t]<[3@16*Q,3@16*Q];

(* CUT 18 *)

cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    [r8_b, r8_t] = [lr_bo1, lr_to1] + [r11_bo1, r11_to1] /\
    [r11_b, r11_t] = [lr_bo1, lr_to1] - [r11_bo1, r11_to1] /\
    [r6_b, r6_t] = [r2_b, r2_t] + [r4_bo1, r4_to1] /\
    [r4_b, r4_t] = [r2_b, r2_t] - [r4_bo1, r4_to1] /\
    eqmod ([r3_bo1, r3_to1] + [r5_bo1, r5_to1] * [1600, 1600])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo1, r3_to1] - [r5_bo1, r5_to1] * [1600, 1600])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r7_bo1, r7_to1] + [r9_bo1, r9_to1] * [1600, 1600])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r7_bo1, r7_to1] - [r9_bo1, r9_to1] * [1600, 1600])
          [r9_b, r9_t] [Q, Q] /\
    [4*NQ,4*NQ]<[r8_b,r8_t]/\[r8_b,r8_t]<[4*Q,4*Q] /\
    [4*NQ,4*NQ]<[r11_b,r11_t]/\[r11_b,r11_t]<[4*Q,4*Q] /\
    [4*NQ,4*NQ]<[r6_b,r6_t]/\[r6_b,r6_t]<[4*Q,4*Q] /\
    [4*NQ,4*NQ]<[r4_b,r4_t]/\[r4_b,r4_t]<[4*Q,4*Q] /\
    [3*NQ,3*NQ]<[r3_b,r3_t]/\[r3_b,r3_t]<[3*Q,3*Q] /\
    [3*NQ,3*NQ]<[r5_b,r5_t]/\[r5_b,r5_t]<[3*Q,3*Q] /\
    [3*NQ,3*NQ]<[r7_b,r7_t]/\[r7_b,r7_t]<[3*Q,3*Q] /\
    [3*NQ,3*NQ]<[r9_b,r9_t]/\[r9_b,r9_t]<[3*Q,3*Q]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [4@16*NQ,4@16*NQ]<[r8_b,r8_t]/\[r8_b,r8_t]<[4@16*Q,4@16*Q] /\
    [4@16*NQ,4@16*NQ]<[r11_b,r11_t]/\[r11_b,r11_t]<[4@16*Q,4@16*Q] /\
    [4@16*NQ,4@16*NQ]<[r6_b,r6_t]/\[r6_b,r6_t]<[4@16*Q,4@16*Q] /\
    [4@16*NQ,4@16*NQ]<[r4_b,r4_t]/\[r4_b,r4_t]<[4@16*Q,4@16*Q] /\
    [3@16*NQ,3@16*NQ]<[r3_b,r3_t]/\[r3_b,r3_t]<[3@16*Q,3@16*Q] /\
    [3@16*NQ,3@16*NQ]<[r5_b,r5_t]/\[r5_b,r5_t]<[3@16*Q,3@16*Q] /\
    [3@16*NQ,3@16*NQ]<[r7_b,r7_t]/\[r7_b,r7_t]<[3@16*Q,3@16*Q] /\
    [3@16*NQ,3@16*NQ]<[r9_b,r9_t]/\[r9_b,r9_t]<[3@16*Q,3@16*Q];

ghost r3_bo2@int16,r3_to2@int16,r4_bo2@int16,r4_to2@int16,
      r5_bo2@int16,r5_to2@int16,r6_bo1@int16,r6_to1@int16,
      r7_bo2@int16,r7_to2@int16,r8_bo2@int16,r8_to2@int16,
      r9_bo2@int16,r9_to2@int16,lr_bo2@int16,lr_to2@int16:
      r3_bo2 = r3_b /\ r3_to2 = r3_t /\ r4_bo2 = r4_b /\ r4_to2 = r4_t /\
      r5_bo2 = r5_b /\ r5_to2 = r5_t /\ r6_bo1 = r6_b /\ r6_to1 = r6_t /\
      r7_bo2 = r7_b /\ r7_to2 = r7_t /\ r8_bo2 = r8_b /\ r8_to2 = r8_t /\
      r9_bo2 = r9_b /\ r9_to2 = r9_t /\ lr_bo2 = lr_b /\ lr_to2 = lr_t
   && r3_bo2 = r3_b /\ r3_to2 = r3_t /\ r4_bo2 = r4_b /\ r4_to2 = r4_t /\
      r5_bo2 = r5_b /\ r5_to2 = r5_t /\ r6_bo1 = r6_b /\ r6_to1 = r6_t /\
      r7_bo2 = r7_b /\ r7_to2 = r7_t /\ r8_bo2 = r8_b /\ r8_to2 = r8_t /\
      r9_bo2 = r9_b /\ r9_to2 = r9_t /\ lr_bo2 = lr_b /\ lr_to2 = lr_t;

(* sadd16	r2, r8, r6                               #! PC = 0x401178 *)
add r2_b r8_b r6_b;
add r2_t r8_t r6_t;
(* ssub16	r6, r8, r6                               #! PC = 0x40117c *)
sub r6_b r8_b r6_b;
sub r6_t r8_t r6_t;
(* vmov	r10, s3                                    #! PC = 0x401180 *)
mov [r10_b, r10_t] [s3_b, s3_t]; mov r10 s3;
(* smulwb	lr, r10, r7                              #! PC = 0x401184 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x401188 *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40118c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x401190 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x401194 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo2 *   40) [Q] /\
       eqmod lr_t (r7_to2 *   40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo2 *   40) [Q] /\
       eqmod lr_t (r7_to2 *   40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x401198 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x40119c *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

(* vmov	r10, s4                                    #! PC = 0x4011a0 *)
mov [r10_b, r10_t] [s4_b, s4_t]; mov r10 s4;
(* smulwb	lr, r10, r4                              #! PC = 0x4011a4 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x4011a8 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4011ac *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x4011b0 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x4011b4 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_bo2 * 1600) [Q] /\
       eqmod lr_t (r4_to2 * 1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_bo2 * 1600) [Q] /\
       eqmod lr_t (r4_to2 * 1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* ssub16	r8, r11, lr                              #! PC = 0x4011b8 *)
sub r8_b r11_b lr_b;
sub r8_t r11_t lr_t;
(* sadd16	r4, r11, lr                              #! PC = 0x4011bc *)
add r4_b r11_b lr_b;
add r4_t r11_t lr_t;

(* vmov	r10, s5                                    #! PC = 0x4011c0 *)
mov [r10_b, r10_t] [s5_b, s5_t]; mov r10 s5;
(* smulwb	lr, r10, r9                              #! PC = 0x4011c4 *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x4011c8 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4011cc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4011d0 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4011d4 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo2 *  749) [Q] /\
       eqmod lr_t (r9_to2 *  749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo2 *  749) [Q] /\
       eqmod lr_t (r9_to2 *  749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x4011d8 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x4011dc *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;

assert [8*NQ,8*NQ]<[r2_b,r2_t]/\[r2_b,r2_t]<[8*Q,8*Q] /\
       [8*NQ,8*NQ]<[r6_b,r6_t]/\[r6_b,r6_t]<[8*Q,8*Q] /\
       [4*NQ,4*NQ]<[r3_b,r3_t]/\[r3_b,r3_t]<[4*Q,4*Q] /\
       [4*NQ,4*NQ]<[r7_b,r7_t]/\[r7_b,r7_t]<[4*Q,4*Q] /\
       [5*NQ,5*NQ]<[r4_b,r4_t]/\[r4_b,r4_t]<[5*Q,5*Q] /\
       [5*NQ,5*NQ]<[r8_b,r8_t]/\[r8_b,r8_t]<[5*Q,5*Q] /\
       [4*NQ,4*NQ]<[r5_b,r5_t]/\[r5_b,r5_t]<[4*Q,4*Q] /\
       [4*NQ,4*NQ]<[r9_b,r9_t]/\[r9_b,r9_t]<[4*Q,4*Q]
       prove with [algebra solver isl] && true;
assume [8*NQ,8*NQ]<[r2_b,r2_t]/\[r2_b,r2_t]<[8*Q,8*Q] /\
       [8*NQ,8*NQ]<[r6_b,r6_t]/\[r6_b,r6_t]<[8*Q,8*Q] /\
       [4*NQ,4*NQ]<[r3_b,r3_t]/\[r3_b,r3_t]<[4*Q,4*Q] /\
       [4*NQ,4*NQ]<[r7_b,r7_t]/\[r7_b,r7_t]<[4*Q,4*Q] /\
       [5*NQ,5*NQ]<[r4_b,r4_t]/\[r4_b,r4_t]<[5*Q,5*Q] /\
       [5*NQ,5*NQ]<[r8_b,r8_t]/\[r8_b,r8_t]<[5*Q,5*Q] /\
       [4*NQ,4*NQ]<[r5_b,r5_t]/\[r5_b,r5_t]<[4*Q,4*Q] /\
       [4*NQ,4*NQ]<[r9_b,r9_t]/\[r9_b,r9_t]<[4*Q,4*Q]
    && [8@16*NQ,8@16*NQ]<[r2_b,r2_t]/\[r2_b,r2_t]<[8@16*Q,8@16*Q] /\
       [8@16*NQ,8@16*NQ]<[r6_b,r6_t]/\[r6_b,r6_t]<[8@16*Q,8@16*Q] /\
       [4@16*NQ,4@16*NQ]<[r3_b,r3_t]/\[r3_b,r3_t]<[4@16*Q,4@16*Q] /\
       [4@16*NQ,4@16*NQ]<[r7_b,r7_t]/\[r7_b,r7_t]<[4@16*Q,4@16*Q] /\
       [5@16*NQ,5@16*NQ]<[r4_b,r4_t]/\[r4_b,r4_t]<[5@16*Q,5@16*Q] /\
       [5@16*NQ,5@16*NQ]<[r8_b,r8_t]/\[r8_b,r8_t]<[5@16*Q,5@16*Q] /\
       [4@16*NQ,4@16*NQ]<[r5_b,r5_t]/\[r5_b,r5_t]<[4@16*Q,4@16*Q] /\
       [4@16*NQ,4@16*NQ]<[r9_b,r9_t]/\[r9_b,r9_t]<[4@16*Q,4@16*Q];

(* CUT 19 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    [r2_b, r2_t] = [r8_bo2, r8_to2] + [r6_bo1, r6_to1] /\
    [r6_b, r6_t] = [r8_bo2, r8_to2] - [r6_bo1, r6_to1] /\
    eqmod ([r3_bo2, r3_to2] + [r7_bo2, r7_to2] * [  40,   40])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo2, r3_to2] - [r7_bo2, r7_to2] * [  40,   40])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r11_b, r11_t] + [r4_bo2, r4_to2] * [1600,  1600])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r11_b, r11_t] - [r4_bo2, r4_to2] * [1600,  1600])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r5_bo2, r5_to2] + [r9_bo2, r9_to2] * [  749,  749])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r5_bo2, r5_to2] - [r9_bo2, r9_to2] * [  749,  749])
          [r9_b, r9_t] [Q, Q] /\
    [8*NQ,8*NQ]<[r2_b,r2_t]/\[r2_b,r2_t]<[8*Q,8*Q] /\
    [8*NQ,8*NQ]<[r6_b,r6_t]/\[r6_b,r6_t]<[8*Q,8*Q] /\
    [4*NQ,4*NQ]<[r3_b,r3_t]/\[r3_b,r3_t]<[4*Q,4*Q] /\
    [4*NQ,4*NQ]<[r7_b,r7_t]/\[r7_b,r7_t]<[4*Q,4*Q] /\
    [5*NQ,5*NQ]<[r4_b,r4_t]/\[r4_b,r4_t]<[5*Q,5*Q] /\
    [5*NQ,5*NQ]<[r8_b,r8_t]/\[r8_b,r8_t]<[5*Q,5*Q] /\
    [4*NQ,4*NQ]<[r5_b,r5_t]/\[r5_b,r5_t]<[4*Q,4*Q] /\
    [4*NQ,4*NQ]<[r9_b,r9_t]/\[r9_b,r9_t]<[4*Q,4*Q]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [8@16*NQ,8@16*NQ]<[r2_b,r2_t]/\[r2_b,r2_t]<[8@16*Q,8@16*Q] /\
    [8@16*NQ,8@16*NQ]<[r6_b,r6_t]/\[r6_b,r6_t]<[8@16*Q,8@16*Q] /\
    [4@16*NQ,4@16*NQ]<[r3_b,r3_t]/\[r3_b,r3_t]<[4@16*Q,4@16*Q] /\
    [4@16*NQ,4@16*NQ]<[r7_b,r7_t]/\[r7_b,r7_t]<[4@16*Q,4@16*Q] /\
    [5@16*NQ,5@16*NQ]<[r4_b,r4_t]/\[r4_b,r4_t]<[5@16*Q,5@16*Q] /\
    [5@16*NQ,5@16*NQ]<[r8_b,r8_t]/\[r8_b,r8_t]<[5@16*Q,5@16*Q] /\
    [4@16*NQ,4@16*NQ]<[r5_b,r5_t]/\[r5_b,r5_t]<[4@16*Q,4@16*Q] /\
    [4@16*NQ,4@16*NQ]<[r9_b,r9_t]/\[r9_b,r9_t]<[4@16*Q,4@16*Q];

ghost r2_bo1@int16,r2_to1@int16,r3_bo3@int16,r3_to3@int16,
      r4_bo3@int16,r4_to3@int16,r5_bo3@int16,r5_to3@int16,
      r6_bo2@int16,r6_to2@int16,r7_bo3@int16,r7_to3@int16,
      r8_bo3@int16,r8_to3@int16,r9_bo3@int16,r9_to3@int16:
      r2_bo1 = r2_b /\ r2_to1 = r2_t /\ r3_bo3 = r3_b /\ r3_to3 = r3_t /\
      r4_bo3 = r4_b /\ r4_to3 = r4_t /\ r5_bo3 = r5_b /\ r5_to3 = r5_t /\
      r6_bo2 = r6_b /\ r6_to2 = r6_t /\ r7_bo3 = r7_b /\ r7_to3 = r7_t /\
      r8_bo3 = r8_b /\ r8_to3 = r8_t /\ r9_bo3 = r9_b /\ r9_to3 = r9_t
   && r2_bo1 = r2_b /\ r2_to1 = r2_t /\ r3_bo3 = r3_b /\ r3_to3 = r3_t /\
      r4_bo3 = r4_b /\ r4_to3 = r4_t /\ r5_bo3 = r5_b /\ r5_to3 = r5_t /\
      r6_bo2 = r6_b /\ r6_to2 = r6_t /\ r7_bo3 = r7_b /\ r7_to3 = r7_t /\
      r8_bo3 = r8_b /\ r8_to3 = r8_t /\ r9_bo3 = r9_b /\ r9_to3 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4018d0; Value = 0x912fe8a0; PC = 0x4011e0 *)
mov [r10, r11] [L0x4018d0, L0x4018d4];
(* smulwb	lr, r10, r2                              #! PC = 0x4011e4 *)
vpc r2_bW@int32 r2_b; mulj tmp r10 r2_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r2, r10, r2                              #! PC = 0x4011e8 *)
vpc r2_tW@int32 r2_t; mulj tmp r10 r2_tW; spl r2_w dc tmp 16;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b; vpc r2_t@int16 r2_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4011ec *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r2, r2, r12, r0                          #! PC = 0x4011f0 *)
mulj prod r2_b r12_t; add r2_w prod r0;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b;
(* pkhtb	r2, r2, lr, asr #16                       #! PC = 0x4011f4 *)
mov tmp_b lr_t; mov tmp_t r2_t;
mov r2_b tmp_b; mov r2_t tmp_t;

assert eqmod r2_b (r2_bo1 * -1441) [Q] /\
       eqmod r2_t (r2_to1 * -1441) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r2_b (r2_bo1 * -1441) [Q] /\
       eqmod r2_t (r2_to1 * -1441) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r2_b, r2_t] /\ [r2_b, r2_t] <s[Q2, Q2];

(* smulwb	lr, r11, r3                              #! PC = 0x4011f8 *)
vpc r3_bW@int32 r3_b; mulj tmp r11 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r11, r3                              #! PC = 0x4011fc *)
vpc r3_tW@int32 r3_t; mulj tmp r11 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401200 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x401204 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x401208 *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov r3_b tmp_b; mov r3_t tmp_t;

assert eqmod r3_b (r3_bo3 *  225) [Q] /\
       eqmod r3_t (r3_to3 *  225) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r3_b (r3_bo3 *  225) [Q] /\
       eqmod r3_t (r3_to3 *  225) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r3_b, r3_t] /\ [r3_b, r3_t] <s[Q2, Q2];

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4018d8; Value = 0xaf7c58e6; PC = 0x40120c *)
mov [r10, r11] [L0x4018d8, L0x4018dc];
(* smulwb	lr, r10, r4                              #! PC = 0x401210 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401214 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401218 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x40121c *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x401220 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov r4_b tmp_b; mov r4_t tmp_t;

assert eqmod r4_b (r4_bo3 * -1047) [Q] /\
       eqmod r4_t (r4_to3 * -1047) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r4_b (r4_bo3 * -1047) [Q] /\
       eqmod r4_t (r4_to3 * -1047) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r4_b, r4_t] /\ [r4_b, r4_t] <s[Q2, Q2];

(* smulwb	lr, r11, r5                              #! PC = 0x401224 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x401228 *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40122c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401230 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x401234 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov r5_b tmp_b; mov r5_t tmp_t;

assert eqmod r5_b (r5_bo3 * -987) [Q] /\
       eqmod r5_t (r5_to3 * -987) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r5_b (r5_bo3 * -987) [Q] /\
       eqmod r5_t (r5_to3 * -987) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r5_b, r5_t] /\ [r5_b, r5_t] <s[Q2, Q2];

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4018e0; Value = 0x6b6de3db; PC = 0x401238 *)
mov [r10, r11] [L0x4018e0, L0x4018e4];
(* smulwb	lr, r10, r6                              #! PC = 0x40123c *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x401240 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401244 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x401248 *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x40124c *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov r6_b tmp_b; mov r6_t tmp_t;

assert eqmod r6_b (r6_bo2 * 1397) [Q] /\
       eqmod r6_t (r6_to2 * 1397) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r6_b (r6_bo2 * 1397) [Q] /\
       eqmod r6_t (r6_to2 * 1397) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r6_b, r6_t] /\ [r6_b, r6_t] <s[Q2, Q2];

(* smulwb	lr, r11, r7                              #! PC = 0x401250 *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x401254 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401258 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x40125c *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x401260 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov r7_b tmp_b; mov r7_t tmp_t;

assert eqmod r7_b (r7_bo3 *  468) [Q] /\
       eqmod r7_t (r7_to3 *  468) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r7_b (r7_bo3 *  468) [Q] /\
       eqmod r7_t (r7_to3 *  468) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r7_b, r7_t] /\ [r7_b, r7_t] <s[Q2, Q2];

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4018e8; Value = 0xc92b9a30; PC = 0x401264 *)
mov [r10, r11] [L0x4018e8, L0x4018ec];
(* smulwb	lr, r10, r8                              #! PC = 0x401268 *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x40126c *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401270 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x401274 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x401278 *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov r8_b tmp_b; mov r8_t tmp_t;

assert eqmod r8_b (r8_bo3 *  -713) [Q] /\
       eqmod r8_t (r8_to3 *  -713) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r8_b (r8_bo3 *  -713) [Q] /\
       eqmod r8_t (r8_to3 *  -713) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r8_b, r8_t] /\ [r8_b, r8_t] <s[Q2, Q2];

(* smulwb	lr, r11, r9                              #! PC = 0x40127c *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x401280 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401284 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401288 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x40128c *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov r9_b tmp_b; mov r9_t tmp_t;

assert eqmod r9_b (r9_bo3 * -1254) [Q] /\
       eqmod r9_t (r9_to3 * -1254) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r9_b (r9_bo3 * -1254) [Q] /\
       eqmod r9_t (r9_to3 * -1254) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r9_b, r9_t] /\ [r9_b, r9_t] <s[Q2, Q2];

(* vmov	r0, s6                                     #! PC = 0x401290 *)
mov [r0_b, r0_t] [s6_b, s6_t];
(* str.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2c4; PC = 0x401294 *)
mov [L0xbefff2c4, L0xbefff2c6] [r6_b, r6_t];
(* str.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff304; PC = 0x401298 *)
mov [L0xbefff304, L0xbefff306] [r7_b, r7_t];
(* str.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff344; PC = 0x40129c *)
mov [L0xbefff344, L0xbefff346] [r8_b, r8_t];
(* str.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff384; PC = 0x4012a0 *)
mov [L0xbefff384, L0xbefff386] [r9_b, r9_t];
(* str.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff204; PC = 0x4012a4 *)
mov [L0xbefff204, L0xbefff206] [r3_b, r3_t];
(* str.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff244; PC = 0x4012a8 *)
mov [L0xbefff244, L0xbefff246] [r4_b, r4_t];
(* str.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff284; PC = 0x4012ac *)
mov [L0xbefff284, L0xbefff286] [r5_b, r5_t];
(* str.w	r2, [r0], #4                              #! EA = L0xbefff1c4; PC = 0x4012b0 *)
mov [L0xbefff1c4, L0xbefff1c6] [r2_b, r2_t];

(* CUT 20 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo1, r2_to1] * [-1441, -1441])
          [L0xbefff1c4, L0xbefff1c6] [Q, Q] /\
    eqmod ([r3_bo3, r3_to3] * [  225,   225])
          [L0xbefff204, L0xbefff206] [Q, Q] /\
    eqmod ([r4_bo3, r4_to3] * [-1047, -1047])
          [L0xbefff244, L0xbefff246] [Q, Q] /\
    eqmod ([r5_bo3, r5_to3] * [ -987,  -987])
          [L0xbefff284, L0xbefff286] [Q, Q] /\
    eqmod ([r6_bo2, r6_to2] * [ 1397,  1397])
          [L0xbefff2c4, L0xbefff2c6] [Q, Q] /\
    eqmod ([r7_bo3, r7_to3] * [  468,   468])
          [L0xbefff304, L0xbefff306] [Q, Q] /\
    eqmod ([r8_bo3, r8_to3] * [ -713,  -713])
          [L0xbefff344, L0xbefff346] [Q, Q] /\
    eqmod ([r9_bo3, r9_to3] * [-1254, -1254])
          [L0xbefff384, L0xbefff386] [Q, Q] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff1c4,L0xbefff1c6,L0xbefff204,L0xbefff206] /\
    [L0xbefff1c4,L0xbefff1c6,L0xbefff204,L0xbefff206]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff244,L0xbefff246,L0xbefff284,L0xbefff286] /\
    [L0xbefff244,L0xbefff246,L0xbefff284,L0xbefff286]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff2c4,L0xbefff2c6,L0xbefff304,L0xbefff306] /\
    [L0xbefff2c4,L0xbefff2c6,L0xbefff304,L0xbefff306]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff344,L0xbefff346,L0xbefff384,L0xbefff386] /\
    [L0xbefff344,L0xbefff346,L0xbefff384,L0xbefff386]< [Q2,Q2,Q2,Q2]
 && [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1c4,L0xbefff1c6,L0xbefff204,L0xbefff206] /\
    [L0xbefff1c4,L0xbefff1c6,L0xbefff204,L0xbefff206]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff244,L0xbefff246,L0xbefff284,L0xbefff286] /\
    [L0xbefff244,L0xbefff246,L0xbefff284,L0xbefff286]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2c4,L0xbefff2c6,L0xbefff304,L0xbefff306] /\
    [L0xbefff2c4,L0xbefff2c6,L0xbefff304,L0xbefff306]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff344,L0xbefff346,L0xbefff384,L0xbefff386] /\
    [L0xbefff344,L0xbefff346,L0xbefff384,L0xbefff386]<s[Q2,Q2,Q2,Q2];

(* add.w	lr, r0, #60	; 0x3c                        #! PC = 0x4012b4 *)
adds dontcare lr r0 60@int32;
(* vmov	s14, lr                                    #! PC = 0x4012b8 *)
mov [s14_b, s14_t] [lr_b, lr_t]; mov s14 lr;
(* vmov	s6, r0                                     #! PC = 0x4012bc *)
mov [s6_b, s6_t] [r0_b, r0_t];
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff1c8; Value = 0x0efa1023; PC = 0x4012c0 *)
mov [r2_b, r2_t] [L0xbefff1c8, L0xbefff1ca];
(* ldr.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff208; Value = 0xf24e187b; PC = 0x4012c4 *)
mov [r3_b, r3_t] [L0xbefff208, L0xbefff20a];
(* ldr.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff248; Value = 0xf7a2f3ae; PC = 0x4012c8 *)
mov [r4_b, r4_t] [L0xbefff248, L0xbefff24a];
(* ldr.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff288; Value = 0xf1d9ff55; PC = 0x4012cc *)
mov [r5_b, r5_t] [L0xbefff288, L0xbefff28a];
(* ldr.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2c8; Value = 0xfc7df7cc; PC = 0x4012d0 *)
mov [r6_b, r6_t] [L0xbefff2c8, L0xbefff2ca];
(* ldr.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff308; Value = 0x0069fa8c; PC = 0x4012d4 *)
mov [r7_b, r7_t] [L0xbefff308, L0xbefff30a];
(* ldr.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff348; Value = 0xed190b40; PC = 0x4012d8 *)
mov [r8_b, r8_t] [L0xbefff348, L0xbefff34a];
(* ldr.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff388; Value = 0xf26bed4d; PC = 0x4012dc *)
mov [r9_b, r9_t] [L0xbefff388, L0xbefff38a];
(* movw	r0, #26632	; 0x6808                        #! PC = 0x4012e0 *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;

ghost r2_bo4@int16,r2_to4@int16,r3_bo4@int16,r3_to4@int16,
      r4_bo4@int16,r4_to4@int16,r5_bo4@int16,r5_to4@int16,
      r6_bo4@int16,r6_to4@int16,r7_bo4@int16,r7_to4@int16,
      r8_bo4@int16,r8_to4@int16,r9_bo4@int16,r9_to4@int16:
      r2_bo4 = r2_b /\ r2_to4 = r2_t /\ r3_bo4 = r3_b /\ r3_to4 = r3_t /\
      r4_bo4 = r4_b /\ r4_to4 = r4_t /\ r5_bo4 = r5_b /\ r5_to4 = r5_t /\
      r6_bo4 = r6_b /\ r6_to4 = r6_t /\ r7_bo4 = r7_b /\ r7_to4 = r7_t /\
      r8_bo4 = r8_b /\ r8_to4 = r8_t /\ r9_bo4 = r9_b /\ r9_to4 = r9_t
   && r2_bo4 = r2_b /\ r2_to4 = r2_t /\ r3_bo4 = r3_b /\ r3_to4 = r3_t /\
      r4_bo4 = r4_b /\ r4_to4 = r4_t /\ r5_bo4 = r5_b /\ r5_to4 = r5_t /\
      r6_bo4 = r6_b /\ r6_to4 = r6_t /\ r7_bo4 = r7_b /\ r7_to4 = r7_t /\
      r8_bo4 = r8_b /\ r8_to4 = r8_t /\ r9_bo4 = r9_b /\ r9_to4 = r9_t;

(* ldr.w	r10, [r1], #4                             #! EA = L0x4018f0; Value = 0x79bb8f1d; PC = 0x4012e4 *)
mov r10 L0x4018f0;
(* smulwb	lr, r10, r3                              #! PC = 0x4012e8 *)
vpc r3_bW@int32 r3_b; mulj tmp r10 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r10, r3                              #! PC = 0x4012ec *)
vpc r3_tW@int32 r3_t; mulj tmp r10 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4012f0 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4012f4 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x4012f8 *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r3_bo4 * 1583) [Q] /\
       eqmod lr_t (r3_to4 * 1583) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r3_bo4 * 1583) [Q] /\
       eqmod lr_t (r3_to4 * 1583) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r3, r2, lr                               #! PC = 0x4012fc *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401300 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;

(* smulwb	lr, r10, r5                              #! PC = 0x401304 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x401308 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40130c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401310 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401314 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo4 * 1583) [Q] /\
       eqmod lr_t (r5_to4 * 1583) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo4 * 1583) [Q] /\
       eqmod lr_t (r5_to4 * 1583) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r4, lr                               #! PC = 0x401318 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x40131c *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;

(* smulwb	lr, r10, r7                              #! PC = 0x401320 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x401324 *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401328 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x40132c *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x401330 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo4 * 1583) [Q] /\
       eqmod lr_t (r7_to4 * 1583) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo4 * 1583) [Q] /\
       eqmod lr_t (r7_to4 * 1583) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r6, lr                               #! PC = 0x401334 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x401338 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;

(* smulwb	lr, r10, r9                              #! PC = 0x40133c *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x401340 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401344 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401348 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x40134c *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;
(* usub16	r9, r8, lr                               #! PC = 0x401350 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x401354 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

assert eqmod lr_b (r9_bo4 * 1583) [Q] /\
       eqmod lr_t (r9_to4 * 1583) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo4 * 1583) [Q] /\
       eqmod lr_t (r9_to4 * 1583) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

assert [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [6*Q2,6*Q2,6*Q2,6*Q2] 
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [6*Q2,6*Q2,6*Q2,6*Q2]
    && [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2];
       
(* CUT 21 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo4, r2_to4] + [r3_bo4, r3_to4] * [ 1583,  1583])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo4, r2_to4] - [r3_bo4, r3_to4] * [ 1583,  1583])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r4_bo4, r4_to4] + [r5_bo4, r5_to4] * [ 1583,  1583])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo4, r4_to4] - [r5_bo4, r5_to4] * [ 1583,  1583])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo4, r6_to4] + [r7_bo4, r7_to4] * [ 1583,  1583])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo4, r6_to4] - [r7_bo4, r7_to4] * [ 1583,  1583])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r8_bo4, r8_to4] + [r9_bo4, r9_to4] * [ 1583,  1583])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r8_bo4, r8_to4] - [r9_bo4, r9_to4] * [ 1583,  1583])
          [r9_b, r9_t] [Q, Q] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [6*Q2,6*Q2,6*Q2,6*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2]
    prove with [cuts [1, 3, 5, 7, 9, 11, 13, 15]];

ghost r2_bo5@int16,r2_to5@int16,r3_bo5@int16,r3_to5@int16,
      r4_bo5@int16,r4_to5@int16,r5_bo5@int16,r5_to5@int16,
      r6_bo5@int16,r6_to5@int16,r7_bo5@int16,r7_to5@int16,
      r8_bo5@int16,r8_to5@int16,r9_bo5@int16,r9_to5@int16:
      r2_bo5 = r2_b /\ r2_to5 = r2_t /\ r3_bo5 = r3_b /\ r3_to5 = r3_t /\
      r4_bo5 = r4_b /\ r4_to5 = r4_t /\ r5_bo5 = r5_b /\ r5_to5 = r5_t /\
      r6_bo5 = r6_b /\ r6_to5 = r6_t /\ r7_bo5 = r7_b /\ r7_to5 = r7_t /\
      r8_bo5 = r8_b /\ r8_to5 = r8_t /\ r9_bo5 = r9_b /\ r9_to5 = r9_t
   && r2_bo5 = r2_b /\ r2_to5 = r2_t /\ r3_bo5 = r3_b /\ r3_to5 = r3_t /\
      r4_bo5 = r4_b /\ r4_to5 = r4_t /\ r5_bo5 = r5_b /\ r5_to5 = r5_t /\
      r6_bo5 = r6_b /\ r6_to5 = r6_t /\ r7_bo5 = r7_b /\ r7_to5 = r7_t /\
      r8_bo5 = r8_b /\ r8_to5 = r8_t /\ r9_bo5 = r9_b /\ r9_to5 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4018f4; Value = 0xc0dd78d0; PC = 0x401358 *)
mov [r10, r11] [L0x4018f4, L0x4018f8];
(* smulwb	lr, r10, r4                              #! PC = 0x40135c *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401360 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401364 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401368 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x40136c *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_bo5 *  -821) [Q] /\
       eqmod lr_t (r4_to5 *  -821) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_bo5 *  -821) [Q] /\
       eqmod lr_t (r4_to5 *  -821) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r4, r2, lr                               #! PC = 0x401370 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401374 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;

(* smulwb	lr, r11, r5                              #! PC = 0x401378 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40137c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401380 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401384 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401388 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo5 *  1355) [Q] /\
       eqmod lr_t (r5_to5 *  1355) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo5 *  1355) [Q] /\
       eqmod lr_t (r5_to5 *  1355) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x40138c *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401390 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

(* smulwb	lr, r10, r8                              #! PC = 0x401394 *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401398 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40139c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4013a0 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x4013a4 *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo5 *  -821) [Q] /\
       eqmod lr_t (r8_to5 *  -821) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo5 *  -821) [Q] /\
       eqmod lr_t (r8_to5 *  -821) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r6, lr                               #! PC = 0x4013a8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x4013ac *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;

(* smulwb	lr, r11, r9                              #! PC = 0x4013b0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4013b4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013b8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4013bc *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4013c0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo5 *  1355) [Q] /\
       eqmod lr_t (r9_to5 *  1355) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo5 *  1355) [Q] /\
       eqmod lr_t (r9_to5 *  1355) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x4013c4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x4013c8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;

assert [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
       prove with [algebra solver isl]
    && true;
assume [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
    && [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2];

(* CUT 22 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo5, r2_to5] + [r4_bo5, r4_to5] * [ -821, -821])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo5, r2_to5] - [r4_bo5, r4_to5] * [ -821, -821])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r3_bo5, r3_to5] + [r5_bo5, r5_to5] * [ 1355, 1355])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo5, r3_to5] - [r5_bo5, r5_to5] * [ 1355, 1355])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo5, r6_to5] + [r8_bo5, r8_to5] * [ -821, -821])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo5, r6_to5] - [r8_bo5, r8_to5] * [ -821, -821])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r7_bo5, r7_to5] + [r9_bo5, r9_to5] * [ 1355, 1355])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r7_bo5, r7_to5] - [r9_bo5, r9_to5] * [ 1355, 1355])
          [r9_b, r9_t] [Q, Q] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2];
    
ghost r2_bo6@int16,r2_to6@int16,r3_bo6@int16,r3_to6@int16,
      r4_bo6@int16,r4_to6@int16,r5_bo6@int16,r5_to6@int16,
      r6_bo6@int16,r6_to6@int16,r7_bo6@int16,r7_to6@int16,
      r8_bo6@int16,r8_to6@int16,r9_bo6@int16,r9_to6@int16:
      r2_bo6 = r2_b /\ r2_to6 = r2_t /\ r3_bo6 = r3_b /\ r3_to6 = r3_t /\
      r4_bo6 = r4_b /\ r4_to6 = r4_t /\ r5_bo6 = r5_b /\ r5_to6 = r5_t /\
      r6_bo6 = r6_b /\ r6_to6 = r6_t /\ r7_bo6 = r7_b /\ r7_to6 = r7_t /\
      r8_bo6 = r8_b /\ r8_to6 = r8_t /\ r9_bo6 = r9_b /\ r9_to6 = r9_t
   && r2_bo6 = r2_b /\ r2_to6 = r2_t /\ r3_bo6 = r3_b /\ r3_to6 = r3_t /\
      r4_bo6 = r4_b /\ r4_to6 = r4_t /\ r5_bo6 = r5_b /\ r5_to6 = r5_t /\
      r6_bo6 = r6_b /\ r6_to6 = r6_t /\ r7_bo6 = r7_b /\ r7_to6 = r7_t /\
      r8_bo6 = r8_b /\ r8_to6 = r8_t /\ r9_bo6 = r9_b /\ r9_to6 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4018fc; Value = 0xba05620d; PC = 0x4013cc *)
mov [r10, r11] [L0x4018fc, L0x401900];

(* smulwb	lr, r10, r6                              #! PC = 0x4013d0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4013d4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013d8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4013dc *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x4013e0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r6_bo6 *  -910) [Q] /\
       eqmod lr_t (r6_to6 *  -910) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r6_bo6 *  -910) [Q] /\
       eqmod lr_t (r6_to6 *  -910) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r6, r2, lr                               #! PC = 0x4013e4 *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4013e8 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;

(* smulwb	lr, r11, r7                              #! PC = 0x4013ec *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4013f0 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013f4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4013f8 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x4013fc *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo6 *   219) [Q] /\
       eqmod lr_t (r7_to6 *   219) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo6 *   219) [Q] /\
       eqmod lr_t (r7_to6 *   219) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x401400 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401404 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401904; Value = 0xa1a4cbf1; PC = 0x401408 *)
mov [r10, r11] [L0x401904, L0x401908];
(* smulwb	lr, r10, r8                              #! PC = 0x40140c *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401410 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401414 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x401418 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x40141c *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo6 * -1227) [Q] /\
       eqmod lr_t (r8_to6 * -1227) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo6 * -1227) [Q] /\
       eqmod lr_t (r8_to6 * -1227) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r4, lr                               #! PC = 0x401420 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x401424 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;

(* smulwb	lr, r11, r9                              #! PC = 0x401428 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x40142c *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401430 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401434 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x401438 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo6 *   855) [Q] /\
       eqmod lr_t (r9_to6 *   855) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo6 *   855) [Q] /\
       eqmod lr_t (r9_to6 *   855) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x40143c *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x401440 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;

assert [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2] 
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2] 
    && [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2];

(* CUT 23 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo6, r2_to6] + [r6_bo6, r6_to6] * [ -910,  -910])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo6, r2_to6] - [r6_bo6, r6_to6] * [ -910,  -910])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r3_bo6, r3_to6] + [r7_bo6, r7_to6] * [  219,   219])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo6, r3_to6] - [r7_bo6, r7_to6] * [  219,   219])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r4_bo6, r4_to6] + [r8_bo6, r8_to6] * [-1227, -1227])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo6, r4_to6] - [r8_bo6, r8_to6] * [-1227, -1227])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r5_bo6, r5_to6] + [r9_bo6, r9_to6] * [  855,   855])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r5_bo6, r5_to6] - [r9_bo6, r9_to6] * [  855,   855])
          [r9_b, r9_t] [Q, Q] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2];
    
ghost r2_bo7@int16,r2_to7@int16,r3_bo7@int16,r3_to7@int16,
      r4_bo7@int16,r4_to7@int16,r5_bo7@int16,r5_to7@int16,
      r6_bo7@int16,r6_to7@int16,r7_bo7@int16,r7_to7@int16,
      r8_bo7@int16,r8_to7@int16,r9_bo7@int16,r9_to7@int16:
      r2_bo7 = r2_b /\ r2_to7 = r2_t /\ r3_bo7 = r3_b /\ r3_to7 = r3_t /\
      r4_bo7 = r4_b /\ r4_to7 = r4_t /\ r5_bo7 = r5_b /\ r5_to7 = r5_t /\
      r6_bo7 = r6_b /\ r6_to7 = r6_t /\ r7_bo7 = r7_b /\ r7_to7 = r7_t /\
      r8_bo7 = r8_b /\ r8_to7 = r8_t /\ r9_bo7 = r9_b /\ r9_to7 = r9_t
   && r2_bo7 = r2_b /\ r2_to7 = r2_t /\ r3_bo7 = r3_b /\ r3_to7 = r3_t /\
      r4_bo7 = r4_b /\ r4_to7 = r4_t /\ r5_bo7 = r5_b /\ r5_to7 = r5_t /\
      r6_bo7 = r6_b /\ r6_to7 = r6_t /\ r7_bo7 = r7_b /\ r7_to7 = r7_t /\
      r8_bo7 = r8_b /\ r8_to7 = r8_t /\ r9_bo7 = r9_b /\ r9_to7 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x40190c; Value = 0x62e4b355; PC = 0x401444 *)
mov [r10, r11] [L0x40190c, L0x401910];
(* smulwb	lr, r10, r2                              #! PC = 0x401448 *)
vpc r2_bW@int32 r2_b; mulj tmp r10 r2_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r2, r10, r2                              #! PC = 0x40144c *)
vpc r2_tW@int32 r2_t; mulj tmp r10 r2_tW; spl r2_w dc tmp 16;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b; vpc r2_t@int16 r2_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401450 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r2, r2, r12, r0                          #! PC = 0x401454 *)
mulj prod r2_b r12_t; add r2_w prod r0;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b;
(* pkhtb	r2, r2, lr, asr #16                       #! PC = 0x401458 *)
mov tmp_b lr_t; mov tmp_t r2_t;
mov r2_b tmp_b; mov r2_t tmp_t;

assert eqmod r2_b (r2_bo7 *  1286) [Q] /\
       eqmod r2_t (r2_to7 *  1286) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r2_b (r2_bo7 *  1286) [Q] /\
       eqmod r2_t (r2_to7 *  1286) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r2_b, r2_t] /\ [r2_b, r2_t] <s[Q2, Q2];

(* smulwb	lr, r11, r3                              #! PC = 0x40145c *)
vpc r3_bW@int32 r3_b; mulj tmp r11 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r11, r3                              #! PC = 0x401460 *)
vpc r3_tW@int32 r3_t; mulj tmp r11 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401464 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x401468 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x40146c *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov r3_b tmp_b; mov r3_t tmp_t;

assert eqmod r3_b (r3_bo7 *  1384) [Q] /\
       eqmod r3_t (r3_to7 *  1384) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r3_b (r3_bo7 *  1384) [Q] /\
       eqmod r3_t (r3_to7 *  1384) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r3_b, r3_t] /\ [r3_b, r3_t] <s[Q2, Q2];

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401914; Value = 0x73bc053b; PC = 0x401470 *)
mov [r10, r11] [L0x401914, L0x401918];
(* smulwb	lr, r10, r4                              #! PC = 0x401474 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401478 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40147c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401480 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x401484 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov r4_b tmp_b; mov r4_t tmp_t;

assert eqmod r4_b (r4_bo7 *  1505) [Q] /\
       eqmod r4_t (r4_to7 *  1505) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r4_b (r4_bo7 *  1505) [Q] /\
       eqmod r4_t (r4_to7 *  1505) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r4_b, r4_t] /\ [r4_b, r4_t] <s[Q2, Q2];

(* smulwb	lr, r11, r5                              #! PC = 0x401488 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40148c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401490 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401494 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x401498 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov r5_b tmp_b; mov r5_t tmp_t;

assert eqmod r5_b (r5_bo7 * -1233) [Q] /\
       eqmod r5_t (r5_to7 * -1233) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r5_b (r5_bo7 * -1233) [Q] /\
       eqmod r5_t (r5_to7 * -1233) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r5_b, r5_t] /\ [r5_b, r5_t] <s[Q2, Q2];

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x40191c; Value = 0x1560d12b; PC = 0x40149c *)
mov [r10, r11] [L0x40191c, L0x401920];
(* smulwb	lr, r10, r6                              #! PC = 0x4014a0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4014a4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014a8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4014ac *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x4014b0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov r6_b tmp_b; mov r6_t tmp_t;

assert eqmod r6_b (r6_bo7 *   278) [Q] /\
       eqmod r6_t (r6_to7 *   278) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r6_b (r6_bo7 *   278) [Q] /\
       eqmod r6_t (r6_to7 *   278) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r6_b, r6_t] /\ [r6_b, r6_t] <s[Q2, Q2];

(* smulwb	lr, r11, r7                              #! PC = 0x4014b4 *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4014b8 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014bc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4014c0 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x4014c4 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov r7_b tmp_b; mov r7_t tmp_t;

assert eqmod r7_b (r7_bo7 *   615) [Q] /\
       eqmod r7_t (r7_to7 *   615) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r7_b (r7_bo7 *   615) [Q] /\
       eqmod r7_t (r7_to7 *   615) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r7_b, r7_t] /\ [r7_b, r7_t] <s[Q2, Q2];

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401924; Value = 0x5720aeb8; PC = 0x4014c8 *)
mov [r10, r11] [L0x401924, L0x401928];
(* smulwb	lr, r10, r8                              #! PC = 0x4014cc *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x4014d0 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014d4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4014d8 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x4014dc *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov r8_b tmp_b; mov r8_t tmp_t;

assert eqmod r8_b (r8_bo7 *  1133) [Q] /\
       eqmod r8_t (r8_to7 *  1133) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r8_b (r8_bo7 *  1133) [Q] /\
       eqmod r8_t (r8_to7 *  1133) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r8_b, r8_t] /\ [r8_b, r8_t] <s[Q2, Q2];

(* smulwb	lr, r11, r9                              #! PC = 0x4014e0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4014e4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014e8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4014ec *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x4014f0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov r9_b tmp_b; mov r9_t tmp_t;

assert eqmod r9_b (r9_bo7 *  1297) [Q] /\
       eqmod r9_t (r9_to7 *  1297) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r9_b (r9_bo7 *  1297) [Q] /\
       eqmod r9_t (r9_to7 *  1297) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r9_b, r9_t] /\ [r9_b, r9_t] <s[Q2, Q2];

(* vmov	r0, s6                                     #! PC = 0x4014f4 *)
mov [r0_b, r0_t] [s6_b, s6_t];
(* str.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2c8; PC = 0x4014f8 *)
mov [L0xbefff2c8, L0xbefff2ca] [r6_b, r6_t];
(* str.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff308; PC = 0x4014fc *)
mov [L0xbefff308, L0xbefff30a] [r7_b, r7_t];
(* str.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff348; PC = 0x401500 *)
mov [L0xbefff348, L0xbefff34a] [r8_b, r8_t];
(* str.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff388; PC = 0x401504 *)
mov [L0xbefff388, L0xbefff38a] [r9_b, r9_t];
(* str.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff208; PC = 0x401508 *)
mov [L0xbefff208, L0xbefff20a] [r3_b, r3_t];
(* str.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff248; PC = 0x40150c *)
mov [L0xbefff248, L0xbefff24a] [r4_b, r4_t];
(* str.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff288; PC = 0x401510 *)
mov [L0xbefff288, L0xbefff28a] [r5_b, r5_t];
(* str.w	r2, [r0], #4                              #! EA = L0xbefff1c8; PC = 0x401514 *)
mov [L0xbefff1c8, L0xbefff1ca] [r2_b, r2_t];
(* vmov	lr, s14                                    #! PC = 0x401518 *)
mov [lr_b, lr_t] [s14_b, s14_t]; mov lr s14;
(* cmp.w	r0, lr                                    #! PC = 0x40151c *)
(* cmp.w r0, lr *)
nop;
(*
assert eqmod (poly X** 32
                   [poly X [L0xbefff1c4,L0xbefff1c6,L0xbefff1c8,L0xbefff1ca],
                    poly X [L0xbefff204,L0xbefff206,L0xbefff208,L0xbefff20a],
                    poly X [L0xbefff244,L0xbefff246,L0xbefff248,L0xbefff24a],
                    poly X [L0xbefff284,L0xbefff286,L0xbefff288,L0xbefff28a],
                    poly X [L0xbefff2c4,L0xbefff2c6,L0xbefff2c8,L0xbefff2ca],
                    poly X [L0xbefff304,L0xbefff306,L0xbefff308,L0xbefff30a],
                    poly X [L0xbefff344,L0xbefff346,L0xbefff348,L0xbefff34a],
                    poly X [L0xbefff384,L0xbefff386,L0xbefff388,L0xbefff38a]])
             (2**16*F**2) [Q, X** 32 - 17**0]
       prove with [all ghosts, all cuts] && true;
*)

(* CUT 24 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo7, r2_to7] * [ 1286,  1286])
          [L0xbefff1c8, L0xbefff1ca] [Q, Q] /\
    eqmod ([r3_bo7, r3_to7] * [ 1384,  1384])
          [L0xbefff208, L0xbefff20a] [Q, Q] /\
    eqmod ([r4_bo7, r4_to7] * [ 1505,  1505])
          [L0xbefff248, L0xbefff24a] [Q, Q] /\
    eqmod ([r5_bo7, r5_to7] * [-1233, -1233])
          [L0xbefff288, L0xbefff28a] [Q, Q] /\
    eqmod ([r6_bo7, r6_to7] * [  278,   278])
          [L0xbefff2c8, L0xbefff2ca] [Q, Q] /\
    eqmod ([r7_bo7, r7_to7] * [  615,   615])
          [L0xbefff308, L0xbefff30a] [Q, Q] /\
    eqmod ([r8_bo7, r8_to7] * [ 1133,  1133])
          [L0xbefff348, L0xbefff34a] [Q, Q] /\
    eqmod ([r9_bo7, r9_to7] * [ 1297,  1297])
          [L0xbefff388, L0xbefff38a] [Q, Q] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff1c8,L0xbefff1ca,L0xbefff208,L0xbefff20a] /\
    [L0xbefff1c8,L0xbefff1ca,L0xbefff208,L0xbefff20a]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff248,L0xbefff24a,L0xbefff288,L0xbefff28a] /\
    [L0xbefff248,L0xbefff24a,L0xbefff288,L0xbefff28a]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff2c8,L0xbefff2ca,L0xbefff308,L0xbefff30a] /\
    [L0xbefff2c8,L0xbefff2ca,L0xbefff308,L0xbefff30a]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff348,L0xbefff34a,L0xbefff388,L0xbefff38a] /\
    [L0xbefff348,L0xbefff34a,L0xbefff388,L0xbefff38a]< [Q2,Q2,Q2,Q2]
 && [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1c8,L0xbefff1ca,L0xbefff208,L0xbefff20a] /\
    [L0xbefff1c8,L0xbefff1ca,L0xbefff208,L0xbefff20a]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff248,L0xbefff24a,L0xbefff288,L0xbefff28a] /\
    [L0xbefff248,L0xbefff24a,L0xbefff288,L0xbefff28a]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2c8,L0xbefff2ca,L0xbefff308,L0xbefff30a] /\
    [L0xbefff2c8,L0xbefff2ca,L0xbefff308,L0xbefff30a]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff348,L0xbefff34a,L0xbefff388,L0xbefff38a] /\
    [L0xbefff348,L0xbefff34a,L0xbefff388,L0xbefff38a]<s[Q2,Q2,Q2,Q2];

(* #bne.w	0x4012bc <invntt_fast+1552>              #! PC = 0x401520 *)
#bne.w	0x4012bc <invntt_fast+1552>              #! 0x401520 = 0x401520;
(* vmov	s6, r0                                     #! PC = 0x4012bc *)
mov [s6_b, s6_t] [r0_b, r0_t];
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff1cc; Value = 0x119605a8; PC = 0x4012c0 *)
mov [r2_b, r2_t] [L0xbefff1cc, L0xbefff1ce];
(* ldr.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff20c; Value = 0xff17f1d1; PC = 0x4012c4 *)
mov [r3_b, r3_t] [L0xbefff20c, L0xbefff20e];
(* ldr.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff24c; Value = 0xe356f5de; PC = 0x4012c8 *)
mov [r4_b, r4_t] [L0xbefff24c, L0xbefff24e];
(* ldr.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff28c; Value = 0xf74b102e; PC = 0x4012cc *)
mov [r5_b, r5_t] [L0xbefff28c, L0xbefff28e];
(* ldr.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2cc; Value = 0x0fdef55e; PC = 0x4012d0 *)
mov [r6_b, r6_t] [L0xbefff2cc, L0xbefff2ce];
(* ldr.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff30c; Value = 0xfc84f239; PC = 0x4012d4 *)
mov [r7_b, r7_t] [L0xbefff30c, L0xbefff30e];
(* ldr.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff34c; Value = 0xf51b0543; PC = 0x4012d8 *)
mov [r8_b, r8_t] [L0xbefff34c, L0xbefff34e];
(* ldr.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff38c; Value = 0xfa4b0610; PC = 0x4012dc *)
mov [r9_b, r9_t] [L0xbefff38c, L0xbefff38e];

ghost r2_bo8@int16, r2_to8@int16, r3_bo8@int16, r3_to8@int16,
      r4_bo8@int16, r4_to8@int16, r5_bo8@int16, r5_to8@int16,
      r6_bo8@int16, r6_to8@int16, r7_bo8@int16, r7_to8@int16,
      r8_bo8@int16, r8_to8@int16, r9_bo8@int16, r9_to8@int16:
      r2_bo8 = r2_b /\ r2_to8 = r2_t /\ r3_bo8 = r3_b /\ r3_to8 = r3_t /\
      r4_bo8 = r4_b /\ r4_to8 = r4_t /\ r5_bo8 = r5_b /\ r5_to8 = r5_t /\
      r6_bo8 = r6_b /\ r6_to8 = r6_t /\ r7_bo8 = r7_b /\ r7_to8 = r7_t /\
      r8_bo8 = r8_b /\ r8_to8 = r8_t /\ r9_bo8 = r9_b /\ r9_to8 = r9_t
   && r2_bo8 = r2_b /\ r2_to8 = r2_t /\ r3_bo8 = r3_b /\ r3_to8 = r3_t /\
      r4_bo8 = r4_b /\ r4_to8 = r4_t /\ r5_bo8 = r5_b /\ r5_to8 = r5_t /\
      r6_bo8 = r6_b /\ r6_to8 = r6_t /\ r7_bo8 = r7_b /\ r7_to8 = r7_t /\
      r8_bo8 = r8_b /\ r8_to8 = r8_t /\ r9_bo8 = r9_b /\ r9_to8 = r9_t;

(* movw	r0, #26632	; 0x6808                        #! PC = 0x4012e0 *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* ldr.w	r10, [r1], #4                             #! EA = L0x40192c; Value = 0xbec9f078; PC = 0x4012e4 *)
mov r10 L0x40192c;
(* smulwb	lr, r10, r3                              #! PC = 0x4012e8 *)
vpc r3_bW@int32 r3_b; mulj tmp r10 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r10, r3                              #! PC = 0x4012ec *)
vpc r3_tW@int32 r3_t; mulj tmp r10 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4012f0 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4012f4 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x4012f8 *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r3_bo8 *  -848) [Q] /\
       eqmod lr_t (r3_to8 *  -848) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r3_bo8 *  -848) [Q] /\
       eqmod lr_t (r3_to8 *  -848) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r3, r2, lr                               #! PC = 0x4012fc *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401300 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x401304 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x401308 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40130c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401310 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401314 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo8 *  -848) [Q] /\
       eqmod lr_t (r5_to8 *  -848) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo8 *  -848) [Q] /\
       eqmod lr_t (r5_to8 *  -848) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r4, lr                               #! PC = 0x401318 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x40131c *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x401320 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x401324 *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401328 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x40132c *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x401330 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo8 *  -848) [Q] /\
       eqmod lr_t (r7_to8 *  -848) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo8 *  -848) [Q] /\
       eqmod lr_t (r7_to8 *  -848) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r6, lr                               #! PC = 0x401334 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x401338 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x40133c *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x401340 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401344 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401348 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x40134c *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo8 *  -848) [Q] /\
       eqmod lr_t (r9_to8 *  -848) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo8 *  -848) [Q] /\
       eqmod lr_t (r9_to8 *  -848) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r8, lr                               #! PC = 0x401350 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x401354 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

assert [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
    && [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2];


(* CUT 25 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo8, r2_to8] + [r3_bo8, r3_to8] * [ -848,  -848])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo8, r2_to8] - [r3_bo8, r3_to8] * [ -848,  -848])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r4_bo8, r4_to8] + [r5_bo8, r5_to8] * [ -848,  -848])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo8, r4_to8] - [r5_bo8, r5_to8] * [ -848,  -848])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo8, r6_to8] + [r7_bo8, r7_to8] * [ -848,  -848])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo8, r6_to8] - [r7_bo8, r7_to8] * [ -848,  -848])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r8_bo8, r8_to8] + [r9_bo8, r9_to8] * [ -848,  -848])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r8_bo8, r8_to8] - [r9_bo8, r9_to8] * [ -848,  -848])
          [r9_b, r9_t] [Q, Q] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2]
    prove with [cuts [1, 3, 5, 7, 9, 11, 13, 15]];

ghost r2_bo9@int16, r2_to9@int16, r3_bo9@int16, r3_to9@int16,
      r4_bo9@int16, r4_to9@int16, r5_bo9@int16, r5_to9@int16,
      r6_bo9@int16, r6_to9@int16, r7_bo9@int16, r7_to9@int16,
      r8_bo9@int16, r8_to9@int16, r9_bo9@int16, r9_to9@int16:
      r2_bo9 = r2_b /\ r2_to9 = r2_t /\ r3_bo9 = r3_b /\ r3_to9 = r3_t /\
      r4_bo9 = r4_b /\ r4_to9 = r4_t /\ r5_bo9 = r5_b /\ r5_to9 = r5_t /\
      r6_bo9 = r6_b /\ r6_to9 = r6_t /\ r7_bo9 = r7_b /\ r7_to9 = r7_t /\
      r8_bo9 = r8_b /\ r8_to9 = r8_t /\ r9_bo9 = r9_b /\ r9_to9 = r9_t
   && r2_bo9 = r2_b /\ r2_to9 = r2_t /\ r3_bo9 = r3_b /\ r3_to9 = r3_t /\
      r4_bo9 = r4_b /\ r4_to9 = r4_t /\ r5_bo9 = r5_b /\ r5_to9 = r5_t /\
      r6_bo9 = r6_b /\ r6_to9 = r6_t /\ r7_bo9 = r7_b /\ r7_to9 = r7_t /\
      r8_bo9 = r8_b /\ r8_to9 = r8_t /\ r9_bo9 = r9_b /\ r9_to9 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401930; Value = 0x79bb8f1d; PC = 0x401358 *)
mov [r10, r11] [L0x401930, L0x401934];
(* smulwb	lr, r10, r4                              #! PC = 0x40135c *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401360 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401364 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401368 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x40136c *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_bo9 *  1583) [Q] /\
       eqmod lr_t (r4_to9 *  1583) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_bo9 *  1583) [Q] /\
       eqmod lr_t (r4_to9 *  1583) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r4, r2, lr                               #! PC = 0x401370 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401374 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401378 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40137c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401380 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401384 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401388 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo9 *  -569) [Q] /\
       eqmod lr_t (r5_to9 *  -569) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo9 *  -569) [Q] /\
       eqmod lr_t (r5_to9 *  -569) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x40138c *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401390 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x401394 *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401398 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40139c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4013a0 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x4013a4 *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo9 *  1583) [Q] /\
       eqmod lr_t (r8_to9 *  1583) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo9 *  1583) [Q] /\
       eqmod lr_t (r8_to9 *  1583) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r6, lr                               #! PC = 0x4013a8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x4013ac *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4013b0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4013b4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013b8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4013bc *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4013c0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo9 *  -569) [Q] /\
       eqmod lr_t (r9_to9 *  -569) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo9 *  -569) [Q] /\
       eqmod lr_t (r9_to9 *  -569) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x4013c4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x4013c8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;

assert [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
       prove with [algebra solver isl] && true;
assume [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
    && [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2];


(* CUT 26 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo9, r2_to9] + [r4_bo9, r4_to9] * [ 1583,  1583])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo9, r2_to9] - [r4_bo9, r4_to9] * [ 1583,  1583])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r3_bo9, r3_to9] + [r5_bo9, r5_to9] * [ -569,  -569])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo9, r3_to9] - [r5_bo9, r5_to9] * [ -569,  -569])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo9, r6_to9] + [r8_bo9, r8_to9] * [ 1583,  1583])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo9, r6_to9] - [r8_bo9, r8_to9] * [ 1583,  1583])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r7_bo9, r7_to9] + [r9_bo9, r9_to9] * [ -569,  -569])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r7_bo9, r7_to9] - [r9_bo9, r9_to9] * [ -569,  -569])
          [r9_b, r9_t] [Q, Q] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2];

ghost r2_bo10@int16, r2_to10@int16, r3_bo10@int16, r3_to10@int16,
      r4_bo10@int16, r4_to10@int16, r5_bo10@int16, r5_to10@int16,
      r6_bo10@int16, r6_to10@int16, r7_bo10@int16, r7_to10@int16,
      r8_bo10@int16, r8_to10@int16, r9_bo10@int16, r9_to10@int16:
      r2_bo10 = r2_b /\ r2_to10 = r2_t /\ r3_bo10 = r3_b /\ r3_to10 = r3_t /\
      r4_bo10 = r4_b /\ r4_to10 = r4_t /\ r5_bo10 = r5_b /\ r5_to10 = r5_t /\
      r6_bo10 = r6_b /\ r6_to10 = r6_t /\ r7_bo10 = r7_b /\ r7_to10 = r7_t /\
      r8_bo10 = r8_b /\ r8_to10 = r8_t /\ r9_bo10 = r9_b /\ r9_to10 = r9_t
   && r2_bo10 = r2_b /\ r2_to10 = r2_t /\ r3_bo10 = r3_b /\ r3_to10 = r3_t /\
      r4_bo10 = r4_b /\ r4_to10 = r4_t /\ r5_bo10 = r5_b /\ r5_to10 = r5_t /\
      r6_bo10 = r6_b /\ r6_to10 = r6_t /\ r7_bo10 = r7_b /\ r7_to10 = r7_t /\
      r8_bo10 = r8_b /\ r8_to10 = r8_t /\ r9_bo10 = r9_b /\ r9_to10 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401938; Value = 0xc0dd78d0; PC = 0x4013cc *)
mov [r10, r11] [L0x401938, L0x40193c];
(* smulwb	lr, r10, r6                              #! PC = 0x4013d0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4013d4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013d8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4013dc *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x4013e0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r6_bo10 *  -821) [Q] /\
       eqmod lr_t (r6_to10 *  -821) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r6_bo10 *  -821) [Q] /\
       eqmod lr_t (r6_to10 *  -821) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r6, r2, lr                               #! PC = 0x4013e4 *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4013e8 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4013ec *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4013f0 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013f4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4013f8 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x4013fc *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo10 *   450) [Q] /\
       eqmod lr_t (r7_to10 *   450) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo10 *   450) [Q] /\
       eqmod lr_t (r7_to10 *   450) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x401400 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401404 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401940; Value = 0x68330fc4; PC = 0x401408 *)
mov [r10, r11] [L0x401940, L0x401944];
(* smulwb	lr, r10, r8                              #! PC = 0x40140c *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401410 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401414 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x401418 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x40141c *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo10 *  1355) [Q] /\
       eqmod lr_t (r8_to10 *  1355) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo10 *  1355) [Q] /\
       eqmod lr_t (r8_to10 *  1355) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r4, lr                               #! PC = 0x401420 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x401424 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x401428 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x40142c *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401430 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401434 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x401438 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo10 *   936) [Q] /\
       eqmod lr_t (r9_to10 *   936) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo10 *   936) [Q] /\
       eqmod lr_t (r9_to10 *   936) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x40143c *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x401440 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;

assert [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
       [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
       [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
       [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [9*Q2,9*Q2,9*Q2,9*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
       [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
       [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
       [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [9*Q2,9*Q2,9*Q2,9*Q2]
    && [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2] /\
       [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2] /\
       [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2] /\
       [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2];


(* CUT 27 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo10, r2_to10] + [r6_bo10, r6_to10] * [ -821,  -821])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo10, r2_to10] - [r6_bo10, r6_to10] * [ -821,  -821])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r3_bo10, r3_to10] + [r7_bo10, r7_to10] * [  450,   450])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo10, r3_to10] - [r7_bo10, r7_to10] * [  450,   450])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r4_bo10, r4_to10] + [r8_bo10, r8_to10] * [ 1355,  1355])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo10, r4_to10] - [r8_bo10, r8_to10] * [ 1355,  1355])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r5_bo10, r5_to10] + [r9_bo10, r9_to10] * [  936,   936])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r5_bo10, r5_to10] - [r9_bo10, r9_to10] * [  936,   936])
          [r9_b, r9_t] [Q, Q] /\
    [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
    [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
    [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
    [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [9*Q2,9*Q2,9*Q2,9*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2] /\
    [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2] /\
    [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2] /\
    [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2];

ghost r2_bo11@int16, r2_to11@int16, r3_bo11@int16, r3_to11@int16,
      r4_bo11@int16, r4_to11@int16, r5_bo11@int16, r5_to11@int16,
      r6_bo11@int16, r6_to11@int16, r7_bo11@int16, r7_to11@int16,
      r8_bo11@int16, r8_to11@int16, r9_bo11@int16, r9_to11@int16:
      r2_bo11 = r2_b /\ r2_to11 = r2_t /\ r3_bo11 = r3_b /\ r3_to11 = r3_t /\
      r4_bo11 = r4_b /\ r4_to11 = r4_t /\ r5_bo11 = r5_b /\ r5_to11 = r5_t /\
      r6_bo11 = r6_b /\ r6_to11 = r6_t /\ r7_bo11 = r7_b /\ r7_to11 = r7_t /\
      r8_bo11 = r8_b /\ r8_to11 = r8_t /\ r9_bo11 = r9_b /\ r9_to11 = r9_t
   && r2_bo11 = r2_b /\ r2_to11 = r2_t /\ r3_bo11 = r3_b /\ r3_to11 = r3_t /\
      r4_bo11 = r4_b /\ r4_to11 = r4_t /\ r5_bo11 = r5_b /\ r5_to11 = r5_t /\
      r6_bo11 = r6_b /\ r6_to11 = r6_t /\ r7_bo11 = r7_b /\ r7_to11 = r7_t /\
      r8_bo11 = r8_b /\ r8_to11 = r8_t /\ r9_bo11 = r9_b /\ r9_to11 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401948; Value = 0xe7b3199c; PC = 0x401444 *)
mov [r10, r11] [L0x401948, L0x40194c];
(* smulwb	lr, r10, r2                              #! PC = 0x401448 *)
vpc r2_bW@int32 r2_b; mulj tmp r10 r2_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r2, r10, r2                              #! PC = 0x40144c *)
vpc r2_tW@int32 r2_t; mulj tmp r10 r2_tW; spl r2_w dc tmp 16;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b; vpc r2_t@int16 r2_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401450 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r2, r2, r12, r0                          #! PC = 0x401454 *)
mulj prod r2_b r12_t; add r2_w prod r0;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b;
(* pkhtb	r2, r2, lr, asr #16                       #! PC = 0x401458 *)
mov tmp_b lr_t; mov tmp_t r2_t;
mov r2_b tmp_b; mov r2_t tmp_t;

assert eqmod r2_b (r2_bo11 *  -316) [Q] /\
       eqmod r2_t (r2_to11 *  -316) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r2_b (r2_bo11 *  -316) [Q] /\
       eqmod r2_t (r2_to11 *  -316) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r2_b, r2_t] /\ [r2_b, r2_t] <s[Q2, Q2];

(* smulwb	lr, r11, r3                              #! PC = 0x40145c *)
vpc r3_bW@int32 r3_b; mulj tmp r11 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r11, r3                              #! PC = 0x401460 *)
vpc r3_tW@int32 r3_t; mulj tmp r11 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401464 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x401468 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x40146c *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov r3_b tmp_b; mov r3_t tmp_t;

assert eqmod r3_b (r3_bo11 *  1648) [Q] /\
       eqmod r3_t (r3_to11 *  1648) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r3_b (r3_bo11 *  1648) [Q] /\
       eqmod r3_t (r3_to11 *  1648) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r3_b, r3_t] /\ [r3_b, r3_t] <s[Q2, Q2];

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401950; Value = 0x33fc004f; PC = 0x401470 *)
mov [r10, r11] [L0x401950, L0x401954];
(* smulwb	lr, r10, r4                              #! PC = 0x401474 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401478 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40147c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401480 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x401484 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov r4_b tmp_b; mov r4_t tmp_t;

assert eqmod r4_b (r4_bo11 *   676) [Q] /\
       eqmod r4_t (r4_to11 *   676) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r4_b (r4_bo11 *   676) [Q] /\
       eqmod r4_t (r4_to11 *   676) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r4_b, r4_t] /\ [r4_b, r4_t] <s[Q2, Q2];

(* smulwb	lr, r11, r5                              #! PC = 0x401488 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40148c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401490 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401494 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x401498 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov r5_b tmp_b; mov r5_t tmp_t;

assert eqmod r5_b (r5_bo11 *  -660) [Q] /\
       eqmod r5_t (r5_to11 *  -660) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r5_b (r5_bo11 *  -660) [Q] /\
       eqmod r5_t (r5_to11 *  -660) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r5_b, r5_t] /\ [r5_b, r5_t] <s[Q2, Q2];

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401958; Value = 0x1f600c4e; PC = 0x40149c *)
mov [r10, r11] [L0x401958, L0x40195c];
(* smulwb	lr, r10, r6                              #! PC = 0x4014a0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4014a4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014a8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4014ac *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x4014b0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov r6_b tmp_b; mov r6_t tmp_t;

assert eqmod r6_b (r6_bo11 *   408) [Q] /\
       eqmod r6_t (r6_to11 *   408) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r6_b (r6_bo11 *   408) [Q] /\
       eqmod r6_t (r6_to11 *   408) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r6_b, r6_t] /\ [r6_b, r6_t] <s[Q2, Q2];

(* smulwb	lr, r11, r7                              #! PC = 0x4014b4 *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4014b8 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014bc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4014c0 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x4014c4 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov r7_b tmp_b; mov r7_t tmp_t;

assert eqmod r7_b (r7_bo11 *   232) [Q] /\
       eqmod r7_t (r7_to11 *   232) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r7_b (r7_bo11 *   232) [Q] /\
       eqmod r7_t (r7_to11 *   232) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r7_b, r7_t] /\ [r7_b, r7_t] <s[Q2, Q2];

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401960; Value = 0xe701ec29; PC = 0x4014c8 *)
mov [r10, r11] [L0x401960, L0x401964];
(* smulwb	lr, r10, r8                              #! PC = 0x4014cc *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x4014d0 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014d4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4014d8 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x4014dc *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov r8_b tmp_b; mov r8_t tmp_t;

assert eqmod r8_b (r8_bo11 *  -325) [Q] /\
       eqmod r8_t (r8_to11 *  -325) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r8_b (r8_bo11 *  -325) [Q] /\
       eqmod r8_t (r8_to11 *  -325) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r8_b, r8_t] /\ [r8_b, r8_t] <s[Q2, Q2];

(* smulwb	lr, r11, r9                              #! PC = 0x4014e0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4014e4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014e8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4014ec *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x4014f0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov r9_b tmp_b; mov r9_t tmp_t;

assert eqmod r9_b (r9_bo11 *  -707) [Q] /\
       eqmod r9_t (r9_to11 *  -707) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r9_b (r9_bo11 *  -707) [Q] /\
       eqmod r9_t (r9_to11 *  -707) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r9_b, r9_t] /\ [r9_b, r9_t] <s[Q2, Q2];

(* vmov	r0, s6                                     #! PC = 0x4014f4 *)
mov [r0_b, r0_t] [s6_b, s6_t];
(* str.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2cc; PC = 0x4014f8 *)
mov [L0xbefff2cc, L0xbefff2ce] [r6_b, r6_t];
(* str.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff30c; PC = 0x4014fc *)
mov [L0xbefff30c, L0xbefff30e] [r7_b, r7_t];
(* str.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff34c; PC = 0x401500 *)
mov [L0xbefff34c, L0xbefff34e] [r8_b, r8_t];
(* str.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff38c; PC = 0x401504 *)
mov [L0xbefff38c, L0xbefff38e] [r9_b, r9_t];
(* str.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff20c; PC = 0x401508 *)
mov [L0xbefff20c, L0xbefff20e] [r3_b, r3_t];
(* str.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff24c; PC = 0x40150c *)
mov [L0xbefff24c, L0xbefff24e] [r4_b, r4_t];
(* str.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff28c; PC = 0x401510 *)
mov [L0xbefff28c, L0xbefff28e] [r5_b, r5_t];
(* str.w	r2, [r0], #4                              #! EA = L0xbefff1cc; PC = 0x401514 *)
mov [L0xbefff1cc, L0xbefff1ce] [r2_b, r2_t];
(* vmov	lr, s14                                    #! PC = 0x401518 *)
mov [lr_b, lr_t] [s14_b, s14_t]; mov lr s14;
(* cmp.w	r0, lr                                    #! PC = 0x40151c *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x4012bc <invntt_fast+1552>              #! PC = 0x401520 *)
#bne.w	0x4012bc <invntt_fast+1552>              #! 0x401520 = 0x401520;


(* CUT 28 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo11, r2_to11] * [ -316,  -316])
          [L0xbefff1cc, L0xbefff1ce] [Q, Q] /\
    eqmod ([r3_bo11, r3_to11] * [ 1648,  1648])
          [L0xbefff20c, L0xbefff20e] [Q, Q] /\
    eqmod ([r4_bo11, r4_to11] * [  676,   676])
          [L0xbefff24c, L0xbefff24e] [Q, Q] /\
    eqmod ([r5_bo11, r5_to11] * [ -660,  -660])
          [L0xbefff28c, L0xbefff28e] [Q, Q] /\
    eqmod ([r6_bo11, r6_to11] * [  408,   408])
          [L0xbefff2cc, L0xbefff2ce] [Q, Q] /\
    eqmod ([r7_bo11, r7_to11] * [  232,   232])
          [L0xbefff30c, L0xbefff30e] [Q, Q] /\
    eqmod ([r8_bo11, r8_to11] * [ -325,  -325])
          [L0xbefff34c, L0xbefff34e] [Q, Q] /\
    eqmod ([r9_bo11, r9_to11] * [ -707,  -707])
          [L0xbefff38c, L0xbefff38e] [Q, Q] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff1cc,L0xbefff1ce,L0xbefff20c,L0xbefff20e] /\
    [L0xbefff1cc,L0xbefff1ce,L0xbefff20c,L0xbefff20e]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff24c,L0xbefff24e,L0xbefff28c,L0xbefff28e] /\
    [L0xbefff24c,L0xbefff24e,L0xbefff28c,L0xbefff28e]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff2cc,L0xbefff2ce,L0xbefff30c,L0xbefff30e] /\
    [L0xbefff2cc,L0xbefff2ce,L0xbefff30c,L0xbefff30e]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff34c,L0xbefff34e,L0xbefff38c,L0xbefff38e] /\
    [L0xbefff34c,L0xbefff34e,L0xbefff38c,L0xbefff38e]< [Q2,Q2,Q2,Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1cc,L0xbefff1ce,L0xbefff20c,L0xbefff20e] /\
    [L0xbefff1cc,L0xbefff1ce,L0xbefff20c,L0xbefff20e]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff24c,L0xbefff24e,L0xbefff28c,L0xbefff28e] /\
    [L0xbefff24c,L0xbefff24e,L0xbefff28c,L0xbefff28e]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2cc,L0xbefff2ce,L0xbefff30c,L0xbefff30e] /\
    [L0xbefff2cc,L0xbefff2ce,L0xbefff30c,L0xbefff30e]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff34c,L0xbefff34e,L0xbefff38c,L0xbefff38e] /\
    [L0xbefff34c,L0xbefff34e,L0xbefff38c,L0xbefff38e]<s[Q2,Q2,Q2,Q2];

(* vmov	s6, r0                                     #! PC = 0x4012bc *)
mov [s6_b, s6_t] [r0_b, r0_t];
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff1d0; Value = 0xfe9909cb; PC = 0x4012c0 *)
mov [r2_b, r2_t] [L0xbefff1d0, L0xbefff1d2];
(* ldr.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff210; Value = 0xf5b4fdec; PC = 0x4012c4 *)
mov [r3_b, r3_t] [L0xbefff210, L0xbefff212];
(* ldr.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff250; Value = 0x028bf2fa; PC = 0x4012c8 *)
mov [r4_b, r4_t] [L0xbefff250, L0xbefff252];
(* ldr.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff290; Value = 0xfc86fc11; PC = 0x4012cc *)
mov [r5_b, r5_t] [L0xbefff290, L0xbefff292];
(* ldr.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2d0; Value = 0x0b7a005f; PC = 0x4012d0 *)
mov [r6_b, r6_t] [L0xbefff2d0, L0xbefff2d2];
(* ldr.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff310; Value = 0xed57f636; PC = 0x4012d4 *)
mov [r7_b, r7_t] [L0xbefff310, L0xbefff312];
(* ldr.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff350; Value = 0xfb7607de; PC = 0x4012d8 *)
mov [r8_b, r8_t] [L0xbefff350, L0xbefff352];
(* ldr.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff390; Value = 0xffebfcdd; PC = 0x4012dc *)
mov [r9_b, r9_t] [L0xbefff390, L0xbefff392];

ghost r2_bo12@int16, r2_to12@int16, r3_bo12@int16, r3_to12@int16,
      r4_bo12@int16, r4_to12@int16, r5_bo12@int16, r5_to12@int16,
      r6_bo12@int16, r6_to12@int16, r7_bo12@int16, r7_to12@int16,
      r8_bo12@int16, r8_to12@int16, r9_bo12@int16, r9_to12@int16:
      r2_bo12 = r2_b /\ r2_to12 = r2_t /\ r3_bo12 = r3_b /\ r3_to12 = r3_t /\
      r4_bo12 = r4_b /\ r4_to12 = r4_t /\ r5_bo12 = r5_b /\ r5_to12 = r5_t /\
      r6_bo12 = r6_b /\ r6_to12 = r6_t /\ r7_bo12 = r7_b /\ r7_to12 = r7_t /\
      r8_bo12 = r8_b /\ r8_to12 = r8_t /\ r9_bo12 = r9_b /\ r9_to12 = r9_t
   && r2_bo12 = r2_b /\ r2_to12 = r2_t /\ r3_bo12 = r3_b /\ r3_to12 = r3_t /\
      r4_bo12 = r4_b /\ r4_to12 = r4_t /\ r5_bo12 = r5_b /\ r5_to12 = r5_t /\
      r6_bo12 = r6_b /\ r6_to12 = r6_t /\ r7_bo12 = r7_b /\ r7_to12 = r7_t /\
      r8_bo12 = r8_b /\ r8_to12 = r8_t /\ r9_bo12 = r9_b /\ r9_to12 = r9_t;

(* movw	r0, #26632	; 0x6808                        #! PC = 0x4012e0 *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* ldr.w	r10, [r1], #4                             #! EA = L0x401968; Value = 0xc2b5f202; PC = 0x4012e4 *)
mov r10 L0x401968;
(* smulwb	lr, r10, r3                              #! PC = 0x4012e8 *)
vpc r3_bW@int32 r3_b; mulj tmp r10 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r10, r3                              #! PC = 0x4012ec *)
vpc r3_tW@int32 r3_t; mulj tmp r10 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4012f0 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4012f4 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x4012f8 *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r3_bo12 *  -797) [Q] /\
       eqmod lr_t (r3_to12 *  -797) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r3_bo12 *  -797) [Q] /\
       eqmod lr_t (r3_to12 *  -797) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r3, r2, lr                               #! PC = 0x4012fc *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401300 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x401304 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x401308 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40130c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401310 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401314 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo12 *  -797) [Q] /\
       eqmod lr_t (r5_to12 *  -797) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo12 *  -797) [Q] /\
       eqmod lr_t (r5_to12 *  -797) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r4, lr                               #! PC = 0x401318 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x40131c *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x401320 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x401324 *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401328 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x40132c *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x401330 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo12 *  -797) [Q] /\
       eqmod lr_t (r7_to12 *  -797) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo12 *  -797) [Q] /\
       eqmod lr_t (r7_to12 *  -797) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r6, lr                               #! PC = 0x401334 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x401338 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x40133c *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x401340 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401344 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401348 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x40134c *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo12 *  -797) [Q] /\
       eqmod lr_t (r9_to12 *  -797) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo12 *  -797) [Q] /\
       eqmod lr_t (r9_to12 *  -797) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r8, lr                               #! PC = 0x401350 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x401354 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

assert [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [6*Q2,6*Q2,6*Q2,6*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [6*Q2,6*Q2,6*Q2,6*Q2]
    && [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2];


(* CUT 29 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo12, r2_to12] + [r3_bo12, r3_to12] * [ -797,  -797])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo12, r2_to12] - [r3_bo12, r3_to12] * [ -797,  -797])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r4_bo12, r4_to12] + [r5_bo12, r5_to12] * [ -797,  -797])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo12, r4_to12] - [r5_bo12, r5_to12] * [ -797,  -797])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo12, r6_to12] + [r7_bo12, r7_to12] * [ -797,  -797])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo12, r6_to12] - [r7_bo12, r7_to12] * [ -797,  -797])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r8_bo12, r8_to12] + [r9_bo12, r9_to12] * [ -797,  -797])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r8_bo12, r8_to12] - [r9_bo12, r9_to12] * [ -797,  -797])
          [r9_b, r9_t] [Q, Q] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [6*Q2,6*Q2,6*Q2,6*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2]
    prove with [cuts [1, 3, 5, 7, 9, 11, 13, 15]];

ghost r2_bo13@int16, r2_to13@int16, r3_bo13@int16, r3_to13@int16,
      r4_bo13@int16, r4_to13@int16, r5_bo13@int16, r5_to13@int16,
      r6_bo13@int16, r6_to13@int16, r7_bo13@int16, r7_to13@int16,
      r8_bo13@int16, r8_to13@int16, r9_bo13@int16, r9_to13@int16:
      r2_bo13 = r2_b /\ r2_to13 = r2_t /\ r3_bo13 = r3_b /\ r3_to13 = r3_t /\
      r4_bo13 = r4_b /\ r4_to13 = r4_t /\ r5_bo13 = r5_b /\ r5_to13 = r5_t /\
      r6_bo13 = r6_b /\ r6_to13 = r6_t /\ r7_bo13 = r7_b /\ r7_to13 = r7_t /\
      r8_bo13 = r8_b /\ r8_to13 = r8_t /\ r9_bo13 = r9_b /\ r9_to13 = r9_t
   && r2_bo13 = r2_b /\ r2_to13 = r2_t /\ r3_bo13 = r3_b /\ r3_to13 = r3_t /\
      r4_bo13 = r4_b /\ r4_to13 = r4_t /\ r5_bo13 = r5_b /\ r5_to13 = r5_t /\
      r6_bo13 = r6_b /\ r6_to13 = r6_t /\ r7_bo13 = r7_b /\ r7_to13 = r7_t /\
      r8_bo13 = r8_b /\ r8_to13 = r8_t /\ r9_bo13 = r9_b /\ r9_to13 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x40196c; Value = 0x997e0a00; PC = 0x401358 *)
mov [r10, r11] [L0x40196c, L0x401970];
(* smulwb	lr, r10, r4                              #! PC = 0x40135c *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401360 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401364 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401368 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x40136c *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_bo13 * -1333) [Q] /\
       eqmod lr_t (r4_to13 * -1333) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_bo13 * -1333) [Q] /\
       eqmod lr_t (r4_to13 * -1333) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r4, r2, lr                               #! PC = 0x401370 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401374 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401378 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40137c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401380 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401384 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401388 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo13 *  1089) [Q] /\
       eqmod lr_t (r5_to13 *  1089) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo13 *  1089) [Q] /\
       eqmod lr_t (r5_to13 *  1089) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x40138c *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401390 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x401394 *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401398 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40139c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4013a0 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x4013a4 *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo13 * -1333) [Q] /\
       eqmod lr_t (r8_to13 * -1333) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo13 * -1333) [Q] /\
       eqmod lr_t (r8_to13 * -1333) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r6, lr                               #! PC = 0x4013a8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x4013ac *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4013b0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4013b4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013b8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4013bc *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4013c0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo13 *  1089) [Q] /\
       eqmod lr_t (r9_to13 *  1089) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo13 *  1089) [Q] /\
       eqmod lr_t (r9_to13 *  1089) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x4013c4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x4013c8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;

assert [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
       prove with [algebra solver isl] && true;
assume [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
    && [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2];


(* CUT 30 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo13, r2_to13] + [r4_bo13, r4_to13] * [-1333, -1333])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo13, r2_to13] - [r4_bo13, r4_to13] * [-1333, -1333])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r3_bo13, r3_to13] + [r5_bo13, r5_to13] * [ 1089,  1089])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo13, r3_to13] - [r5_bo13, r5_to13] * [ 1089,  1089])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo13, r6_to13] + [r8_bo13, r8_to13] * [-1333, -1333])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo13, r6_to13] - [r8_bo13, r8_to13] * [-1333, -1333])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r7_bo13, r7_to13] + [r9_bo13, r9_to13] * [ 1089,  1089])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r7_bo13, r7_to13] - [r9_bo13, r9_to13] * [ 1089,  1089])
          [r9_b, r9_t] [Q, Q] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2];

ghost r2_bo14@int16, r2_to14@int16, r3_bo14@int16, r3_to14@int16,
      r4_bo14@int16, r4_to14@int16, r5_bo14@int16, r5_to14@int16,
      r6_bo14@int16, r6_to14@int16, r7_bo14@int16, r7_to14@int16,
      r8_bo14@int16, r8_to14@int16, r9_bo14@int16, r9_to14@int16:
      r2_bo14 = r2_b /\ r2_to14 = r2_t /\ r3_bo14 = r3_b /\ r3_to14 = r3_t /\
      r4_bo14 = r4_b /\ r4_to14 = r4_t /\ r5_bo14 = r5_b /\ r5_to14 = r5_t /\
      r6_bo14 = r6_b /\ r6_to14 = r6_t /\ r7_bo14 = r7_b /\ r7_to14 = r7_t /\
      r8_bo14 = r8_b /\ r8_to14 = r8_t /\ r9_bo14 = r9_b /\ r9_to14 = r9_t
   && r2_bo14 = r2_b /\ r2_to14 = r2_t /\ r3_bo14 = r3_b /\ r3_to14 = r3_t /\
      r4_bo14 = r4_b /\ r4_to14 = r4_t /\ r5_bo14 = r5_b /\ r5_to14 = r5_t /\
      r6_bo14 = r6_b /\ r6_to14 = r6_t /\ r7_bo14 = r7_b /\ r7_to14 = r7_t /\
      r8_bo14 = r8_b /\ r8_to14 = r8_t /\ r9_bo14 = r9_b /\ r9_to14 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401974; Value = 0x6cbc8f09; PC = 0x4013cc *)
mov [r10, r11] [L0x401974, L0x401978];
(* smulwb	lr, r10, r6                              #! PC = 0x4013d0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4013d4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013d8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4013dc *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x4013e0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r6_bo14 *  1414) [Q] /\
       eqmod lr_t (r6_to14 *  1414) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r6_bo14 *  1414) [Q] /\
       eqmod lr_t (r6_to14 *  1414) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r6, r2, lr                               #! PC = 0x4013e4 *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4013e8 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4013ec *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4013f0 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013f4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4013f8 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x4013fc *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo14 *   -33) [Q] /\
       eqmod lr_t (r7_to14 *   -33) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo14 *   -33) [Q] /\
       eqmod lr_t (r7_to14 *   -33) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x401400 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401404 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x40197c; Value = 0x9a7df650; PC = 0x401408 *)
mov [r10, r11] [L0x40197c, L0x401980];
(* smulwb	lr, r10, r8                              #! PC = 0x40140c *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401410 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401414 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x401418 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x40141c *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo14 * -1320) [Q] /\
       eqmod lr_t (r8_to14 * -1320) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo14 * -1320) [Q] /\
       eqmod lr_t (r8_to14 * -1320) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r4, lr                               #! PC = 0x401420 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x401424 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x401428 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x40142c *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401430 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401434 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x401438 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo14 *   464) [Q] /\
       eqmod lr_t (r9_to14 *   464) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo14 *   464) [Q] /\
       eqmod lr_t (r9_to14 *   464) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x40143c *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x401440 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;

assert [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
    && [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2];


(* CUT 31 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo14, r2_to14] + [r6_bo14, r6_to14] * [ 1414,  1414])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo14, r2_to14] - [r6_bo14, r6_to14] * [ 1414,  1414])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r3_bo14, r3_to14] + [r7_bo14, r7_to14] * [  -33,   -33])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo14, r3_to14] - [r7_bo14, r7_to14] * [  -33,   -33])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r4_bo14, r4_to14] + [r8_bo14, r8_to14] * [-1320, -1320])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo14, r4_to14] - [r8_bo14, r8_to14] * [-1320, -1320])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r5_bo14, r5_to14] + [r9_bo14, r9_to14] * [  464,   464])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r5_bo14, r5_to14] - [r9_bo14, r9_to14] * [  464,   464])
          [r9_b, r9_t] [Q, Q] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2];

ghost r2_bo15@int16, r2_to15@int16, r3_bo15@int16, r3_to15@int16,
      r4_bo15@int16, r4_to15@int16, r5_bo15@int16, r5_to15@int16,
      r6_bo15@int16, r6_to15@int16, r7_bo15@int16, r7_to15@int16,
      r8_bo15@int16, r8_to15@int16, r9_bo15@int16, r9_to15@int16:
      r2_bo15 = r2_b /\ r2_to15 = r2_t /\ r3_bo15 = r3_b /\ r3_to15 = r3_t /\
      r4_bo15 = r4_b /\ r4_to15 = r4_t /\ r5_bo15 = r5_b /\ r5_to15 = r5_t /\
      r6_bo15 = r6_b /\ r6_to15 = r6_t /\ r7_bo15 = r7_b /\ r7_to15 = r7_t /\
      r8_bo15 = r8_b /\ r8_to15 = r8_t /\ r9_bo15 = r9_b /\ r9_to15 = r9_t
   && r2_bo15 = r2_b /\ r2_to15 = r2_t /\ r3_bo15 = r3_b /\ r3_to15 = r3_t /\
      r4_bo15 = r4_b /\ r4_to15 = r4_t /\ r5_bo15 = r5_b /\ r5_to15 = r5_t /\
      r6_bo15 = r6_b /\ r6_to15 = r6_t /\ r7_bo15 = r7_b /\ r7_to15 = r7_t /\
      r8_bo15 = r8_b /\ r8_to15 = r8_t /\ r9_bo15 = r9_b /\ r9_to15 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401984; Value = 0x770a890a; PC = 0x401444 *)
mov [r10, r11] [L0x401984, L0x401988];
(* smulwb	lr, r10, r2                              #! PC = 0x401448 *)
vpc r2_bW@int32 r2_b; mulj tmp r10 r2_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r2, r10, r2                              #! PC = 0x40144c *)
vpc r2_tW@int32 r2_t; mulj tmp r10 r2_tW; spl r2_w dc tmp 16;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b; vpc r2_t@int16 r2_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401450 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r2, r2, r12, r0                          #! PC = 0x401454 *)
mulj prod r2_b r12_t; add r2_w prod r0;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b;
(* pkhtb	r2, r2, lr, asr #16                       #! PC = 0x401458 *)
mov tmp_b lr_t; mov tmp_t r2_t;
mov r2_b tmp_b; mov r2_t tmp_t;
(* smulwb	lr, r11, r3                              #! PC = 0x40145c *)
vpc r3_bW@int32 r3_b; mulj tmp r11 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r11, r3                              #! PC = 0x401460 *)
vpc r3_tW@int32 r3_t; mulj tmp r11 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401464 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x401468 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x40146c *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov r3_b tmp_b; mov r3_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x40198c; Value = 0x99a5696f; PC = 0x401470 *)
mov [r10, r11] [L0x40198c, L0x401990];
(* smulwb	lr, r10, r4                              #! PC = 0x401474 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401478 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40147c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401480 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x401484 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov r4_b tmp_b; mov r4_t tmp_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401488 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40148c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401490 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401494 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x401498 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov r5_b tmp_b; mov r5_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401994; Value = 0x01d87932; PC = 0x40149c *)
mov [r10, r11] [L0x401994, L0x401998];
(* smulwb	lr, r10, r6                              #! PC = 0x4014a0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4014a4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014a8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4014ac *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x4014b0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov r6_b tmp_b; mov r6_t tmp_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4014b4 *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4014b8 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014bc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4014c0 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x4014c4 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov r7_b tmp_b; mov r7_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x40199c; Value = 0x49d2efc7; PC = 0x4014c8 *)
mov [r10, r11] [L0x40199c, L0x4019a0];
(* smulwb	lr, r10, r8                              #! PC = 0x4014cc *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x4014d0 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014d4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4014d8 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x4014dc *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov r8_b tmp_b; mov r8_t tmp_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4014e0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4014e4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014e8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4014ec *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x4014f0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov r9_b tmp_b; mov r9_t tmp_t;

assert eqmod r2_b (r2_bo15 *  1548) [Q] /\
       eqmod r2_t (r2_to15 *  1548) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r2_b (r2_bo15 *  1548) [Q] /\
       eqmod r2_t (r2_to15 *  1548) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r2_b, r2_t] /\ [r2_b, r2_t] <s[Q2, Q2];


assert eqmod r3_b (r3_bo15 * -1078) [Q] /\
       eqmod r3_t (r3_to15 * -1078) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r3_b (r3_bo15 * -1078) [Q] /\
       eqmod r3_t (r3_to15 * -1078) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r3_b, r3_t] /\ [r3_b, r3_t] <s[Q2, Q2];


assert eqmod r4_b (r4_bo15 * -1331) [Q] /\
       eqmod r4_t (r4_to15 * -1331) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r4_b (r4_bo15 * -1331) [Q] /\
       eqmod r4_t (r4_to15 * -1331) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r4_b, r4_t] /\ [r4_b, r4_t] <s[Q2, Q2];


assert eqmod r5_b (r5_bo15 *   157) [Q] /\
       eqmod r5_t (r5_to15 *   157) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r5_b (r5_bo15 *   157) [Q] /\
       eqmod r5_t (r5_to15 *   157) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r5_b, r5_t] /\ [r5_b, r5_t] <s[Q2, Q2];


assert eqmod r6_b (r6_bo15 *    24) [Q] /\
       eqmod r6_t (r6_to15 *    24) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r6_b (r6_bo15 *    24) [Q] /\
       eqmod r6_t (r6_to15 *    24) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r6_b, r6_t] /\ [r6_b, r6_t] <s[Q2, Q2];


assert eqmod r7_b (r7_bo15 *  -378) [Q] /\
       eqmod r7_t (r7_to15 *  -378) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r7_b (r7_bo15 *  -378) [Q] /\
       eqmod r7_t (r7_to15 *  -378) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r7_b, r7_t] /\ [r7_b, r7_t] <s[Q2, Q2];


assert eqmod r8_b (r8_bo15 *   960) [Q] /\
       eqmod r8_t (r8_to15 *   960) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r8_b (r8_bo15 *   960) [Q] /\
       eqmod r8_t (r8_to15 *   960) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r8_b, r8_t] /\ [r8_b, r8_t] <s[Q2, Q2];


assert eqmod r9_b (r9_bo15 *  1525) [Q] /\
       eqmod r9_t (r9_to15 *  1525) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r9_b (r9_bo15 *  1525) [Q] /\
       eqmod r9_t (r9_to15 *  1525) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r9_b, r9_t] /\ [r9_b, r9_t] <s[Q2, Q2];

(* vmov	r0, s6                                     #! PC = 0x4014f4 *)
mov [r0_b, r0_t] [s6_b, s6_t];
(* str.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2d0; PC = 0x4014f8 *)
mov [L0xbefff2d0, L0xbefff2d2] [r6_b, r6_t];
(* str.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff310; PC = 0x4014fc *)
mov [L0xbefff310, L0xbefff312] [r7_b, r7_t];
(* str.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff350; PC = 0x401500 *)
mov [L0xbefff350, L0xbefff352] [r8_b, r8_t];
(* str.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff390; PC = 0x401504 *)
mov [L0xbefff390, L0xbefff392] [r9_b, r9_t];
(* str.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff210; PC = 0x401508 *)
mov [L0xbefff210, L0xbefff212] [r3_b, r3_t];
(* str.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff250; PC = 0x40150c *)
mov [L0xbefff250, L0xbefff252] [r4_b, r4_t];
(* str.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff290; PC = 0x401510 *)
mov [L0xbefff290, L0xbefff292] [r5_b, r5_t];
(* str.w	r2, [r0], #4                              #! EA = L0xbefff1d0; PC = 0x401514 *)
mov [L0xbefff1d0, L0xbefff1d2] [r2_b, r2_t];
(* vmov	lr, s14                                    #! PC = 0x401518 *)
mov [lr_b, lr_t] [s14_b, s14_t]; mov lr s14;
(* cmp.w	r0, lr                                    #! PC = 0x40151c *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x4012bc <invntt_fast+1552>              #! PC = 0x401520 *)
#bne.w	0x4012bc <invntt_fast+1552>              #! 0x401520 = 0x401520;

(* CUT 32 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo15, r2_to15] * [ 1548,  1548])
          [L0xbefff1d0, L0xbefff1d2] [Q, Q] /\
    eqmod ([r3_bo15, r3_to15] * [-1078, -1078])
          [L0xbefff210, L0xbefff212] [Q, Q] /\
    eqmod ([r4_bo15, r4_to15] * [-1331, -1331])
          [L0xbefff250, L0xbefff252] [Q, Q] /\
    eqmod ([r5_bo15, r5_to15] * [  157,   157])
          [L0xbefff290, L0xbefff292] [Q, Q] /\
    eqmod ([r6_bo15, r6_to15] * [   24,    24])
          [L0xbefff2d0, L0xbefff2d2] [Q, Q] /\
    eqmod ([r7_bo15, r7_to15] * [ -378,  -378])
          [L0xbefff310, L0xbefff312] [Q, Q] /\
    eqmod ([r8_bo15, r8_to15] * [  960,   960])
          [L0xbefff350, L0xbefff352] [Q, Q] /\
    eqmod ([r9_bo15, r9_to15] * [ 1525,  1525])
          [L0xbefff390, L0xbefff392] [Q, Q] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff1d0,L0xbefff1d2,L0xbefff210,L0xbefff212] /\
    [L0xbefff1d0,L0xbefff1d2,L0xbefff210,L0xbefff212]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff250,L0xbefff252,L0xbefff290,L0xbefff292] /\
    [L0xbefff250,L0xbefff252,L0xbefff290,L0xbefff292]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff2d0,L0xbefff2d2,L0xbefff310,L0xbefff312] /\
    [L0xbefff2d0,L0xbefff2d2,L0xbefff310,L0xbefff312]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff350,L0xbefff352,L0xbefff390,L0xbefff392] /\
    [L0xbefff350,L0xbefff352,L0xbefff390,L0xbefff392]< [Q2,Q2,Q2,Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1d0,L0xbefff1d2,L0xbefff210,L0xbefff212] /\
    [L0xbefff1d0,L0xbefff1d2,L0xbefff210,L0xbefff212]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff250,L0xbefff252,L0xbefff290,L0xbefff292] /\
    [L0xbefff250,L0xbefff252,L0xbefff290,L0xbefff292]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2d0,L0xbefff2d2,L0xbefff310,L0xbefff312] /\
    [L0xbefff2d0,L0xbefff2d2,L0xbefff310,L0xbefff312]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff350,L0xbefff352,L0xbefff390,L0xbefff392] /\
    [L0xbefff350,L0xbefff352,L0xbefff390,L0xbefff392]<s[Q2,Q2,Q2,Q2];


(* vmov	s6, r0                                     #! PC = 0x4012bc *)
mov [s6_b, s6_t] [r0_b, r0_t];
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff1d4; Value = 0x18c71413; PC = 0x4012c0 *)
mov [r2_b, r2_t] [L0xbefff1d4, L0xbefff1d6];
(* ldr.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff214; Value = 0xe32e2b64; PC = 0x4012c4 *)
mov [r3_b, r3_t] [L0xbefff214, L0xbefff216];
(* ldr.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff254; Value = 0xe2e10956; PC = 0x4012c8 *)
mov [r4_b, r4_t] [L0xbefff254, L0xbefff256];
(* ldr.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff294; Value = 0xdd5b2970; PC = 0x4012cc *)
mov [r5_b, r5_t] [L0xbefff294, L0xbefff296];
(* ldr.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2d4; Value = 0xe72429bc; PC = 0x4012d0 *)
mov [r6_b, r6_t] [L0xbefff2d4, L0xbefff2d6];
(* ldr.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff314; Value = 0xeefcf124; PC = 0x4012d4 *)
mov [r7_b, r7_t] [L0xbefff314, L0xbefff316];
(* ldr.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff354; Value = 0x0675e759; PC = 0x4012d8 *)
mov [r8_b, r8_t] [L0xbefff354, L0xbefff356];
(* ldr.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff394; Value = 0xdd55ddc3; PC = 0x4012dc *)
mov [r9_b, r9_t] [L0xbefff394, L0xbefff396];

ghost r2_bo16@int16, r2_to16@int16, r3_bo16@int16, r3_to16@int16,
      r4_bo16@int16, r4_to16@int16, r5_bo16@int16, r5_to16@int16,
      r6_bo16@int16, r6_to16@int16, r7_bo16@int16, r7_to16@int16,
      r8_bo16@int16, r8_to16@int16, r9_bo16@int16, r9_to16@int16:
      r2_bo16 = r2_b /\ r2_to16 = r2_t /\ r3_bo16 = r3_b /\ r3_to16 = r3_t /\
      r4_bo16 = r4_b /\ r4_to16 = r4_t /\ r5_bo16 = r5_b /\ r5_to16 = r5_t /\
      r6_bo16 = r6_b /\ r6_to16 = r6_t /\ r7_bo16 = r7_b /\ r7_to16 = r7_t /\
      r8_bo16 = r8_b /\ r8_to16 = r8_t /\ r9_bo16 = r9_b /\ r9_to16 = r9_t
   && r2_bo16 = r2_b /\ r2_to16 = r2_t /\ r3_bo16 = r3_b /\ r3_to16 = r3_t /\
      r4_bo16 = r4_b /\ r4_to16 = r4_t /\ r5_bo16 = r5_b /\ r5_to16 = r5_t /\
      r6_bo16 = r6_b /\ r6_to16 = r6_t /\ r7_bo16 = r7_b /\ r7_to16 = r7_t /\
      r8_bo16 = r8_b /\ r8_to16 = r8_t /\ r9_bo16 = r9_b /\ r9_to16 = r9_t;

(* movw	r0, #26632	; 0x6808                        #! PC = 0x4012e0 *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* ldr.w	r10, [r1], #4                             #! EA = L0x4019a4; Value = 0x031374a9; PC = 0x4012e4 *)
mov r10 L0x4019a4;
(* smulwb	lr, r10, r3                              #! PC = 0x4012e8 *)
vpc r3_bW@int32 r3_b; mulj tmp r10 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r10, r3                              #! PC = 0x4012ec *)
vpc r3_tW@int32 r3_t; mulj tmp r10 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4012f0 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4012f4 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x4012f8 *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r3_bo16 *    40) [Q] /\
       eqmod lr_t (r3_to16 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r3_bo16 *    40) [Q] /\
       eqmod lr_t (r3_to16 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r3, r2, lr                               #! PC = 0x4012fc *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401300 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x401304 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x401308 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40130c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401310 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401314 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo16 *    40) [Q] /\
       eqmod lr_t (r5_to16 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo16 *    40) [Q] /\
       eqmod lr_t (r5_to16 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r4, lr                               #! PC = 0x401318 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x40131c *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x401320 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x401324 *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401328 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x40132c *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x401330 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo16 *    40) [Q] /\
       eqmod lr_t (r7_to16 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo16 *    40) [Q] /\
       eqmod lr_t (r7_to16 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r6, lr                               #! PC = 0x401334 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x401338 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x40133c *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x401340 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401344 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401348 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x40134c *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo16 *    40) [Q] /\
       eqmod lr_t (r9_to16 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo16 *    40) [Q] /\
       eqmod lr_t (r9_to16 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r8, lr                               #! PC = 0x401350 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x401354 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

assert [10*NQ2,10*NQ2,10*NQ2,10*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [10*Q2,10*Q2,10*Q2,10*Q2] /\
       [10*NQ2,10*NQ2,10*NQ2,10*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [10*Q2,10*Q2,10*Q2,10*Q2] /\
       [10*NQ2,10*NQ2,10*NQ2,10*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [10*Q2,10*Q2,10*Q2,10*Q2] /\
       [10*NQ2,10*NQ2,10*NQ2,10*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [10*Q2,10*Q2,10*Q2,10*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [10*NQ2,10*NQ2,10*NQ2,10*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [10*Q2,10*Q2,10*Q2,10*Q2] /\
       [10*NQ2,10*NQ2,10*NQ2,10*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [10*Q2,10*Q2,10*Q2,10*Q2] /\
       [10*NQ2,10*NQ2,10*NQ2,10*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [10*Q2,10*Q2,10*Q2,10*Q2] /\
       [10*NQ2,10*NQ2,10*NQ2,10*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [10*Q2,10*Q2,10*Q2,10*Q2]
    && [10@16*NQ2,10@16*NQ2,10@16*NQ2,10@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[10@16*Q2,10@16*Q2,10@16*Q2,10@16*Q2] /\
       [10@16*NQ2,10@16*NQ2,10@16*NQ2,10@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[10@16*Q2,10@16*Q2,10@16*Q2,10@16*Q2] /\
       [10@16*NQ2,10@16*NQ2,10@16*NQ2,10@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[10@16*Q2,10@16*Q2,10@16*Q2,10@16*Q2] /\
       [10@16*NQ2,10@16*NQ2,10@16*NQ2,10@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[10@16*Q2,10@16*Q2,10@16*Q2,10@16*Q2];


(* CUT 33 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo16, r2_to16] + [r3_bo16, r3_to16] * [   40,    40])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo16, r2_to16] - [r3_bo16, r3_to16] * [   40,    40])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r4_bo16, r4_to16] + [r5_bo16, r5_to16] * [   40,    40])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo16, r4_to16] - [r5_bo16, r5_to16] * [   40,    40])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo16, r6_to16] + [r7_bo16, r7_to16] * [   40,    40])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo16, r6_to16] - [r7_bo16, r7_to16] * [   40,    40])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r8_bo16, r8_to16] + [r9_bo16, r9_to16] * [   40,    40])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r8_bo16, r8_to16] - [r9_bo16, r9_to16] * [   40,    40])
          [r9_b, r9_t] [Q, Q] /\
    [10*NQ2,10*NQ2,10*NQ2,10*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [10*Q2,10*Q2,10*Q2,10*Q2] /\
    [10*NQ2,10*NQ2,10*NQ2,10*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [10*Q2,10*Q2,10*Q2,10*Q2] /\
    [10*NQ2,10*NQ2,10*NQ2,10*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [10*Q2,10*Q2,10*Q2,10*Q2] /\
    [10*NQ2,10*NQ2,10*NQ2,10*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [10*Q2,10*Q2,10*Q2,10*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [10@16*NQ2,10@16*NQ2,10@16*NQ2,10@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[10@16*Q2,10@16*Q2,10@16*Q2,10@16*Q2] /\
    [10@16*NQ2,10@16*NQ2,10@16*NQ2,10@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[10@16*Q2,10@16*Q2,10@16*Q2,10@16*Q2] /\
    [10@16*NQ2,10@16*NQ2,10@16*NQ2,10@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[10@16*Q2,10@16*Q2,10@16*Q2,10@16*Q2] /\
    [10@16*NQ2,10@16*NQ2,10@16*NQ2,10@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[10@16*Q2,10@16*Q2,10@16*Q2,10@16*Q2]
    prove with [cuts [1, 3, 5, 7, 9, 11, 13, 15]];

ghost r2_bo17@int16, r2_to17@int16, r3_bo17@int16, r3_to17@int16,
      r4_bo17@int16, r4_to17@int16, r5_bo17@int16, r5_to17@int16,
      r6_bo17@int16, r6_to17@int16, r7_bo17@int16, r7_to17@int16,
      r8_bo17@int16, r8_to17@int16, r9_bo17@int16, r9_to17@int16:
      r2_bo17 = r2_b /\ r2_to17 = r2_t /\ r3_bo17 = r3_b /\ r3_to17 = r3_t /\
      r4_bo17 = r4_b /\ r4_to17 = r4_t /\ r5_bo17 = r5_b /\ r5_to17 = r5_t /\
      r6_bo17 = r6_b /\ r6_to17 = r6_t /\ r7_bo17 = r7_b /\ r7_to17 = r7_t /\
      r8_bo17 = r8_b /\ r8_to17 = r8_t /\ r9_bo17 = r9_b /\ r9_to17 = r9_t
   && r2_bo17 = r2_b /\ r2_to17 = r2_t /\ r3_bo17 = r3_b /\ r3_to17 = r3_t /\
      r4_bo17 = r4_b /\ r4_to17 = r4_t /\ r5_bo17 = r5_b /\ r5_to17 = r5_t /\
      r6_bo17 = r6_b /\ r6_to17 = r6_t /\ r7_bo17 = r7_b /\ r7_to17 = r7_t /\
      r8_bo17 = r8_b /\ r8_to17 = r8_t /\ r9_bo17 = r9_b /\ r9_to17 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4019a8; Value = 0xbec9f078; PC = 0x401358 *)
mov [r10, r11] [L0x4019a8, L0x4019ac];
(* smulwb	lr, r10, r4                              #! PC = 0x40135c *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401360 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401364 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401368 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x40136c *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_bo17 *  -848) [Q] /\
       eqmod lr_t (r4_to17 *  -848) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_bo17 *  -848) [Q] /\
       eqmod lr_t (r4_to17 *  -848) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r4, r2, lr                               #! PC = 0x401370 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401374 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401378 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40137c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401380 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401384 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401388 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo17 *  1432) [Q] /\
       eqmod lr_t (r5_to17 *  1432) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo17 *  1432) [Q] /\
       eqmod lr_t (r5_to17 *  1432) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x40138c *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401390 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x401394 *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401398 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40139c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4013a0 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x4013a4 *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo17 *  -848) [Q] /\
       eqmod lr_t (r8_to17 *  -848) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo17 *  -848) [Q] /\
       eqmod lr_t (r8_to17 *  -848) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r6, lr                               #! PC = 0x4013a8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x4013ac *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4013b0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4013b4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013b8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4013bc *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4013c0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo17 *  1432) [Q] /\
       eqmod lr_t (r9_to17 *  1432) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo17 *  1432) [Q] /\
       eqmod lr_t (r9_to17 *  1432) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x4013c4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x4013c8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;

assert [11*NQ2,11*NQ2,11*NQ2,11*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [11*Q2,11*Q2,11*Q2,11*Q2] /\
       [11*NQ2,11*NQ2,11*NQ2,11*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [11*Q2,11*Q2,11*Q2,11*Q2] /\
       [11*NQ2,11*NQ2,11*NQ2,11*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [11*Q2,11*Q2,11*Q2,11*Q2] /\
       [11*NQ2,11*NQ2,11*NQ2,11*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [11*Q2,11*Q2,11*Q2,11*Q2]
       prove with [algebra solver isl] && true;
assume [11*NQ2,11*NQ2,11*NQ2,11*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [11*Q2,11*Q2,11*Q2,11*Q2] /\
       [11*NQ2,11*NQ2,11*NQ2,11*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [11*Q2,11*Q2,11*Q2,11*Q2] /\
       [11*NQ2,11*NQ2,11*NQ2,11*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [11*Q2,11*Q2,11*Q2,11*Q2] /\
       [11*NQ2,11*NQ2,11*NQ2,11*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [11*Q2,11*Q2,11*Q2,11*Q2]
    && [11@16*NQ2,11@16*NQ2,11@16*NQ2,11@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[11@16*Q2,11@16*Q2,11@16*Q2,11@16*Q2] /\
       [11@16*NQ2,11@16*NQ2,11@16*NQ2,11@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[11@16*Q2,11@16*Q2,11@16*Q2,11@16*Q2] /\
       [11@16*NQ2,11@16*NQ2,11@16*NQ2,11@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[11@16*Q2,11@16*Q2,11@16*Q2,11@16*Q2] /\
       [11@16*NQ2,11@16*NQ2,11@16*NQ2,11@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[11@16*Q2,11@16*Q2,11@16*Q2,11@16*Q2];


(* CUT 34 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo17, r2_to17] + [r4_bo17, r4_to17] * [ -848,  -848])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo17, r2_to17] - [r4_bo17, r4_to17] * [ -848,  -848])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r3_bo17, r3_to17] + [r5_bo17, r5_to17] * [ 1432,  1432])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo17, r3_to17] - [r5_bo17, r5_to17] * [ 1432,  1432])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo17, r6_to17] + [r8_bo17, r8_to17] * [ -848,  -848])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo17, r6_to17] - [r8_bo17, r8_to17] * [ -848,  -848])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r7_bo17, r7_to17] + [r9_bo17, r9_to17] * [ 1432,  1432])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r7_bo17, r7_to17] - [r9_bo17, r9_to17] * [ 1432,  1432])
          [r9_b, r9_t] [Q, Q] /\
    [11*NQ2,11*NQ2,11*NQ2,11*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [11*Q2,11*Q2,11*Q2,11*Q2] /\
    [11*NQ2,11*NQ2,11*NQ2,11*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [11*Q2,11*Q2,11*Q2,11*Q2] /\
    [11*NQ2,11*NQ2,11*NQ2,11*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [11*Q2,11*Q2,11*Q2,11*Q2] /\
    [11*NQ2,11*NQ2,11*NQ2,11*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [11*Q2,11*Q2,11*Q2,11*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [11@16*NQ2,11@16*NQ2,11@16*NQ2,11@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[11@16*Q2,11@16*Q2,11@16*Q2,11@16*Q2] /\
    [11@16*NQ2,11@16*NQ2,11@16*NQ2,11@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[11@16*Q2,11@16*Q2,11@16*Q2,11@16*Q2] /\
    [11@16*NQ2,11@16*NQ2,11@16*NQ2,11@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[11@16*Q2,11@16*Q2,11@16*Q2,11@16*Q2] /\
    [11@16*NQ2,11@16*NQ2,11@16*NQ2,11@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[11@16*Q2,11@16*Q2,11@16*Q2,11@16*Q2];

ghost r2_bo18@int16, r2_to18@int16, r3_bo18@int16, r3_to18@int16,
      r4_bo18@int16, r4_to18@int16, r5_bo18@int16, r5_to18@int16,
      r6_bo18@int16, r6_to18@int16, r7_bo18@int16, r7_to18@int16,
      r8_bo18@int16, r8_to18@int16, r9_bo18@int16, r9_to18@int16:
      r2_bo18 = r2_b /\ r2_to18 = r2_t /\ r3_bo18 = r3_b /\ r3_to18 = r3_t /\
      r4_bo18 = r4_b /\ r4_to18 = r4_t /\ r5_bo18 = r5_b /\ r5_to18 = r5_t /\
      r6_bo18 = r6_b /\ r6_to18 = r6_t /\ r7_bo18 = r7_b /\ r7_to18 = r7_t /\
      r8_bo18 = r8_b /\ r8_to18 = r8_t /\ r9_bo18 = r9_b /\ r9_to18 = r9_t
   && r2_bo18 = r2_b /\ r2_to18 = r2_t /\ r3_bo18 = r3_b /\ r3_to18 = r3_t /\
      r4_bo18 = r4_b /\ r4_to18 = r4_t /\ r5_bo18 = r5_b /\ r5_to18 = r5_t /\
      r6_bo18 = r6_b /\ r6_to18 = r6_t /\ r7_bo18 = r7_b /\ r7_to18 = r7_t /\
      r8_bo18 = r8_b /\ r8_to18 = r8_t /\ r9_bo18 = r9_b /\ r9_to18 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4019b0; Value = 0x79bb8f1d; PC = 0x4013cc *)
mov [r10, r11] [L0x4019b0, L0x4019b4];
(* smulwb	lr, r10, r6                              #! PC = 0x4013d0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4013d4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013d8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4013dc *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x4013e0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r6_bo18 *  1583) [Q] /\
       eqmod lr_t (r6_to18 *  1583) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r6_bo18 *  1583) [Q] /\
       eqmod lr_t (r6_to18 *  1583) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r6, r2, lr                               #! PC = 0x4013e4 *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4013e8 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4013ec *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4013f0 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013f4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4013f8 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x4013fc *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo18 *    69) [Q] /\
       eqmod lr_t (r7_to18 *    69) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo18 *    69) [Q] /\
       eqmod lr_t (r7_to18 *    69) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x401400 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401404 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4019b8; Value = 0xd43e715a; PC = 0x401408 *)
mov [r10, r11] [L0x4019b8, L0x4019bc];
(* smulwb	lr, r10, r8                              #! PC = 0x40140c *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401410 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401414 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x401418 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x40141c *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo18 *  -569) [Q] /\
       eqmod lr_t (r8_to18 *  -569) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo18 *  -569) [Q] /\
       eqmod lr_t (r8_to18 *  -569) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r4, lr                               #! PC = 0x401420 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x401424 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x401428 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x40142c *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401430 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401434 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x401438 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo18 *   543) [Q] /\
       eqmod lr_t (r9_to18 *   543) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo18 *   543) [Q] /\
       eqmod lr_t (r9_to18 *   543) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x40143c *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x401440 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;

assert [12*NQ2,12*NQ2,12*NQ2,12*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [12*Q2,12*Q2,12*Q2,12*Q2] /\
       [12*NQ2,12*NQ2,12*NQ2,12*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [12*Q2,12*Q2,12*Q2,12*Q2] /\
       [12*NQ2,12*NQ2,12*NQ2,12*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [12*Q2,12*Q2,12*Q2,12*Q2] /\
       [12*NQ2,12*NQ2,12*NQ2,12*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [12*Q2,12*Q2,12*Q2,12*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [12*NQ2,12*NQ2,12*NQ2,12*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [12*Q2,12*Q2,12*Q2,12*Q2] /\
       [12*NQ2,12*NQ2,12*NQ2,12*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [12*Q2,12*Q2,12*Q2,12*Q2] /\
       [12*NQ2,12*NQ2,12*NQ2,12*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [12*Q2,12*Q2,12*Q2,12*Q2] /\
       [12*NQ2,12*NQ2,12*NQ2,12*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [12*Q2,12*Q2,12*Q2,12*Q2]
    && [12@16*NQ2,12@16*NQ2,12@16*NQ2,12@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[12@16*Q2,12@16*Q2,12@16*Q2,12@16*Q2] /\
       [12@16*NQ2,12@16*NQ2,12@16*NQ2,12@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[12@16*Q2,12@16*Q2,12@16*Q2,12@16*Q2] /\
       [12@16*NQ2,12@16*NQ2,12@16*NQ2,12@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[12@16*Q2,12@16*Q2,12@16*Q2,12@16*Q2] /\
       [12@16*NQ2,12@16*NQ2,12@16*NQ2,12@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[12@16*Q2,12@16*Q2,12@16*Q2,12@16*Q2];


(* CUT 35 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo18, r2_to18] + [r6_bo18, r6_to18] * [ 1583,  1583])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo18, r2_to18] - [r6_bo18, r6_to18] * [ 1583,  1583])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r3_bo18, r3_to18] + [r7_bo18, r7_to18] * [   69,    69])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo18, r3_to18] - [r7_bo18, r7_to18] * [   69,    69])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r4_bo18, r4_to18] + [r8_bo18, r8_to18] * [ -569,  -569])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo18, r4_to18] - [r8_bo18, r8_to18] * [ -569,  -569])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r5_bo18, r5_to18] + [r9_bo18, r9_to18] * [  543,   543])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r5_bo18, r5_to18] - [r9_bo18, r9_to18] * [  543,   543])
          [r9_b, r9_t] [Q, Q] /\
    [12*NQ2,12*NQ2,12*NQ2,12*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [12*Q2,12*Q2,12*Q2,12*Q2] /\
    [12*NQ2,12*NQ2,12*NQ2,12*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [12*Q2,12*Q2,12*Q2,12*Q2] /\
    [12*NQ2,12*NQ2,12*NQ2,12*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [12*Q2,12*Q2,12*Q2,12*Q2] /\
    [12*NQ2,12*NQ2,12*NQ2,12*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [12*Q2,12*Q2,12*Q2,12*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [12@16*NQ2,12@16*NQ2,12@16*NQ2,12@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[12@16*Q2,12@16*Q2,12@16*Q2,12@16*Q2] /\
    [12@16*NQ2,12@16*NQ2,12@16*NQ2,12@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[12@16*Q2,12@16*Q2,12@16*Q2,12@16*Q2] /\
    [12@16*NQ2,12@16*NQ2,12@16*NQ2,12@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[12@16*Q2,12@16*Q2,12@16*Q2,12@16*Q2] /\
    [12@16*NQ2,12@16*NQ2,12@16*NQ2,12@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[12@16*Q2,12@16*Q2,12@16*Q2,12@16*Q2];


ghost r2_bo19@int16, r2_to19@int16, r3_bo19@int16, r3_to19@int16,
      r4_bo19@int16, r4_to19@int16, r5_bo19@int16, r5_to19@int16,
      r6_bo19@int16, r6_to19@int16, r7_bo19@int16, r7_to19@int16,
      r8_bo19@int16, r8_to19@int16, r9_bo19@int16, r9_to19@int16:
      r2_bo19 = r2_b /\ r2_to19 = r2_t /\ r3_bo19 = r3_b /\ r3_to19 = r3_t /\
      r4_bo19 = r4_b /\ r4_to19 = r4_t /\ r5_bo19 = r5_b /\ r5_to19 = r5_t /\
      r6_bo19 = r6_b /\ r6_to19 = r6_t /\ r7_bo19 = r7_b /\ r7_to19 = r7_t /\
      r8_bo19 = r8_b /\ r8_to19 = r8_t /\ r9_bo19 = r9_b /\ r9_to19 = r9_t
   && r2_bo19 = r2_b /\ r2_to19 = r2_t /\ r3_bo19 = r3_b /\ r3_to19 = r3_t /\
      r4_bo19 = r4_b /\ r4_to19 = r4_t /\ r5_bo19 = r5_b /\ r5_to19 = r5_t /\
      r6_bo19 = r6_b /\ r6_to19 = r6_t /\ r7_bo19 = r7_b /\ r7_to19 = r7_t /\
      r8_bo19 = r8_b /\ r8_to19 = r8_t /\ r9_bo19 = r9_b /\ r9_to19 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4019c0; Value = 0x615af901; PC = 0x401444 *)
mov [r10, r11] [L0x4019c0, L0x4019c4];
(* smulwb	lr, r10, r2                              #! PC = 0x401448 *)
vpc r2_bW@int32 r2_b; mulj tmp r10 r2_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r2, r10, r2                              #! PC = 0x40144c *)
vpc r2_tW@int32 r2_t; mulj tmp r10 r2_tW; spl r2_w dc tmp 16;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b; vpc r2_t@int16 r2_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401450 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r2, r2, r12, r0                          #! PC = 0x401454 *)
mulj prod r2_b r12_t; add r2_w prod r0;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b;
(* pkhtb	r2, r2, lr, asr #16                       #! PC = 0x401458 *)
mov tmp_b lr_t; mov tmp_t r2_t;
mov r2_b tmp_b; mov r2_t tmp_t;
(* smulwb	lr, r11, r3                              #! PC = 0x40145c *)
vpc r3_bW@int32 r3_b; mulj tmp r11 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r11, r3                              #! PC = 0x401460 *)
vpc r3_tW@int32 r3_t; mulj tmp r11 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401464 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x401468 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x40146c *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov r3_b tmp_b; mov r3_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4019c8; Value = 0x3636e816; PC = 0x401470 *)
mov [r10, r11] [L0x4019c8, L0x4019cc];
(* smulwb	lr, r10, r4                              #! PC = 0x401474 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401478 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40147c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401480 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x401484 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov r4_b tmp_b; mov r4_t tmp_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401488 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40148c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401490 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401494 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x401498 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov r5_b tmp_b; mov r5_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4019d0; Value = 0x7894435e; PC = 0x40149c *)
mov [r10, r11] [L0x4019d0, L0x4019d4];
(* smulwb	lr, r10, r6                              #! PC = 0x4014a0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4014a4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014a8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4014ac *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x4014b0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov r6_b tmp_b; mov r6_t tmp_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4014b4 *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4014b8 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014bc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4014c0 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x4014c4 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov r7_b tmp_b; mov r7_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4019d8; Value = 0xd72a8694; PC = 0x4014c8 *)
mov [r10, r11] [L0x4019d8, L0x4019dc];
(* smulwb	lr, r10, r8                              #! PC = 0x4014cc *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x4014d0 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014d4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4014d8 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x4014dc *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov r8_b tmp_b; mov r8_t tmp_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4014e0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4014e4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014e8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4014ec *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x4014f0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov r9_b tmp_b; mov r9_t tmp_t;

assert eqmod r2_b (r2_bo19 *  1266) [Q] /\
       eqmod r2_t (r2_to19 *  1266) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r2_b (r2_bo19 *  1266) [Q] /\
       eqmod r2_t (r2_to19 *  1266) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r2_b, r2_t] /\ [r2_b, r2_t] <s[Q2, Q2];


assert eqmod r3_b (r3_bo19 * -1630) [Q] /\
       eqmod r3_t (r3_to19 * -1630) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r3_b (r3_bo19 * -1630) [Q] /\
       eqmod r3_t (r3_to19 * -1630) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r3_b, r3_t] /\ [r3_b, r3_t] <s[Q2, Q2];


assert eqmod r4_b (r4_bo19 *   705) [Q] /\
       eqmod r4_t (r4_to19 *   705) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r4_b (r4_bo19 *   705) [Q] /\
       eqmod r4_t (r4_to19 *   705) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r4_b, r4_t] /\ [r4_b, r4_t] <s[Q2, Q2];


assert eqmod r5_b (r5_bo19 *  1380) [Q] /\
       eqmod r5_t (r5_to19 *  1380) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r5_b (r5_bo19 *  1380) [Q] /\
       eqmod r5_t (r5_to19 *  1380) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r5_b, r5_t] /\ [r5_b, r5_t] <s[Q2, Q2];


assert eqmod r6_b (r6_bo19 *  1568) [Q] /\
       eqmod r6_t (r6_to19 *  1568) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r6_b (r6_bo19 *  1568) [Q] /\
       eqmod r6_t (r6_to19 *  1568) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r6_b, r6_t] /\ [r6_b, r6_t] <s[Q2, Q2];


assert eqmod r7_b (r7_bo19 * -1393) [Q] /\
       eqmod r7_t (r7_to19 * -1393) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r7_b (r7_bo19 * -1393) [Q] /\
       eqmod r7_t (r7_to19 * -1393) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r7_b, r7_t] /\ [r7_b, r7_t] <s[Q2, Q2];


assert eqmod r8_b (r8_bo19 *  -531) [Q] /\
       eqmod r8_t (r8_to19 *  -531) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r8_b (r8_bo19 *  -531) [Q] /\
       eqmod r8_t (r8_to19 *  -531) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r8_b, r8_t] /\ [r8_b, r8_t] <s[Q2, Q2];


assert eqmod r9_b (r9_bo19 *   873) [Q] /\
       eqmod r9_t (r9_to19 *   873) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r9_b (r9_bo19 *   873) [Q] /\
       eqmod r9_t (r9_to19 *   873) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r9_b, r9_t] /\ [r9_b, r9_t] <s[Q2, Q2];


(* vmov	r0, s6                                     #! PC = 0x4014f4 *)
mov [r0_b, r0_t] [s6_b, s6_t];
(* str.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2d4; PC = 0x4014f8 *)
mov [L0xbefff2d4, L0xbefff2d6] [r6_b, r6_t];
(* str.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff314; PC = 0x4014fc *)
mov [L0xbefff314, L0xbefff316] [r7_b, r7_t];
(* str.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff354; PC = 0x401500 *)
mov [L0xbefff354, L0xbefff356] [r8_b, r8_t];
(* str.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff394; PC = 0x401504 *)
mov [L0xbefff394, L0xbefff396] [r9_b, r9_t];
(* str.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff214; PC = 0x401508 *)
mov [L0xbefff214, L0xbefff216] [r3_b, r3_t];
(* str.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff254; PC = 0x40150c *)
mov [L0xbefff254, L0xbefff256] [r4_b, r4_t];
(* str.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff294; PC = 0x401510 *)
mov [L0xbefff294, L0xbefff296] [r5_b, r5_t];
(* str.w	r2, [r0], #4                              #! EA = L0xbefff1d4; PC = 0x401514 *)
mov [L0xbefff1d4, L0xbefff1d6] [r2_b, r2_t];
(* vmov	lr, s14                                    #! PC = 0x401518 *)
mov [lr_b, lr_t] [s14_b, s14_t]; mov lr s14;
(* cmp.w	r0, lr                                    #! PC = 0x40151c *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x4012bc <invntt_fast+1552>              #! PC = 0x401520 *)
#bne.w	0x4012bc <invntt_fast+1552>              #! 0x401520 = 0x401520;

(* CUT 36 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo19, r2_to19] * [ 1266,  1266])
          [L0xbefff1d4, L0xbefff1d6] [Q, Q] /\
    eqmod ([r3_bo19, r3_to19] * [-1630, -1630])
          [L0xbefff214, L0xbefff216] [Q, Q] /\
    eqmod ([r4_bo19, r4_to19] * [  705,   705])
          [L0xbefff254, L0xbefff256] [Q, Q] /\
    eqmod ([r5_bo19, r5_to19] * [ 1380,  1380])
          [L0xbefff294, L0xbefff296] [Q, Q] /\
    eqmod ([r6_bo19, r6_to19] * [ 1568,  1568])
          [L0xbefff2d4, L0xbefff2d6] [Q, Q] /\
    eqmod ([r7_bo19, r7_to19] * [-1393, -1393])
          [L0xbefff314, L0xbefff316] [Q, Q] /\
    eqmod ([r8_bo19, r8_to19] * [ -531,  -531])
          [L0xbefff354, L0xbefff356] [Q, Q] /\
    eqmod ([r9_bo19, r9_to19] * [  873,   873])
          [L0xbefff394, L0xbefff396] [Q, Q] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff1d4,L0xbefff1d6,L0xbefff214,L0xbefff216] /\
    [L0xbefff1d4,L0xbefff1d6,L0xbefff214,L0xbefff216]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff254,L0xbefff256,L0xbefff294,L0xbefff296] /\
    [L0xbefff254,L0xbefff256,L0xbefff294,L0xbefff296]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff2d4,L0xbefff2d6,L0xbefff314,L0xbefff316] /\
    [L0xbefff2d4,L0xbefff2d6,L0xbefff314,L0xbefff316]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff354,L0xbefff356,L0xbefff394,L0xbefff396] /\
    [L0xbefff354,L0xbefff356,L0xbefff394,L0xbefff396]< [Q2,Q2,Q2,Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1d4,L0xbefff1d6,L0xbefff214,L0xbefff216] /\
    [L0xbefff1d4,L0xbefff1d6,L0xbefff214,L0xbefff216]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff254,L0xbefff256,L0xbefff294,L0xbefff296] /\
    [L0xbefff254,L0xbefff256,L0xbefff294,L0xbefff296]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2d4,L0xbefff2d6,L0xbefff314,L0xbefff316] /\
    [L0xbefff2d4,L0xbefff2d6,L0xbefff314,L0xbefff316]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff354,L0xbefff356,L0xbefff394,L0xbefff396] /\
    [L0xbefff354,L0xbefff356,L0xbefff394,L0xbefff396]<s[Q2,Q2,Q2,Q2];


(* vmov	s6, r0                                     #! PC = 0x4012bc *)
mov [s6_b, s6_t] [r0_b, r0_t];
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff1d8; Value = 0x0d300fa8; PC = 0x4012c0 *)
mov [r2_b, r2_t] [L0xbefff1d8, L0xbefff1da];
(* ldr.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff218; Value = 0xff650d82; PC = 0x4012c4 *)
mov [r3_b, r3_t] [L0xbefff218, L0xbefff21a];
(* ldr.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff258; Value = 0xe992f68c; PC = 0x4012c8 *)
mov [r4_b, r4_t] [L0xbefff258, L0xbefff25a];
(* ldr.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff298; Value = 0xf23a0854; PC = 0x4012cc *)
mov [r5_b, r5_t] [L0xbefff298, L0xbefff29a];
(* ldr.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2d8; Value = 0x0680fb44; PC = 0x4012d0 *)
mov [r6_b, r6_t] [L0xbefff2d8, L0xbefff2da];
(* ldr.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff318; Value = 0xfa5bf4c1; PC = 0x4012d4 *)
mov [r7_b, r7_t] [L0xbefff318, L0xbefff31a];
(* ldr.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff358; Value = 0x01ab0748; PC = 0x4012d8 *)
mov [r8_b, r8_t] [L0xbefff358, L0xbefff35a];
(* ldr.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff398; Value = 0xf03eefa8; PC = 0x4012dc *)
mov [r9_b, r9_t] [L0xbefff398, L0xbefff39a];

ghost r2_bo20@int16, r2_to20@int16, r3_bo20@int16, r3_to20@int16,
      r4_bo20@int16, r4_to20@int16, r5_bo20@int16, r5_to20@int16,
      r6_bo20@int16, r6_to20@int16, r7_bo20@int16, r7_to20@int16,
      r8_bo20@int16, r8_to20@int16, r9_bo20@int16, r9_to20@int16:
      r2_bo20 = r2_b /\ r2_to20 = r2_t /\ r3_bo20 = r3_b /\ r3_to20 = r3_t /\
      r4_bo20 = r4_b /\ r4_to20 = r4_t /\ r5_bo20 = r5_b /\ r5_to20 = r5_t /\
      r6_bo20 = r6_b /\ r6_to20 = r6_t /\ r7_bo20 = r7_b /\ r7_to20 = r7_t /\
      r8_bo20 = r8_b /\ r8_to20 = r8_t /\ r9_bo20 = r9_b /\ r9_to20 = r9_t
   && r2_bo20 = r2_b /\ r2_to20 = r2_t /\ r3_bo20 = r3_b /\ r3_to20 = r3_t /\
      r4_bo20 = r4_b /\ r4_to20 = r4_t /\ r5_bo20 = r5_b /\ r5_to20 = r5_t /\
      r6_bo20 = r6_b /\ r6_to20 = r6_t /\ r7_bo20 = r7_b /\ r7_to20 = r7_t /\
      r8_bo20 = r8_b /\ r8_to20 = r8_t /\ r9_bo20 = r9_b /\ r9_to20 = r9_t;

(* movw	r0, #26632	; 0x6808                        #! PC = 0x4012e0 *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* ldr.w	r10, [r1], #4                             #! EA = L0x4019e0; Value = 0x054e5c70; PC = 0x4012e4 *)
mov r10 L0x4019e0;
(* smulwb	lr, r10, r3                              #! PC = 0x4012e8 *)
vpc r3_bW@int32 r3_b; mulj tmp r10 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r10, r3                              #! PC = 0x4012ec *)
vpc r3_tW@int32 r3_t; mulj tmp r10 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4012f0 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4012f4 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x4012f8 *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r3_bo20 *    69) [Q] /\
       eqmod lr_t (r3_to20 *    69) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r3_bo20 *    69) [Q] /\
       eqmod lr_t (r3_to20 *    69) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r3, r2, lr                               #! PC = 0x4012fc *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401300 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x401304 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x401308 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40130c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401310 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401314 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo20 *    69) [Q] /\
       eqmod lr_t (r5_to20 *    69) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo20 *    69) [Q] /\
       eqmod lr_t (r5_to20 *    69) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r4, lr                               #! PC = 0x401318 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x40131c *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x401320 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x401324 *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401328 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x40132c *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x401330 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo20 *    69) [Q] /\
       eqmod lr_t (r7_to20 *    69) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo20 *    69) [Q] /\
       eqmod lr_t (r7_to20 *    69) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r6, lr                               #! PC = 0x401334 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x401338 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x40133c *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x401340 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401344 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401348 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x40134c *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo20 *    69) [Q] /\
       eqmod lr_t (r9_to20 *    69) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo20 *    69) [Q] /\
       eqmod lr_t (r9_to20 *    69) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r8, lr                               #! PC = 0x401350 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x401354 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

assert [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [6*Q2,6*Q2,6*Q2,6*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [6*Q2,6*Q2,6*Q2,6*Q2]
    && [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2];


(* CUT 37 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo20, r2_to20] + [r3_bo20, r3_to20] * [   69,    69])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo20, r2_to20] - [r3_bo20, r3_to20] * [   69,    69])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r4_bo20, r4_to20] + [r5_bo20, r5_to20] * [   69,    69])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo20, r4_to20] - [r5_bo20, r5_to20] * [   69,    69])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo20, r6_to20] + [r7_bo20, r7_to20] * [   69,    69])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo20, r6_to20] - [r7_bo20, r7_to20] * [   69,    69])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r8_bo20, r8_to20] + [r9_bo20, r9_to20] * [   69,    69])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r8_bo20, r8_to20] - [r9_bo20, r9_to20] * [   69,    69])
          [r9_b, r9_t] [Q, Q] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [6*Q2,6*Q2,6*Q2,6*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2]
    prove with [cuts [1, 3, 5, 7, 9, 11, 13, 15]];

ghost r2_bo21@int16, r2_to21@int16, r3_bo21@int16, r3_to21@int16,
      r4_bo21@int16, r4_to21@int16, r5_bo21@int16, r5_to21@int16,
      r6_bo21@int16, r6_to21@int16, r7_bo21@int16, r7_to21@int16,
      r8_bo21@int16, r8_to21@int16, r9_bo21@int16, r9_to21@int16:
      r2_bo21 = r2_b /\ r2_to21 = r2_t /\ r3_bo21 = r3_b /\ r3_to21 = r3_t /\
      r4_bo21 = r4_b /\ r4_to21 = r4_t /\ r5_bo21 = r5_b /\ r5_to21 = r5_t /\
      r6_bo21 = r6_b /\ r6_to21 = r6_t /\ r7_bo21 = r7_b /\ r7_to21 = r7_t /\
      r8_bo21 = r8_b /\ r8_to21 = r8_t /\ r9_bo21 = r9_b /\ r9_to21 = r9_t
   && r2_bo21 = r2_b /\ r2_to21 = r2_t /\ r3_bo21 = r3_b /\ r3_to21 = r3_t /\
      r4_bo21 = r4_b /\ r4_to21 = r4_t /\ r5_bo21 = r5_b /\ r5_to21 = r5_t /\
      r6_bo21 = r6_b /\ r6_to21 = r6_t /\ r7_bo21 = r7_b /\ r7_to21 = r7_t /\
      r8_bo21 = r8_b /\ r8_to21 = r8_t /\ r9_bo21 = r9_b /\ r9_to21 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4019e4; Value = 0x225fd13f; PC = 0x401358 *)
mov [r10, r11] [L0x4019e4, L0x4019e8];
(* smulwb	lr, r10, r4                              #! PC = 0x40135c *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401360 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401364 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401368 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x40136c *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_bo21 *   447) [Q] /\
       eqmod lr_t (r4_to21 *   447) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_bo21 *   447) [Q] /\
       eqmod lr_t (r4_to21 *   447) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r4, r2, lr                               #! PC = 0x401370 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401374 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401378 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40137c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401380 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401384 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401388 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo21 *  -535) [Q] /\
       eqmod lr_t (r5_to21 *  -535) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo21 *  -535) [Q] /\
       eqmod lr_t (r5_to21 *  -535) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x40138c *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401390 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x401394 *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401398 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40139c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4013a0 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x4013a4 *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo21 *   447) [Q] /\
       eqmod lr_t (r8_to21 *   447) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo21 *   447) [Q] /\
       eqmod lr_t (r8_to21 *   447) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r6, lr                               #! PC = 0x4013a8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x4013ac *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4013b0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4013b4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013b8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4013bc *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4013c0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo21 *  -535) [Q] /\
       eqmod lr_t (r9_to21 *  -535) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo21 *  -535) [Q] /\
       eqmod lr_t (r9_to21 *  -535) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x4013c4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x4013c8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;

assert [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
       prove with [algebra solver isl] && true;
assume [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
    && [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2];


(* CUT 38 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo21, r2_to21] + [r4_bo21, r4_to21] * [  447,   447])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo21, r2_to21] - [r4_bo21, r4_to21] * [  447,   447])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r3_bo21, r3_to21] + [r5_bo21, r5_to21] * [ -535,  -535])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo21, r3_to21] - [r5_bo21, r5_to21] * [ -535,  -535])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo21, r6_to21] + [r8_bo21, r8_to21] * [  447,   447])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo21, r6_to21] - [r8_bo21, r8_to21] * [  447,   447])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r7_bo21, r7_to21] + [r9_bo21, r9_to21] * [ -535,  -535])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r7_bo21, r7_to21] - [r9_bo21, r9_to21] * [ -535,  -535])
          [r9_b, r9_t] [Q, Q] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2];

ghost r2_bo22@int16, r2_to22@int16, r3_bo22@int16, r3_to22@int16,
      r4_bo22@int16, r4_to22@int16, r5_bo22@int16, r5_to22@int16,
      r6_bo22@int16, r6_to22@int16, r7_bo22@int16, r7_to22@int16,
      r8_bo22@int16, r8_to22@int16, r9_bo22@int16, r9_to22@int16:
      r2_bo22 = r2_b /\ r2_to22 = r2_t /\ r3_bo22 = r3_b /\ r3_to22 = r3_t /\
      r4_bo22 = r4_b /\ r4_to22 = r4_t /\ r5_bo22 = r5_b /\ r5_to22 = r5_t /\
      r6_bo22 = r6_b /\ r6_to22 = r6_t /\ r7_bo22 = r7_b /\ r7_to22 = r7_t /\
      r8_bo22 = r8_b /\ r8_to22 = r8_t /\ r9_bo22 = r9_b /\ r9_to22 = r9_t
   && r2_bo22 = r2_b /\ r2_to22 = r2_t /\ r3_bo22 = r3_b /\ r3_to22 = r3_t /\
      r4_bo22 = r4_b /\ r4_to22 = r4_t /\ r5_bo22 = r5_b /\ r5_to22 = r5_t /\
      r6_bo22 = r6_b /\ r6_to22 = r6_t /\ r7_bo22 = r7_b /\ r7_to22 = r7_t /\
      r8_bo22 = r8_b /\ r8_to22 = r8_t /\ r9_bo22 = r9_b /\ r9_to22 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4019ec; Value = 0x47494922; PC = 0x4013cc *)
mov [r10, r11] [L0x4019ec, L0x4019f0];
(* smulwb	lr, r10, r6                              #! PC = 0x4013d0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4013d4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013d8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4013dc *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x4013e0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r6_bo22 *   927) [Q] /\
       eqmod lr_t (r6_to22 *   927) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r6_bo22 *   927) [Q] /\
       eqmod lr_t (r6_to22 *   927) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r6, r2, lr                               #! PC = 0x4013e4 *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4013e8 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4013ec *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4013f0 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013f4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4013f8 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x4013fc *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo22 *   461) [Q] /\
       eqmod lr_t (r7_to22 *   461) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo22 *   461) [Q] /\
       eqmod lr_t (r7_to22 *   461) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x401400 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401404 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4019f4; Value = 0x8a0912ff; PC = 0x401408 *)
mov [r10, r11] [L0x4019f4, L0x4019f8];
(* smulwb	lr, r10, r8                              #! PC = 0x40140c *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401410 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401414 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x401418 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x40141c *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo22 * -1534) [Q] /\
       eqmod lr_t (r8_to22 * -1534) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo22 * -1534) [Q] /\
       eqmod lr_t (r8_to22 * -1534) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r4, lr                               #! PC = 0x401420 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x401424 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x401428 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x40142c *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401430 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401434 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x401438 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo22 * -1438) [Q] /\
       eqmod lr_t (r9_to22 * -1438) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo22 * -1438) [Q] /\
       eqmod lr_t (r9_to22 * -1438) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x40143c *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x401440 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;

assert [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
    && [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2];


(* CUT 39 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo22, r2_to22] + [r6_bo22, r6_to22] * [  927,   927])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo22, r2_to22] - [r6_bo22, r6_to22] * [  927,   927])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r3_bo22, r3_to22] + [r7_bo22, r7_to22] * [  461,   461])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo22, r3_to22] - [r7_bo22, r7_to22] * [  461,   461])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r4_bo22, r4_to22] + [r8_bo22, r8_to22] * [-1534, -1534])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo22, r4_to22] - [r8_bo22, r8_to22] * [-1534, -1534])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r5_bo22, r5_to22] + [r9_bo22, r9_to22] * [-1438, -1438])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r5_bo22, r5_to22] - [r9_bo22, r9_to22] * [-1438, -1438])
          [r9_b, r9_t] [Q, Q] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2];



ghost r2_bo23@int16, r2_to23@int16, r3_bo23@int16, r3_to23@int16,
      r4_bo23@int16, r4_to23@int16, r5_bo23@int16, r5_to23@int16,
      r6_bo23@int16, r6_to23@int16, r7_bo23@int16, r7_to23@int16,
      r8_bo23@int16, r8_to23@int16, r9_bo23@int16, r9_to23@int16:
      r2_bo23 = r2_b /\ r2_to23 = r2_t /\ r3_bo23 = r3_b /\ r3_to23 = r3_t /\
      r4_bo23 = r4_b /\ r4_to23 = r4_t /\ r5_bo23 = r5_b /\ r5_to23 = r5_t /\
      r6_bo23 = r6_b /\ r6_to23 = r6_t /\ r7_bo23 = r7_b /\ r7_to23 = r7_t /\
      r8_bo23 = r8_b /\ r8_to23 = r8_t /\ r9_bo23 = r9_b /\ r9_to23 = r9_t
   && r2_bo23 = r2_b /\ r2_to23 = r2_t /\ r3_bo23 = r3_b /\ r3_to23 = r3_t /\
      r4_bo23 = r4_b /\ r4_to23 = r4_t /\ r5_bo23 = r5_b /\ r5_to23 = r5_t /\
      r6_bo23 = r6_b /\ r6_to23 = r6_t /\ r7_bo23 = r7_b /\ r7_to23 = r7_t /\
      r8_bo23 = r8_b /\ r8_to23 = r8_t /\ r9_bo23 = r9_b /\ r9_to23 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4019fc; Value = 0xd88ce179; PC = 0x401444 *)
mov [r10, r11] [L0x4019fc, L0x401a00];
(* smulwb	lr, r10, r2                              #! PC = 0x401448 *)
vpc r2_bW@int32 r2_b; mulj tmp r10 r2_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r2, r10, r2                              #! PC = 0x40144c *)
vpc r2_tW@int32 r2_t; mulj tmp r10 r2_tW; spl r2_w dc tmp 16;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b; vpc r2_t@int16 r2_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401450 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r2, r2, r12, r0                          #! PC = 0x401454 *)
mulj prod r2_b r12_t; add r2_w prod r0;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b;
(* pkhtb	r2, r2, lr, asr #16                       #! PC = 0x401458 *)
mov tmp_b lr_t; mov tmp_t r2_t;
mov r2_b tmp_b; mov r2_t tmp_t;
(* smulwb	lr, r11, r3                              #! PC = 0x40145c *)
vpc r3_bW@int32 r3_b; mulj tmp r11 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r11, r3                              #! PC = 0x401460 *)
vpc r3_tW@int32 r3_t; mulj tmp r11 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401464 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x401468 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x40146c *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov r3_b tmp_b; mov r3_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401a04; Value = 0xd6033ad5; PC = 0x401470 *)
mov [r10, r11] [L0x401a04, L0x401a08];
(* smulwb	lr, r10, r4                              #! PC = 0x401474 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401478 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40147c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401480 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x401484 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov r4_b tmp_b; mov r4_t tmp_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401488 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40148c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401490 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401494 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x401498 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov r5_b tmp_b; mov r5_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401a0c; Value = 0x70813124; PC = 0x40149c *)
mov [r10, r11] [L0x401a0c, L0x401a10];
(* smulwb	lr, r10, r6                              #! PC = 0x4014a0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4014a4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014a8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4014ac *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x4014b0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov r6_b tmp_b; mov r6_t tmp_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4014b4 *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4014b8 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014bc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4014c0 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x4014c4 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov r7_b tmp_b; mov r7_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401a14; Value = 0x942fad91; PC = 0x4014c8 *)
mov [r10, r11] [L0x401a14, L0x401a18];
(* smulwb	lr, r10, r8                              #! PC = 0x4014cc *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x4014d0 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014d4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4014d8 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x4014dc *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov r8_b tmp_b; mov r8_t tmp_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4014e0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4014e4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014e8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4014ec *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x4014f0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov r9_b tmp_b; mov r9_t tmp_t;

assert eqmod r2_b (r2_bo23 *  -513) [Q] /\
       eqmod r2_t (r2_to23 *  -513) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r2_b (r2_bo23 *  -513) [Q] /\
       eqmod r2_t (r2_to23 *  -513) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r2_b, r2_t] /\ [r2_b, r2_t] <s[Q2, Q2];


assert eqmod r3_b (r3_bo23 * -1075) [Q] /\
       eqmod r3_t (r3_to23 * -1075) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r3_b (r3_bo23 * -1075) [Q] /\
       eqmod r3_t (r3_to23 * -1075) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r3_b, r3_t] /\ [r3_b, r3_t] <s[Q2, Q2];


assert eqmod r4_b (r4_bo23 *  -546) [Q] /\
       eqmod r4_t (r4_to23 *  -546) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r4_b (r4_bo23 *  -546) [Q] /\
       eqmod r4_t (r4_to23 *  -546) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r4_b, r4_t] /\ [r4_b, r4_t] <s[Q2, Q2];


assert eqmod r5_b (r5_bo23 *   277) [Q] /\
       eqmod r5_t (r5_to23 *   277) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r5_b (r5_bo23 *   277) [Q] /\
       eqmod r5_t (r5_to23 *   277) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r5_b, r5_t] /\ [r5_b, r5_t] <s[Q2, Q2];


assert eqmod r6_b (r6_bo23 *  1463) [Q] /\
       eqmod r6_t (r6_to23 *  1463) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r6_b (r6_bo23 *  1463) [Q] /\
       eqmod r6_t (r6_to23 *  1463) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r6_b, r6_t] /\ [r6_b, r6_t] <s[Q2, Q2];


assert eqmod r7_b (r7_bo23 *  1093) [Q] /\
       eqmod r7_t (r7_to23 *  1093) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r7_b (r7_bo23 *  1093) [Q] /\
       eqmod r7_t (r7_to23 *  1093) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r7_b, r7_t] /\ [r7_b, r7_t] <s[Q2, Q2];


assert eqmod r8_b (r8_bo23 * -1402) [Q] /\
       eqmod r8_t (r8_to23 * -1402) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r8_b (r8_bo23 * -1402) [Q] /\
       eqmod r8_t (r8_to23 * -1402) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r8_b, r8_t] /\ [r8_b, r8_t] <s[Q2, Q2];


assert eqmod r9_b (r9_bo23 *   443) [Q] /\
       eqmod r9_t (r9_to23 *   443) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r9_b (r9_bo23 *   443) [Q] /\
       eqmod r9_t (r9_to23 *   443) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r9_b, r9_t] /\ [r9_b, r9_t] <s[Q2, Q2];

(* vmov	r0, s6                                     #! PC = 0x4014f4 *)
mov [r0_b, r0_t] [s6_b, s6_t];
(* str.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2d8; PC = 0x4014f8 *)
mov [L0xbefff2d8, L0xbefff2da] [r6_b, r6_t];
(* str.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff318; PC = 0x4014fc *)
mov [L0xbefff318, L0xbefff31a] [r7_b, r7_t];
(* str.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff358; PC = 0x401500 *)
mov [L0xbefff358, L0xbefff35a] [r8_b, r8_t];
(* str.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff398; PC = 0x401504 *)
mov [L0xbefff398, L0xbefff39a] [r9_b, r9_t];
(* str.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff218; PC = 0x401508 *)
mov [L0xbefff218, L0xbefff21a] [r3_b, r3_t];
(* str.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff258; PC = 0x40150c *)
mov [L0xbefff258, L0xbefff25a] [r4_b, r4_t];
(* str.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff298; PC = 0x401510 *)
mov [L0xbefff298, L0xbefff29a] [r5_b, r5_t];
(* str.w	r2, [r0], #4                              #! EA = L0xbefff1d8; PC = 0x401514 *)
mov [L0xbefff1d8, L0xbefff1da] [r2_b, r2_t];
(* vmov	lr, s14                                    #! PC = 0x401518 *)
mov [lr_b, lr_t] [s14_b, s14_t]; mov lr s14;
(* cmp.w	r0, lr                                    #! PC = 0x40151c *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x4012bc <invntt_fast+1552>              #! PC = 0x401520 *)
#bne.w	0x4012bc <invntt_fast+1552>              #! 0x401520 = 0x401520;

(* CUT 40 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo23, r2_to23] * [ -513,  -513])
          [L0xbefff1d8, L0xbefff1da] [Q, Q] /\
    eqmod ([r3_bo23, r3_to23] * [-1075, -1075])
          [L0xbefff218, L0xbefff21a] [Q, Q] /\
    eqmod ([r4_bo23, r4_to23] * [ -546,  -546])
          [L0xbefff258, L0xbefff25a] [Q, Q] /\
    eqmod ([r5_bo23, r5_to23] * [  277,   277])
          [L0xbefff298, L0xbefff29a] [Q, Q] /\
    eqmod ([r6_bo23, r6_to23] * [ 1463,  1463])
          [L0xbefff2d8, L0xbefff2da] [Q, Q] /\
    eqmod ([r7_bo23, r7_to23] * [ 1093,  1093])
          [L0xbefff318, L0xbefff31a] [Q, Q] /\
    eqmod ([r8_bo23, r8_to23] * [-1402, -1402])
          [L0xbefff358, L0xbefff35a] [Q, Q] /\
    eqmod ([r9_bo23, r9_to23] * [  443,   443])
          [L0xbefff398, L0xbefff39a] [Q, Q] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff1d8,L0xbefff1da,L0xbefff218,L0xbefff21a] /\
    [L0xbefff1d8,L0xbefff1da,L0xbefff218,L0xbefff21a]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff258,L0xbefff25a,L0xbefff298,L0xbefff29a] /\
    [L0xbefff258,L0xbefff25a,L0xbefff298,L0xbefff29a]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff2d8,L0xbefff2da,L0xbefff318,L0xbefff31a] /\
    [L0xbefff2d8,L0xbefff2da,L0xbefff318,L0xbefff31a]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff358,L0xbefff35a,L0xbefff398,L0xbefff39a] /\
    [L0xbefff358,L0xbefff35a,L0xbefff398,L0xbefff39a]< [Q2,Q2,Q2,Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1d8,L0xbefff1da,L0xbefff218,L0xbefff21a] /\
    [L0xbefff1d8,L0xbefff1da,L0xbefff218,L0xbefff21a]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff258,L0xbefff25a,L0xbefff298,L0xbefff29a] /\
    [L0xbefff258,L0xbefff25a,L0xbefff298,L0xbefff29a]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2d8,L0xbefff2da,L0xbefff318,L0xbefff31a] /\
    [L0xbefff2d8,L0xbefff2da,L0xbefff318,L0xbefff31a]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff358,L0xbefff35a,L0xbefff398,L0xbefff39a] /\
    [L0xbefff358,L0xbefff35a,L0xbefff398,L0xbefff39a]<s[Q2,Q2,Q2,Q2];

(* vmov	s6, r0                                     #! PC = 0x4012bc *)
mov [s6_b, s6_t] [r0_b, r0_t];
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff1dc; Value = 0x098103f6; PC = 0x4012c0 *)
mov [r2_b, r2_t] [L0xbefff1dc, L0xbefff1de];
(* ldr.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff21c; Value = 0x02dce5c1; PC = 0x4012c4 *)
mov [r3_b, r3_t] [L0xbefff21c, L0xbefff21e];
(* ldr.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff25c; Value = 0xecb5eb69; PC = 0x4012c8 *)
mov [r4_b, r4_t] [L0xbefff25c, L0xbefff25e];
(* ldr.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff29c; Value = 0xfa1c0e4f; PC = 0x4012cc *)
mov [r5_b, r5_t] [L0xbefff29c, L0xbefff29e];
(* ldr.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2dc; Value = 0x090aee84; PC = 0x4012d0 *)
mov [r6_b, r6_t] [L0xbefff2dc, L0xbefff2de];
(* ldr.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff31c; Value = 0xf4bae035; PC = 0x4012d4 *)
mov [r7_b, r7_t] [L0xbefff31c, L0xbefff31e];
(* ldr.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff35c; Value = 0xf88d078f; PC = 0x4012d8 *)
mov [r8_b, r8_t] [L0xbefff35c, L0xbefff35e];
(* ldr.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff39c; Value = 0x0450099e; PC = 0x4012dc *)
mov [r9_b, r9_t] [L0xbefff39c, L0xbefff39e];

ghost r2_bo24@int16, r2_to24@int16, r3_bo24@int16, r3_to24@int16,
      r4_bo24@int16, r4_to24@int16, r5_bo24@int16, r5_to24@int16,
      r6_bo24@int16, r6_to24@int16, r7_bo24@int16, r7_to24@int16,
      r8_bo24@int16, r8_to24@int16, r9_bo24@int16, r9_to24@int16:
      r2_bo24 = r2_b /\ r2_to24 = r2_t /\ r3_bo24 = r3_b /\ r3_to24 = r3_t /\
      r4_bo24 = r4_b /\ r4_to24 = r4_t /\ r5_bo24 = r5_b /\ r5_to24 = r5_t /\
      r6_bo24 = r6_b /\ r6_to24 = r6_t /\ r7_bo24 = r7_b /\ r7_to24 = r7_t /\
      r8_bo24 = r8_b /\ r8_to24 = r8_t /\ r9_bo24 = r9_b /\ r9_to24 = r9_t
   && r2_bo24 = r2_b /\ r2_to24 = r2_t /\ r3_bo24 = r3_b /\ r3_to24 = r3_t /\
      r4_bo24 = r4_b /\ r4_to24 = r4_t /\ r5_bo24 = r5_b /\ r5_to24 = r5_t /\
      r6_bo24 = r6_b /\ r6_to24 = r6_t /\ r7_bo24 = r7_b /\ r7_to24 = r7_t /\
      r8_bo24 = r8_b /\ r8_to24 = r8_t /\ r9_bo24 = r9_b /\ r9_to24 = r9_t;

(* movw	r0, #26632	; 0x6808                        #! PC = 0x4012e0 *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* ldr.w	r10, [r1], #4                             #! EA = L0x401a1c; Value = 0xcf8d92a6; PC = 0x4012e4 *)
mov r10 L0x401a1c;
(* smulwb	lr, r10, r3                              #! PC = 0x4012e8 *)
vpc r3_bW@int32 r3_b; mulj tmp r10 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r10, r3                              #! PC = 0x4012ec *)
vpc r3_tW@int32 r3_t; mulj tmp r10 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4012f0 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4012f4 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x4012f8 *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r3_bo24 *  -630) [Q] /\
       eqmod lr_t (r3_to24 *  -630) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r3_bo24 *  -630) [Q] /\
       eqmod lr_t (r3_to24 *  -630) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r3, r2, lr                               #! PC = 0x4012fc *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401300 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x401304 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x401308 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40130c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401310 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401314 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo24 *  -630) [Q] /\
       eqmod lr_t (r5_to24 *  -630) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo24 *  -630) [Q] /\
       eqmod lr_t (r5_to24 *  -630) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r4, lr                               #! PC = 0x401318 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x40131c *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x401320 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x401324 *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401328 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x40132c *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x401330 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo24 *  -630) [Q] /\
       eqmod lr_t (r7_to24 *  -630) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo24 *  -630) [Q] /\
       eqmod lr_t (r7_to24 *  -630) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r6, lr                               #! PC = 0x401334 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x401338 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x40133c *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x401340 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401344 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401348 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x40134c *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo24 *  -630) [Q] /\
       eqmod lr_t (r9_to24 *  -630) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo24 *  -630) [Q] /\
       eqmod lr_t (r9_to24 *  -630) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r8, lr                               #! PC = 0x401350 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x401354 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

assert [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
    && [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2];


(* CUT 41 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo24, r2_to24] + [r3_bo24, r3_to24] * [ -630,  -630])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo24, r2_to24] - [r3_bo24, r3_to24] * [ -630,  -630])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r4_bo24, r4_to24] + [r5_bo24, r5_to24] * [ -630,  -630])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo24, r4_to24] - [r5_bo24, r5_to24] * [ -630,  -630])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo24, r6_to24] + [r7_bo24, r7_to24] * [ -630,  -630])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo24, r6_to24] - [r7_bo24, r7_to24] * [ -630,  -630])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r8_bo24, r8_to24] + [r9_bo24, r9_to24] * [ -630,  -630])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r8_bo24, r8_to24] - [r9_bo24, r9_to24] * [ -630,  -630])
          [r9_b, r9_t] [Q, Q] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2]
    prove with [cuts [1, 3, 5, 7, 9, 11, 13, 15]];

ghost r2_bo25@int16, r2_to25@int16, r3_bo25@int16, r3_to25@int16,
      r4_bo25@int16, r4_to25@int16, r5_bo25@int16, r5_to25@int16,
      r6_bo25@int16, r6_to25@int16, r7_bo25@int16, r7_to25@int16,
      r8_bo25@int16, r8_to25@int16, r9_bo25@int16, r9_to25@int16:
      r2_bo25 = r2_b /\ r2_to25 = r2_t /\ r3_bo25 = r3_b /\ r3_to25 = r3_t /\
      r4_bo25 = r4_b /\ r4_to25 = r4_t /\ r5_bo25 = r5_b /\ r5_to25 = r5_t /\
      r6_bo25 = r6_b /\ r6_to25 = r6_t /\ r7_bo25 = r7_b /\ r7_to25 = r7_t /\
      r8_bo25 = r8_b /\ r8_to25 = r8_t /\ r9_bo25 = r9_b /\ r9_to25 = r9_t
   && r2_bo25 = r2_b /\ r2_to25 = r2_t /\ r3_bo25 = r3_b /\ r3_to25 = r3_t /\
      r4_bo25 = r4_b /\ r4_to25 = r4_t /\ r5_bo25 = r5_b /\ r5_to25 = r5_t /\
      r6_bo25 = r6_b /\ r6_to25 = r6_t /\ r7_bo25 = r7_b /\ r7_to25 = r7_t /\
      r8_bo25 = r8_b /\ r8_to25 = r8_t /\ r9_bo25 = r9_b /\ r9_to25 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401a20; Value = 0xc2b5f202; PC = 0x401358 *)
mov [r10, r11] [L0x401a20, L0x401a24];
(* smulwb	lr, r10, r4                              #! PC = 0x40135c *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401360 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401364 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401368 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x40136c *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_bo25 *  -797) [Q] /\
       eqmod lr_t (r4_to25 *  -797) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_bo25 *  -797) [Q] /\
       eqmod lr_t (r4_to25 *  -797) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r4, r2, lr                               #! PC = 0x401370 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401374 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401378 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40137c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401380 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401384 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401388 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo25 *  -193) [Q] /\
       eqmod lr_t (r5_to25 *  -193) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo25 *  -193) [Q] /\
       eqmod lr_t (r5_to25 *  -193) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x40138c *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401390 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x401394 *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401398 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40139c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4013a0 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x4013a4 *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo25 *  -797) [Q] /\
       eqmod lr_t (r8_to25 *  -797) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo25 *  -797) [Q] /\
       eqmod lr_t (r8_to25 *  -797) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r6, lr                               #! PC = 0x4013a8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x4013ac *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4013b0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4013b4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013b8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4013bc *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4013c0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo25 *  -193) [Q] /\
       eqmod lr_t (r9_to25 *  -193) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo25 *  -193) [Q] /\
       eqmod lr_t (r9_to25 *  -193) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x4013c4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x4013c8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;

assert [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
       prove with [algebra solver isl] && true;
assume [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
    && [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2];


(* CUT 42 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo25, r2_to25] + [r4_bo25, r4_to25] * [ -797,  -797])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo25, r2_to25] - [r4_bo25, r4_to25] * [ -797,  -797])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r3_bo25, r3_to25] + [r5_bo25, r5_to25] * [ -193,  -193])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo25, r3_to25] - [r5_bo25, r5_to25] * [ -193,  -193])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo25, r6_to25] + [r8_bo25, r8_to25] * [ -797,  -797])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo25, r6_to25] - [r8_bo25, r8_to25] * [ -797,  -797])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r7_bo25, r7_to25] + [r9_bo25, r9_to25] * [ -193,  -193])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r7_bo25, r7_to25] - [r9_bo25, r9_to25] * [ -193,  -193])
          [r9_b, r9_t] [Q, Q] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2];

ghost r2_bo26@int16, r2_to26@int16, r3_bo26@int16, r3_to26@int16,
      r4_bo26@int16, r4_to26@int16, r5_bo26@int16, r5_to26@int16,
      r6_bo26@int16, r6_to26@int16, r7_bo26@int16, r7_to26@int16,
      r8_bo26@int16, r8_to26@int16, r9_bo26@int16, r9_to26@int16:
      r2_bo26 = r2_b /\ r2_to26 = r2_t /\ r3_bo26 = r3_b /\ r3_to26 = r3_t /\
      r4_bo26 = r4_b /\ r4_to26 = r4_t /\ r5_bo26 = r5_b /\ r5_to26 = r5_t /\
      r6_bo26 = r6_b /\ r6_to26 = r6_t /\ r7_bo26 = r7_b /\ r7_to26 = r7_t /\
      r8_bo26 = r8_b /\ r8_to26 = r8_t /\ r9_bo26 = r9_b /\ r9_to26 = r9_t
   && r2_bo26 = r2_b /\ r2_to26 = r2_t /\ r3_bo26 = r3_b /\ r3_to26 = r3_t /\
      r4_bo26 = r4_b /\ r4_to26 = r4_t /\ r5_bo26 = r5_b /\ r5_to26 = r5_t /\
      r6_bo26 = r6_b /\ r6_to26 = r6_t /\ r7_bo26 = r7_b /\ r7_to26 = r7_t /\
      r8_bo26 = r8_b /\ r8_to26 = r8_t /\ r9_bo26 = r9_b /\ r9_to26 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401a28; Value = 0x997e0a00; PC = 0x4013cc *)
mov [r10, r11] [L0x401a28, L0x401a2c];
(* smulwb	lr, r10, r6                              #! PC = 0x4013d0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4013d4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013d8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4013dc *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x4013e0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r6_bo26 * -1333) [Q] /\
       eqmod lr_t (r6_to26 * -1333) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r6_bo26 * -1333) [Q] /\
       eqmod lr_t (r6_to26 * -1333) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r6, r2, lr                               #! PC = 0x4013e4 *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4013e8 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4013ec *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4013f0 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013f4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4013f8 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x4013fc *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo26 *   -56) [Q] /\
       eqmod lr_t (r7_to26 *   -56) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo26 *   -56) [Q] /\
       eqmod lr_t (r7_to26 *   -56) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x401400 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401404 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401a30; Value = 0x53be7b32; PC = 0x401408 *)
mov [r10, r11] [L0x401a30, L0x401a34];
(* smulwb	lr, r10, r8                              #! PC = 0x40140c *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401410 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401414 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x401418 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x40141c *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo26 *  1089) [Q] /\
       eqmod lr_t (r8_to26 *  1089) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo26 *  1089) [Q] /\
       eqmod lr_t (r8_to26 *  1089) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r4, lr                               #! PC = 0x401420 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x401424 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x401428 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x40142c *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401430 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401434 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x401438 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo26 *   283) [Q] /\
       eqmod lr_t (r9_to26 *   283) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo26 *   283) [Q] /\
       eqmod lr_t (r9_to26 *   283) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x40143c *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x401440 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;

assert [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
       [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
       [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
       [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [9*Q2,9*Q2,9*Q2,9*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
       [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
       [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
       [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [9*Q2,9*Q2,9*Q2,9*Q2]
    && [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2] /\
       [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2] /\
       [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2] /\
       [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2];


(* CUT 43 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo26, r2_to26] + [r6_bo26, r6_to26] * [-1333, -1333])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo26, r2_to26] - [r6_bo26, r6_to26] * [-1333, -1333])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r3_bo26, r3_to26] + [r7_bo26, r7_to26] * [  -56,   -56])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo26, r3_to26] - [r7_bo26, r7_to26] * [  -56,   -56])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r4_bo26, r4_to26] + [r8_bo26, r8_to26] * [ 1089,  1089])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo26, r4_to26] - [r8_bo26, r8_to26] * [ 1089,  1089])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r5_bo26, r5_to26] + [r9_bo26, r9_to26] * [  283,   283])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r5_bo26, r5_to26] - [r9_bo26, r9_to26] * [  283,   283])
          [r9_b, r9_t] [Q, Q] /\
    [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
    [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
    [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
    [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [9*Q2,9*Q2,9*Q2,9*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2] /\
    [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2] /\
    [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2] /\
    [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2];



ghost r2_bo27@int16, r2_to27@int16, r3_bo27@int16, r3_to27@int16,
      r4_bo27@int16, r4_to27@int16, r5_bo27@int16, r5_to27@int16,
      r6_bo27@int16, r6_to27@int16, r7_bo27@int16, r7_to27@int16,
      r8_bo27@int16, r8_to27@int16, r9_bo27@int16, r9_to27@int16:
      r2_bo27 = r2_b /\ r2_to27 = r2_t /\ r3_bo27 = r3_b /\ r3_to27 = r3_t /\
      r4_bo27 = r4_b /\ r4_to27 = r4_t /\ r5_bo27 = r5_b /\ r5_to27 = r5_t /\
      r6_bo27 = r6_b /\ r6_to27 = r6_t /\ r7_bo27 = r7_b /\ r7_to27 = r7_t /\
      r8_bo27 = r8_b /\ r8_to27 = r8_t /\ r9_bo27 = r9_b /\ r9_to27 = r9_t
   && r2_bo27 = r2_b /\ r2_to27 = r2_t /\ r3_bo27 = r3_b /\ r3_to27 = r3_t /\
      r4_bo27 = r4_b /\ r4_to27 = r4_t /\ r5_bo27 = r5_b /\ r5_to27 = r5_t /\
      r6_bo27 = r6_b /\ r6_to27 = r6_t /\ r7_bo27 = r7_b /\ r7_to27 = r7_t /\
      r8_bo27 = r8_b /\ r8_to27 = r8_t /\ r9_bo27 = r9_b /\ r9_to27 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401a38; Value = 0xee9ee017; PC = 0x401444 *)
mov [r10, r11] [L0x401a38, L0x401a3c];
(* smulwb	lr, r10, r2                              #! PC = 0x401448 *)
vpc r2_bW@int32 r2_b; mulj tmp r10 r2_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r2, r10, r2                              #! PC = 0x40144c *)
vpc r2_tW@int32 r2_t; mulj tmp r10 r2_tW; spl r2_w dc tmp 16;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b; vpc r2_t@int16 r2_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401450 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r2, r2, r12, r0                          #! PC = 0x401454 *)
mulj prod r2_b r12_t; add r2_w prod r0;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b;
(* pkhtb	r2, r2, lr, asr #16                       #! PC = 0x401458 *)
mov tmp_b lr_t; mov tmp_t r2_t;
mov r2_b tmp_b; mov r2_t tmp_t;
(* smulwb	lr, r11, r3                              #! PC = 0x40145c *)
vpc r3_bW@int32 r3_b; mulj tmp r11 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r11, r3                              #! PC = 0x401460 *)
vpc r3_tW@int32 r3_t; mulj tmp r11 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401464 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x401468 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x40146c *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov r3_b tmp_b; mov r3_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401a40; Value = 0x48d30376; PC = 0x401470 *)
mov [r10, r11] [L0x401a40, L0x401a44];
(* smulwb	lr, r10, r4                              #! PC = 0x401474 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401478 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40147c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401480 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x401484 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov r4_b tmp_b; mov r4_t tmp_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401488 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40148c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401490 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401494 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x401498 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov r5_b tmp_b; mov r5_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401a48; Value = 0x60f88a6c; PC = 0x40149c *)
mov [r10, r11] [L0x401a48, L0x401a4c];
(* smulwb	lr, r10, r6                              #! PC = 0x4014a0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4014a4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014a8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4014ac *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x4014b0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov r6_b tmp_b; mov r6_t tmp_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4014b4 *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4014b8 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014bc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4014c0 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x4014c4 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov r7_b tmp_b; mov r7_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401a50; Value = 0x26d5a0cd; PC = 0x4014c8 *)
mov [r10, r11] [L0x401a50, L0x401a54];
(* smulwb	lr, r10, r8                              #! PC = 0x4014cc *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x4014d0 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014d4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4014d8 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x4014dc *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov r8_b tmp_b; mov r8_t tmp_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4014e0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4014e4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014e8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4014ec *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x4014f0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov r9_b tmp_b; mov r9_t tmp_t;

assert eqmod r2_b (r2_bo27 *  -226) [Q] /\
       eqmod r2_t (r2_to27 *  -226) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r2_b (r2_bo27 *  -226) [Q] /\
       eqmod r2_t (r2_to27 *  -226) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r2_b, r2_t] /\ [r2_b, r2_t] <s[Q2, Q2];


assert eqmod r3_b (r3_bo27 * -1434) [Q] /\
       eqmod r3_t (r3_to27 * -1434) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r3_b (r3_bo27 * -1434) [Q] /\
       eqmod r3_t (r3_to27 * -1434) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r3_b, r3_t] /\ [r3_b, r3_t] <s[Q2, Q2];


assert eqmod r4_b (r4_bo27 *   947) [Q] /\
       eqmod r4_t (r4_to27 *   947) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r4_b (r4_bo27 *   947) [Q] /\
       eqmod r4_t (r4_to27 *   947) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r4_b, r4_t] /\ [r4_b, r4_t] <s[Q2, Q2];


assert eqmod r5_b (r5_bo27 *  -767) [Q] /\
       eqmod r5_t (r5_to27 *  -767) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r5_b (r5_bo27 *  -767) [Q] /\
       eqmod r5_t (r5_to27 *  -767) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r5_b, r5_t] /\ [r5_b, r5_t] <s[Q2, Q2];


assert eqmod r6_b (r6_bo27 *  1261) [Q] /\
       eqmod r6_t (r6_to27 *  1261) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r6_b (r6_bo27 *  1261) [Q] /\
       eqmod r6_t (r6_to27 *  1261) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r6_b, r6_t] /\ [r6_b, r6_t] <s[Q2, Q2];


assert eqmod r7_b (r7_bo27 *  -719) [Q] /\
       eqmod r7_t (r7_to27 *  -719) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r7_b (r7_bo27 *  -719) [Q] /\
       eqmod r7_t (r7_to27 *  -719) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r7_b, r7_t] /\ [r7_b, r7_t] <s[Q2, Q2];


assert eqmod r8_b (r8_bo27 *   505) [Q] /\
       eqmod r8_t (r8_to27 *   505) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r8_b (r8_bo27 *   505) [Q] /\
       eqmod r8_t (r8_to27 *   505) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r8_b, r8_t] /\ [r8_b, r8_t] <s[Q2, Q2];


assert eqmod r9_b (r9_bo27 *  1201) [Q] /\
       eqmod r9_t (r9_to27 *  1201) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r9_b (r9_bo27 *  1201) [Q] /\
       eqmod r9_t (r9_to27 *  1201) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r9_b, r9_t] /\ [r9_b, r9_t] <s[Q2, Q2];

(* vmov	r0, s6                                     #! PC = 0x4014f4 *)
mov [r0_b, r0_t] [s6_b, s6_t];
(* str.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2dc; PC = 0x4014f8 *)
mov [L0xbefff2dc, L0xbefff2de] [r6_b, r6_t];
(* str.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff31c; PC = 0x4014fc *)
mov [L0xbefff31c, L0xbefff31e] [r7_b, r7_t];
(* str.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff35c; PC = 0x401500 *)
mov [L0xbefff35c, L0xbefff35e] [r8_b, r8_t];
(* str.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff39c; PC = 0x401504 *)
mov [L0xbefff39c, L0xbefff39e] [r9_b, r9_t];
(* str.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff21c; PC = 0x401508 *)
mov [L0xbefff21c, L0xbefff21e] [r3_b, r3_t];
(* str.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff25c; PC = 0x40150c *)
mov [L0xbefff25c, L0xbefff25e] [r4_b, r4_t];
(* str.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff29c; PC = 0x401510 *)
mov [L0xbefff29c, L0xbefff29e] [r5_b, r5_t];
(* str.w	r2, [r0], #4                              #! EA = L0xbefff1dc; PC = 0x401514 *)
mov [L0xbefff1dc, L0xbefff1de] [r2_b, r2_t];
(* vmov	lr, s14                                    #! PC = 0x401518 *)
mov [lr_b, lr_t] [s14_b, s14_t]; mov lr s14;
(* cmp.w	r0, lr                                    #! PC = 0x40151c *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x4012bc <invntt_fast+1552>              #! PC = 0x401520 *)
#bne.w	0x4012bc <invntt_fast+1552>              #! 0x401520 = 0x401520;

(* CUT 44 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo27, r2_to27] * [ -226,  -226])
          [L0xbefff1dc, L0xbefff1de] [Q, Q] /\
    eqmod ([r3_bo27, r3_to27] * [-1434, -1434])
          [L0xbefff21c, L0xbefff21e] [Q, Q] /\
    eqmod ([r4_bo27, r4_to27] * [  947,   947])
          [L0xbefff25c, L0xbefff25e] [Q, Q] /\
    eqmod ([r5_bo27, r5_to27] * [ -767,  -767])
          [L0xbefff29c, L0xbefff29e] [Q, Q] /\
    eqmod ([r6_bo27, r6_to27] * [ 1261,  1261])
          [L0xbefff2dc, L0xbefff2de] [Q, Q] /\
    eqmod ([r7_bo27, r7_to27] * [ -719,  -719])
          [L0xbefff31c, L0xbefff31e] [Q, Q] /\
    eqmod ([r8_bo27, r8_to27] * [  505,   505])
          [L0xbefff35c, L0xbefff35e] [Q, Q] /\
    eqmod ([r9_bo27, r9_to27] * [ 1201,  1201])
          [L0xbefff39c, L0xbefff39e] [Q, Q] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff1dc,L0xbefff1de,L0xbefff21c,L0xbefff21e] /\
    [L0xbefff1dc,L0xbefff1de,L0xbefff21c,L0xbefff21e]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff25c,L0xbefff25e,L0xbefff29c,L0xbefff29e] /\
    [L0xbefff25c,L0xbefff25e,L0xbefff29c,L0xbefff29e]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff2dc,L0xbefff2de,L0xbefff31c,L0xbefff31e] /\
    [L0xbefff2dc,L0xbefff2de,L0xbefff31c,L0xbefff31e]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff35c,L0xbefff35e,L0xbefff39c,L0xbefff39e] /\
    [L0xbefff35c,L0xbefff35e,L0xbefff39c,L0xbefff39e]< [Q2,Q2,Q2,Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1dc,L0xbefff1de,L0xbefff21c,L0xbefff21e] /\
    [L0xbefff1dc,L0xbefff1de,L0xbefff21c,L0xbefff21e]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff25c,L0xbefff25e,L0xbefff29c,L0xbefff29e] /\
    [L0xbefff25c,L0xbefff25e,L0xbefff29c,L0xbefff29e]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2dc,L0xbefff2de,L0xbefff31c,L0xbefff31e] /\
    [L0xbefff2dc,L0xbefff2de,L0xbefff31c,L0xbefff31e]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff35c,L0xbefff35e,L0xbefff39c,L0xbefff39e] /\
    [L0xbefff35c,L0xbefff35e,L0xbefff39c,L0xbefff39e]<s[Q2,Q2,Q2,Q2];

(* vmov	s6, r0                                     #! PC = 0x4012bc *)
mov [s6_b, s6_t] [r0_b, r0_t];
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff1e0; Value = 0x0b8f074c; PC = 0x4012c0 *)
mov [r2_b, r2_t] [L0xbefff1e0, L0xbefff1e2];
(* ldr.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff220; Value = 0xf31b0f00; PC = 0x4012c4 *)
mov [r3_b, r3_t] [L0xbefff220, L0xbefff222];
(* ldr.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff260; Value = 0xf7a3fd85; PC = 0x4012c8 *)
mov [r4_b, r4_t] [L0xbefff260, L0xbefff262];
(* ldr.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff2a0; Value = 0x0174fea4; PC = 0x4012cc *)
mov [r5_b, r5_t] [L0xbefff2a0, L0xbefff2a2];
(* ldr.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2e0; Value = 0x0ea40a40; PC = 0x4012d0 *)
mov [r6_b, r6_t] [L0xbefff2e0, L0xbefff2e2];
(* ldr.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff320; Value = 0xf838014b; PC = 0x4012d4 *)
mov [r7_b, r7_t] [L0xbefff320, L0xbefff322];
(* ldr.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff360; Value = 0xfe1f0f2a; PC = 0x4012d8 *)
mov [r8_b, r8_t] [L0xbefff360, L0xbefff362];
(* ldr.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff3a0; Value = 0xf77bfbba; PC = 0x4012dc *)
mov [r9_b, r9_t] [L0xbefff3a0, L0xbefff3a2];

ghost r2_bo28@int16, r2_to28@int16, r3_bo28@int16, r3_to28@int16,
      r4_bo28@int16, r4_to28@int16, r5_bo28@int16, r5_to28@int16,
      r6_bo28@int16, r6_to28@int16, r7_bo28@int16, r7_to28@int16,
      r8_bo28@int16, r8_to28@int16, r9_bo28@int16, r9_to28@int16:
      r2_bo28 = r2_b /\ r2_to28 = r2_t /\ r3_bo28 = r3_b /\ r3_to28 = r3_t /\
      r4_bo28 = r4_b /\ r4_to28 = r4_t /\ r5_bo28 = r5_b /\ r5_to28 = r5_t /\
      r6_bo28 = r6_b /\ r6_to28 = r6_t /\ r7_bo28 = r7_b /\ r7_to28 = r7_t /\
      r8_bo28 = r8_b /\ r8_to28 = r8_t /\ r9_bo28 = r9_b /\ r9_to28 = r9_t
   && r2_bo28 = r2_b /\ r2_to28 = r2_t /\ r3_bo28 = r3_b /\ r3_to28 = r3_t /\
      r4_bo28 = r4_b /\ r4_to28 = r4_t /\ r5_bo28 = r5_b /\ r5_to28 = r5_t /\
      r6_bo28 = r6_b /\ r6_to28 = r6_t /\ r7_bo28 = r7_b /\ r7_to28 = r7_t /\
      r8_bo28 = r8_b /\ r8_to28 = r8_t /\ r9_bo28 = r9_b /\ r9_to28 = r9_t;

(* movw	r0, #26632	; 0x6808                        #! PC = 0x4012e0 *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* ldr.w	r10, [r1], #4                             #! EA = L0x401a58; Value = 0x6c6dd02c; PC = 0x4012e4 *)
mov r10 L0x401a58;
(* smulwb	lr, r10, r3                              #! PC = 0x4012e8 *)
vpc r3_bW@int32 r3_b; mulj tmp r10 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r10, r3                              #! PC = 0x4012ec *)
vpc r3_tW@int32 r3_t; mulj tmp r10 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4012f0 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4012f4 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x4012f8 *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r3_bo28 *  1410) [Q] /\
       eqmod lr_t (r3_to28 *  1410) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r3_bo28 *  1410) [Q] /\
       eqmod lr_t (r3_to28 *  1410) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r3, r2, lr                               #! PC = 0x4012fc *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401300 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x401304 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x401308 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40130c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401310 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401314 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo28 *  1410) [Q] /\
       eqmod lr_t (r5_to28 *  1410) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo28 *  1410) [Q] /\
       eqmod lr_t (r5_to28 *  1410) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r4, lr                               #! PC = 0x401318 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x40131c *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x401320 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x401324 *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401328 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x40132c *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x401330 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo28 *  1410) [Q] /\
       eqmod lr_t (r7_to28 *  1410) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo28 *  1410) [Q] /\
       eqmod lr_t (r7_to28 *  1410) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r6, lr                               #! PC = 0x401334 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x401338 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x40133c *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x401340 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401344 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401348 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x40134c *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo28 *  1410) [Q] /\
       eqmod lr_t (r9_to28 *  1410) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo28 *  1410) [Q] /\
       eqmod lr_t (r9_to28 *  1410) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r8, lr                               #! PC = 0x401350 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x401354 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

assert [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [6*Q2,6*Q2,6*Q2,6*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [6*Q2,6*Q2,6*Q2,6*Q2]
    && [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2];


(* CUT 45 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo28, r2_to28] + [r3_bo28, r3_to28] * [ 1410,  1410])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo28, r2_to28] - [r3_bo28, r3_to28] * [ 1410,  1410])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r4_bo28, r4_to28] + [r5_bo28, r5_to28] * [ 1410,  1410])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo28, r4_to28] - [r5_bo28, r5_to28] * [ 1410,  1410])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo28, r6_to28] + [r7_bo28, r7_to28] * [ 1410,  1410])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo28, r6_to28] - [r7_bo28, r7_to28] * [ 1410,  1410])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r8_bo28, r8_to28] + [r9_bo28, r9_to28] * [ 1410,  1410])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r8_bo28, r8_to28] - [r9_bo28, r9_to28] * [ 1410,  1410])
          [r9_b, r9_t] [Q, Q] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [6*Q2,6*Q2,6*Q2,6*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2]
    prove with [cuts [1, 3, 5, 7, 9, 11, 13, 15]];

ghost r2_bo29@int16, r2_to29@int16, r3_bo29@int16, r3_to29@int16,
      r4_bo29@int16, r4_to29@int16, r5_bo29@int16, r5_to29@int16,
      r6_bo29@int16, r6_to29@int16, r7_bo29@int16, r7_to29@int16,
      r8_bo29@int16, r8_to29@int16, r9_bo29@int16, r9_to29@int16:
      r2_bo29 = r2_b /\ r2_to29 = r2_t /\ r3_bo29 = r3_b /\ r3_to29 = r3_t /\
      r4_bo29 = r4_b /\ r4_to29 = r4_t /\ r5_bo29 = r5_b /\ r5_to29 = r5_t /\
      r6_bo29 = r6_b /\ r6_to29 = r6_t /\ r7_bo29 = r7_b /\ r7_to29 = r7_t /\
      r8_bo29 = r8_b /\ r8_to29 = r8_t /\ r9_bo29 = r9_b /\ r9_to29 = r9_t
   && r2_bo29 = r2_b /\ r2_to29 = r2_t /\ r3_bo29 = r3_b /\ r3_to29 = r3_t /\
      r4_bo29 = r4_b /\ r4_to29 = r4_t /\ r5_bo29 = r5_b /\ r5_to29 = r5_t /\
      r6_bo29 = r6_b /\ r6_to29 = r6_t /\ r7_bo29 = r7_b /\ r7_to29 = r7_t /\
      r8_bo29 = r8_b /\ r8_to29 = r8_t /\ r9_bo29 = r9_b /\ r9_to29 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401a5c; Value = 0x8e7ee28d; PC = 0x401358 *)
mov [r10, r11] [L0x401a5c, L0x401a60];
(* smulwb	lr, r10, r4                              #! PC = 0x40135c *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401360 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401364 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401368 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x40136c *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_bo29 * -1476) [Q] /\
       eqmod lr_t (r4_to29 * -1476) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_bo29 * -1476) [Q] /\
       eqmod lr_t (r4_to29 * -1476) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r4, r2, lr                               #! PC = 0x401370 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401374 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401378 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40137c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401380 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401384 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401388 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo29 * -1339) [Q] /\
       eqmod lr_t (r5_to29 * -1339) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo29 * -1339) [Q] /\
       eqmod lr_t (r5_to29 * -1339) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x40138c *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401390 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x401394 *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401398 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40139c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4013a0 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x4013a4 *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo29 * -1476) [Q] /\
       eqmod lr_t (r8_to29 * -1476) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo29 * -1476) [Q] /\
       eqmod lr_t (r8_to29 * -1476) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r6, lr                               #! PC = 0x4013a8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x4013ac *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4013b0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4013b4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013b8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4013bc *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4013c0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo29 * -1339) [Q] /\
       eqmod lr_t (r9_to29 * -1339) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo29 * -1339) [Q] /\
       eqmod lr_t (r9_to29 * -1339) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x4013c4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x4013c8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;

assert [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
       prove with [algebra solver isl] && true;
assume [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
    && [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2];


(* CUT 46 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo29, r2_to29] + [r4_bo29, r4_to29] * [-1476, -1476])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo29, r2_to29] - [r4_bo29, r4_to29] * [-1476, -1476])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r3_bo29, r3_to29] + [r5_bo29, r5_to29] * [-1339, -1339])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo29, r3_to29] - [r5_bo29, r5_to29] * [-1339, -1339])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo29, r6_to29] + [r8_bo29, r8_to29] * [-1476, -1476])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo29, r6_to29] - [r8_bo29, r8_to29] * [-1476, -1476])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r7_bo29, r7_to29] + [r9_bo29, r9_to29] * [-1339, -1339])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r7_bo29, r7_to29] - [r9_bo29, r9_to29] * [-1339, -1339])
          [r9_b, r9_t] [Q, Q] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2];



ghost r2_bo30@int16, r2_to30@int16, r3_bo30@int16, r3_to30@int16,
      r4_bo30@int16, r4_to30@int16, r5_bo30@int16, r5_to30@int16,
      r6_bo30@int16, r6_to30@int16, r7_bo30@int16, r7_to30@int16,
      r8_bo30@int16, r8_to30@int16, r9_bo30@int16, r9_to30@int16:
      r2_bo30 = r2_b /\ r2_to30 = r2_t /\ r3_bo30 = r3_b /\ r3_to30 = r3_t /\
      r4_bo30 = r4_b /\ r4_to30 = r4_t /\ r5_bo30 = r5_b /\ r5_to30 = r5_t /\
      r6_bo30 = r6_b /\ r6_to30 = r6_t /\ r7_bo30 = r7_b /\ r7_to30 = r7_t /\
      r8_bo30 = r8_b /\ r8_to30 = r8_t /\ r9_bo30 = r9_b /\ r9_to30 = r9_t
   && r2_bo30 = r2_b /\ r2_to30 = r2_t /\ r3_bo30 = r3_b /\ r3_to30 = r3_t /\
      r4_bo30 = r4_b /\ r4_to30 = r4_t /\ r5_bo30 = r5_b /\ r5_to30 = r5_t /\
      r6_bo30 = r6_b /\ r6_to30 = r6_t /\ r7_bo30 = r7_b /\ r7_to30 = r7_t /\
      r8_bo30 = r8_b /\ r8_to30 = r8_t /\ r9_bo30 = r9_b /\ r9_to30 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401a64; Value = 0x61f876bc; PC = 0x4013cc *)
mov [r10, r11] [L0x401a64, L0x401a68];
(* smulwb	lr, r10, r6                              #! PC = 0x4013d0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4013d4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013d8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4013dc *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x4013e0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r6_bo30 *  1274) [Q] /\
       eqmod lr_t (r6_to30 *  1274) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r6_bo30 *  1274) [Q] /\
       eqmod lr_t (r6_to30 *  1274) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r6, r2, lr                               #! PC = 0x4013e4 *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4013e8 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4013ec *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4013f0 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013f4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4013f8 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x4013fc *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo30 *  1025) [Q] /\
       eqmod lr_t (r7_to30 *  1025) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo30 *  1025) [Q] /\
       eqmod lr_t (r7_to30 *  1025) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x401400 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401404 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401a6c; Value = 0x50e615b0; PC = 0x401408 *)
mov [r10, r11] [L0x401a6c, L0x401a70];
(* smulwb	lr, r10, r8                              #! PC = 0x40140c *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401410 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401414 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x401418 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x40141c *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo30 *  1052) [Q] /\
       eqmod lr_t (r8_to30 *  1052) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo30 *  1052) [Q] /\
       eqmod lr_t (r8_to30 *  1052) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];


(* usub16	r8, r4, lr                               #! PC = 0x401420 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x401424 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x401428 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x40142c *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401430 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401434 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x401438 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo30 * -1197) [Q] /\
       eqmod lr_t (r9_to30 * -1197) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo30 * -1197) [Q] /\
       eqmod lr_t (r9_to30 * -1197) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x40143c *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x401440 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;

assert [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
    && [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2];


(* CUT 47 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo30, r2_to30] + [r6_bo30, r6_to30] * [ 1274,  1274])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo30, r2_to30] - [r6_bo30, r6_to30] * [ 1274,  1274])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r3_bo30, r3_to30] + [r7_bo30, r7_to30] * [ 1025,  1025])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo30, r3_to30] - [r7_bo30, r7_to30] * [ 1025,  1025])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r4_bo30, r4_to30] + [r8_bo30, r8_to30] * [ 1052,  1052])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo30, r4_to30] - [r8_bo30, r8_to30] * [ 1052,  1052])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r5_bo30, r5_to30] + [r9_bo30, r9_to30] * [-1197, -1197])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r5_bo30, r5_to30] - [r9_bo30, r9_to30] * [-1197, -1197])
          [r9_b, r9_t] [Q, Q] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2];



ghost r2_bo31@int16, r2_to31@int16, r3_bo31@int16, r3_to31@int16,
      r4_bo31@int16, r4_to31@int16, r5_bo31@int16, r5_to31@int16,
      r6_bo31@int16, r6_to31@int16, r7_bo31@int16, r7_to31@int16,
      r8_bo31@int16, r8_to31@int16, r9_bo31@int16, r9_to31@int16:
      r2_bo31 = r2_b /\ r2_to31 = r2_t /\ r3_bo31 = r3_b /\ r3_to31 = r3_t /\
      r4_bo31 = r4_b /\ r4_to31 = r4_t /\ r5_bo31 = r5_b /\ r5_to31 = r5_t /\
      r6_bo31 = r6_b /\ r6_to31 = r6_t /\ r7_bo31 = r7_b /\ r7_to31 = r7_t /\
      r8_bo31 = r8_b /\ r8_to31 = r8_t /\ r9_bo31 = r9_b /\ r9_to31 = r9_t
   && r2_bo31 = r2_b /\ r2_to31 = r2_t /\ r3_bo31 = r3_b /\ r3_to31 = r3_t /\
      r4_bo31 = r4_b /\ r4_to31 = r4_t /\ r5_bo31 = r5_b /\ r5_to31 = r5_t /\
      r6_bo31 = r6_b /\ r6_to31 = r6_t /\ r7_bo31 = r7_b /\ r7_to31 = r7_t /\
      r8_bo31 = r8_b /\ r8_to31 = r8_t /\ r9_bo31 = r9_b /\ r9_to31 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401a74; Value = 0x3b3685a7; PC = 0x401444 *)
mov [r10, r11] [L0x401a74, L0x401a78];
(* smulwb	lr, r10, r2                              #! PC = 0x401448 *)
vpc r2_bW@int32 r2_b; mulj tmp r10 r2_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r2, r10, r2                              #! PC = 0x40144c *)
vpc r2_tW@int32 r2_t; mulj tmp r10 r2_tW; spl r2_w dc tmp 16;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b; vpc r2_t@int16 r2_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401450 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r2, r2, r12, r0                          #! PC = 0x401454 *)
mulj prod r2_b r12_t; add r2_w prod r0;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b;
(* pkhtb	r2, r2, lr, asr #16                       #! PC = 0x401458 *)
mov tmp_b lr_t; mov tmp_t r2_t;
mov r2_b tmp_b; mov r2_t tmp_t;
(* smulwb	lr, r11, r3                              #! PC = 0x40145c *)
vpc r3_bW@int32 r3_b; mulj tmp r11 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r11, r3                              #! PC = 0x401460 *)
vpc r3_tW@int32 r3_t; mulj tmp r11 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401464 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x401468 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x40146c *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov r3_b tmp_b; mov r3_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401a7c; Value = 0x4084e216; PC = 0x401470 *)
mov [r10, r11] [L0x401a7c, L0x401a80];
(* smulwb	lr, r10, r4                              #! PC = 0x401474 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401478 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40147c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401480 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x401484 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov r4_b tmp_b; mov r4_t tmp_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401488 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40148c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401490 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401494 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x401498 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov r5_b tmp_b; mov r5_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401a84; Value = 0x14c35370; PC = 0x40149c *)
mov [r10, r11] [L0x401a84, L0x401a88];
(* smulwb	lr, r10, r6                              #! PC = 0x4014a0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4014a4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014a8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4014ac *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x4014b0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov r6_b tmp_b; mov r6_t tmp_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4014b4 *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4014b8 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014bc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4014c0 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x4014c4 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov r7_b tmp_b; mov r7_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401a8c; Value = 0x3e850976; PC = 0x4014c8 *)
mov [r10, r11] [L0x401a8c, L0x401a90];
(* smulwb	lr, r10, r8                              #! PC = 0x4014cc *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x4014d0 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014d4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4014d8 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x4014dc *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov r8_b tmp_b; mov r8_t tmp_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4014e0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4014e4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014e8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4014ec *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x4014f0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov r9_b tmp_b; mov r9_t tmp_t;

assert eqmod r2_b (r2_bo31 *   770) [Q] /\
       eqmod r2_t (r2_to31 *   770) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r2_b (r2_bo31 *   770) [Q] /\
       eqmod r2_t (r2_to31 *   770) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r2_b, r2_t] /\ [r2_b, r2_t] <s[Q2, Q2];


assert eqmod r3_b (r3_bo31 *  -476) [Q] /\
       eqmod r3_t (r3_to31 *  -476) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r3_b (r3_bo31 *  -476) [Q] /\
       eqmod r3_t (r3_to31 *  -476) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r3_b, r3_t] /\ [r3_b, r3_t] <s[Q2, Q2];


assert eqmod r4_b (r4_bo31 *   839) [Q] /\
       eqmod r4_t (r4_to31 *   839) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r4_b (r4_bo31 *   839) [Q] /\
       eqmod r4_t (r4_to31 *   839) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r4_b, r4_t] /\ [r4_b, r4_t] <s[Q2, Q2];


assert eqmod r5_b (r5_bo31 *   934) [Q] /\
       eqmod r5_t (r5_to31 *   934) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r5_b (r5_bo31 *   934) [Q] /\
       eqmod r5_t (r5_to31 *   934) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r5_b, r5_t] /\ [r5_b, r5_t] <s[Q2, Q2];


assert eqmod r6_b (r6_bo31 *   270) [Q] /\
       eqmod r6_t (r6_to31 *   270) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r6_b (r6_bo31 *   270) [Q] /\
       eqmod r6_t (r6_to31 *   270) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r6_b, r6_t] /\ [r6_b, r6_t] <s[Q2, Q2];


assert eqmod r7_b (r7_bo31 *   741) [Q] /\
       eqmod r7_t (r7_to31 *   741) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r7_b (r7_bo31 *   741) [Q] /\
       eqmod r7_t (r7_to31 *   741) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r7_b, r7_t] /\ [r7_b, r7_t] <s[Q2, Q2];


assert eqmod r8_b (r8_bo31 *   813) [Q] /\
       eqmod r8_t (r8_to31 *   813) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r8_b (r8_bo31 *   813) [Q] /\
       eqmod r8_t (r8_to31 *   813) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r8_b, r8_t] /\ [r8_b, r8_t] <s[Q2, Q2];


assert eqmod r9_b (r9_bo31 *  -321) [Q] /\
       eqmod r9_t (r9_to31 *  -321) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r9_b (r9_bo31 *  -321) [Q] /\
       eqmod r9_t (r9_to31 *  -321) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r9_b, r9_t] /\ [r9_b, r9_t] <s[Q2, Q2];

(* vmov	r0, s6                                     #! PC = 0x4014f4 *)
mov [r0_b, r0_t] [s6_b, s6_t];
(* str.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2e0; PC = 0x4014f8 *)
mov [L0xbefff2e0, L0xbefff2e2] [r6_b, r6_t];
(* str.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff320; PC = 0x4014fc *)
mov [L0xbefff320, L0xbefff322] [r7_b, r7_t];
(* str.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff360; PC = 0x401500 *)
mov [L0xbefff360, L0xbefff362] [r8_b, r8_t];
(* str.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff3a0; PC = 0x401504 *)
mov [L0xbefff3a0, L0xbefff3a2] [r9_b, r9_t];
(* str.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff220; PC = 0x401508 *)
mov [L0xbefff220, L0xbefff222] [r3_b, r3_t];
(* str.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff260; PC = 0x40150c *)
mov [L0xbefff260, L0xbefff262] [r4_b, r4_t];
(* str.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff2a0; PC = 0x401510 *)
mov [L0xbefff2a0, L0xbefff2a2] [r5_b, r5_t];
(* str.w	r2, [r0], #4                              #! EA = L0xbefff1e0; PC = 0x401514 *)
mov [L0xbefff1e0, L0xbefff1e2] [r2_b, r2_t];
(* vmov	lr, s14                                    #! PC = 0x401518 *)
mov [lr_b, lr_t] [s14_b, s14_t]; mov lr s14;
(* cmp.w	r0, lr                                    #! PC = 0x40151c *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x4012bc <invntt_fast+1552>              #! PC = 0x401520 *)
#bne.w	0x4012bc <invntt_fast+1552>              #! 0x401520 = 0x401520;

(* CUT 48 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo31, r2_to31] * [  770,   770])
          [L0xbefff1e0, L0xbefff1e2] [Q, Q] /\
    eqmod ([r3_bo31, r3_to31] * [ -476,  -476])
          [L0xbefff220, L0xbefff222] [Q, Q] /\
    eqmod ([r4_bo31, r4_to31] * [  839,   839])
          [L0xbefff260, L0xbefff262] [Q, Q] /\
    eqmod ([r5_bo31, r5_to31] * [  934,   934])
          [L0xbefff2a0, L0xbefff2a2] [Q, Q] /\
    eqmod ([r6_bo31, r6_to31] * [  270,   270])
          [L0xbefff2e0, L0xbefff2e2] [Q, Q] /\
    eqmod ([r7_bo31, r7_to31] * [  741,   741])
          [L0xbefff320, L0xbefff322] [Q, Q] /\
    eqmod ([r8_bo31, r8_to31] * [  813,   813])
          [L0xbefff360, L0xbefff362] [Q, Q] /\
    eqmod ([r9_bo31, r9_to31] * [ -321,  -321])
          [L0xbefff3a0, L0xbefff3a2] [Q, Q] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff1e0,L0xbefff1e2,L0xbefff220,L0xbefff222] /\
    [L0xbefff1e0,L0xbefff1e2,L0xbefff220,L0xbefff222]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff260,L0xbefff262,L0xbefff2a0,L0xbefff2a2] /\
    [L0xbefff260,L0xbefff262,L0xbefff2a0,L0xbefff2a2]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff2e0,L0xbefff2e2,L0xbefff320,L0xbefff322] /\
    [L0xbefff2e0,L0xbefff2e2,L0xbefff320,L0xbefff322]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff360,L0xbefff362,L0xbefff3a0,L0xbefff3a2] /\
    [L0xbefff360,L0xbefff362,L0xbefff3a0,L0xbefff3a2]< [Q2,Q2,Q2,Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1e0,L0xbefff1e2,L0xbefff220,L0xbefff222] /\
    [L0xbefff1e0,L0xbefff1e2,L0xbefff220,L0xbefff222]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff260,L0xbefff262,L0xbefff2a0,L0xbefff2a2] /\
    [L0xbefff260,L0xbefff262,L0xbefff2a0,L0xbefff2a2]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2e0,L0xbefff2e2,L0xbefff320,L0xbefff322] /\
    [L0xbefff2e0,L0xbefff2e2,L0xbefff320,L0xbefff322]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff360,L0xbefff362,L0xbefff3a0,L0xbefff3a2] /\
    [L0xbefff360,L0xbefff362,L0xbefff3a0,L0xbefff3a2]<s[Q2,Q2,Q2,Q2];

(* vmov	s6, r0                                     #! PC = 0x4012bc *)
mov [s6_b, s6_t] [r0_b, r0_t];
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff1e4; Value = 0x51c09b70; PC = 0x4012c0 *)
mov [r2_b, r2_t] [L0xbefff1e4, L0xbefff1e6];
(* ldr.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff224; Value = 0x18b0e490; PC = 0x4012c4 *)
mov [r3_b, r3_t] [L0xbefff224, L0xbefff226];
(* ldr.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff264; Value = 0xeb1006a0; PC = 0x4012c8 *)
mov [r4_b, r4_t] [L0xbefff264, L0xbefff266];
(* ldr.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff2a4; Value = 0xa9d0c940; PC = 0x4012cc *)
mov [r5_b, r5_t] [L0xbefff2a4, L0xbefff2a6];
(* ldr.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2e4; Value = 0xedf05c60; PC = 0x4012d0 *)
mov [r6_b, r6_t] [L0xbefff2e4, L0xbefff2e6];
(* ldr.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff324; Value = 0x41900df0; PC = 0x4012d4 *)
mov [r7_b, r7_t] [L0xbefff324, L0xbefff326];
(* ldr.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff364; Value = 0x9d80f360; PC = 0x4012d8 *)
mov [r8_b, r8_t] [L0xbefff364, L0xbefff366];
(* ldr.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff3a4; Value = 0xb0f00640; PC = 0x4012dc *)
mov [r9_b, r9_t] [L0xbefff3a4, L0xbefff3a6];

ghost r2_bo32@int16, r2_to32@int16, r3_bo32@int16, r3_to32@int16,
      r4_bo32@int16, r4_to32@int16, r5_bo32@int16, r5_to32@int16,
      r6_bo32@int16, r6_to32@int16, r7_bo32@int16, r7_to32@int16,
      r8_bo32@int16, r8_to32@int16, r9_bo32@int16, r9_to32@int16:
      r2_bo32 = r2_b /\ r2_to32 = r2_t /\ r3_bo32 = r3_b /\ r3_to32 = r3_t /\
      r4_bo32 = r4_b /\ r4_to32 = r4_t /\ r5_bo32 = r5_b /\ r5_to32 = r5_t /\
      r6_bo32 = r6_b /\ r6_to32 = r6_t /\ r7_bo32 = r7_b /\ r7_to32 = r7_t /\
      r8_bo32 = r8_b /\ r8_to32 = r8_t /\ r9_bo32 = r9_b /\ r9_to32 = r9_t
   && r2_bo32 = r2_b /\ r2_to32 = r2_t /\ r3_bo32 = r3_b /\ r3_to32 = r3_t /\
      r4_bo32 = r4_b /\ r4_to32 = r4_t /\ r5_bo32 = r5_b /\ r5_to32 = r5_t /\
      r6_bo32 = r6_b /\ r6_to32 = r6_t /\ r7_bo32 = r7_b /\ r7_to32 = r7_t /\
      r8_bo32 = r8_b /\ r8_to32 = r8_t /\ r9_bo32 = r9_b /\ r9_to32 = r9_t;

(* movw	r0, #26632	; 0x6808                        #! PC = 0x4012e0 *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* ldr.w	r10, [r1], #4                             #! EA = L0x401a94; Value = 0x7b0a3a4b; PC = 0x4012e4 *)
mov r10 L0x401a94;
(* smulwb	lr, r10, r3                              #! PC = 0x4012e8 *)
vpc r3_bW@int32 r3_b; mulj tmp r10 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r10, r3                              #! PC = 0x4012ec *)
vpc r3_tW@int32 r3_t; mulj tmp r10 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4012f0 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4012f4 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x4012f8 *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r3_bo32 *  1600) [Q] /\
       eqmod lr_t (r3_to32 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r3_bo32 *  1600) [Q] /\
       eqmod lr_t (r3_to32 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r3, r2, lr                               #! PC = 0x4012fc *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401300 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x401304 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x401308 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40130c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401310 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401314 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo32 *  1600) [Q] /\
       eqmod lr_t (r5_to32 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo32 *  1600) [Q] /\
       eqmod lr_t (r5_to32 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r4, lr                               #! PC = 0x401318 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x40131c *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x401320 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x401324 *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401328 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x40132c *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x401330 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo32 *  1600) [Q] /\
       eqmod lr_t (r7_to32 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo32 *  1600) [Q] /\
       eqmod lr_t (r7_to32 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r6, lr                               #! PC = 0x401334 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x401338 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x40133c *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x401340 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401344 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401348 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x40134c *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo32 *  1600) [Q] /\
       eqmod lr_t (r9_to32 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo32 *  1600) [Q] /\
       eqmod lr_t (r9_to32 *  1600) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r8, lr                               #! PC = 0x401350 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x401354 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

assert [17*NQ2,17*NQ2,17*NQ2,17*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [17*Q2,17*Q2,17*Q2,17*Q2] /\
       [17*NQ2,17*NQ2,17*NQ2,17*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [17*Q2,17*Q2,17*Q2,17*Q2] /\
       [17*NQ2,17*NQ2,17*NQ2,17*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [17*Q2,17*Q2,17*Q2,17*Q2] /\
       [17*NQ2,17*NQ2,17*NQ2,17*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [17*Q2,17*Q2,17*Q2,17*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [17*NQ2,17*NQ2,17*NQ2,17*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [17*Q2,17*Q2,17*Q2,17*Q2] /\
       [17*NQ2,17*NQ2,17*NQ2,17*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [17*Q2,17*Q2,17*Q2,17*Q2] /\
       [17*NQ2,17*NQ2,17*NQ2,17*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [17*Q2,17*Q2,17*Q2,17*Q2] /\
       [17*NQ2,17*NQ2,17*NQ2,17*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [17*Q2,17*Q2,17*Q2,17*Q2]
    && [17@16*NQ2,17@16*NQ2,17@16*NQ2,17@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[17@16*Q2,17@16*Q2,17@16*Q2,17@16*Q2] /\
       [17@16*NQ2,17@16*NQ2,17@16*NQ2,17@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[17@16*Q2,17@16*Q2,17@16*Q2,17@16*Q2] /\
       [17@16*NQ2,17@16*NQ2,17@16*NQ2,17@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[17@16*Q2,17@16*Q2,17@16*Q2,17@16*Q2] /\
       [17@16*NQ2,17@16*NQ2,17@16*NQ2,17@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[17@16*Q2,17@16*Q2,17@16*Q2,17@16*Q2];


(* CUT 49 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo32, r2_to32] + [r3_bo32, r3_to32] * [ 1600,  1600])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo32, r2_to32] - [r3_bo32, r3_to32] * [ 1600,  1600])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r4_bo32, r4_to32] + [r5_bo32, r5_to32] * [ 1600,  1600])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo32, r4_to32] - [r5_bo32, r5_to32] * [ 1600,  1600])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo32, r6_to32] + [r7_bo32, r7_to32] * [ 1600,  1600])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo32, r6_to32] - [r7_bo32, r7_to32] * [ 1600,  1600])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r8_bo32, r8_to32] + [r9_bo32, r9_to32] * [ 1600,  1600])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r8_bo32, r8_to32] - [r9_bo32, r9_to32] * [ 1600,  1600])
          [r9_b, r9_t] [Q, Q] /\
    [17*NQ2,17*NQ2,17*NQ2,17*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [17*Q2,17*Q2,17*Q2,17*Q2] /\
    [17*NQ2,17*NQ2,17*NQ2,17*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [17*Q2,17*Q2,17*Q2,17*Q2] /\
    [17*NQ2,17*NQ2,17*NQ2,17*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [17*Q2,17*Q2,17*Q2,17*Q2] /\
    [17*NQ2,17*NQ2,17*NQ2,17*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [17*Q2,17*Q2,17*Q2,17*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [17@16*NQ2,17@16*NQ2,17@16*NQ2,17@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[17@16*Q2,17@16*Q2,17@16*Q2,17@16*Q2] /\
    [17@16*NQ2,17@16*NQ2,17@16*NQ2,17@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[17@16*Q2,17@16*Q2,17@16*Q2,17@16*Q2] /\
    [17@16*NQ2,17@16*NQ2,17@16*NQ2,17@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[17@16*Q2,17@16*Q2,17@16*Q2,17@16*Q2] /\
    [17@16*NQ2,17@16*NQ2,17@16*NQ2,17@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[17@16*Q2,17@16*Q2,17@16*Q2,17@16*Q2]
    prove with [cuts [1, 3, 5, 7, 9, 11, 13, 15]];



ghost r2_bo33@int16, r2_to33@int16, r3_bo33@int16, r3_to33@int16,
      r4_bo33@int16, r4_to33@int16, r5_bo33@int16, r5_to33@int16,
      r6_bo33@int16, r6_to33@int16, r7_bo33@int16, r7_to33@int16,
      r8_bo33@int16, r8_to33@int16, r9_bo33@int16, r9_to33@int16:
      r2_bo33 = r2_b /\ r2_to33 = r2_t /\ r3_bo33 = r3_b /\ r3_to33 = r3_t /\
      r4_bo33 = r4_b /\ r4_to33 = r4_t /\ r5_bo33 = r5_b /\ r5_to33 = r5_t /\
      r6_bo33 = r6_b /\ r6_to33 = r6_t /\ r7_bo33 = r7_b /\ r7_to33 = r7_t /\
      r8_bo33 = r8_b /\ r8_to33 = r8_t /\ r9_bo33 = r9_b /\ r9_to33 = r9_t
   && r2_bo33 = r2_b /\ r2_to33 = r2_t /\ r3_bo33 = r3_b /\ r3_to33 = r3_t /\
      r4_bo33 = r4_b /\ r4_to33 = r4_t /\ r5_bo33 = r5_b /\ r5_to33 = r5_t /\
      r6_bo33 = r6_b /\ r6_to33 = r6_t /\ r7_bo33 = r7_b /\ r7_to33 = r7_t /\
      r8_bo33 = r8_b /\ r8_to33 = r8_t /\ r9_bo33 = r9_b /\ r9_to33 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401a98; Value = 0x031374a9; PC = 0x401358 *)
mov [r10, r11] [L0x401a98, L0x401a9c];
(* smulwb	lr, r10, r4                              #! PC = 0x40135c *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401360 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401364 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401368 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x40136c *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_bo33 *    40) [Q] /\
       eqmod lr_t (r4_to33 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_bo33 *    40) [Q] /\
       eqmod lr_t (r4_to33 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r4, r2, lr                               #! PC = 0x401370 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401374 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401378 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40137c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401380 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401384 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401388 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo33 *   749) [Q] /\
       eqmod lr_t (r5_to33 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo33 *   749) [Q] /\
       eqmod lr_t (r5_to33 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x40138c *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401390 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x401394 *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401398 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40139c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4013a0 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x4013a4 *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo33 *    40) [Q] /\
       eqmod lr_t (r8_to33 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo33 *    40) [Q] /\
       eqmod lr_t (r8_to33 *    40) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r6, lr                               #! PC = 0x4013a8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x4013ac *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4013b0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4013b4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013b8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4013bc *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4013c0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo33 *   749) [Q] /\
       eqmod lr_t (r9_to33 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo33 *   749) [Q] /\
       eqmod lr_t (r9_to33 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x4013c4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x4013c8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;

assert [18*NQ2,18*NQ2,18*NQ2,18*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [18*Q2,18*Q2,18*Q2,18*Q2] /\
       [18*NQ2,18*NQ2,18*NQ2,18*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [18*Q2,18*Q2,18*Q2,18*Q2] /\
       [18*NQ2,18*NQ2,18*NQ2,18*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [18*Q2,18*Q2,18*Q2,18*Q2] /\
       [18*NQ2,18*NQ2,18*NQ2,18*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [18*Q2,18*Q2,18*Q2,18*Q2]
       prove with [algebra solver isl] && true;
assume [18*NQ2,18*NQ2,18*NQ2,18*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [18*Q2,18*Q2,18*Q2,18*Q2] /\
       [18*NQ2,18*NQ2,18*NQ2,18*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [18*Q2,18*Q2,18*Q2,18*Q2] /\
       [18*NQ2,18*NQ2,18*NQ2,18*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [18*Q2,18*Q2,18*Q2,18*Q2] /\
       [18*NQ2,18*NQ2,18*NQ2,18*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [18*Q2,18*Q2,18*Q2,18*Q2]
    && [18@16*NQ2,18@16*NQ2,18@16*NQ2,18@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[18@16*Q2,18@16*Q2,18@16*Q2,18@16*Q2] /\
       [18@16*NQ2,18@16*NQ2,18@16*NQ2,18@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[18@16*Q2,18@16*Q2,18@16*Q2,18@16*Q2] /\
       [18@16*NQ2,18@16*NQ2,18@16*NQ2,18@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[18@16*Q2,18@16*Q2,18@16*Q2,18@16*Q2] /\
       [18@16*NQ2,18@16*NQ2,18@16*NQ2,18@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[18@16*Q2,18@16*Q2,18@16*Q2,18@16*Q2];


(* CUT 50 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo33, r2_to33] + [r4_bo33, r4_to33] * [   40,    40])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo33, r2_to33] - [r4_bo33, r4_to33] * [   40,    40])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r3_bo33, r3_to33] + [r5_bo33, r5_to33] * [  749,   749])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo33, r3_to33] - [r5_bo33, r5_to33] * [  749,   749])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo33, r6_to33] + [r8_bo33, r8_to33] * [   40,    40])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo33, r6_to33] - [r8_bo33, r8_to33] * [   40,    40])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r7_bo33, r7_to33] + [r9_bo33, r9_to33] * [  749,   749])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r7_bo33, r7_to33] - [r9_bo33, r9_to33] * [  749,   749])
          [r9_b, r9_t] [Q, Q] /\
    [18*NQ2,18*NQ2,18*NQ2,18*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [18*Q2,18*Q2,18*Q2,18*Q2] /\
    [18*NQ2,18*NQ2,18*NQ2,18*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [18*Q2,18*Q2,18*Q2,18*Q2] /\
    [18*NQ2,18*NQ2,18*NQ2,18*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [18*Q2,18*Q2,18*Q2,18*Q2] /\
    [18*NQ2,18*NQ2,18*NQ2,18*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [18*Q2,18*Q2,18*Q2,18*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [18@16*NQ2,18@16*NQ2,18@16*NQ2,18@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[18@16*Q2,18@16*Q2,18@16*Q2,18@16*Q2] /\
    [18@16*NQ2,18@16*NQ2,18@16*NQ2,18@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[18@16*Q2,18@16*Q2,18@16*Q2,18@16*Q2] /\
    [18@16*NQ2,18@16*NQ2,18@16*NQ2,18@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[18@16*Q2,18@16*Q2,18@16*Q2,18@16*Q2] /\
    [18@16*NQ2,18@16*NQ2,18@16*NQ2,18@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[18@16*Q2,18@16*Q2,18@16*Q2,18@16*Q2];



ghost r2_bo34@int16, r2_to34@int16, r3_bo34@int16, r3_to34@int16,
      r4_bo34@int16, r4_to34@int16, r5_bo34@int16, r5_to34@int16,
      r6_bo34@int16, r6_to34@int16, r7_bo34@int16, r7_to34@int16,
      r8_bo34@int16, r8_to34@int16, r9_bo34@int16, r9_to34@int16:
      r2_bo34 = r2_b /\ r2_to34 = r2_t /\ r3_bo34 = r3_b /\ r3_to34 = r3_t /\
      r4_bo34 = r4_b /\ r4_to34 = r4_t /\ r5_bo34 = r5_b /\ r5_to34 = r5_t /\
      r6_bo34 = r6_b /\ r6_to34 = r6_t /\ r7_bo34 = r7_b /\ r7_to34 = r7_t /\
      r8_bo34 = r8_b /\ r8_to34 = r8_t /\ r9_bo34 = r9_b /\ r9_to34 = r9_t
   && r2_bo34 = r2_b /\ r2_to34 = r2_t /\ r3_bo34 = r3_b /\ r3_to34 = r3_t /\
      r4_bo34 = r4_b /\ r4_to34 = r4_t /\ r5_bo34 = r5_b /\ r5_to34 = r5_t /\
      r6_bo34 = r6_b /\ r6_to34 = r6_t /\ r7_bo34 = r7_b /\ r7_to34 = r7_t /\
      r8_bo34 = r8_b /\ r8_to34 = r8_t /\ r9_bo34 = r9_b /\ r9_to34 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401aa0; Value = 0xbec9f078; PC = 0x4013cc *)
mov [r10, r11] [L0x401aa0, L0x401aa4];
(* smulwb	lr, r10, r6                              #! PC = 0x4013d0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4013d4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013d8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4013dc *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x4013e0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r6_bo34 *  -848) [Q] /\
       eqmod lr_t (r6_to34 *  -848) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r6_bo34 *  -848) [Q] /\
       eqmod lr_t (r6_to34 *  -848) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r6, r2, lr                               #! PC = 0x4013e4 *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4013e8 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4013ec *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4013f0 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013f4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4013f8 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x4013fc *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo34 *  -630) [Q] /\
       eqmod lr_t (r7_to34 *  -630) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo34 *  -630) [Q] /\
       eqmod lr_t (r7_to34 *  -630) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x401400 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401404 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401aa8; Value = 0x6e1ee9ef; PC = 0x401408 *)
mov [r10, r11] [L0x401aa8, L0x401aac];
(* smulwb	lr, r10, r8                              #! PC = 0x40140c *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401410 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401414 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x401418 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x40141c *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo34 *  1432) [Q] /\
       eqmod lr_t (r8_to34 *  1432) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo34 *  1432) [Q] /\
       eqmod lr_t (r8_to34 *  1432) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r4, lr                               #! PC = 0x401420 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x401424 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x401428 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x40142c *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401430 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401434 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x401438 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo34 *   687) [Q] /\
       eqmod lr_t (r9_to34 *   687) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo34 *   687) [Q] /\
       eqmod lr_t (r9_to34 *   687) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x40143c *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x401440 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;

assert [19*NQ2,19*NQ2,19*NQ2,19*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [19*Q2,19*Q2,19*Q2,19*Q2] /\
       [19*NQ2,19*NQ2,19*NQ2,19*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [19*Q2,19*Q2,19*Q2,19*Q2] /\
       [19*NQ2,19*NQ2,19*NQ2,19*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [19*Q2,19*Q2,19*Q2,19*Q2] /\
       [19*NQ2,19*NQ2,19*NQ2,19*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [19*Q2,19*Q2,19*Q2,19*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [19*NQ2,19*NQ2,19*NQ2,19*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [19*Q2,19*Q2,19*Q2,19*Q2] /\
       [19*NQ2,19*NQ2,19*NQ2,19*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [19*Q2,19*Q2,19*Q2,19*Q2] /\
       [19*NQ2,19*NQ2,19*NQ2,19*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [19*Q2,19*Q2,19*Q2,19*Q2] /\
       [19*NQ2,19*NQ2,19*NQ2,19*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [19*Q2,19*Q2,19*Q2,19*Q2]
    && [19@16*NQ2,19@16*NQ2,19@16*NQ2,19@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[19@16*Q2,19@16*Q2,19@16*Q2,19@16*Q2] /\
       [19@16*NQ2,19@16*NQ2,19@16*NQ2,19@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[19@16*Q2,19@16*Q2,19@16*Q2,19@16*Q2] /\
       [19@16*NQ2,19@16*NQ2,19@16*NQ2,19@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[19@16*Q2,19@16*Q2,19@16*Q2,19@16*Q2] /\
       [19@16*NQ2,19@16*NQ2,19@16*NQ2,19@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[19@16*Q2,19@16*Q2,19@16*Q2,19@16*Q2];


(* CUT 51 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo34, r2_to34] + [r6_bo34, r6_to34] * [ -848,  -848])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo34, r2_to34] - [r6_bo34, r6_to34] * [ -848,  -848])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r3_bo34, r3_to34] + [r7_bo34, r7_to34] * [ -630,  -630])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo34, r3_to34] - [r7_bo34, r7_to34] * [ -630,  -630])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r4_bo34, r4_to34] + [r8_bo34, r8_to34] * [ 1432,  1432])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo34, r4_to34] - [r8_bo34, r8_to34] * [ 1432,  1432])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r5_bo34, r5_to34] + [r9_bo34, r9_to34] * [  687,   687])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r5_bo34, r5_to34] - [r9_bo34, r9_to34] * [  687,   687])
          [r9_b, r9_t] [Q, Q] /\
    [19*NQ2,19*NQ2,19*NQ2,19*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [19*Q2,19*Q2,19*Q2,19*Q2] /\
    [19*NQ2,19*NQ2,19*NQ2,19*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [19*Q2,19*Q2,19*Q2,19*Q2] /\
    [19*NQ2,19*NQ2,19*NQ2,19*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [19*Q2,19*Q2,19*Q2,19*Q2] /\
    [19*NQ2,19*NQ2,19*NQ2,19*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [19*Q2,19*Q2,19*Q2,19*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [19@16*NQ2,19@16*NQ2,19@16*NQ2,19@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[19@16*Q2,19@16*Q2,19@16*Q2,19@16*Q2] /\
    [19@16*NQ2,19@16*NQ2,19@16*NQ2,19@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[19@16*Q2,19@16*Q2,19@16*Q2,19@16*Q2] /\
    [19@16*NQ2,19@16*NQ2,19@16*NQ2,19@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[19@16*Q2,19@16*Q2,19@16*Q2,19@16*Q2] /\
    [19@16*NQ2,19@16*NQ2,19@16*NQ2,19@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[19@16*Q2,19@16*Q2,19@16*Q2,19@16*Q2];



ghost r2_bo35@int16, r2_to35@int16, r3_bo35@int16, r3_to35@int16,
      r4_bo35@int16, r4_to35@int16, r5_bo35@int16, r5_to35@int16,
      r6_bo35@int16, r6_to35@int16, r7_bo35@int16, r7_to35@int16,
      r8_bo35@int16, r8_to35@int16, r9_bo35@int16, r9_to35@int16:
      r2_bo35 = r2_b /\ r2_to35 = r2_t /\ r3_bo35 = r3_b /\ r3_to35 = r3_t /\
      r4_bo35 = r4_b /\ r4_to35 = r4_t /\ r5_bo35 = r5_b /\ r5_to35 = r5_t /\
      r6_bo35 = r6_b /\ r6_to35 = r6_t /\ r7_bo35 = r7_b /\ r7_to35 = r7_t /\
      r8_bo35 = r8_b /\ r8_to35 = r8_t /\ r9_bo35 = r9_b /\ r9_to35 = r9_t
   && r2_bo35 = r2_b /\ r2_to35 = r2_t /\ r3_bo35 = r3_b /\ r3_to35 = r3_t /\
      r4_bo35 = r4_b /\ r4_to35 = r4_t /\ r5_bo35 = r5_b /\ r5_to35 = r5_t /\
      r6_bo35 = r6_b /\ r6_to35 = r6_t /\ r7_bo35 = r7_b /\ r7_to35 = r7_t /\
      r8_bo35 = r8_b /\ r8_to35 = r8_t /\ r9_bo35 = r9_b /\ r9_to35 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401ab0; Value = 0xc73f7147; PC = 0x401444 *)
mov [r10, r11] [L0x401ab0, L0x401ab4];
(* smulwb	lr, r10, r2                              #! PC = 0x401448 *)
vpc r2_bW@int32 r2_b; mulj tmp r10 r2_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r2, r10, r2                              #! PC = 0x40144c *)
vpc r2_tW@int32 r2_t; mulj tmp r10 r2_tW; spl r2_w dc tmp 16;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b; vpc r2_t@int16 r2_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401450 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r2, r2, r12, r0                          #! PC = 0x401454 *)
mulj prod r2_b r12_t; add r2_w prod r0;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b;
(* pkhtb	r2, r2, lr, asr #16                       #! PC = 0x401458 *)
mov tmp_b lr_t; mov tmp_t r2_t;
mov r2_b tmp_b; mov r2_t tmp_t;
(* smulwb	lr, r11, r3                              #! PC = 0x40145c *)
vpc r3_bW@int32 r3_b; mulj tmp r11 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r11, r3                              #! PC = 0x401460 *)
vpc r3_tW@int32 r3_t; mulj tmp r11 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401464 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x401468 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x40146c *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov r3_b tmp_b; mov r3_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401ab8; Value = 0x21e9b2f3; PC = 0x401470 *)
mov [r10, r11] [L0x401ab8, L0x401abc];
(* smulwb	lr, r10, r4                              #! PC = 0x401474 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401478 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40147c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401480 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x401484 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov r4_b tmp_b; mov r4_t tmp_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401488 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40148c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401490 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401494 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x401498 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov r5_b tmp_b; mov r5_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401ac0; Value = 0x4c83f5da; PC = 0x40149c *)
mov [r10, r11] [L0x401ac0, L0x401ac4];
(* smulwb	lr, r10, r6                              #! PC = 0x4014a0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4014a4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014a8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4014ac *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x4014b0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov r6_b tmp_b; mov r6_t tmp_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4014b4 *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4014b8 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014bc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4014c0 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x4014c4 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov r7_b tmp_b; mov r7_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401ac8; Value = 0xf49e69f8; PC = 0x4014c8 *)
mov [r10, r11] [L0x401ac8, L0x401acc];
(* smulwb	lr, r10, r8                              #! PC = 0x4014cc *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x4014d0 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014d4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4014d8 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x4014dc *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov r8_b tmp_b; mov r8_t tmp_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4014e0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4014e4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014e8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4014ec *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x4014f0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov r9_b tmp_b; mov r9_t tmp_t;

assert eqmod r2_b (r2_bo35 *  -738) [Q] /\
       eqmod r2_t (r2_to35 *  -738) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r2_b (r2_bo35 *  -738) [Q] /\
       eqmod r2_t (r2_to35 *  -738) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r2_b, r2_t] /\ [r2_b, r2_t] <s[Q2, Q2];


assert eqmod r3_b (r3_bo35 *   -28) [Q] /\
       eqmod r3_t (r3_to35 *   -28) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r3_b (r3_bo35 *   -28) [Q] /\
       eqmod r3_t (r3_to35 *   -28) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r3_b, r3_t] /\ [r3_b, r3_t] <s[Q2, Q2];


assert eqmod r4_b (r4_bo35 *   441) [Q] /\
       eqmod r4_t (r4_to35 *   441) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r4_b (r4_bo35 *   441) [Q] /\
       eqmod r4_t (r4_to35 *   441) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r4_b, r4_t] /\ [r4_b, r4_t] <s[Q2, Q2];


assert eqmod r5_b (r5_bo35 * -1120) [Q] /\
       eqmod r5_t (r5_to35 * -1120) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r5_b (r5_bo35 * -1120) [Q] /\
       eqmod r5_t (r5_to35 * -1120) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r5_b, r5_t] /\ [r5_b, r5_t] <s[Q2, Q2];


assert eqmod r6_b (r6_bo35 *   995) [Q] /\
       eqmod r6_t (r6_to35 *   995) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r6_b (r6_bo35 *   995) [Q] /\
       eqmod r6_t (r6_to35 *   995) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r6_b, r6_t] /\ [r6_b, r6_t] <s[Q2, Q2];


assert eqmod r7_b (r7_bo35 * -1523) [Q] /\
       eqmod r7_t (r7_to35 * -1523) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r7_b (r7_bo35 * -1523) [Q] /\
       eqmod r7_t (r7_to35 * -1523) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r7_b, r7_t] /\ [r7_b, r7_t] <s[Q2, Q2];


assert eqmod r8_b (r8_bo35 *  -148) [Q] /\
       eqmod r8_t (r8_to35 *  -148) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r8_b (r8_bo35 *  -148) [Q] /\
       eqmod r8_t (r8_to35 *  -148) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r8_b, r8_t] /\ [r8_b, r8_t] <s[Q2, Q2];


assert eqmod r9_b (r9_bo35 *  -998) [Q] /\
       eqmod r9_t (r9_to35 *  -998) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r9_b (r9_bo35 *  -998) [Q] /\
       eqmod r9_t (r9_to35 *  -998) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r9_b, r9_t] /\ [r9_b, r9_t] <s[Q2, Q2];

(* vmov	r0, s6                                     #! PC = 0x4014f4 *)
mov [r0_b, r0_t] [s6_b, s6_t];
(* str.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2e4; PC = 0x4014f8 *)
mov [L0xbefff2e4, L0xbefff2e6] [r6_b, r6_t];
(* str.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff324; PC = 0x4014fc *)
mov [L0xbefff324, L0xbefff326] [r7_b, r7_t];
(* str.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff364; PC = 0x401500 *)
mov [L0xbefff364, L0xbefff366] [r8_b, r8_t];
(* str.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff3a4; PC = 0x401504 *)
mov [L0xbefff3a4, L0xbefff3a6] [r9_b, r9_t];
(* str.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff224; PC = 0x401508 *)
mov [L0xbefff224, L0xbefff226] [r3_b, r3_t];
(* str.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff264; PC = 0x40150c *)
mov [L0xbefff264, L0xbefff266] [r4_b, r4_t];
(* str.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff2a4; PC = 0x401510 *)
mov [L0xbefff2a4, L0xbefff2a6] [r5_b, r5_t];
(* str.w	r2, [r0], #4                              #! EA = L0xbefff1e4; PC = 0x401514 *)
mov [L0xbefff1e4, L0xbefff1e6] [r2_b, r2_t];
(* vmov	lr, s14                                    #! PC = 0x401518 *)
mov [lr_b, lr_t] [s14_b, s14_t]; mov lr s14;
(* cmp.w	r0, lr                                    #! PC = 0x40151c *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x4012bc <invntt_fast+1552>              #! PC = 0x401520 *)
#bne.w	0x4012bc <invntt_fast+1552>              #! 0x401520 = 0x401520;

(* CUT 52 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo35, r2_to35] * [ -738,  -738])
          [L0xbefff1e4, L0xbefff1e6] [Q, Q] /\
    eqmod ([r3_bo35, r3_to35] * [  -28,   -28])
          [L0xbefff224, L0xbefff226] [Q, Q] /\
    eqmod ([r4_bo35, r4_to35] * [  441,   441])
          [L0xbefff264, L0xbefff266] [Q, Q] /\
    eqmod ([r5_bo35, r5_to35] * [-1120, -1120])
          [L0xbefff2a4, L0xbefff2a6] [Q, Q] /\
    eqmod ([r6_bo35, r6_to35] * [  995,   995])
          [L0xbefff2e4, L0xbefff2e6] [Q, Q] /\
    eqmod ([r7_bo35, r7_to35] * [-1523, -1523])
          [L0xbefff324, L0xbefff326] [Q, Q] /\
    eqmod ([r8_bo35, r8_to35] * [ -148,  -148])
          [L0xbefff364, L0xbefff366] [Q, Q] /\
    eqmod ([r9_bo35, r9_to35] * [ -998,  -998])
          [L0xbefff3a4, L0xbefff3a6] [Q, Q] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff1e4,L0xbefff1e6,L0xbefff224,L0xbefff226] /\
    [L0xbefff1e4,L0xbefff1e6,L0xbefff224,L0xbefff226]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff264,L0xbefff266,L0xbefff2a4,L0xbefff2a6] /\
    [L0xbefff264,L0xbefff266,L0xbefff2a4,L0xbefff2a6]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff2e4,L0xbefff2e6,L0xbefff324,L0xbefff326] /\
    [L0xbefff2e4,L0xbefff2e6,L0xbefff324,L0xbefff326]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff364,L0xbefff366,L0xbefff3a4,L0xbefff3a6] /\
    [L0xbefff364,L0xbefff366,L0xbefff3a4,L0xbefff3a6]< [Q2,Q2,Q2,Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1e4,L0xbefff1e6,L0xbefff224,L0xbefff226] /\
    [L0xbefff1e4,L0xbefff1e6,L0xbefff224,L0xbefff226]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff264,L0xbefff266,L0xbefff2a4,L0xbefff2a6] /\
    [L0xbefff264,L0xbefff266,L0xbefff2a4,L0xbefff2a6]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2e4,L0xbefff2e6,L0xbefff324,L0xbefff326] /\
    [L0xbefff2e4,L0xbefff2e6,L0xbefff324,L0xbefff326]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff364,L0xbefff366,L0xbefff3a4,L0xbefff3a6] /\
    [L0xbefff364,L0xbefff366,L0xbefff3a4,L0xbefff3a6]<s[Q2,Q2,Q2,Q2];

(* vmov	s6, r0                                     #! PC = 0x4012bc *)
mov [s6_b, s6_t] [r0_b, r0_t];
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff1e8; Value = 0x171014a5; PC = 0x4012c0 *)
mov [r2_b, r2_t] [L0xbefff1e8, L0xbefff1ea];
(* ldr.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff228; Value = 0xf6a61143; PC = 0x4012c4 *)
mov [r3_b, r3_t] [L0xbefff228, L0xbefff22a];
(* ldr.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff268; Value = 0xf222f866; PC = 0x4012c8 *)
mov [r4_b, r4_t] [L0xbefff268, L0xbefff26a];
(* ldr.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff2a8; Value = 0xfc51072f; PC = 0x4012cc *)
mov [r5_b, r5_t] [L0xbefff2a8, L0xbefff2aa];
(* ldr.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2e8; Value = 0xfe0ff1d8; PC = 0x4012d0 *)
mov [r6_b, r6_t] [L0xbefff2e8, L0xbefff2ea];
(* ldr.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff328; Value = 0x02d104d2; PC = 0x4012d4 *)
mov [r7_b, r7_t] [L0xbefff328, L0xbefff32a];
(* ldr.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff368; Value = 0xf76f085a; PC = 0x4012d8 *)
mov [r8_b, r8_t] [L0xbefff368, L0xbefff36a];
(* ldr.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff3a8; Value = 0xf193f991; PC = 0x4012dc *)
mov [r9_b, r9_t] [L0xbefff3a8, L0xbefff3aa];

ghost r2_bo36@int16, r2_to36@int16, r3_bo36@int16, r3_to36@int16,
      r4_bo36@int16, r4_to36@int16, r5_bo36@int16, r5_to36@int16,
      r6_bo36@int16, r6_to36@int16, r7_bo36@int16, r7_to36@int16,
      r8_bo36@int16, r8_to36@int16, r9_bo36@int16, r9_to36@int16:
      r2_bo36 = r2_b /\ r2_to36 = r2_t /\ r3_bo36 = r3_b /\ r3_to36 = r3_t /\
      r4_bo36 = r4_b /\ r4_to36 = r4_t /\ r5_bo36 = r5_b /\ r5_to36 = r5_t /\
      r6_bo36 = r6_b /\ r6_to36 = r6_t /\ r7_bo36 = r7_b /\ r7_to36 = r7_t /\
      r8_bo36 = r8_b /\ r8_to36 = r8_t /\ r9_bo36 = r9_b /\ r9_to36 = r9_t
   && r2_bo36 = r2_b /\ r2_to36 = r2_t /\ r3_bo36 = r3_b /\ r3_to36 = r3_t /\
      r4_bo36 = r4_b /\ r4_to36 = r4_t /\ r5_bo36 = r5_b /\ r5_to36 = r5_t /\
      r6_bo36 = r6_b /\ r6_to36 = r6_t /\ r7_bo36 = r7_b /\ r7_to36 = r7_t /\
      r8_bo36 = r8_b /\ r8_to36 = r8_t /\ r9_bo36 = r9_b /\ r9_to36 = r9_t;

(* movw	r0, #26632	; 0x6808                        #! PC = 0x4012e0 *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* ldr.w	r10, [r1], #4                             #! EA = L0x401ad0; Value = 0xd43e715a; PC = 0x4012e4 *)
mov r10 L0x401ad0;
(* smulwb	lr, r10, r3                              #! PC = 0x4012e8 *)
vpc r3_bW@int32 r3_b; mulj tmp r10 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r10, r3                              #! PC = 0x4012ec *)
vpc r3_tW@int32 r3_t; mulj tmp r10 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4012f0 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4012f4 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x4012f8 *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r3_bo36 *  -569) [Q] /\
       eqmod lr_t (r3_to36 *  -569) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r3_bo36 *  -569) [Q] /\
       eqmod lr_t (r3_to36 *  -569) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r3, r2, lr                               #! PC = 0x4012fc *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401300 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x401304 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x401308 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40130c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401310 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401314 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo36 *  -569) [Q] /\
       eqmod lr_t (r5_to36 *  -569) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo36 *  -569) [Q] /\
       eqmod lr_t (r5_to36 *  -569) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r4, lr                               #! PC = 0x401318 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x40131c *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x401320 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x401324 *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401328 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x40132c *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x401330 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo36 *  -569) [Q] /\
       eqmod lr_t (r7_to36 *  -569) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo36 *  -569) [Q] /\
       eqmod lr_t (r7_to36 *  -569) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r6, lr                               #! PC = 0x401334 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x401338 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x40133c *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x401340 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401344 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401348 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x40134c *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo36 *  -569) [Q] /\
       eqmod lr_t (r9_to36 *  -569) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo36 *  -569) [Q] /\
       eqmod lr_t (r9_to36 *  -569) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r8, lr                               #! PC = 0x401350 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x401354 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

assert [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [6*Q2,6*Q2,6*Q2,6*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [6*Q2,6*Q2,6*Q2,6*Q2]
    && [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2];


(* CUT 53 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo36, r2_to36] + [r3_bo36, r3_to36] * [ -569,  -569])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo36, r2_to36] - [r3_bo36, r3_to36] * [ -569,  -569])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r4_bo36, r4_to36] + [r5_bo36, r5_to36] * [ -569,  -569])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo36, r4_to36] - [r5_bo36, r5_to36] * [ -569,  -569])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo36, r6_to36] + [r7_bo36, r7_to36] * [ -569,  -569])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo36, r6_to36] - [r7_bo36, r7_to36] * [ -569,  -569])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r8_bo36, r8_to36] + [r9_bo36, r9_to36] * [ -569,  -569])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r8_bo36, r8_to36] - [r9_bo36, r9_to36] * [ -569,  -569])
          [r9_b, r9_t] [Q, Q] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [6*Q2,6*Q2,6*Q2,6*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2]
    prove with [cuts [1, 3, 5, 7, 9, 11, 13, 15]];

ghost r2_bo37@int16, r2_to37@int16, r3_bo37@int16, r3_to37@int16,
      r4_bo37@int16, r4_to37@int16, r5_bo37@int16, r5_to37@int16,
      r6_bo37@int16, r6_to37@int16, r7_bo37@int16, r7_to37@int16,
      r8_bo37@int16, r8_to37@int16, r9_bo37@int16, r9_to37@int16:
      r2_bo37 = r2_b /\ r2_to37 = r2_t /\ r3_bo37 = r3_b /\ r3_to37 = r3_t /\
      r4_bo37 = r4_b /\ r4_to37 = r4_t /\ r5_bo37 = r5_b /\ r5_to37 = r5_t /\
      r6_bo37 = r6_b /\ r6_to37 = r6_t /\ r7_bo37 = r7_b /\ r7_to37 = r7_t /\
      r8_bo37 = r8_b /\ r8_to37 = r8_t /\ r9_bo37 = r9_b /\ r9_to37 = r9_t
   && r2_bo37 = r2_b /\ r2_to37 = r2_t /\ r3_bo37 = r3_b /\ r3_to37 = r3_t /\
      r4_bo37 = r4_b /\ r4_to37 = r4_t /\ r5_bo37 = r5_b /\ r5_to37 = r5_t /\
      r6_bo37 = r6_b /\ r6_to37 = r6_t /\ r7_bo37 = r7_b /\ r7_to37 = r7_t /\
      r8_bo37 = r8_b /\ r8_to37 = r8_t /\ r9_bo37 = r9_b /\ r9_to37 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401ad4; Value = 0x229ae065; PC = 0x401358 *)
mov [r10, r11] [L0x401ad4, L0x401ad8];
(* smulwb	lr, r10, r4                              #! PC = 0x40135c *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401360 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401364 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401368 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x40136c *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_bo37 *   450) [Q] /\
       eqmod lr_t (r4_to37 *   450) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_bo37 *   450) [Q] /\
       eqmod lr_t (r4_to37 *   450) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r4, r2, lr                               #! PC = 0x401370 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401374 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401378 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40137c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401380 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401384 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401388 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo37 *   936) [Q] /\
       eqmod lr_t (r5_to37 *   936) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo37 *   936) [Q] /\
       eqmod lr_t (r5_to37 *   936) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x40138c *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401390 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x401394 *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401398 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40139c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4013a0 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x4013a4 *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo37 *   450) [Q] /\
       eqmod lr_t (r8_to37 *   450) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo37 *   450) [Q] /\
       eqmod lr_t (r8_to37 *   450) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r6, lr                               #! PC = 0x4013a8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x4013ac *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4013b0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4013b4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013b8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4013bc *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4013c0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo37 *   936) [Q] /\
       eqmod lr_t (r9_to37 *   936) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo37 *   936) [Q] /\
       eqmod lr_t (r9_to37 *   936) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x4013c4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x4013c8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;

assert [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
       prove with [algebra solver isl] && true;
assume [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
    && [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2];


(* CUT 54 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo37, r2_to37] + [r4_bo37, r4_to37] * [  450,   450])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo37, r2_to37] - [r4_bo37, r4_to37] * [  450,   450])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r3_bo37, r3_to37] + [r5_bo37, r5_to37] * [  936,   936])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo37, r3_to37] - [r5_bo37, r5_to37] * [  936,   936])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo37, r6_to37] + [r8_bo37, r8_to37] * [  450,   450])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo37, r6_to37] - [r8_bo37, r8_to37] * [  450,   450])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r7_bo37, r7_to37] + [r9_bo37, r9_to37] * [  936,   936])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r7_bo37, r7_to37] - [r9_bo37, r9_to37] * [  936,   936])
          [r9_b, r9_t] [Q, Q] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2];



ghost r2_bo38@int16, r2_to38@int16, r3_bo38@int16, r3_to38@int16,
      r4_bo38@int16, r4_to38@int16, r5_bo38@int16, r5_to38@int16,
      r6_bo38@int16, r6_to38@int16, r7_bo38@int16, r7_to38@int16,
      r8_bo38@int16, r8_to38@int16, r9_bo38@int16, r9_to38@int16:
      r2_bo38 = r2_b /\ r2_to38 = r2_t /\ r3_bo38 = r3_b /\ r3_to38 = r3_t /\
      r4_bo38 = r4_b /\ r4_to38 = r4_t /\ r5_bo38 = r5_b /\ r5_to38 = r5_t /\
      r6_bo38 = r6_b /\ r6_to38 = r6_t /\ r7_bo38 = r7_b /\ r7_to38 = r7_t /\
      r8_bo38 = r8_b /\ r8_to38 = r8_t /\ r9_bo38 = r9_b /\ r9_to38 = r9_t
   && r2_bo38 = r2_b /\ r2_to38 = r2_t /\ r3_bo38 = r3_b /\ r3_to38 = r3_t /\
      r4_bo38 = r4_b /\ r4_to38 = r4_t /\ r5_bo38 = r5_b /\ r5_to38 = r5_t /\
      r6_bo38 = r6_b /\ r6_to38 = r6_t /\ r7_bo38 = r7_b /\ r7_to38 = r7_t /\
      r8_bo38 = r8_b /\ r8_to38 = r8_t /\ r9_bo38 = r9_b /\ r9_to38 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401adc; Value = 0xce2b37c1; PC = 0x4013cc *)
mov [r10, r11] [L0x401adc, L0x401ae0];
(* smulwb	lr, r10, r6                              #! PC = 0x4013d0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4013d4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013d8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4013dc *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x4013e0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r6_bo38 *  -648) [Q] /\
       eqmod lr_t (r6_to38 *  -648) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r6_bo38 *  -648) [Q] /\
       eqmod lr_t (r6_to38 *  -648) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r6, r2, lr                               #! PC = 0x4013e4 *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4013e8 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4013ec *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4013f0 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013f4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4013f8 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x4013fc *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo38 *   712) [Q] /\
       eqmod lr_t (r7_to38 *   712) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo38 *   712) [Q] /\
       eqmod lr_t (r7_to38 *   712) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x401400 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401404 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401ae4; Value = 0x8e1c73f8; PC = 0x401408 *)
mov [r10, r11] [L0x401ae4, L0x401ae8];
(* smulwb	lr, r10, r8                              #! PC = 0x40140c *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401410 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401414 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x401418 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x40141c *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo38 * -1481) [Q] /\
       eqmod lr_t (r8_to38 * -1481) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo38 * -1481) [Q] /\
       eqmod lr_t (r8_to38 * -1481) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r4, lr                               #! PC = 0x401420 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x401424 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x401428 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x40142c *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401430 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401434 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x401438 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo38 *   682) [Q] /\
       eqmod lr_t (r9_to38 *   682) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo38 *   682) [Q] /\
       eqmod lr_t (r9_to38 *   682) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x40143c *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x401440 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;

assert [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
    && [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2];


(* CUT 55 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo38, r2_to38] + [r6_bo38, r6_to38] * [ -648,  -648])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo38, r2_to38] - [r6_bo38, r6_to38] * [ -648,  -648])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r3_bo38, r3_to38] + [r7_bo38, r7_to38] * [  712,   712])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo38, r3_to38] - [r7_bo38, r7_to38] * [  712,   712])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r4_bo38, r4_to38] + [r8_bo38, r8_to38] * [-1481, -1481])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo38, r4_to38] - [r8_bo38, r8_to38] * [-1481, -1481])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r5_bo38, r5_to38] + [r9_bo38, r9_to38] * [  682,   682])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r5_bo38, r5_to38] - [r9_bo38, r9_to38] * [  682,   682])
          [r9_b, r9_t] [Q, Q] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2];



ghost r2_bo39@int16, r2_to39@int16, r3_bo39@int16, r3_to39@int16,
      r4_bo39@int16, r4_to39@int16, r5_bo39@int16, r5_to39@int16,
      r6_bo39@int16, r6_to39@int16, r7_bo39@int16, r7_to39@int16,
      r8_bo39@int16, r8_to39@int16, r9_bo39@int16, r9_to39@int16:
      r2_bo39 = r2_b /\ r2_to39 = r2_t /\ r3_bo39 = r3_b /\ r3_to39 = r3_t /\
      r4_bo39 = r4_b /\ r4_to39 = r4_t /\ r5_bo39 = r5_b /\ r5_to39 = r5_t /\
      r6_bo39 = r6_b /\ r6_to39 = r6_t /\ r7_bo39 = r7_b /\ r7_to39 = r7_t /\
      r8_bo39 = r8_b /\ r8_to39 = r8_t /\ r9_bo39 = r9_b /\ r9_to39 = r9_t
   && r2_bo39 = r2_b /\ r2_to39 = r2_t /\ r3_bo39 = r3_b /\ r3_to39 = r3_t /\
      r4_bo39 = r4_b /\ r4_to39 = r4_t /\ r5_bo39 = r5_b /\ r5_to39 = r5_t /\
      r6_bo39 = r6_b /\ r6_to39 = r6_t /\ r7_bo39 = r7_b /\ r7_to39 = r7_t /\
      r8_bo39 = r8_b /\ r8_to39 = r8_t /\ r9_bo39 = r9_b /\ r9_to39 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401aec; Value = 0x8430e88c; PC = 0x401444 *)
mov [r10, r11] [L0x401aec, L0x401af0];
(* smulwb	lr, r10, r2                              #! PC = 0x401448 *)
vpc r2_bW@int32 r2_b; mulj tmp r10 r2_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r2, r10, r2                              #! PC = 0x40144c *)
vpc r2_tW@int32 r2_t; mulj tmp r10 r2_tW; spl r2_w dc tmp 16;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b; vpc r2_t@int16 r2_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401450 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r2, r2, r12, r0                          #! PC = 0x401454 *)
mulj prod r2_b r12_t; add r2_w prod r0;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b;
(* pkhtb	r2, r2, lr, asr #16                       #! PC = 0x401458 *)
mov tmp_b lr_t; mov tmp_t r2_t;
mov r2_b tmp_b; mov r2_t tmp_t;
(* smulwb	lr, r11, r3                              #! PC = 0x40145c *)
vpc r3_bW@int32 r3_b; mulj tmp r11 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r11, r3                              #! PC = 0x401460 *)
vpc r3_tW@int32 r3_t; mulj tmp r11 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401464 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x401468 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x40146c *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov r3_b tmp_b; mov r3_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401af4; Value = 0xa7a455d3; PC = 0x401470 *)
mov [r10, r11] [L0x401af4, L0x401af8];
(* smulwb	lr, r10, r4                              #! PC = 0x401474 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401478 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40147c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401480 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x401484 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov r4_b tmp_b; mov r4_t tmp_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401488 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40148c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401490 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401494 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x401498 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov r5_b tmp_b; mov r5_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401afc; Value = 0x31ad68d1; PC = 0x40149c *)
mov [r10, r11] [L0x401afc, L0x401b00];
(* smulwb	lr, r10, r6                              #! PC = 0x4014a0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4014a4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014a8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4014ac *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x4014b0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov r6_b tmp_b; mov r6_t tmp_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4014b4 *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4014b8 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014bc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4014c0 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x4014c4 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov r7_b tmp_b; mov r7_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401b04; Value = 0xc3186097; PC = 0x4014c8 *)
mov [r10, r11] [L0x401b04, L0x401b08];
(* smulwb	lr, r10, r8                              #! PC = 0x4014cc *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x4014d0 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014d4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4014d8 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x4014dc *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov r8_b tmp_b; mov r8_t tmp_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4014e0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4014e4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014e8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4014ec *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x4014f0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov r9_b tmp_b; mov r9_t tmp_t;

assert eqmod r2_b (r2_bo39 * -1610) [Q] /\
       eqmod r2_t (r2_to39 * -1610) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r2_b (r2_bo39 * -1610) [Q] /\
       eqmod r2_t (r2_to39 * -1610) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r2_b, r2_t] /\ [r2_b, r2_t] <s[Q2, Q2];


assert eqmod r3_b (r3_bo39 *   390) [Q] /\
       eqmod r3_t (r3_to39 *   390) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r3_b (r3_bo39 *   390) [Q] /\
       eqmod r3_t (r3_to39 *   390) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r3_b, r3_t] /\ [r3_b, r3_t] <s[Q2, Q2];


assert eqmod r4_b (r4_bo39 * -1149) [Q] /\
       eqmod r4_t (r4_to39 * -1149) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r4_b (r4_bo39 * -1149) [Q] /\
       eqmod r4_t (r4_to39 * -1149) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r4_b, r4_t] /\ [r4_b, r4_t] <s[Q2, Q2];


assert eqmod r5_b (r5_bo39 * -1045) [Q] /\
       eqmod r5_t (r5_to39 * -1045) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r5_b (r5_bo39 * -1045) [Q] /\
       eqmod r5_t (r5_to39 * -1045) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r5_b, r5_t] /\ [r5_b, r5_t] <s[Q2, Q2];


assert eqmod r6_b (r6_bo39 *   646) [Q] /\
       eqmod r6_t (r6_to39 *   646) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r6_b (r6_bo39 *   646) [Q] /\
       eqmod r6_t (r6_to39 *   646) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r6_b, r6_t] /\ [r6_b, r6_t] <s[Q2, Q2];


assert eqmod r7_b (r7_bo39 *  1477) [Q] /\
       eqmod r7_t (r7_to39 *  1477) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r7_b (r7_bo39 *  1477) [Q] /\
       eqmod r7_t (r7_to39 *  1477) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r7_b, r7_t] /\ [r7_b, r7_t] <s[Q2, Q2];


assert eqmod r8_b (r8_bo39 *  -792) [Q] /\
       eqmod r8_t (r8_to39 *  -792) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r8_b (r8_bo39 *  -792) [Q] /\
       eqmod r8_t (r8_to39 *  -792) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r8_b, r8_t] /\ [r8_b, r8_t] <s[Q2, Q2];


assert eqmod r9_b (r9_bo39 *  -842) [Q] /\
       eqmod r9_t (r9_to39 *  -842) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r9_b (r9_bo39 *  -842) [Q] /\
       eqmod r9_t (r9_to39 *  -842) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r9_b, r9_t] /\ [r9_b, r9_t] <s[Q2, Q2];

(* vmov	r0, s6                                     #! PC = 0x4014f4 *)
mov [r0_b, r0_t] [s6_b, s6_t];
(* str.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2e8; PC = 0x4014f8 *)
mov [L0xbefff2e8, L0xbefff2ea] [r6_b, r6_t];
(* str.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff328; PC = 0x4014fc *)
mov [L0xbefff328, L0xbefff32a] [r7_b, r7_t];
(* str.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff368; PC = 0x401500 *)
mov [L0xbefff368, L0xbefff36a] [r8_b, r8_t];
(* str.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff3a8; PC = 0x401504 *)
mov [L0xbefff3a8, L0xbefff3aa] [r9_b, r9_t];
(* str.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff228; PC = 0x401508 *)
mov [L0xbefff228, L0xbefff22a] [r3_b, r3_t];
(* str.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff268; PC = 0x40150c *)
mov [L0xbefff268, L0xbefff26a] [r4_b, r4_t];
(* str.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff2a8; PC = 0x401510 *)
mov [L0xbefff2a8, L0xbefff2aa] [r5_b, r5_t];
(* str.w	r2, [r0], #4                              #! EA = L0xbefff1e8; PC = 0x401514 *)
mov [L0xbefff1e8, L0xbefff1ea] [r2_b, r2_t];
(* vmov	lr, s14                                    #! PC = 0x401518 *)
mov [lr_b, lr_t] [s14_b, s14_t]; mov lr s14;
(* cmp.w	r0, lr                                    #! PC = 0x40151c *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x4012bc <invntt_fast+1552>              #! PC = 0x401520 *)
#bne.w	0x4012bc <invntt_fast+1552>              #! 0x401520 = 0x401520;

(* CUT 56 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo39, r2_to39] * [-1610, -1610])
          [L0xbefff1e8, L0xbefff1ea] [Q, Q] /\
    eqmod ([r3_bo39, r3_to39] * [  390,   390])
          [L0xbefff228, L0xbefff22a] [Q, Q] /\
    eqmod ([r4_bo39, r4_to39] * [-1149, -1149])
          [L0xbefff268, L0xbefff26a] [Q, Q] /\
    eqmod ([r5_bo39, r5_to39] * [-1045, -1045])
          [L0xbefff2a8, L0xbefff2aa] [Q, Q] /\
    eqmod ([r6_bo39, r6_to39] * [  646,   646])
          [L0xbefff2e8, L0xbefff2ea] [Q, Q] /\
    eqmod ([r7_bo39, r7_to39] * [ 1477,  1477])
          [L0xbefff328, L0xbefff32a] [Q, Q] /\
    eqmod ([r8_bo39, r8_to39] * [ -792,  -792])
          [L0xbefff368, L0xbefff36a] [Q, Q] /\
    eqmod ([r9_bo39, r9_to39] * [ -842,  -842])
          [L0xbefff3a8, L0xbefff3aa] [Q, Q] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff1e8,L0xbefff1ea,L0xbefff228,L0xbefff22a] /\
    [L0xbefff1e8,L0xbefff1ea,L0xbefff228,L0xbefff22a]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff268,L0xbefff26a,L0xbefff2a8,L0xbefff2aa] /\
    [L0xbefff268,L0xbefff26a,L0xbefff2a8,L0xbefff2aa]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff2e8,L0xbefff2ea,L0xbefff328,L0xbefff32a] /\
    [L0xbefff2e8,L0xbefff2ea,L0xbefff328,L0xbefff32a]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff368,L0xbefff36a,L0xbefff3a8,L0xbefff3aa] /\
    [L0xbefff368,L0xbefff36a,L0xbefff3a8,L0xbefff3aa]< [Q2,Q2,Q2,Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1e8,L0xbefff1ea,L0xbefff228,L0xbefff22a] /\
    [L0xbefff1e8,L0xbefff1ea,L0xbefff228,L0xbefff22a]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff268,L0xbefff26a,L0xbefff2a8,L0xbefff2aa] /\
    [L0xbefff268,L0xbefff26a,L0xbefff2a8,L0xbefff2aa]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2e8,L0xbefff2ea,L0xbefff328,L0xbefff32a] /\
    [L0xbefff2e8,L0xbefff2ea,L0xbefff328,L0xbefff32a]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff368,L0xbefff36a,L0xbefff3a8,L0xbefff3aa] /\
    [L0xbefff368,L0xbefff36a,L0xbefff3a8,L0xbefff3aa]<s[Q2,Q2,Q2,Q2];

(* vmov	s6, r0                                     #! PC = 0x4012bc *)
mov [s6_b, s6_t] [r0_b, r0_t];
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff1ec; Value = 0x0b2c0a36; PC = 0x4012c0 *)
mov [r2_b, r2_t] [L0xbefff1ec, L0xbefff1ee];
(* ldr.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff22c; Value = 0x0acdfc67; PC = 0x4012c4 *)
mov [r3_b, r3_t] [L0xbefff22c, L0xbefff22e];
(* ldr.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff26c; Value = 0xedeeeeb2; PC = 0x4012c8 *)
mov [r4_b, r4_t] [L0xbefff26c, L0xbefff26e];
(* ldr.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff2ac; Value = 0x004316bc; PC = 0x4012cc *)
mov [r5_b, r5_t] [L0xbefff2ac, L0xbefff2ae];
(* ldr.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2ec; Value = 0x1c1af6e6; PC = 0x4012d0 *)
mov [r6_b, r6_t] [L0xbefff2ec, L0xbefff2ee];
(* ldr.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff32c; Value = 0xfb70e62d; PC = 0x4012d4 *)
mov [r7_b, r7_t] [L0xbefff32c, L0xbefff32e];
(* ldr.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff36c; Value = 0xe883084b; PC = 0x4012d8 *)
mov [r8_b, r8_t] [L0xbefff36c, L0xbefff36e];
(* ldr.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff3ac; Value = 0x031900b2; PC = 0x4012dc *)
mov [r9_b, r9_t] [L0xbefff3ac, L0xbefff3ae];

ghost r2_bo40@int16, r2_to40@int16, r3_bo40@int16, r3_to40@int16,
      r4_bo40@int16, r4_to40@int16, r5_bo40@int16, r5_to40@int16,
      r6_bo40@int16, r6_to40@int16, r7_bo40@int16, r7_to40@int16,
      r8_bo40@int16, r8_to40@int16, r9_bo40@int16, r9_to40@int16:
      r2_bo40 = r2_b /\ r2_to40 = r2_t /\ r3_bo40 = r3_b /\ r3_to40 = r3_t /\
      r4_bo40 = r4_b /\ r4_to40 = r4_t /\ r5_bo40 = r5_b /\ r5_to40 = r5_t /\
      r6_bo40 = r6_b /\ r6_to40 = r6_t /\ r7_bo40 = r7_b /\ r7_to40 = r7_t /\
      r8_bo40 = r8_b /\ r8_to40 = r8_t /\ r9_bo40 = r9_b /\ r9_to40 = r9_t
   && r2_bo40 = r2_b /\ r2_to40 = r2_t /\ r3_bo40 = r3_b /\ r3_to40 = r3_t /\
      r4_bo40 = r4_b /\ r4_to40 = r4_t /\ r5_bo40 = r5_b /\ r5_to40 = r5_t /\
      r6_bo40 = r6_b /\ r6_to40 = r6_t /\ r7_bo40 = r7_b /\ r7_to40 = r7_t /\
      r8_bo40 = r8_b /\ r8_to40 = r8_t /\ r9_bo40 = r9_b /\ r9_to40 = r9_t;

(* movw	r0, #26632	; 0x6808                        #! PC = 0x4012e0 *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* ldr.w	r10, [r1], #4                             #! EA = L0x401b0c; Value = 0x6e1ee9ef; PC = 0x4012e4 *)
mov r10 L0x401b0c;
(* smulwb	lr, r10, r3                              #! PC = 0x4012e8 *)
vpc r3_bW@int32 r3_b; mulj tmp r10 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r10, r3                              #! PC = 0x4012ec *)
vpc r3_tW@int32 r3_t; mulj tmp r10 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4012f0 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4012f4 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x4012f8 *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r3_bo40 *  1432) [Q] /\
       eqmod lr_t (r3_to40 *  1432) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r3_bo40 *  1432) [Q] /\
       eqmod lr_t (r3_to40 *  1432) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r3, r2, lr                               #! PC = 0x4012fc *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401300 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x401304 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x401308 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40130c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401310 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401314 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo40 *  1432) [Q] /\
       eqmod lr_t (r5_to40 *  1432) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo40 *  1432) [Q] /\
       eqmod lr_t (r5_to40 *  1432) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r4, lr                               #! PC = 0x401318 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x40131c *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x401320 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x401324 *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401328 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x40132c *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x401330 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo40 *  1432) [Q] /\
       eqmod lr_t (r7_to40 *  1432) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo40 *  1432) [Q] /\
       eqmod lr_t (r7_to40 *  1432) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r6, lr                               #! PC = 0x401334 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x401338 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x40133c *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x401340 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401344 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401348 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x40134c *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo40 *  1432) [Q] /\
       eqmod lr_t (r9_to40 *  1432) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo40 *  1432) [Q] /\
       eqmod lr_t (r9_to40 *  1432) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r8, lr                               #! PC = 0x401350 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x401354 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

assert [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
    && [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2];


(* CUT 57 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo40, r2_to40] + [r3_bo40, r3_to40] * [ 1432,  1432])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo40, r2_to40] - [r3_bo40, r3_to40] * [ 1432,  1432])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r4_bo40, r4_to40] + [r5_bo40, r5_to40] * [ 1432,  1432])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo40, r4_to40] - [r5_bo40, r5_to40] * [ 1432,  1432])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo40, r6_to40] + [r7_bo40, r7_to40] * [ 1432,  1432])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo40, r6_to40] - [r7_bo40, r7_to40] * [ 1432,  1432])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r8_bo40, r8_to40] + [r9_bo40, r9_to40] * [ 1432,  1432])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r8_bo40, r8_to40] - [r9_bo40, r9_to40] * [ 1432,  1432])
          [r9_b, r9_t] [Q, Q] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2]
    prove with [cuts [1, 3, 5, 7, 9, 11, 13, 15]];



ghost r2_bo41@int16, r2_to41@int16, r3_bo41@int16, r3_to41@int16,
      r4_bo41@int16, r4_to41@int16, r5_bo41@int16, r5_to41@int16,
      r6_bo41@int16, r6_to41@int16, r7_bo41@int16, r7_to41@int16,
      r8_bo41@int16, r8_to41@int16, r9_bo41@int16, r9_to41@int16:
      r2_bo41 = r2_b /\ r2_to41 = r2_t /\ r3_bo41 = r3_b /\ r3_to41 = r3_t /\
      r4_bo41 = r4_b /\ r4_to41 = r4_t /\ r5_bo41 = r5_b /\ r5_to41 = r5_t /\
      r6_bo41 = r6_b /\ r6_to41 = r6_t /\ r7_bo41 = r7_b /\ r7_to41 = r7_t /\
      r8_bo41 = r8_b /\ r8_to41 = r8_t /\ r9_bo41 = r9_b /\ r9_to41 = r9_t
   && r2_bo41 = r2_b /\ r2_to41 = r2_t /\ r3_bo41 = r3_b /\ r3_to41 = r3_t /\
      r4_bo41 = r4_b /\ r4_to41 = r4_t /\ r5_bo41 = r5_b /\ r5_to41 = r5_t /\
      r6_bo41 = r6_b /\ r6_to41 = r6_t /\ r7_bo41 = r7_b /\ r7_to41 = r7_t /\
      r8_bo41 = r8_b /\ r8_to41 = r8_t /\ r9_bo41 = r9_b /\ r9_to41 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401b10; Value = 0x054e5c70; PC = 0x401358 *)
mov [r10, r11] [L0x401b10, L0x401b14];
(* smulwb	lr, r10, r4                              #! PC = 0x40135c *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401360 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401364 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401368 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x40136c *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_bo41 *    69) [Q] /\
       eqmod lr_t (r4_to41 *    69) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_bo41 *    69) [Q] /\
       eqmod lr_t (r4_to41 *    69) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r4, r2, lr                               #! PC = 0x401370 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401374 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401378 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40137c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401380 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401384 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401388 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo41 *   543) [Q] /\
       eqmod lr_t (r5_to41 *   543) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo41 *   543) [Q] /\
       eqmod lr_t (r5_to41 *   543) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x40138c *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401390 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x401394 *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401398 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40139c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4013a0 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x4013a4 *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo41 *    69) [Q] /\
       eqmod lr_t (r8_to41 *    69) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo41 *    69) [Q] /\
       eqmod lr_t (r8_to41 *    69) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r6, lr                               #! PC = 0x4013a8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x4013ac *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4013b0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4013b4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013b8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4013bc *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4013c0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo41 *   543) [Q] /\
       eqmod lr_t (r9_to41 *   543) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo41 *   543) [Q] /\
       eqmod lr_t (r9_to41 *   543) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x4013c4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x4013c8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;

assert [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
       prove with [algebra solver isl] && true;
assume [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
    && [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2];


(* CUT 58 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo41, r2_to41] + [r4_bo41, r4_to41] * [   69,    69])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo41, r2_to41] - [r4_bo41, r4_to41] * [   69,    69])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r3_bo41, r3_to41] + [r5_bo41, r5_to41] * [  543,   543])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo41, r3_to41] - [r5_bo41, r5_to41] * [  543,   543])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo41, r6_to41] + [r8_bo41, r8_to41] * [   69,    69])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo41, r6_to41] - [r8_bo41, r8_to41] * [   69,    69])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r7_bo41, r7_to41] + [r9_bo41, r9_to41] * [  543,   543])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r7_bo41, r7_to41] - [r9_bo41, r9_to41] * [  543,   543])
          [r9_b, r9_t] [Q, Q] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2];



ghost r2_bo42@int16, r2_to42@int16, r3_bo42@int16, r3_to42@int16,
      r4_bo42@int16, r4_to42@int16, r5_bo42@int16, r5_to42@int16,
      r6_bo42@int16, r6_to42@int16, r7_bo42@int16, r7_to42@int16,
      r8_bo42@int16, r8_to42@int16, r9_bo42@int16, r9_to42@int16:
      r2_bo42 = r2_b /\ r2_to42 = r2_t /\ r3_bo42 = r3_b /\ r3_to42 = r3_t /\
      r4_bo42 = r4_b /\ r4_to42 = r4_t /\ r5_bo42 = r5_b /\ r5_to42 = r5_t /\
      r6_bo42 = r6_b /\ r6_to42 = r6_t /\ r7_bo42 = r7_b /\ r7_to42 = r7_t /\
      r8_bo42 = r8_b /\ r8_to42 = r8_t /\ r9_bo42 = r9_b /\ r9_to42 = r9_t
   && r2_bo42 = r2_b /\ r2_to42 = r2_t /\ r3_bo42 = r3_b /\ r3_to42 = r3_t /\
      r4_bo42 = r4_b /\ r4_to42 = r4_t /\ r5_bo42 = r5_b /\ r5_to42 = r5_t /\
      r6_bo42 = r6_b /\ r6_to42 = r6_t /\ r7_bo42 = r7_b /\ r7_to42 = r7_t /\
      r8_bo42 = r8_b /\ r8_to42 = r8_t /\ r9_bo42 = r9_b /\ r9_to42 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401b18; Value = 0x225fd13f; PC = 0x4013cc *)
mov [r10, r11] [L0x401b18, L0x401b1c];
(* smulwb	lr, r10, r6                              #! PC = 0x4013d0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4013d4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013d8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4013dc *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x4013e0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r6_bo42 *   447) [Q] /\
       eqmod lr_t (r6_to42 *   447) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r6_bo42 *   447) [Q] /\
       eqmod lr_t (r6_to42 *   447) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r6, r2, lr                               #! PC = 0x4013e4 *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4013e8 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4013ec *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4013f0 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013f4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4013f8 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x4013fc *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo42 *  1235) [Q] /\
       eqmod lr_t (r7_to42 *  1235) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo42 *  1235) [Q] /\
       eqmod lr_t (r7_to42 *  1235) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x401400 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401404 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401b20; Value = 0xd6dbc7b6; PC = 0x401408 *)
mov [r10, r11] [L0x401b20, L0x401b24];
(* smulwb	lr, r10, r8                              #! PC = 0x40140c *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401410 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401414 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x401418 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x40141c *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo42 *  -535) [Q] /\
       eqmod lr_t (r8_to42 *  -535) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo42 *  -535) [Q] /\
       eqmod lr_t (r8_to42 *  -535) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r4, lr                               #! PC = 0x401420 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x401424 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x401428 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x40142c *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401430 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401434 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x401438 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo42 * -1426) [Q] /\
       eqmod lr_t (r9_to42 * -1426) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo42 * -1426) [Q] /\
       eqmod lr_t (r9_to42 * -1426) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x40143c *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x401440 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;

assert [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
       [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
       [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
       [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [9*Q2,9*Q2,9*Q2,9*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
       [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
       [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
       [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [9*Q2,9*Q2,9*Q2,9*Q2]
    && [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2] /\
       [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2] /\
       [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2] /\
       [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2];


(* CUT 59 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo42, r2_to42] + [r6_bo42, r6_to42] * [  447,   447])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo42, r2_to42] - [r6_bo42, r6_to42] * [  447,   447])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r3_bo42, r3_to42] + [r7_bo42, r7_to42] * [ 1235,  1235])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo42, r3_to42] - [r7_bo42, r7_to42] * [ 1235,  1235])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r4_bo42, r4_to42] + [r8_bo42, r8_to42] * [ -535,  -535])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo42, r4_to42] - [r8_bo42, r8_to42] * [ -535,  -535])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r5_bo42, r5_to42] + [r9_bo42, r9_to42] * [-1426, -1426])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r5_bo42, r5_to42] - [r9_bo42, r9_to42] * [-1426, -1426])
          [r9_b, r9_t] [Q, Q] /\
    [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
    [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
    [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
    [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [9*Q2,9*Q2,9*Q2,9*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2] /\
    [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2] /\
    [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2] /\
    [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2];



ghost r2_bo43@int16, r2_to43@int16, r3_bo43@int16, r3_to43@int16,
      r4_bo43@int16, r4_to43@int16, r5_bo43@int16, r5_to43@int16,
      r6_bo43@int16, r6_to43@int16, r7_bo43@int16, r7_to43@int16,
      r8_bo43@int16, r8_to43@int16, r9_bo43@int16, r9_to43@int16:
      r2_bo43 = r2_b /\ r2_to43 = r2_t /\ r3_bo43 = r3_b /\ r3_to43 = r3_t /\
      r4_bo43 = r4_b /\ r4_to43 = r4_t /\ r5_bo43 = r5_b /\ r5_to43 = r5_t /\
      r6_bo43 = r6_b /\ r6_to43 = r6_t /\ r7_bo43 = r7_b /\ r7_to43 = r7_t /\
      r8_bo43 = r8_b /\ r8_to43 = r8_t /\ r9_bo43 = r9_b /\ r9_to43 = r9_t
   && r2_bo43 = r2_b /\ r2_to43 = r2_t /\ r3_bo43 = r3_b /\ r3_to43 = r3_t /\
      r4_bo43 = r4_b /\ r4_to43 = r4_t /\ r5_bo43 = r5_b /\ r5_to43 = r5_t /\
      r6_bo43 = r6_b /\ r6_to43 = r6_t /\ r7_bo43 = r7_b /\ r7_to43 = r7_t /\
      r8_bo43 = r8_b /\ r8_to43 = r8_t /\ r9_bo43 = r9_b /\ r9_to43 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401b28; Value = 0xbc7b58fa; PC = 0x401444 *)
mov [r10, r11] [L0x401b28, L0x401b2c];
(* smulwb	lr, r10, r2                              #! PC = 0x401448 *)
vpc r2_bW@int32 r2_b; mulj tmp r10 r2_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r2, r10, r2                              #! PC = 0x40144c *)
vpc r2_tW@int32 r2_t; mulj tmp r10 r2_tW; spl r2_w dc tmp 16;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b; vpc r2_t@int16 r2_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401450 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r2, r2, r12, r0                          #! PC = 0x401454 *)
mulj prod r2_b r12_t; add r2_w prod r0;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b;
(* pkhtb	r2, r2, lr, asr #16                       #! PC = 0x401458 *)
mov tmp_b lr_t; mov tmp_t r2_t;
mov r2_b tmp_b; mov r2_t tmp_t;
(* smulwb	lr, r11, r3                              #! PC = 0x40145c *)
vpc r3_bW@int32 r3_b; mulj tmp r11 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r11, r3                              #! PC = 0x401460 *)
vpc r3_tW@int32 r3_t; mulj tmp r11 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401464 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x401468 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x40146c *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov r3_b tmp_b; mov r3_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401b30; Value = 0x7345e6ef; PC = 0x401470 *)
mov [r10, r11] [L0x401b30, L0x401b34];
(* smulwb	lr, r10, r4                              #! PC = 0x401474 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401478 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40147c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401480 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x401484 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov r4_b tmp_b; mov r4_t tmp_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401488 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40148c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401490 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401494 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x401498 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov r5_b tmp_b; mov r5_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401b38; Value = 0x02ec153a; PC = 0x40149c *)
mov [r10, r11] [L0x401b38, L0x401b3c];
(* smulwb	lr, r10, r6                              #! PC = 0x4014a0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4014a4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014a8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4014ac *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x4014b0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov r6_b tmp_b; mov r6_t tmp_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4014b4 *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4014b8 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014bc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4014c0 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x4014c4 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov r7_b tmp_b; mov r7_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401b40; Value = 0x74e350fa; PC = 0x4014c8 *)
mov [r10, r11] [L0x401b40, L0x401b44];
(* smulwb	lr, r10, r8                              #! PC = 0x4014cc *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x4014d0 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014d4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4014d8 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x4014dc *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov r8_b tmp_b; mov r8_t tmp_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4014e0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4014e4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014e8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4014ec *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x4014f0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov r9_b tmp_b; mov r9_t tmp_t;

assert eqmod r2_b (r2_bo43 *  -878) [Q] /\
       eqmod r2_t (r2_to43 *  -878) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r2_b (r2_bo43 *  -878) [Q] /\
       eqmod r2_t (r2_to43 *  -878) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r2_b, r2_t] /\ [r2_b, r2_t] <s[Q2, Q2];


assert eqmod r3_b (r3_bo43 * -1152) [Q] /\
       eqmod r3_t (r3_to43 * -1152) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r3_b (r3_bo43 * -1152) [Q] /\
       eqmod r3_t (r3_to43 * -1152) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r3_b, r3_t] /\ [r3_b, r3_t] <s[Q2, Q2];


assert eqmod r4_b (r4_bo43 *  1499) [Q] /\
       eqmod r4_t (r4_to43 *  1499) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r4_b (r4_bo43 *  1499) [Q] /\
       eqmod r4_t (r4_to43 *  1499) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r4_b, r4_t] /\ [r4_b, r4_t] <s[Q2, Q2];


assert eqmod r5_b (r5_bo43 *   526) [Q] /\
       eqmod r5_t (r5_to43 *   526) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r5_b (r5_bo43 *   526) [Q] /\
       eqmod r5_t (r5_to43 *   526) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r5_b, r5_t] /\ [r5_b, r5_t] <s[Q2, Q2];


assert eqmod r6_b (r6_bo43 *    38) [Q] /\
       eqmod r6_t (r6_to43 *    38) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r6_b (r6_bo43 *    38) [Q] /\
       eqmod r6_t (r6_to43 *    38) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r6_b, r6_t] /\ [r6_b, r6_t] <s[Q2, Q2];


assert eqmod r7_b (r7_bo43 *  1066) [Q] /\
       eqmod r7_t (r7_to43 *  1066) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r7_b (r7_bo43 *  1066) [Q] /\
       eqmod r7_t (r7_to43 *  1066) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r7_b, r7_t] /\ [r7_b, r7_t] <s[Q2, Q2];


assert eqmod r8_b (r8_bo43 *  1520) [Q] /\
       eqmod r8_t (r8_to43 *  1520) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r8_b (r8_bo43 *  1520) [Q] /\
       eqmod r8_t (r8_to43 *  1520) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r8_b, r8_t] /\ [r8_b, r8_t] <s[Q2, Q2];


assert eqmod r9_b (r9_bo43 *  -637) [Q] /\
       eqmod r9_t (r9_to43 *  -637) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r9_b (r9_bo43 *  -637) [Q] /\
       eqmod r9_t (r9_to43 *  -637) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r9_b, r9_t] /\ [r9_b, r9_t] <s[Q2, Q2];

(* vmov	r0, s6                                     #! PC = 0x4014f4 *)
mov [r0_b, r0_t] [s6_b, s6_t];
(* str.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2ec; PC = 0x4014f8 *)
mov [L0xbefff2ec, L0xbefff2ee] [r6_b, r6_t];
(* str.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff32c; PC = 0x4014fc *)
mov [L0xbefff32c, L0xbefff32e] [r7_b, r7_t];
(* str.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff36c; PC = 0x401500 *)
mov [L0xbefff36c, L0xbefff36e] [r8_b, r8_t];
(* str.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff3ac; PC = 0x401504 *)
mov [L0xbefff3ac, L0xbefff3ae] [r9_b, r9_t];
(* str.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff22c; PC = 0x401508 *)
mov [L0xbefff22c, L0xbefff22e] [r3_b, r3_t];
(* str.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff26c; PC = 0x40150c *)
mov [L0xbefff26c, L0xbefff26e] [r4_b, r4_t];
(* str.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff2ac; PC = 0x401510 *)
mov [L0xbefff2ac, L0xbefff2ae] [r5_b, r5_t];
(* str.w	r2, [r0], #4                              #! EA = L0xbefff1ec; PC = 0x401514 *)
mov [L0xbefff1ec, L0xbefff1ee] [r2_b, r2_t];
(* vmov	lr, s14                                    #! PC = 0x401518 *)
mov [lr_b, lr_t] [s14_b, s14_t]; mov lr s14;
(* cmp.w	r0, lr                                    #! PC = 0x40151c *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x4012bc <invntt_fast+1552>              #! PC = 0x401520 *)
#bne.w	0x4012bc <invntt_fast+1552>              #! 0x401520 = 0x401520;

(* CUT 60 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo43, r2_to43] * [ -878,  -878])
          [L0xbefff1ec, L0xbefff1ee] [Q, Q] /\
    eqmod ([r3_bo43, r3_to43] * [-1152, -1152])
          [L0xbefff22c, L0xbefff22e] [Q, Q] /\
    eqmod ([r4_bo43, r4_to43] * [ 1499,  1499])
          [L0xbefff26c, L0xbefff26e] [Q, Q] /\
    eqmod ([r5_bo43, r5_to43] * [  526,   526])
          [L0xbefff2ac, L0xbefff2ae] [Q, Q] /\
    eqmod ([r6_bo43, r6_to43] * [   38,    38])
          [L0xbefff2ec, L0xbefff2ee] [Q, Q] /\
    eqmod ([r7_bo43, r7_to43] * [ 1066,  1066])
          [L0xbefff32c, L0xbefff32e] [Q, Q] /\
    eqmod ([r8_bo43, r8_to43] * [ 1520,  1520])
          [L0xbefff36c, L0xbefff36e] [Q, Q] /\
    eqmod ([r9_bo43, r9_to43] * [ -637,  -637])
          [L0xbefff3ac, L0xbefff3ae] [Q, Q] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff1ec,L0xbefff1ee,L0xbefff22c,L0xbefff22e] /\
    [L0xbefff1ec,L0xbefff1ee,L0xbefff22c,L0xbefff22e]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff26c,L0xbefff26e,L0xbefff2ac,L0xbefff2ae] /\
    [L0xbefff26c,L0xbefff26e,L0xbefff2ac,L0xbefff2ae]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff2ec,L0xbefff2ee,L0xbefff32c,L0xbefff32e] /\
    [L0xbefff2ec,L0xbefff2ee,L0xbefff32c,L0xbefff32e]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff36c,L0xbefff36e,L0xbefff3ac,L0xbefff3ae] /\
    [L0xbefff36c,L0xbefff36e,L0xbefff3ac,L0xbefff3ae]< [Q2,Q2,Q2,Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1ec,L0xbefff1ee,L0xbefff22c,L0xbefff22e] /\
    [L0xbefff1ec,L0xbefff1ee,L0xbefff22c,L0xbefff22e]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff26c,L0xbefff26e,L0xbefff2ac,L0xbefff2ae] /\
    [L0xbefff26c,L0xbefff26e,L0xbefff2ac,L0xbefff2ae]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2ec,L0xbefff2ee,L0xbefff32c,L0xbefff32e] /\
    [L0xbefff2ec,L0xbefff2ee,L0xbefff32c,L0xbefff32e]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff36c,L0xbefff36e,L0xbefff3ac,L0xbefff3ae] /\
    [L0xbefff36c,L0xbefff36e,L0xbefff3ac,L0xbefff3ae]<s[Q2,Q2,Q2,Q2];

(* vmov	s6, r0                                     #! PC = 0x4012bc *)
mov [s6_b, s6_t] [r0_b, r0_t];
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff1f0; Value = 0x038bffbb; PC = 0x4012c0 *)
mov [r2_b, r2_t] [L0xbefff1f0, L0xbefff1f2];
(* ldr.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff230; Value = 0xfae8fad6; PC = 0x4012c4 *)
mov [r3_b, r3_t] [L0xbefff230, L0xbefff232];
(* ldr.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff270; Value = 0x0235ee48; PC = 0x4012c8 *)
mov [r4_b, r4_t] [L0xbefff270, L0xbefff272];
(* ldr.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff2b0; Value = 0x06800897; PC = 0x4012cc *)
mov [r5_b, r5_t] [L0xbefff2b0, L0xbefff2b2];
(* ldr.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2f0; Value = 0x0c34fd9f; PC = 0x4012d0 *)
mov [r6_b, r6_t] [L0xbefff2f0, L0xbefff2f2];
(* ldr.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff330; Value = 0xf34dfe9c; PC = 0x4012d4 *)
mov [r7_b, r7_t] [L0xbefff330, L0xbefff332];
(* ldr.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff370; Value = 0xfd4e130e; PC = 0x4012d8 *)
mov [r8_b, r8_t] [L0xbefff370, L0xbefff372];
(* ldr.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff3b0; Value = 0xf8fff38f; PC = 0x4012dc *)
mov [r9_b, r9_t] [L0xbefff3b0, L0xbefff3b2];

ghost r2_bo44@int16, r2_to44@int16, r3_bo44@int16, r3_to44@int16,
      r4_bo44@int16, r4_to44@int16, r5_bo44@int16, r5_to44@int16,
      r6_bo44@int16, r6_to44@int16, r7_bo44@int16, r7_to44@int16,
      r8_bo44@int16, r8_to44@int16, r9_bo44@int16, r9_to44@int16:
      r2_bo44 = r2_b /\ r2_to44 = r2_t /\ r3_bo44 = r3_b /\ r3_to44 = r3_t /\
      r4_bo44 = r4_b /\ r4_to44 = r4_t /\ r5_bo44 = r5_b /\ r5_to44 = r5_t /\
      r6_bo44 = r6_b /\ r6_to44 = r6_t /\ r7_bo44 = r7_b /\ r7_to44 = r7_t /\
      r8_bo44 = r8_b /\ r8_to44 = r8_t /\ r9_bo44 = r9_b /\ r9_to44 = r9_t
   && r2_bo44 = r2_b /\ r2_to44 = r2_t /\ r3_bo44 = r3_b /\ r3_to44 = r3_t /\
      r4_bo44 = r4_b /\ r4_to44 = r4_t /\ r5_bo44 = r5_b /\ r5_to44 = r5_t /\
      r6_bo44 = r6_b /\ r6_to44 = r6_t /\ r7_bo44 = r7_b /\ r7_to44 = r7_t /\
      r8_bo44 = r8_b /\ r8_to44 = r8_t /\ r9_bo44 = r9_b /\ r9_to44 = r9_t;

(* movw	r0, #26632	; 0x6808                        #! PC = 0x4012e0 *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* ldr.w	r10, [r1], #4                             #! EA = L0x401b48; Value = 0xf12886bb; PC = 0x4012e4 *)
mov r10 L0x401b48;
(* smulwb	lr, r10, r3                              #! PC = 0x4012e8 *)
vpc r3_bW@int32 r3_b; mulj tmp r10 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r10, r3                              #! PC = 0x4012ec *)
vpc r3_tW@int32 r3_t; mulj tmp r10 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4012f0 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4012f4 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x4012f8 *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r3_bo44 *  -193) [Q] /\
       eqmod lr_t (r3_to44 *  -193) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r3_bo44 *  -193) [Q] /\
       eqmod lr_t (r3_to44 *  -193) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r3, r2, lr                               #! PC = 0x4012fc *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401300 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x401304 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x401308 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40130c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401310 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401314 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo44 *  -193) [Q] /\
       eqmod lr_t (r5_to44 *  -193) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo44 *  -193) [Q] /\
       eqmod lr_t (r5_to44 *  -193) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r4, lr                               #! PC = 0x401318 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x40131c *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x401320 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x401324 *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401328 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x40132c *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x401330 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo44 *  -193) [Q] /\
       eqmod lr_t (r7_to44 *  -193) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo44 *  -193) [Q] /\
       eqmod lr_t (r7_to44 *  -193) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r6, lr                               #! PC = 0x401334 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x401338 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x40133c *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x401340 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401344 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401348 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x40134c *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo44 *  -193) [Q] /\
       eqmod lr_t (r9_to44 *  -193) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo44 *  -193) [Q] /\
       eqmod lr_t (r9_to44 *  -193) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r8, lr                               #! PC = 0x401350 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x401354 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

assert [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [6*Q2,6*Q2,6*Q2,6*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [6*Q2,6*Q2,6*Q2,6*Q2]
    && [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2];


(* CUT 61 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo44, r2_to44] + [r3_bo44, r3_to44] * [ -193,  -193])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo44, r2_to44] - [r3_bo44, r3_to44] * [ -193,  -193])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r4_bo44, r4_to44] + [r5_bo44, r5_to44] * [ -193,  -193])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo44, r4_to44] - [r5_bo44, r5_to44] * [ -193,  -193])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo44, r6_to44] + [r7_bo44, r7_to44] * [ -193,  -193])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo44, r6_to44] - [r7_bo44, r7_to44] * [ -193,  -193])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r8_bo44, r8_to44] + [r9_bo44, r9_to44] * [ -193,  -193])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r8_bo44, r8_to44] - [r9_bo44, r9_to44] * [ -193,  -193])
          [r9_b, r9_t] [Q, Q] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [6*Q2,6*Q2,6*Q2,6*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2]
    prove with [cuts [1, 3, 5, 7, 9, 11, 13, 15]];



ghost r2_bo45@int16, r2_to45@int16, r3_bo45@int16, r3_to45@int16,
      r4_bo45@int16, r4_to45@int16, r5_bo45@int16, r5_to45@int16,
      r6_bo45@int16, r6_to45@int16, r7_bo45@int16, r7_to45@int16,
      r8_bo45@int16, r8_to45@int16, r9_bo45@int16, r9_to45@int16:
      r2_bo45 = r2_b /\ r2_to45 = r2_t /\ r3_bo45 = r3_b /\ r3_to45 = r3_t /\
      r4_bo45 = r4_b /\ r4_to45 = r4_t /\ r5_bo45 = r5_b /\ r5_to45 = r5_t /\
      r6_bo45 = r6_b /\ r6_to45 = r6_t /\ r7_bo45 = r7_b /\ r7_to45 = r7_t /\
      r8_bo45 = r8_b /\ r8_to45 = r8_t /\ r9_bo45 = r9_b /\ r9_to45 = r9_t
   && r2_bo45 = r2_b /\ r2_to45 = r2_t /\ r3_bo45 = r3_b /\ r3_to45 = r3_t /\
      r4_bo45 = r4_b /\ r4_to45 = r4_t /\ r5_bo45 = r5_b /\ r5_to45 = r5_t /\
      r6_bo45 = r6_b /\ r6_to45 = r6_t /\ r7_bo45 = r7_b /\ r7_to45 = r7_t /\
      r8_bo45 = r8_b /\ r8_to45 = r8_t /\ r9_bo45 = r9_b /\ r9_to45 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401b4c; Value = 0xfbb18fe2; PC = 0x401358 *)
mov [r10, r11] [L0x401b4c, L0x401b50];
(* smulwb	lr, r10, r4                              #! PC = 0x40135c *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401360 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401364 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401368 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x40136c *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_bo45 *   -56) [Q] /\
       eqmod lr_t (r4_to45 *   -56) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_bo45 *   -56) [Q] /\
       eqmod lr_t (r4_to45 *   -56) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r4, r2, lr                               #! PC = 0x401370 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401374 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401378 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40137c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401380 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401384 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401388 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo45 *   283) [Q] /\
       eqmod lr_t (r5_to45 *   283) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo45 *   283) [Q] /\
       eqmod lr_t (r5_to45 *   283) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x40138c *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401390 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x401394 *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401398 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40139c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4013a0 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x4013a4 *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo45 *   -56) [Q] /\
       eqmod lr_t (r8_to45 *   -56) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo45 *   -56) [Q] /\
       eqmod lr_t (r8_to45 *   -56) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r6, lr                               #! PC = 0x4013a8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x4013ac *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4013b0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4013b4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013b8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4013bc *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4013c0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo45 *   283) [Q] /\
       eqmod lr_t (r9_to45 *   283) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo45 *   283) [Q] /\
       eqmod lr_t (r9_to45 *   283) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x4013c4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x4013c8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;

assert [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
       prove with [algebra solver isl] && true;
assume [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
    && [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2];


(* CUT 62 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo45, r2_to45] + [r4_bo45, r4_to45] * [  -56,   -56])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo45, r2_to45] - [r4_bo45, r4_to45] * [  -56,   -56])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r3_bo45, r3_to45] + [r5_bo45, r5_to45] * [  283,   283])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo45, r3_to45] - [r5_bo45, r5_to45] * [  283,   283])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo45, r6_to45] + [r8_bo45, r8_to45] * [  -56,   -56])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo45, r6_to45] - [r8_bo45, r8_to45] * [  -56,   -56])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r7_bo45, r7_to45] + [r9_bo45, r9_to45] * [  283,   283])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r7_bo45, r7_to45] - [r9_bo45, r9_to45] * [  283,   283])
          [r9_b, r9_t] [Q, Q] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2];



ghost r2_bo46@int16, r2_to46@int16, r3_bo46@int16, r3_to46@int16,
      r4_bo46@int16, r4_to46@int16, r5_bo46@int16, r5_to46@int16,
      r6_bo46@int16, r6_to46@int16, r7_bo46@int16, r7_to46@int16,
      r8_bo46@int16, r8_to46@int16, r9_bo46@int16, r9_to46@int16:
      r2_bo46 = r2_b /\ r2_to46 = r2_t /\ r3_bo46 = r3_b /\ r3_to46 = r3_t /\
      r4_bo46 = r4_b /\ r4_to46 = r4_t /\ r5_bo46 = r5_b /\ r5_to46 = r5_t /\
      r6_bo46 = r6_b /\ r6_to46 = r6_t /\ r7_bo46 = r7_b /\ r7_to46 = r7_t /\
      r8_bo46 = r8_b /\ r8_to46 = r8_t /\ r9_bo46 = r9_b /\ r9_to46 = r9_t
   && r2_bo46 = r2_b /\ r2_to46 = r2_t /\ r3_bo46 = r3_b /\ r3_to46 = r3_t /\
      r4_bo46 = r4_b /\ r4_to46 = r4_t /\ r5_bo46 = r5_b /\ r5_to46 = r5_t /\
      r6_bo46 = r6_b /\ r6_to46 = r6_t /\ r7_bo46 = r7_b /\ r7_to46 = r7_t /\
      r8_bo46 = r8_b /\ r8_to46 = r8_t /\ r9_bo46 = r9_b /\ r9_to46 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401b54; Value = 0xcf663338; PC = 0x4013cc *)
mov [r10, r11] [L0x401b54, L0x401b58];
(* smulwb	lr, r10, r6                              #! PC = 0x4013d0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4013d4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013d8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4013dc *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x4013e0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r6_bo46 *  -632) [Q] /\
       eqmod lr_t (r6_to46 *  -632) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r6_bo46 *  -632) [Q] /\
       eqmod lr_t (r6_to46 *  -632) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r6, r2, lr                               #! PC = 0x4013e4 *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4013e8 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4013ec *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4013f0 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013f4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4013f8 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x4013fc *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo46 *  1352) [Q] /\
       eqmod lr_t (r7_to46 *  1352) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo46 *  1352) [Q] /\
       eqmod lr_t (r7_to46 *  1352) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x401400 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401404 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401b5c; Value = 0x3ec0189c; PC = 0x401408 *)
mov [r10, r11] [L0x401b5c, L0x401b60];
(* smulwb	lr, r10, r8                              #! PC = 0x40140c *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401410 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401414 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x401418 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x40141c *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo46 *   816) [Q] /\
       eqmod lr_t (r8_to46 *   816) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo46 *   816) [Q] /\
       eqmod lr_t (r8_to46 *   816) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r4, lr                               #! PC = 0x401420 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x401424 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x401428 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x40142c *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401430 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401434 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x401438 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo46 *  -650) [Q] /\
       eqmod lr_t (r9_to46 *  -650) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo46 *  -650) [Q] /\
       eqmod lr_t (r9_to46 *  -650) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x40143c *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x401440 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;

assert [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
    && [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2];


(* CUT 63 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo46, r2_to46] + [r6_bo46, r6_to46] * [ -632,  -632])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo46, r2_to46] - [r6_bo46, r6_to46] * [ -632,  -632])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r3_bo46, r3_to46] + [r7_bo46, r7_to46] * [ 1352,  1352])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo46, r3_to46] - [r7_bo46, r7_to46] * [ 1352,  1352])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r4_bo46, r4_to46] + [r8_bo46, r8_to46] * [  816,   816])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo46, r4_to46] - [r8_bo46, r8_to46] * [  816,   816])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r5_bo46, r5_to46] + [r9_bo46, r9_to46] * [ -650,  -650])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r5_bo46, r5_to46] - [r9_bo46, r9_to46] * [ -650,  -650])
          [r9_b, r9_t] [Q, Q] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2];



ghost r2_bo47@int16, r2_to47@int16, r3_bo47@int16, r3_to47@int16,
      r4_bo47@int16, r4_to47@int16, r5_bo47@int16, r5_to47@int16,
      r6_bo47@int16, r6_to47@int16, r7_bo47@int16, r7_to47@int16,
      r8_bo47@int16, r8_to47@int16, r9_bo47@int16, r9_to47@int16:
      r2_bo47 = r2_b /\ r2_to47 = r2_t /\ r3_bo47 = r3_b /\ r3_to47 = r3_t /\
      r4_bo47 = r4_b /\ r4_to47 = r4_t /\ r5_bo47 = r5_b /\ r5_to47 = r5_t /\
      r6_bo47 = r6_b /\ r6_to47 = r6_t /\ r7_bo47 = r7_b /\ r7_to47 = r7_t /\
      r8_bo47 = r8_b /\ r8_to47 = r8_t /\ r9_bo47 = r9_b /\ r9_to47 = r9_t
   && r2_bo47 = r2_b /\ r2_to47 = r2_t /\ r3_bo47 = r3_b /\ r3_to47 = r3_t /\
      r4_bo47 = r4_b /\ r4_to47 = r4_t /\ r5_bo47 = r5_b /\ r5_to47 = r5_t /\
      r6_bo47 = r6_b /\ r6_to47 = r6_t /\ r7_bo47 = r7_b /\ r7_to47 = r7_t /\
      r8_bo47 = r8_b /\ r8_to47 = r8_t /\ r9_bo47 = r9_b /\ r9_to47 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401b64; Value = 0x1a255f97; PC = 0x401444 *)
mov [r10, r11] [L0x401b64, L0x401b68];
(* smulwb	lr, r10, r2                              #! PC = 0x401448 *)
vpc r2_bW@int32 r2_b; mulj tmp r10 r2_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r2, r10, r2                              #! PC = 0x40144c *)
vpc r2_tW@int32 r2_t; mulj tmp r10 r2_tW; spl r2_w dc tmp 16;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b; vpc r2_t@int16 r2_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401450 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r2, r2, r12, r0                          #! PC = 0x401454 *)
mulj prod r2_b r12_t; add r2_w prod r0;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b;
(* pkhtb	r2, r2, lr, asr #16                       #! PC = 0x401458 *)
mov tmp_b lr_t; mov tmp_t r2_t;
mov r2_b tmp_b; mov r2_t tmp_t;
(* smulwb	lr, r11, r3                              #! PC = 0x40145c *)
vpc r3_bW@int32 r3_b; mulj tmp r11 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r11, r3                              #! PC = 0x401460 *)
vpc r3_tW@int32 r3_t; mulj tmp r11 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401464 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x401468 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x40146c *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov r3_b tmp_b; mov r3_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401b6c; Value = 0x15d6ef78; PC = 0x401470 *)
mov [r10, r11] [L0x401b6c, L0x401b70];
(* smulwb	lr, r10, r4                              #! PC = 0x401474 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401478 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40147c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401480 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x401484 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov r4_b tmp_b; mov r4_t tmp_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401488 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40148c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401490 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401494 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x401498 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov r5_b tmp_b; mov r5_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401b74; Value = 0x69956aaa; PC = 0x40149c *)
mov [r10, r11] [L0x401b74, L0x401b78];
(* smulwb	lr, r10, r6                              #! PC = 0x4014a0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4014a4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014a8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4014ac *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x4014b0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov r6_b tmp_b; mov r6_t tmp_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4014b4 *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4014b8 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014bc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4014c0 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x4014c4 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov r7_b tmp_b; mov r7_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401b7c; Value = 0x7f58aa6a; PC = 0x4014c8 *)
mov [r10, r11] [L0x401b7c, L0x401b80];
(* smulwb	lr, r10, r8                              #! PC = 0x4014cc *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x4014d0 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014d4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4014d8 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x4014dc *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov r8_b tmp_b; mov r8_t tmp_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4014e0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4014e4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014e8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4014ec *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x4014f0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov r9_b tmp_b; mov r9_t tmp_t;

assert eqmod r2_b (r2_bo47 *   340) [Q] /\
       eqmod r2_t (r2_to47 *   340) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r2_b (r2_bo47 *   340) [Q] /\
       eqmod r2_t (r2_to47 *   340) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r2_b, r2_t] /\ [r2_b, r2_t] <s[Q2, Q2];


assert eqmod r3_b (r3_bo47 *  1303) [Q] /\
       eqmod r3_t (r3_to47 *  1303) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r3_b (r3_bo47 *  1303) [Q] /\
       eqmod r3_t (r3_to47 *  1303) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r3_b, r3_t] /\ [r3_b, r3_t] <s[Q2, Q2];


assert eqmod r4_b (r4_bo47 *   284) [Q] /\
       eqmod r4_t (r4_to47 *   284) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r4_b (r4_bo47 *   284) [Q] /\
       eqmod r4_t (r4_to47 *   284) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r4_b, r4_t] /\ [r4_b, r4_t] <s[Q2, Q2];


assert eqmod r5_b (r5_bo47 * -1144) [Q] /\
       eqmod r5_t (r5_to47 * -1144) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r5_b (r5_bo47 * -1144) [Q] /\
       eqmod r5_t (r5_to47 * -1144) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r5_b, r5_t] /\ [r5_b, r5_t] <s[Q2, Q2];


assert eqmod r6_b (r6_bo47 *  1373) [Q] /\
       eqmod r6_t (r6_to47 *  1373) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r6_b (r6_bo47 *  1373) [Q] /\
       eqmod r6_t (r6_to47 *  1373) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r6_b, r6_t] /\ [r6_b, r6_t] <s[Q2, Q2];


assert eqmod r7_b (r7_bo47 *   846) [Q] /\
       eqmod r7_t (r7_to47 *   846) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r7_b (r7_bo47 *   846) [Q] /\
       eqmod r7_t (r7_to47 *   846) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r7_b, r7_t] /\ [r7_b, r7_t] <s[Q2, Q2];


assert eqmod r8_b (r8_bo47 *  1656) [Q] /\
       eqmod r8_t (r8_to47 *  1656) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r8_b (r8_bo47 *  1656) [Q] /\
       eqmod r8_t (r8_to47 *  1656) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r8_b, r8_t] /\ [r8_b, r8_t] <s[Q2, Q2];


assert eqmod r9_b (r9_bo47 *   550) [Q] /\
       eqmod r9_t (r9_to47 *   550) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r9_b (r9_bo47 *   550) [Q] /\
       eqmod r9_t (r9_to47 *   550) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r9_b, r9_t] /\ [r9_b, r9_t] <s[Q2, Q2];

(* vmov	r0, s6                                     #! PC = 0x4014f4 *)
mov [r0_b, r0_t] [s6_b, s6_t];
(* str.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2f0; PC = 0x4014f8 *)
mov [L0xbefff2f0, L0xbefff2f2] [r6_b, r6_t];
(* str.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff330; PC = 0x4014fc *)
mov [L0xbefff330, L0xbefff332] [r7_b, r7_t];
(* str.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff370; PC = 0x401500 *)
mov [L0xbefff370, L0xbefff372] [r8_b, r8_t];
(* str.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff3b0; PC = 0x401504 *)
mov [L0xbefff3b0, L0xbefff3b2] [r9_b, r9_t];
(* str.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff230; PC = 0x401508 *)
mov [L0xbefff230, L0xbefff232] [r3_b, r3_t];
(* str.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff270; PC = 0x40150c *)
mov [L0xbefff270, L0xbefff272] [r4_b, r4_t];
(* str.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff2b0; PC = 0x401510 *)
mov [L0xbefff2b0, L0xbefff2b2] [r5_b, r5_t];
(* str.w	r2, [r0], #4                              #! EA = L0xbefff1f0; PC = 0x401514 *)
mov [L0xbefff1f0, L0xbefff1f2] [r2_b, r2_t];
(* vmov	lr, s14                                    #! PC = 0x401518 *)
mov [lr_b, lr_t] [s14_b, s14_t]; mov lr s14;
(* cmp.w	r0, lr                                    #! PC = 0x40151c *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x4012bc <invntt_fast+1552>              #! PC = 0x401520 *)
#bne.w	0x4012bc <invntt_fast+1552>              #! 0x401520 = 0x401520;

(* CUT 64 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo47, r2_to47] * [  340,   340])
          [L0xbefff1f0, L0xbefff1f2] [Q, Q] /\
    eqmod ([r3_bo47, r3_to47] * [ 1303,  1303])
          [L0xbefff230, L0xbefff232] [Q, Q] /\
    eqmod ([r4_bo47, r4_to47] * [  284,   284])
          [L0xbefff270, L0xbefff272] [Q, Q] /\
    eqmod ([r5_bo47, r5_to47] * [-1144, -1144])
          [L0xbefff2b0, L0xbefff2b2] [Q, Q] /\
    eqmod ([r6_bo47, r6_to47] * [ 1373,  1373])
          [L0xbefff2f0, L0xbefff2f2] [Q, Q] /\
    eqmod ([r7_bo47, r7_to47] * [  846,   846])
          [L0xbefff330, L0xbefff332] [Q, Q] /\
    eqmod ([r8_bo47, r8_to47] * [ 1656,  1656])
          [L0xbefff370, L0xbefff372] [Q, Q] /\
    eqmod ([r9_bo47, r9_to47] * [  550,   550])
          [L0xbefff3b0, L0xbefff3b2] [Q, Q] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff1f0,L0xbefff1f2,L0xbefff230,L0xbefff232] /\
    [L0xbefff1f0,L0xbefff1f2,L0xbefff230,L0xbefff232]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff270,L0xbefff272,L0xbefff2b0,L0xbefff2b2] /\
    [L0xbefff270,L0xbefff272,L0xbefff2b0,L0xbefff2b2]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff2f0,L0xbefff2f2,L0xbefff330,L0xbefff332] /\
    [L0xbefff2f0,L0xbefff2f2,L0xbefff330,L0xbefff332]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff370,L0xbefff372,L0xbefff3b0,L0xbefff3b2] /\
    [L0xbefff370,L0xbefff372,L0xbefff3b0,L0xbefff3b2]< [Q2,Q2,Q2,Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1f0,L0xbefff1f2,L0xbefff230,L0xbefff232] /\
    [L0xbefff1f0,L0xbefff1f2,L0xbefff230,L0xbefff232]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff270,L0xbefff272,L0xbefff2b0,L0xbefff2b2] /\
    [L0xbefff270,L0xbefff272,L0xbefff2b0,L0xbefff2b2]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2f0,L0xbefff2f2,L0xbefff330,L0xbefff332] /\
    [L0xbefff2f0,L0xbefff2f2,L0xbefff330,L0xbefff332]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff370,L0xbefff372,L0xbefff3b0,L0xbefff3b2] /\
    [L0xbefff370,L0xbefff372,L0xbefff3b0,L0xbefff3b2]<s[Q2,Q2,Q2,Q2];

(* vmov	s6, r0                                     #! PC = 0x4012bc *)
mov [s6_b, s6_t] [r0_b, r0_t];
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff1f4; Value = 0x1bd910cd; PC = 0x4012c0 *)
mov [r2_b, r2_t] [L0xbefff1f4, L0xbefff1f6];
(* ldr.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff234; Value = 0xec22283c; PC = 0x4012c4 *)
mov [r3_b, r3_t] [L0xbefff234, L0xbefff236];
(* ldr.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff274; Value = 0xef5f046a; PC = 0x4012c8 *)
mov [r4_b, r4_t] [L0xbefff274, L0xbefff276];
(* ldr.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff2b4; Value = 0xe3e52de0; PC = 0x4012cc *)
mov [r5_b, r5_t] [L0xbefff2b4, L0xbefff2b6];
(* ldr.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2f4; Value = 0xed5c27f4; PC = 0x4012d0 *)
mov [r6_b, r6_t] [L0xbefff2f4, L0xbefff2f6];
(* ldr.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff334; Value = 0xf6a4fbdc; PC = 0x4012d4 *)
mov [r7_b, r7_t] [L0xbefff334, L0xbefff336];
(* ldr.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff374; Value = 0xfaebda87; PC = 0x4012d8 *)
mov [r8_b, r8_t] [L0xbefff374, L0xbefff376];
(* ldr.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff3b4; Value = 0xdc1bd1ad; PC = 0x4012dc *)
mov [r9_b, r9_t] [L0xbefff3b4, L0xbefff3b6];

ghost r2_bo48@int16, r2_to48@int16, r3_bo48@int16, r3_to48@int16,
      r4_bo48@int16, r4_to48@int16, r5_bo48@int16, r5_to48@int16,
      r6_bo48@int16, r6_to48@int16, r7_bo48@int16, r7_to48@int16,
      r8_bo48@int16, r8_to48@int16, r9_bo48@int16, r9_to48@int16:
      r2_bo48 = r2_b /\ r2_to48 = r2_t /\ r3_bo48 = r3_b /\ r3_to48 = r3_t /\
      r4_bo48 = r4_b /\ r4_to48 = r4_t /\ r5_bo48 = r5_b /\ r5_to48 = r5_t /\
      r6_bo48 = r6_b /\ r6_to48 = r6_t /\ r7_bo48 = r7_b /\ r7_to48 = r7_t /\
      r8_bo48 = r8_b /\ r8_to48 = r8_t /\ r9_bo48 = r9_b /\ r9_to48 = r9_t
   && r2_bo48 = r2_b /\ r2_to48 = r2_t /\ r3_bo48 = r3_b /\ r3_to48 = r3_t /\
      r4_bo48 = r4_b /\ r4_to48 = r4_t /\ r5_bo48 = r5_b /\ r5_to48 = r5_t /\
      r6_bo48 = r6_b /\ r6_to48 = r6_t /\ r7_bo48 = r7_b /\ r7_to48 = r7_t /\
      r8_bo48 = r8_b /\ r8_to48 = r8_t /\ r9_bo48 = r9_b /\ r9_to48 = r9_t;

(* movw	r0, #26632	; 0x6808                        #! PC = 0x4012e0 *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* ldr.w	r10, [r1], #4                             #! EA = L0x401b84; Value = 0x39991b9c; PC = 0x4012e4 *)
mov r10 L0x401b84;
(* smulwb	lr, r10, r3                              #! PC = 0x4012e8 *)
vpc r3_bW@int32 r3_b; mulj tmp r10 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r10, r3                              #! PC = 0x4012ec *)
vpc r3_tW@int32 r3_t; mulj tmp r10 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4012f0 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4012f4 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x4012f8 *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r3_bo48 *   749) [Q] /\
       eqmod lr_t (r3_to48 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r3_bo48 *   749) [Q] /\
       eqmod lr_t (r3_to48 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r3, r2, lr                               #! PC = 0x4012fc *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401300 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x401304 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x401308 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40130c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401310 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401314 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo48 *   749) [Q] /\
       eqmod lr_t (r5_to48 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo48 *   749) [Q] /\
       eqmod lr_t (r5_to48 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r4, lr                               #! PC = 0x401318 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x40131c *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x401320 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x401324 *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401328 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x40132c *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x401330 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo48 *   749) [Q] /\
       eqmod lr_t (r7_to48 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo48 *   749) [Q] /\
       eqmod lr_t (r7_to48 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r6, lr                               #! PC = 0x401334 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x401338 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x40133c *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x401340 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401344 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401348 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x40134c *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo48 *   749) [Q] /\
       eqmod lr_t (r9_to48 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo48 *   749) [Q] /\
       eqmod lr_t (r9_to48 *   749) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r8, lr                               #! PC = 0x401350 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x401354 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

assert [10*NQ2,10*NQ2,10*NQ2,10*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [10*Q2,10*Q2,10*Q2,10*Q2] /\
       [10*NQ2,10*NQ2,10*NQ2,10*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [10*Q2,10*Q2,10*Q2,10*Q2] /\
       [10*NQ2,10*NQ2,10*NQ2,10*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [10*Q2,10*Q2,10*Q2,10*Q2] /\
       [10*NQ2,10*NQ2,10*NQ2,10*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [10*Q2,10*Q2,10*Q2,10*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [10*NQ2,10*NQ2,10*NQ2,10*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [10*Q2,10*Q2,10*Q2,10*Q2] /\
       [10*NQ2,10*NQ2,10*NQ2,10*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [10*Q2,10*Q2,10*Q2,10*Q2] /\
       [10*NQ2,10*NQ2,10*NQ2,10*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [10*Q2,10*Q2,10*Q2,10*Q2] /\
       [10*NQ2,10*NQ2,10*NQ2,10*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [10*Q2,10*Q2,10*Q2,10*Q2]
    && [10@16*NQ2,10@16*NQ2,10@16*NQ2,10@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[10@16*Q2,10@16*Q2,10@16*Q2,10@16*Q2] /\
       [10@16*NQ2,10@16*NQ2,10@16*NQ2,10@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[10@16*Q2,10@16*Q2,10@16*Q2,10@16*Q2] /\
       [10@16*NQ2,10@16*NQ2,10@16*NQ2,10@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[10@16*Q2,10@16*Q2,10@16*Q2,10@16*Q2] /\
       [10@16*NQ2,10@16*NQ2,10@16*NQ2,10@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[10@16*Q2,10@16*Q2,10@16*Q2,10@16*Q2];


(* CUT 65 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo48, r2_to48] + [r3_bo48, r3_to48] * [  749,   749])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo48, r2_to48] - [r3_bo48, r3_to48] * [  749,   749])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r4_bo48, r4_to48] + [r5_bo48, r5_to48] * [  749,   749])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo48, r4_to48] - [r5_bo48, r5_to48] * [  749,   749])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo48, r6_to48] + [r7_bo48, r7_to48] * [  749,   749])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo48, r6_to48] - [r7_bo48, r7_to48] * [  749,   749])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r8_bo48, r8_to48] + [r9_bo48, r9_to48] * [  749,   749])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r8_bo48, r8_to48] - [r9_bo48, r9_to48] * [  749,   749])
          [r9_b, r9_t] [Q, Q] /\
    [10*NQ2,10*NQ2,10*NQ2,10*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [10*Q2,10*Q2,10*Q2,10*Q2] /\
    [10*NQ2,10*NQ2,10*NQ2,10*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [10*Q2,10*Q2,10*Q2,10*Q2] /\
    [10*NQ2,10*NQ2,10*NQ2,10*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [10*Q2,10*Q2,10*Q2,10*Q2] /\
    [10*NQ2,10*NQ2,10*NQ2,10*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [10*Q2,10*Q2,10*Q2,10*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [10@16*NQ2,10@16*NQ2,10@16*NQ2,10@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[10@16*Q2,10@16*Q2,10@16*Q2,10@16*Q2] /\
    [10@16*NQ2,10@16*NQ2,10@16*NQ2,10@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[10@16*Q2,10@16*Q2,10@16*Q2,10@16*Q2] /\
    [10@16*NQ2,10@16*NQ2,10@16*NQ2,10@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[10@16*Q2,10@16*Q2,10@16*Q2,10@16*Q2] /\
    [10@16*NQ2,10@16*NQ2,10@16*NQ2,10@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[10@16*Q2,10@16*Q2,10@16*Q2,10@16*Q2]
    prove with [cuts [1, 3, 5, 7, 9, 11, 13, 15]];



ghost r2_bo49@int16, r2_to49@int16, r3_bo49@int16, r3_to49@int16,
      r4_bo49@int16, r4_to49@int16, r5_bo49@int16, r5_to49@int16,
      r6_bo49@int16, r6_to49@int16, r7_bo49@int16, r7_to49@int16,
      r8_bo49@int16, r8_to49@int16, r9_bo49@int16, r9_to49@int16:
      r2_bo49 = r2_b /\ r2_to49 = r2_t /\ r3_bo49 = r3_b /\ r3_to49 = r3_t /\
      r4_bo49 = r4_b /\ r4_to49 = r4_t /\ r5_bo49 = r5_b /\ r5_to49 = r5_t /\
      r6_bo49 = r6_b /\ r6_to49 = r6_t /\ r7_bo49 = r7_b /\ r7_to49 = r7_t /\
      r8_bo49 = r8_b /\ r8_to49 = r8_t /\ r9_bo49 = r9_b /\ r9_to49 = r9_t
   && r2_bo49 = r2_b /\ r2_to49 = r2_t /\ r3_bo49 = r3_b /\ r3_to49 = r3_t /\
      r4_bo49 = r4_b /\ r4_to49 = r4_t /\ r5_bo49 = r5_b /\ r5_to49 = r5_t /\
      r6_bo49 = r6_b /\ r6_to49 = r6_t /\ r7_bo49 = r7_b /\ r7_to49 = r7_t /\
      r8_bo49 = r8_b /\ r8_to49 = r8_t /\ r9_bo49 = r9_b /\ r9_to49 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401b88; Value = 0xcf8d92a6; PC = 0x401358 *)
mov [r10, r11] [L0x401b88, L0x401b8c];
(* smulwb	lr, r10, r4                              #! PC = 0x40135c *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401360 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401364 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401368 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x40136c *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_bo49 *  -630) [Q] /\
       eqmod lr_t (r4_to49 *  -630) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_bo49 *  -630) [Q] /\
       eqmod lr_t (r4_to49 *  -630) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r4, r2, lr                               #! PC = 0x401370 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401374 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401378 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40137c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401380 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401384 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401388 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo49 *   687) [Q] /\
       eqmod lr_t (r5_to49 *   687) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo49 *   687) [Q] /\
       eqmod lr_t (r5_to49 *   687) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x40138c *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401390 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x401394 *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401398 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40139c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4013a0 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x4013a4 *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo49 *  -630) [Q] /\
       eqmod lr_t (r8_to49 *  -630) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo49 *  -630) [Q] /\
       eqmod lr_t (r8_to49 *  -630) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r6, lr                               #! PC = 0x4013a8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x4013ac *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4013b0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4013b4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013b8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4013bc *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4013c0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo49 *   687) [Q] /\
       eqmod lr_t (r9_to49 *   687) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo49 *   687) [Q] /\
       eqmod lr_t (r9_to49 *   687) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x4013c4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x4013c8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;

assert [11*NQ2,11*NQ2,11*NQ2,11*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [11*Q2,11*Q2,11*Q2,11*Q2] /\
       [11*NQ2,11*NQ2,11*NQ2,11*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [11*Q2,11*Q2,11*Q2,11*Q2] /\
       [11*NQ2,11*NQ2,11*NQ2,11*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [11*Q2,11*Q2,11*Q2,11*Q2] /\
       [11*NQ2,11*NQ2,11*NQ2,11*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [11*Q2,11*Q2,11*Q2,11*Q2]
       prove with [algebra solver isl] && true;
assume [11*NQ2,11*NQ2,11*NQ2,11*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [11*Q2,11*Q2,11*Q2,11*Q2] /\
       [11*NQ2,11*NQ2,11*NQ2,11*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [11*Q2,11*Q2,11*Q2,11*Q2] /\
       [11*NQ2,11*NQ2,11*NQ2,11*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [11*Q2,11*Q2,11*Q2,11*Q2] /\
       [11*NQ2,11*NQ2,11*NQ2,11*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [11*Q2,11*Q2,11*Q2,11*Q2]
    && [11@16*NQ2,11@16*NQ2,11@16*NQ2,11@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[11@16*Q2,11@16*Q2,11@16*Q2,11@16*Q2] /\
       [11@16*NQ2,11@16*NQ2,11@16*NQ2,11@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[11@16*Q2,11@16*Q2,11@16*Q2,11@16*Q2] /\
       [11@16*NQ2,11@16*NQ2,11@16*NQ2,11@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[11@16*Q2,11@16*Q2,11@16*Q2,11@16*Q2] /\
       [11@16*NQ2,11@16*NQ2,11@16*NQ2,11@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[11@16*Q2,11@16*Q2,11@16*Q2,11@16*Q2];


(* CUT 66 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo49, r2_to49] + [r4_bo49, r4_to49] * [ -630,  -630])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo49, r2_to49] - [r4_bo49, r4_to49] * [ -630,  -630])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r3_bo49, r3_to49] + [r5_bo49, r5_to49] * [  687,   687])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo49, r3_to49] - [r5_bo49, r5_to49] * [  687,   687])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo49, r6_to49] + [r8_bo49, r8_to49] * [ -630,  -630])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo49, r6_to49] - [r8_bo49, r8_to49] * [ -630,  -630])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r7_bo49, r7_to49] + [r9_bo49, r9_to49] * [  687,   687])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r7_bo49, r7_to49] - [r9_bo49, r9_to49] * [  687,   687])
          [r9_b, r9_t] [Q, Q] /\
    [11*NQ2,11*NQ2,11*NQ2,11*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [11*Q2,11*Q2,11*Q2,11*Q2] /\
    [11*NQ2,11*NQ2,11*NQ2,11*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [11*Q2,11*Q2,11*Q2,11*Q2] /\
    [11*NQ2,11*NQ2,11*NQ2,11*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [11*Q2,11*Q2,11*Q2,11*Q2] /\
    [11*NQ2,11*NQ2,11*NQ2,11*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [11*Q2,11*Q2,11*Q2,11*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [11@16*NQ2,11@16*NQ2,11@16*NQ2,11@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[11@16*Q2,11@16*Q2,11@16*Q2,11@16*Q2] /\
    [11@16*NQ2,11@16*NQ2,11@16*NQ2,11@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[11@16*Q2,11@16*Q2,11@16*Q2,11@16*Q2] /\
    [11@16*NQ2,11@16*NQ2,11@16*NQ2,11@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[11@16*Q2,11@16*Q2,11@16*Q2,11@16*Q2] /\
    [11@16*NQ2,11@16*NQ2,11@16*NQ2,11@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[11@16*Q2,11@16*Q2,11@16*Q2,11@16*Q2];



ghost r2_bo50@int16, r2_to50@int16, r3_bo50@int16, r3_to50@int16,
      r4_bo50@int16, r4_to50@int16, r5_bo50@int16, r5_to50@int16,
      r6_bo50@int16, r6_to50@int16, r7_bo50@int16, r7_to50@int16,
      r8_bo50@int16, r8_to50@int16, r9_bo50@int16, r9_to50@int16:
      r2_bo50 = r2_b /\ r2_to50 = r2_t /\ r3_bo50 = r3_b /\ r3_to50 = r3_t /\
      r4_bo50 = r4_b /\ r4_to50 = r4_t /\ r5_bo50 = r5_b /\ r5_to50 = r5_t /\
      r6_bo50 = r6_b /\ r6_to50 = r6_t /\ r7_bo50 = r7_b /\ r7_to50 = r7_t /\
      r8_bo50 = r8_b /\ r8_to50 = r8_t /\ r9_bo50 = r9_b /\ r9_to50 = r9_t
   && r2_bo50 = r2_b /\ r2_to50 = r2_t /\ r3_bo50 = r3_b /\ r3_to50 = r3_t /\
      r4_bo50 = r4_b /\ r4_to50 = r4_t /\ r5_bo50 = r5_b /\ r5_to50 = r5_t /\
      r6_bo50 = r6_b /\ r6_to50 = r6_t /\ r7_bo50 = r7_b /\ r7_to50 = r7_t /\
      r8_bo50 = r8_b /\ r8_to50 = r8_t /\ r9_bo50 = r9_b /\ r9_to50 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401b90; Value = 0xc2b5f202; PC = 0x4013cc *)
mov [r10, r11] [L0x401b90, L0x401b94];
(* smulwb	lr, r10, r6                              #! PC = 0x4013d0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4013d4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013d8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4013dc *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x4013e0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r6_bo50 *  -797) [Q] /\
       eqmod lr_t (r6_to50 *  -797) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r6_bo50 *  -797) [Q] /\
       eqmod lr_t (r6_to50 *  -797) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r6, r2, lr                               #! PC = 0x4013e4 *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4013e8 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4013ec *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4013f0 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013f4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4013f8 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x4013fc *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo50 *  1410) [Q] /\
       eqmod lr_t (r7_to50 *  1410) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo50 *  1410) [Q] /\
       eqmod lr_t (r7_to50 *  1410) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x401400 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401404 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401b98; Value = 0xf12886bb; PC = 0x401408 *)
mov [r10, r11] [L0x401b98, L0x401b9c];
(* smulwb	lr, r10, r8                              #! PC = 0x40140c *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401410 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401414 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x401418 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x40141c *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo50 *  -193) [Q] /\
       eqmod lr_t (r8_to50 *  -193) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo50 *  -193) [Q] /\
       eqmod lr_t (r8_to50 *  -193) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r4, lr                               #! PC = 0x401420 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x401424 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x401428 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x40142c *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401430 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401434 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x401438 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo50 * -1062) [Q] /\
       eqmod lr_t (r9_to50 * -1062) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo50 * -1062) [Q] /\
       eqmod lr_t (r9_to50 * -1062) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x40143c *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x401440 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;

assert [12*NQ2,12*NQ2,12*NQ2,12*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [12*Q2,12*Q2,12*Q2,12*Q2] /\
       [12*NQ2,12*NQ2,12*NQ2,12*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [12*Q2,12*Q2,12*Q2,12*Q2] /\
       [12*NQ2,12*NQ2,12*NQ2,12*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [12*Q2,12*Q2,12*Q2,12*Q2] /\
       [12*NQ2,12*NQ2,12*NQ2,12*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [12*Q2,12*Q2,12*Q2,12*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [12*NQ2,12*NQ2,12*NQ2,12*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [12*Q2,12*Q2,12*Q2,12*Q2] /\
       [12*NQ2,12*NQ2,12*NQ2,12*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [12*Q2,12*Q2,12*Q2,12*Q2] /\
       [12*NQ2,12*NQ2,12*NQ2,12*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [12*Q2,12*Q2,12*Q2,12*Q2] /\
       [12*NQ2,12*NQ2,12*NQ2,12*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [12*Q2,12*Q2,12*Q2,12*Q2]
    && [12@16*NQ2,12@16*NQ2,12@16*NQ2,12@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[12@16*Q2,12@16*Q2,12@16*Q2,12@16*Q2] /\
       [12@16*NQ2,12@16*NQ2,12@16*NQ2,12@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[12@16*Q2,12@16*Q2,12@16*Q2,12@16*Q2] /\
       [12@16*NQ2,12@16*NQ2,12@16*NQ2,12@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[12@16*Q2,12@16*Q2,12@16*Q2,12@16*Q2] /\
       [12@16*NQ2,12@16*NQ2,12@16*NQ2,12@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[12@16*Q2,12@16*Q2,12@16*Q2,12@16*Q2];


(* CUT 67 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo50, r2_to50] + [r6_bo50, r6_to50] * [ -797,  -797])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo50, r2_to50] - [r6_bo50, r6_to50] * [ -797,  -797])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r3_bo50, r3_to50] + [r7_bo50, r7_to50] * [ 1410,  1410])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo50, r3_to50] - [r7_bo50, r7_to50] * [ 1410,  1410])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r4_bo50, r4_to50] + [r8_bo50, r8_to50] * [ -193,  -193])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo50, r4_to50] - [r8_bo50, r8_to50] * [ -193,  -193])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r5_bo50, r5_to50] + [r9_bo50, r9_to50] * [-1062, -1062])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r5_bo50, r5_to50] - [r9_bo50, r9_to50] * [-1062, -1062])
          [r9_b, r9_t] [Q, Q] /\
    [12*NQ2,12*NQ2,12*NQ2,12*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [12*Q2,12*Q2,12*Q2,12*Q2] /\
    [12*NQ2,12*NQ2,12*NQ2,12*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [12*Q2,12*Q2,12*Q2,12*Q2] /\
    [12*NQ2,12*NQ2,12*NQ2,12*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [12*Q2,12*Q2,12*Q2,12*Q2] /\
    [12*NQ2,12*NQ2,12*NQ2,12*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [12*Q2,12*Q2,12*Q2,12*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [12@16*NQ2,12@16*NQ2,12@16*NQ2,12@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[12@16*Q2,12@16*Q2,12@16*Q2,12@16*Q2] /\
    [12@16*NQ2,12@16*NQ2,12@16*NQ2,12@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[12@16*Q2,12@16*Q2,12@16*Q2,12@16*Q2] /\
    [12@16*NQ2,12@16*NQ2,12@16*NQ2,12@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[12@16*Q2,12@16*Q2,12@16*Q2,12@16*Q2] /\
    [12@16*NQ2,12@16*NQ2,12@16*NQ2,12@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[12@16*Q2,12@16*Q2,12@16*Q2,12@16*Q2];



ghost r2_bo51@int16, r2_to51@int16, r3_bo51@int16, r3_to51@int16,
      r4_bo51@int16, r4_to51@int16, r5_bo51@int16, r5_to51@int16,
      r6_bo51@int16, r6_to51@int16, r7_bo51@int16, r7_to51@int16,
      r8_bo51@int16, r8_to51@int16, r9_bo51@int16, r9_to51@int16:
      r2_bo51 = r2_b /\ r2_to51 = r2_t /\ r3_bo51 = r3_b /\ r3_to51 = r3_t /\
      r4_bo51 = r4_b /\ r4_to51 = r4_t /\ r5_bo51 = r5_b /\ r5_to51 = r5_t /\
      r6_bo51 = r6_b /\ r6_to51 = r6_t /\ r7_bo51 = r7_b /\ r7_to51 = r7_t /\
      r8_bo51 = r8_b /\ r8_to51 = r8_t /\ r9_bo51 = r9_b /\ r9_to51 = r9_t
   && r2_bo51 = r2_b /\ r2_to51 = r2_t /\ r3_bo51 = r3_b /\ r3_to51 = r3_t /\
      r4_bo51 = r4_b /\ r4_to51 = r4_t /\ r5_bo51 = r5_b /\ r5_to51 = r5_t /\
      r6_bo51 = r6_b /\ r6_to51 = r6_t /\ r7_bo51 = r7_b /\ r7_to51 = r7_t /\
      r8_bo51 = r8_b /\ r8_to51 = r8_t /\ r9_bo51 = r9_b /\ r9_to51 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401ba0; Value = 0x0189ba55; PC = 0x401444 *)
mov [r10, r11] [L0x401ba0, L0x401ba4];
(* smulwb	lr, r10, r2                              #! PC = 0x401448 *)
vpc r2_bW@int32 r2_b; mulj tmp r10 r2_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r2, r10, r2                              #! PC = 0x40144c *)
vpc r2_tW@int32 r2_t; mulj tmp r10 r2_tW; spl r2_w dc tmp 16;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b; vpc r2_t@int16 r2_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401450 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r2, r2, r12, r0                          #! PC = 0x401454 *)
mulj prod r2_b r12_t; add r2_w prod r0;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b;
(* pkhtb	r2, r2, lr, asr #16                       #! PC = 0x401458 *)
mov tmp_b lr_t; mov tmp_t r2_t;
mov r2_b tmp_b; mov r2_t tmp_t;
(* smulwb	lr, r11, r3                              #! PC = 0x40145c *)
vpc r3_bW@int32 r3_b; mulj tmp r11 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r11, r3                              #! PC = 0x401460 *)
vpc r3_tW@int32 r3_t; mulj tmp r11 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401464 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x401468 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x40146c *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov r3_b tmp_b; mov r3_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401ba8; Value = 0x3d851d26; PC = 0x401470 *)
mov [r10, r11] [L0x401ba8, L0x401bac];
(* smulwb	lr, r10, r4                              #! PC = 0x401474 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401478 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40147c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401480 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x401484 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov r4_b tmp_b; mov r4_t tmp_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401488 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40148c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401490 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401494 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x401498 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov r5_b tmp_b; mov r5_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401bb0; Value = 0x9ccc8dce; PC = 0x40149c *)
mov [r10, r11] [L0x401bb0, L0x401bb4];
(* smulwb	lr, r10, r6                              #! PC = 0x4014a0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4014a4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014a8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4014ac *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x4014b0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov r6_b tmp_b; mov r6_t tmp_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4014b4 *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4014b8 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014bc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4014c0 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x4014c4 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov r7_b tmp_b; mov r7_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401bb8; Value = 0x7ff62825; PC = 0x4014c8 *)
mov [r10, r11] [L0x401bb8, L0x401bbc];
(* smulwb	lr, r10, r8                              #! PC = 0x4014cc *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x4014d0 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014d4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4014d8 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x4014dc *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov r8_b tmp_b; mov r8_t tmp_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4014e0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4014e4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014e8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4014ec *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x4014f0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov r9_b tmp_b; mov r9_t tmp_t;

assert eqmod r2_b (r2_bo51 *    20) [Q] /\
       eqmod r2_t (r2_to51 *    20) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r2_b (r2_bo51 *    20) [Q] /\
       eqmod r2_t (r2_to51 *    20) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r2_b, r2_t] /\ [r2_b, r2_t] <s[Q2, Q2];


assert eqmod r3_b (r3_bo51 *  -315) [Q] /\
       eqmod r3_t (r3_to51 *  -315) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r3_b (r3_bo51 *  -315) [Q] /\
       eqmod r3_t (r3_to51 *  -315) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r3_b, r3_t] /\ [r3_b, r3_t] <s[Q2, Q2];


assert eqmod r4_b (r4_bo51 *   800) [Q] /\
       eqmod r4_t (r4_to51 *   800) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r4_b (r4_bo51 *   800) [Q] /\
       eqmod r4_t (r4_to51 *   800) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r4_b, r4_t] /\ [r4_b, r4_t] <s[Q2, Q2];


assert eqmod r5_b (r5_bo51 *   716) [Q] /\
       eqmod r5_t (r5_to51 *   716) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r5_b (r5_bo51 *   716) [Q] /\
       eqmod r5_t (r5_to51 *   716) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r5_b, r5_t] /\ [r5_b, r5_t] <s[Q2, Q2];


assert eqmod r6_b (r6_bo51 * -1290) [Q] /\
       eqmod r6_t (r6_to51 * -1290) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r6_b (r6_bo51 * -1290) [Q] /\
       eqmod r6_t (r6_to51 * -1290) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r6_b, r6_t] /\ [r6_b, r6_t] <s[Q2, Q2];


assert eqmod r7_b (r7_bo51 * -1321) [Q] /\
       eqmod r7_t (r7_to51 * -1321) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r7_b (r7_bo51 * -1321) [Q] /\
       eqmod r7_t (r7_to51 * -1321) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r7_b, r7_t] /\ [r7_b, r7_t] <s[Q2, Q2];


assert eqmod r8_b (r8_bo51 *  1664) [Q] /\
       eqmod r8_t (r8_to51 *  1664) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r8_b (r8_bo51 *  1664) [Q] /\
       eqmod r8_t (r8_to51 *  1664) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r8_b, r8_t] /\ [r8_b, r8_t] <s[Q2, Q2];


assert eqmod r9_b (r9_bo51 *   424) [Q] /\
       eqmod r9_t (r9_to51 *   424) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r9_b (r9_bo51 *   424) [Q] /\
       eqmod r9_t (r9_to51 *   424) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r9_b, r9_t] /\ [r9_b, r9_t] <s[Q2, Q2];

(* vmov	r0, s6                                     #! PC = 0x4014f4 *)
mov [r0_b, r0_t] [s6_b, s6_t];
(* str.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2f4; PC = 0x4014f8 *)
mov [L0xbefff2f4, L0xbefff2f6] [r6_b, r6_t];
(* str.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff334; PC = 0x4014fc *)
mov [L0xbefff334, L0xbefff336] [r7_b, r7_t];
(* str.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff374; PC = 0x401500 *)
mov [L0xbefff374, L0xbefff376] [r8_b, r8_t];
(* str.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff3b4; PC = 0x401504 *)
mov [L0xbefff3b4, L0xbefff3b6] [r9_b, r9_t];
(* str.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff234; PC = 0x401508 *)
mov [L0xbefff234, L0xbefff236] [r3_b, r3_t];
(* str.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff274; PC = 0x40150c *)
mov [L0xbefff274, L0xbefff276] [r4_b, r4_t];
(* str.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff2b4; PC = 0x401510 *)
mov [L0xbefff2b4, L0xbefff2b6] [r5_b, r5_t];
(* str.w	r2, [r0], #4                              #! EA = L0xbefff1f4; PC = 0x401514 *)
mov [L0xbefff1f4, L0xbefff1f6] [r2_b, r2_t];
(* vmov	lr, s14                                    #! PC = 0x401518 *)
mov [lr_b, lr_t] [s14_b, s14_t]; mov lr s14;
(* cmp.w	r0, lr                                    #! PC = 0x40151c *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x4012bc <invntt_fast+1552>              #! PC = 0x401520 *)
#bne.w	0x4012bc <invntt_fast+1552>              #! 0x401520 = 0x401520;

(* CUT 68 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo51, r2_to51] * [   20,    20])
          [L0xbefff1f4, L0xbefff1f6] [Q, Q] /\
    eqmod ([r3_bo51, r3_to51] * [ -315,  -315])
          [L0xbefff234, L0xbefff236] [Q, Q] /\
    eqmod ([r4_bo51, r4_to51] * [  800,   800])
          [L0xbefff274, L0xbefff276] [Q, Q] /\
    eqmod ([r5_bo51, r5_to51] * [  716,   716])
          [L0xbefff2b4, L0xbefff2b6] [Q, Q] /\
    eqmod ([r6_bo51, r6_to51] * [-1290, -1290])
          [L0xbefff2f4, L0xbefff2f6] [Q, Q] /\
    eqmod ([r7_bo51, r7_to51] * [-1321, -1321])
          [L0xbefff334, L0xbefff336] [Q, Q] /\
    eqmod ([r8_bo51, r8_to51] * [ 1664,  1664])
          [L0xbefff374, L0xbefff376] [Q, Q] /\
    eqmod ([r9_bo51, r9_to51] * [  424,   424])
          [L0xbefff3b4, L0xbefff3b6] [Q, Q] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff1f4,L0xbefff1f6,L0xbefff234,L0xbefff236] /\
    [L0xbefff1f4,L0xbefff1f6,L0xbefff234,L0xbefff236]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff274,L0xbefff276,L0xbefff2b4,L0xbefff2b6] /\
    [L0xbefff274,L0xbefff276,L0xbefff2b4,L0xbefff2b6]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff2f4,L0xbefff2f6,L0xbefff334,L0xbefff336] /\
    [L0xbefff2f4,L0xbefff2f6,L0xbefff334,L0xbefff336]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff374,L0xbefff376,L0xbefff3b4,L0xbefff3b6] /\
    [L0xbefff374,L0xbefff376,L0xbefff3b4,L0xbefff3b6]< [Q2,Q2,Q2,Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1f4,L0xbefff1f6,L0xbefff234,L0xbefff236] /\
    [L0xbefff1f4,L0xbefff1f6,L0xbefff234,L0xbefff236]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff274,L0xbefff276,L0xbefff2b4,L0xbefff2b6] /\
    [L0xbefff274,L0xbefff276,L0xbefff2b4,L0xbefff2b6]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2f4,L0xbefff2f6,L0xbefff334,L0xbefff336] /\
    [L0xbefff2f4,L0xbefff2f6,L0xbefff334,L0xbefff336]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff374,L0xbefff376,L0xbefff3b4,L0xbefff3b6] /\
    [L0xbefff374,L0xbefff376,L0xbefff3b4,L0xbefff3b6]<s[Q2,Q2,Q2,Q2];

(* vmov	s6, r0                                     #! PC = 0x4012bc *)
mov [s6_b, s6_t] [r0_b, r0_t];
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff1f8; Value = 0x09ae0614; PC = 0x4012c0 *)
mov [r2_b, r2_t] [L0xbefff1f8, L0xbefff1fa];
(* ldr.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff238; Value = 0xfc2703e8; PC = 0x4012c4 *)
mov [r3_b, r3_t] [L0xbefff238, L0xbefff23a];
(* ldr.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff278; Value = 0xf402f3e4; PC = 0x4012c8 *)
mov [r4_b, r4_t] [L0xbefff278, L0xbefff27a];
(* ldr.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff2b8; Value = 0xf2340710; PC = 0x4012cc *)
mov [r5_b, r5_t] [L0xbefff2b8, L0xbefff2ba];
(* ldr.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2f8; Value = 0x05c0069c; PC = 0x4012d0 *)
mov [r6_b, r6_t] [L0xbefff2f8, L0xbefff2fa];
(* ldr.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff338; Value = 0x01dbf359; PC = 0x4012d4 *)
mov [r7_b, r7_t] [L0xbefff338, L0xbefff33a];
(* ldr.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff378; Value = 0xfa510b72; PC = 0x4012d8 *)
mov [r8_b, r8_t] [L0xbefff378, L0xbefff37a];
(* ldr.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff3b8; Value = 0xf0d8f09a; PC = 0x4012dc *)
mov [r9_b, r9_t] [L0xbefff3b8, L0xbefff3ba];

ghost r2_bo52@int16, r2_to52@int16, r3_bo52@int16, r3_to52@int16,
      r4_bo52@int16, r4_to52@int16, r5_bo52@int16, r5_to52@int16,
      r6_bo52@int16, r6_to52@int16, r7_bo52@int16, r7_to52@int16,
      r8_bo52@int16, r8_to52@int16, r9_bo52@int16, r9_to52@int16:
      r2_bo52 = r2_b /\ r2_to52 = r2_t /\ r3_bo52 = r3_b /\ r3_to52 = r3_t /\
      r4_bo52 = r4_b /\ r4_to52 = r4_t /\ r5_bo52 = r5_b /\ r5_to52 = r5_t /\
      r6_bo52 = r6_b /\ r6_to52 = r6_t /\ r7_bo52 = r7_b /\ r7_to52 = r7_t /\
      r8_bo52 = r8_b /\ r8_to52 = r8_t /\ r9_bo52 = r9_b /\ r9_to52 = r9_t
   && r2_bo52 = r2_b /\ r2_to52 = r2_t /\ r3_bo52 = r3_b /\ r3_to52 = r3_t /\
      r4_bo52 = r4_b /\ r4_to52 = r4_t /\ r5_bo52 = r5_b /\ r5_to52 = r5_t /\
      r6_bo52 = r6_b /\ r6_to52 = r6_t /\ r7_bo52 = r7_b /\ r7_to52 = r7_t /\
      r8_bo52 = r8_b /\ r8_to52 = r8_t /\ r9_bo52 = r9_b /\ r9_to52 = r9_t;

(* movw	r0, #26632	; 0x6808                        #! PC = 0x4012e0 *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* ldr.w	r10, [r1], #4                             #! EA = L0x401bc0; Value = 0x29c1b606; PC = 0x4012e4 *)
mov r10 L0x401bc0;
(* smulwb	lr, r10, r3                              #! PC = 0x4012e8 *)
vpc r3_bW@int32 r3_b; mulj tmp r10 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r10, r3                              #! PC = 0x4012ec *)
vpc r3_tW@int32 r3_t; mulj tmp r10 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4012f0 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4012f4 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x4012f8 *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r3_bo52 *   543) [Q] /\
       eqmod lr_t (r3_to52 *   543) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r3_bo52 *   543) [Q] /\
       eqmod lr_t (r3_to52 *   543) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r3, r2, lr                               #! PC = 0x4012fc *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401300 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x401304 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x401308 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40130c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401310 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401314 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo52 *   543) [Q] /\
       eqmod lr_t (r5_to52 *   543) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo52 *   543) [Q] /\
       eqmod lr_t (r5_to52 *   543) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r4, lr                               #! PC = 0x401318 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x40131c *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x401320 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x401324 *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401328 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x40132c *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x401330 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo52 *   543) [Q] /\
       eqmod lr_t (r7_to52 *   543) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo52 *   543) [Q] /\
       eqmod lr_t (r7_to52 *   543) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r6, lr                               #! PC = 0x401334 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x401338 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x40133c *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x401340 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401344 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401348 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x40134c *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo52 *   543) [Q] /\
       eqmod lr_t (r9_to52 *   543) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo52 *   543) [Q] /\
       eqmod lr_t (r9_to52 *   543) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r8, lr                               #! PC = 0x401350 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x401354 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

assert [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [6*Q2,6*Q2,6*Q2,6*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [6*Q2,6*Q2,6*Q2,6*Q2]
    && [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2];


(* CUT 69 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo52, r2_to52] + [r3_bo52, r3_to52] * [  543,   543])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo52, r2_to52] - [r3_bo52, r3_to52] * [  543,   543])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r4_bo52, r4_to52] + [r5_bo52, r5_to52] * [  543,   543])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo52, r4_to52] - [r5_bo52, r5_to52] * [  543,   543])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo52, r6_to52] + [r7_bo52, r7_to52] * [  543,   543])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo52, r6_to52] - [r7_bo52, r7_to52] * [  543,   543])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r8_bo52, r8_to52] + [r9_bo52, r9_to52] * [  543,   543])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r8_bo52, r8_to52] - [r9_bo52, r9_to52] * [  543,   543])
          [r9_b, r9_t] [Q, Q] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [6*Q2,6*Q2,6*Q2,6*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2]
    prove with [cuts [1, 3, 5, 7, 9, 11, 13, 15]];



ghost r2_bo53@int16, r2_to53@int16, r3_bo53@int16, r3_to53@int16,
      r4_bo53@int16, r4_to53@int16, r5_bo53@int16, r5_to53@int16,
      r6_bo53@int16, r6_to53@int16, r7_bo53@int16, r7_to53@int16,
      r8_bo53@int16, r8_to53@int16, r9_bo53@int16, r9_to53@int16:
      r2_bo53 = r2_b /\ r2_to53 = r2_t /\ r3_bo53 = r3_b /\ r3_to53 = r3_t /\
      r4_bo53 = r4_b /\ r4_to53 = r4_t /\ r5_bo53 = r5_b /\ r5_to53 = r5_t /\
      r6_bo53 = r6_b /\ r6_to53 = r6_t /\ r7_bo53 = r7_b /\ r7_to53 = r7_t /\
      r8_bo53 = r8_b /\ r8_to53 = r8_t /\ r9_bo53 = r9_b /\ r9_to53 = r9_t
   && r2_bo53 = r2_b /\ r2_to53 = r2_t /\ r3_bo53 = r3_b /\ r3_to53 = r3_t /\
      r4_bo53 = r4_b /\ r4_to53 = r4_t /\ r5_bo53 = r5_b /\ r5_to53 = r5_t /\
      r6_bo53 = r6_b /\ r6_to53 = r6_t /\ r7_bo53 = r7_b /\ r7_to53 = r7_t /\
      r8_bo53 = r8_b /\ r8_to53 = r8_t /\ r9_bo53 = r9_b /\ r9_to53 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401bc4; Value = 0x5ef8b1cb; PC = 0x401358 *)
mov [r10, r11] [L0x401bc4, L0x401bc8];
(* smulwb	lr, r10, r4                              #! PC = 0x40135c *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401360 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401364 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401368 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x40136c *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_bo53 *  1235) [Q] /\
       eqmod lr_t (r4_to53 *  1235) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_bo53 *  1235) [Q] /\
       eqmod lr_t (r4_to53 *  1235) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r4, r2, lr                               #! PC = 0x401370 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401374 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401378 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40137c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401380 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401384 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401388 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo53 * -1426) [Q] /\
       eqmod lr_t (r5_to53 * -1426) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo53 * -1426) [Q] /\
       eqmod lr_t (r5_to53 * -1426) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x40138c *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401390 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x401394 *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401398 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40139c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4013a0 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x4013a4 *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo53 *  1235) [Q] /\
       eqmod lr_t (r8_to53 *  1235) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo53 *  1235) [Q] /\
       eqmod lr_t (r8_to53 *  1235) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r6, lr                               #! PC = 0x4013a8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x4013ac *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4013b0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4013b4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013b8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4013bc *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4013c0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo53 * -1426) [Q] /\
       eqmod lr_t (r9_to53 * -1426) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo53 * -1426) [Q] /\
       eqmod lr_t (r9_to53 * -1426) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x4013c4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x4013c8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;

assert [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
       prove with [algebra solver isl] && true;
assume [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
    && [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2];


(* CUT 70 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo53, r2_to53] + [r4_bo53, r4_to53] * [ 1235,  1235])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo53, r2_to53] - [r4_bo53, r4_to53] * [ 1235,  1235])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r3_bo53, r3_to53] + [r5_bo53, r5_to53] * [-1426, -1426])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo53, r3_to53] - [r5_bo53, r5_to53] * [-1426, -1426])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo53, r6_to53] + [r8_bo53, r8_to53] * [ 1235,  1235])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo53, r6_to53] - [r8_bo53, r8_to53] * [ 1235,  1235])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r7_bo53, r7_to53] + [r9_bo53, r9_to53] * [-1426, -1426])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r7_bo53, r7_to53] - [r9_bo53, r9_to53] * [-1426, -1426])
          [r9_b, r9_t] [Q, Q] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2];



ghost r2_bo54@int16, r2_to54@int16, r3_bo54@int16, r3_to54@int16,
      r4_bo54@int16, r4_to54@int16, r5_bo54@int16, r5_to54@int16,
      r6_bo54@int16, r6_to54@int16, r7_bo54@int16, r7_to54@int16,
      r8_bo54@int16, r8_to54@int16, r9_bo54@int16, r9_to54@int16:
      r2_bo54 = r2_b /\ r2_to54 = r2_t /\ r3_bo54 = r3_b /\ r3_to54 = r3_t /\
      r4_bo54 = r4_b /\ r4_to54 = r4_t /\ r5_bo54 = r5_b /\ r5_to54 = r5_t /\
      r6_bo54 = r6_b /\ r6_to54 = r6_t /\ r7_bo54 = r7_b /\ r7_to54 = r7_t /\
      r8_bo54 = r8_b /\ r8_to54 = r8_t /\ r9_bo54 = r9_b /\ r9_to54 = r9_t
   && r2_bo54 = r2_b /\ r2_to54 = r2_t /\ r3_bo54 = r3_b /\ r3_to54 = r3_t /\
      r4_bo54 = r4_b /\ r4_to54 = r4_t /\ r5_bo54 = r5_b /\ r5_to54 = r5_t /\
      r6_bo54 = r6_b /\ r6_to54 = r6_t /\ r7_bo54 = r7_b /\ r7_to54 = r7_t /\
      r8_bo54 = r8_b /\ r8_to54 = r8_t /\ r9_bo54 = r9_b /\ r9_to54 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401bcc; Value = 0xdd3dc02d; PC = 0x4013cc *)
mov [r10, r11] [L0x401bcc, L0x401bd0];
(* smulwb	lr, r10, r6                              #! PC = 0x4013d0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4013d4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013d8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4013dc *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x4013e0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r6_bo54 *  -452) [Q] /\
       eqmod lr_t (r6_to54 *  -452) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r6_bo54 *  -452) [Q] /\
       eqmod lr_t (r6_to54 *  -452) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r6, r2, lr                               #! PC = 0x4013e4 *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4013e8 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4013ec *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4013f0 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013f4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4013f8 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x4013fc *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo54 * -1435) [Q] /\
       eqmod lr_t (r7_to54 * -1435) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo54 * -1435) [Q] /\
       eqmod lr_t (r7_to54 * -1435) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x401400 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401404 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401bd4; Value = 0xc1f114d8; PC = 0x401408 *)
mov [r10, r11] [L0x401bd4, L0x401bd8];
(* smulwb	lr, r10, r8                              #! PC = 0x40140c *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401410 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401414 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x401418 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x40141c *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo54 *  -807) [Q] /\
       eqmod lr_t (r8_to54 *  -807) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo54 *  -807) [Q] /\
       eqmod lr_t (r8_to54 *  -807) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r4, lr                               #! PC = 0x401420 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x401424 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x401428 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x40142c *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401430 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401434 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x401438 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo54 *  1010) [Q] /\
       eqmod lr_t (r9_to54 *  1010) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo54 *  1010) [Q] /\
       eqmod lr_t (r9_to54 *  1010) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x40143c *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x401440 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;

assert [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
    && [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2];


(* CUT 71 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo54, r2_to54] + [r6_bo54, r6_to54] * [ -452,  -452])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo54, r2_to54] - [r6_bo54, r6_to54] * [ -452,  -452])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r3_bo54, r3_to54] + [r7_bo54, r7_to54] * [-1435, -1435])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo54, r3_to54] - [r7_bo54, r7_to54] * [-1435, -1435])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r4_bo54, r4_to54] + [r8_bo54, r8_to54] * [ -807,  -807])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo54, r4_to54] - [r8_bo54, r8_to54] * [ -807,  -807])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r5_bo54, r5_to54] + [r9_bo54, r9_to54] * [ 1010,  1010])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r5_bo54, r5_to54] - [r9_bo54, r9_to54] * [ 1010,  1010])
          [r9_b, r9_t] [Q, Q] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2];



ghost r2_bo55@int16, r2_to55@int16, r3_bo55@int16, r3_to55@int16,
      r4_bo55@int16, r4_to55@int16, r5_bo55@int16, r5_to55@int16,
      r6_bo55@int16, r6_to55@int16, r7_bo55@int16, r7_to55@int16,
      r8_bo55@int16, r8_to55@int16, r9_bo55@int16, r9_to55@int16:
      r2_bo55 = r2_b /\ r2_to55 = r2_t /\ r3_bo55 = r3_b /\ r3_to55 = r3_t /\
      r4_bo55 = r4_b /\ r4_to55 = r4_t /\ r5_bo55 = r5_b /\ r5_to55 = r5_t /\
      r6_bo55 = r6_b /\ r6_to55 = r6_t /\ r7_bo55 = r7_b /\ r7_to55 = r7_t /\
      r8_bo55 = r8_b /\ r8_to55 = r8_t /\ r9_bo55 = r9_b /\ r9_to55 = r9_t
   && r2_bo55 = r2_b /\ r2_to55 = r2_t /\ r3_bo55 = r3_b /\ r3_to55 = r3_t /\
      r4_bo55 = r4_b /\ r4_to55 = r4_t /\ r5_bo55 = r5_b /\ r5_to55 = r5_t /\
      r6_bo55 = r6_b /\ r6_to55 = r6_t /\ r7_bo55 = r7_b /\ r7_to55 = r7_t /\
      r8_bo55 = r8_b /\ r8_to55 = r8_t /\ r9_bo55 = r9_b /\ r9_to55 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401bdc; Value = 0x0f263824; PC = 0x401444 *)
mov [r10, r11] [L0x401bdc, L0x401be0];
(* smulwb	lr, r10, r2                              #! PC = 0x401448 *)
vpc r2_bW@int32 r2_b; mulj tmp r10 r2_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r2, r10, r2                              #! PC = 0x40144c *)
vpc r2_tW@int32 r2_t; mulj tmp r10 r2_tW; spl r2_w dc tmp 16;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b; vpc r2_t@int16 r2_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401450 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r2, r2, r12, r0                          #! PC = 0x401454 *)
mulj prod r2_b r12_t; add r2_w prod r0;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b;
(* pkhtb	r2, r2, lr, asr #16                       #! PC = 0x401458 *)
mov tmp_b lr_t; mov tmp_t r2_t;
mov r2_b tmp_b; mov r2_t tmp_t;
(* smulwb	lr, r11, r3                              #! PC = 0x40145c *)
vpc r3_bW@int32 r3_b; mulj tmp r11 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r11, r3                              #! PC = 0x401460 *)
vpc r3_tW@int32 r3_t; mulj tmp r11 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401464 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x401468 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x40146c *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov r3_b tmp_b; mov r3_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401be4; Value = 0x5df8c57b; PC = 0x401470 *)
mov [r10, r11] [L0x401be4, L0x401be8];
(* smulwb	lr, r10, r4                              #! PC = 0x401474 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401478 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40147c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401480 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x401484 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov r4_b tmp_b; mov r4_t tmp_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401488 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40148c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401490 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401494 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x401498 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov r5_b tmp_b; mov r5_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401bec; Value = 0xaededb2b; PC = 0x40149c *)
mov [r10, r11] [L0x401bec, L0x401bf0];
(* smulwb	lr, r10, r6                              #! PC = 0x4014a0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4014a4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014a8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4014ac *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x4014b0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov r6_b tmp_b; mov r6_t tmp_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4014b4 *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4014b8 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014bc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4014c0 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x4014c4 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov r7_b tmp_b; mov r7_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401bf4; Value = 0x52d23e99; PC = 0x4014c8 *)
mov [r10, r11] [L0x401bf4, L0x401bf8];
(* smulwb	lr, r10, r8                              #! PC = 0x4014cc *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x4014d0 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014d4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4014d8 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x4014dc *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov r8_b tmp_b; mov r8_t tmp_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4014e0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4014e4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014e8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4014ec *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x4014f0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov r9_b tmp_b; mov r9_t tmp_t;

assert eqmod r2_b (r2_bo55 *   197) [Q] /\
       eqmod r2_t (r2_to55 *   197) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r2_b (r2_bo55 *   197) [Q] /\
       eqmod r2_t (r2_to55 *   197) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r2_b, r2_t] /\ [r2_b, r2_t] <s[Q2, Q2];


assert eqmod r3_b (r3_bo55 *  -606) [Q] /\
       eqmod r3_t (r3_to55 *  -606) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r3_b (r3_bo55 *  -606) [Q] /\
       eqmod r3_t (r3_to55 *  -606) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r3_b, r3_t] /\ [r3_b, r3_t] <s[Q2, Q2];


assert eqmod r4_b (r4_bo55 *  1222) [Q] /\
       eqmod r4_t (r4_to55 *  1222) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r4_b (r4_bo55 *  1222) [Q] /\
       eqmod r4_t (r4_to55 *  1222) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r4_b, r4_t] /\ [r4_b, r4_t] <s[Q2, Q2];


assert eqmod r5_b (r5_bo55 *  -937) [Q] /\
       eqmod r5_t (r5_to55 *  -937) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r5_b (r5_bo55 *  -937) [Q] /\
       eqmod r5_t (r5_to55 *  -937) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r5_b, r5_t] /\ [r5_b, r5_t] <s[Q2, Q2];


assert eqmod r6_b (r6_bo55 * -1055) [Q] /\
       eqmod r6_t (r6_to55 * -1055) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r6_b (r6_bo55 * -1055) [Q] /\
       eqmod r6_t (r6_to55 * -1055) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r6_b, r6_t] /\ [r6_b, r6_t] <s[Q2, Q2];


assert eqmod r7_b (r7_bo55 *  -861) [Q] /\
       eqmod r7_t (r7_to55 *  -861) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r7_b (r7_bo55 *  -861) [Q] /\
       eqmod r7_t (r7_to55 *  -861) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r7_b, r7_t] /\ [r7_b, r7_t] <s[Q2, Q2];


assert eqmod r8_b (r8_bo55 *  1077) [Q] /\
       eqmod r8_t (r8_to55 *  1077) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r8_b (r8_bo55 *  1077) [Q] /\
       eqmod r8_t (r8_to55 *  1077) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r8_b, r8_t] /\ [r8_b, r8_t] <s[Q2, Q2];


assert eqmod r9_b (r9_bo55 * -1150) [Q] /\
       eqmod r9_t (r9_to55 * -1150) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r9_b (r9_bo55 * -1150) [Q] /\
       eqmod r9_t (r9_to55 * -1150) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r9_b, r9_t] /\ [r9_b, r9_t] <s[Q2, Q2];

(* vmov	r0, s6                                     #! PC = 0x4014f4 *)
mov [r0_b, r0_t] [s6_b, s6_t];
(* str.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2f8; PC = 0x4014f8 *)
mov [L0xbefff2f8, L0xbefff2fa] [r6_b, r6_t];
(* str.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff338; PC = 0x4014fc *)
mov [L0xbefff338, L0xbefff33a] [r7_b, r7_t];
(* str.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff378; PC = 0x401500 *)
mov [L0xbefff378, L0xbefff37a] [r8_b, r8_t];
(* str.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff3b8; PC = 0x401504 *)
mov [L0xbefff3b8, L0xbefff3ba] [r9_b, r9_t];
(* str.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff238; PC = 0x401508 *)
mov [L0xbefff238, L0xbefff23a] [r3_b, r3_t];
(* str.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff278; PC = 0x40150c *)
mov [L0xbefff278, L0xbefff27a] [r4_b, r4_t];
(* str.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff2b8; PC = 0x401510 *)
mov [L0xbefff2b8, L0xbefff2ba] [r5_b, r5_t];
(* str.w	r2, [r0], #4                              #! EA = L0xbefff1f8; PC = 0x401514 *)
mov [L0xbefff1f8, L0xbefff1fa] [r2_b, r2_t];
(* vmov	lr, s14                                    #! PC = 0x401518 *)
mov [lr_b, lr_t] [s14_b, s14_t]; mov lr s14;
(* cmp.w	r0, lr                                    #! PC = 0x40151c *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x4012bc <invntt_fast+1552>              #! PC = 0x401520 *)
#bne.w	0x4012bc <invntt_fast+1552>              #! 0x401520 = 0x401520;

(* CUT 72 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo55, r2_to55] * [  197,   197])
          [L0xbefff1f8, L0xbefff1fa] [Q, Q] /\
    eqmod ([r3_bo55, r3_to55] * [ -606,  -606])
          [L0xbefff238, L0xbefff23a] [Q, Q] /\
    eqmod ([r4_bo55, r4_to55] * [ 1222,  1222])
          [L0xbefff278, L0xbefff27a] [Q, Q] /\
    eqmod ([r5_bo55, r5_to55] * [ -937,  -937])
          [L0xbefff2b8, L0xbefff2ba] [Q, Q] /\
    eqmod ([r6_bo55, r6_to55] * [-1055, -1055])
          [L0xbefff2f8, L0xbefff2fa] [Q, Q] /\
    eqmod ([r7_bo55, r7_to55] * [ -861,  -861])
          [L0xbefff338, L0xbefff33a] [Q, Q] /\
    eqmod ([r8_bo55, r8_to55] * [ 1077,  1077])
          [L0xbefff378, L0xbefff37a] [Q, Q] /\
    eqmod ([r9_bo55, r9_to55] * [-1150, -1150])
          [L0xbefff3b8, L0xbefff3ba] [Q, Q] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff1f8,L0xbefff1fa,L0xbefff238,L0xbefff23a] /\
    [L0xbefff1f8,L0xbefff1fa,L0xbefff238,L0xbefff23a]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff278,L0xbefff27a,L0xbefff2b8,L0xbefff2ba] /\
    [L0xbefff278,L0xbefff27a,L0xbefff2b8,L0xbefff2ba]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff2f8,L0xbefff2fa,L0xbefff338,L0xbefff33a] /\
    [L0xbefff2f8,L0xbefff2fa,L0xbefff338,L0xbefff33a]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff378,L0xbefff37a,L0xbefff3b8,L0xbefff3ba] /\
    [L0xbefff378,L0xbefff37a,L0xbefff3b8,L0xbefff3ba]< [Q2,Q2,Q2,Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1f8,L0xbefff1fa,L0xbefff238,L0xbefff23a] /\
    [L0xbefff1f8,L0xbefff1fa,L0xbefff238,L0xbefff23a]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff278,L0xbefff27a,L0xbefff2b8,L0xbefff2ba] /\
    [L0xbefff278,L0xbefff27a,L0xbefff2b8,L0xbefff2ba]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2f8,L0xbefff2fa,L0xbefff338,L0xbefff33a] /\
    [L0xbefff2f8,L0xbefff2fa,L0xbefff338,L0xbefff33a]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff378,L0xbefff37a,L0xbefff3b8,L0xbefff3ba] /\
    [L0xbefff378,L0xbefff37a,L0xbefff3b8,L0xbefff3ba]<s[Q2,Q2,Q2,Q2];

(* vmov	s6, r0                                     #! PC = 0x4012bc *)
mov [s6_b, s6_t] [r0_b, r0_t];
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff1fc; Value = 0x10bd039c; PC = 0x4012c0 *)
mov [r2_b, r2_t] [L0xbefff1fc, L0xbefff1fe];
(* ldr.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff23c; Value = 0x0b60f1d7; PC = 0x4012c4 *)
mov [r3_b, r3_t] [L0xbefff23c, L0xbefff23e];
(* ldr.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff27c; Value = 0xe477f637; PC = 0x4012c8 *)
mov [r4_b, r4_t] [L0xbefff27c, L0xbefff27e];
(* ldr.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff2bc; Value = 0xfeb60c37; PC = 0x4012cc *)
mov [r5_b, r5_t] [L0xbefff2bc, L0xbefff2be];
(* ldr.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2fc; Value = 0x0eaee408; PC = 0x4012d0 *)
mov [r6_b, r6_t] [L0xbefff2fc, L0xbefff2fe];
(* ldr.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff33c; Value = 0xfc02e205; PC = 0x4012d4 *)
mov [r7_b, r7_t] [L0xbefff33c, L0xbefff33e];
(* ldr.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff37c; Value = 0xfb15fdf3; PC = 0x4012d8 *)
mov [r8_b, r8_t] [L0xbefff37c, L0xbefff37e];
(* ldr.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff3bc; Value = 0x01fc0020; PC = 0x4012dc *)
mov [r9_b, r9_t] [L0xbefff3bc, L0xbefff3be];

ghost r2_bo56@int16, r2_to56@int16, r3_bo56@int16, r3_to56@int16,
      r4_bo56@int16, r4_to56@int16, r5_bo56@int16, r5_to56@int16,
      r6_bo56@int16, r6_to56@int16, r7_bo56@int16, r7_to56@int16,
      r8_bo56@int16, r8_to56@int16, r9_bo56@int16, r9_to56@int16:
      r2_bo56 = r2_b /\ r2_to56 = r2_t /\ r3_bo56 = r3_b /\ r3_to56 = r3_t /\
      r4_bo56 = r4_b /\ r4_to56 = r4_t /\ r5_bo56 = r5_b /\ r5_to56 = r5_t /\
      r6_bo56 = r6_b /\ r6_to56 = r6_t /\ r7_bo56 = r7_b /\ r7_to56 = r7_t /\
      r8_bo56 = r8_b /\ r8_to56 = r8_t /\ r9_bo56 = r9_b /\ r9_to56 = r9_t
   && r2_bo56 = r2_b /\ r2_to56 = r2_t /\ r3_bo56 = r3_b /\ r3_to56 = r3_t /\
      r4_bo56 = r4_b /\ r4_to56 = r4_t /\ r5_bo56 = r5_b /\ r5_to56 = r5_t /\
      r6_bo56 = r6_b /\ r6_to56 = r6_t /\ r7_bo56 = r7_b /\ r7_to56 = r7_t /\
      r8_bo56 = r8_b /\ r8_to56 = r8_t /\ r9_bo56 = r9_b /\ r9_to56 = r9_t;

(* movw	r0, #26632	; 0x6808                        #! PC = 0x4012e0 *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* ldr.w	r10, [r1], #4                             #! EA = L0x401bfc; Value = 0x34d48d31; PC = 0x4012e4 *)
mov r10 L0x401bfc;
(* smulwb	lr, r10, r3                              #! PC = 0x4012e8 *)
vpc r3_bW@int32 r3_b; mulj tmp r10 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r10, r3                              #! PC = 0x4012ec *)
vpc r3_tW@int32 r3_t; mulj tmp r10 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4012f0 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4012f4 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x4012f8 *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r3_bo56 *   687) [Q] /\
       eqmod lr_t (r3_to56 *   687) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r3_bo56 *   687) [Q] /\
       eqmod lr_t (r3_to56 *   687) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r3, r2, lr                               #! PC = 0x4012fc *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401300 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x401304 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x401308 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40130c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401310 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401314 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo56 *   687) [Q] /\
       eqmod lr_t (r5_to56 *   687) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo56 *   687) [Q] /\
       eqmod lr_t (r5_to56 *   687) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r4, lr                               #! PC = 0x401318 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x40131c *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x401320 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x401324 *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401328 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x40132c *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x401330 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo56 *   687) [Q] /\
       eqmod lr_t (r7_to56 *   687) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo56 *   687) [Q] /\
       eqmod lr_t (r7_to56 *   687) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r6, lr                               #! PC = 0x401334 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x401338 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x40133c *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x401340 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401344 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401348 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x40134c *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo56 *   687) [Q] /\
       eqmod lr_t (r9_to56 *   687) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo56 *   687) [Q] /\
       eqmod lr_t (r9_to56 *   687) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r8, lr                               #! PC = 0x401350 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x401354 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

assert [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
    && [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2];


(* CUT 73 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo56, r2_to56] + [r3_bo56, r3_to56] * [  687,   687])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo56, r2_to56] - [r3_bo56, r3_to56] * [  687,   687])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r4_bo56, r4_to56] + [r5_bo56, r5_to56] * [  687,   687])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo56, r4_to56] - [r5_bo56, r5_to56] * [  687,   687])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo56, r6_to56] + [r7_bo56, r7_to56] * [  687,   687])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo56, r6_to56] - [r7_bo56, r7_to56] * [  687,   687])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r8_bo56, r8_to56] + [r9_bo56, r9_to56] * [  687,   687])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r8_bo56, r8_to56] - [r9_bo56, r9_to56] * [  687,   687])
          [r9_b, r9_t] [Q, Q] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2]
    prove with [cuts [1, 3, 5, 7, 9, 11, 13, 15]];



ghost r2_bo57@int16, r2_to57@int16, r3_bo57@int16, r3_to57@int16,
      r4_bo57@int16, r4_to57@int16, r5_bo57@int16, r5_to57@int16,
      r6_bo57@int16, r6_to57@int16, r7_bo57@int16, r7_to57@int16,
      r8_bo57@int16, r8_to57@int16, r9_bo57@int16, r9_to57@int16:
      r2_bo57 = r2_b /\ r2_to57 = r2_t /\ r3_bo57 = r3_b /\ r3_to57 = r3_t /\
      r4_bo57 = r4_b /\ r4_to57 = r4_t /\ r5_bo57 = r5_b /\ r5_to57 = r5_t /\
      r6_bo57 = r6_b /\ r6_to57 = r6_t /\ r7_bo57 = r7_b /\ r7_to57 = r7_t /\
      r8_bo57 = r8_b /\ r8_to57 = r8_t /\ r9_bo57 = r9_b /\ r9_to57 = r9_t
   && r2_bo57 = r2_b /\ r2_to57 = r2_t /\ r3_bo57 = r3_b /\ r3_to57 = r3_t /\
      r4_bo57 = r4_b /\ r4_to57 = r4_t /\ r5_bo57 = r5_b /\ r5_to57 = r5_t /\
      r6_bo57 = r6_b /\ r6_to57 = r6_t /\ r7_bo57 = r7_b /\ r7_to57 = r7_t /\
      r8_bo57 = r8_b /\ r8_to57 = r8_t /\ r9_bo57 = r9_b /\ r9_to57 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401c00; Value = 0x6c6dd02c; PC = 0x401358 *)
mov [r10, r11] [L0x401c00, L0x401c04];
(* smulwb	lr, r10, r4                              #! PC = 0x40135c *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401360 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401364 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401368 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x40136c *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_bo57 *  1410) [Q] /\
       eqmod lr_t (r4_to57 *  1410) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_bo57 *  1410) [Q] /\
       eqmod lr_t (r4_to57 *  1410) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r4, r2, lr                               #! PC = 0x401370 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401374 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401378 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40137c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401380 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401384 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401388 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo57 * -1062) [Q] /\
       eqmod lr_t (r5_to57 * -1062) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo57 * -1062) [Q] /\
       eqmod lr_t (r5_to57 * -1062) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x40138c *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401390 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x401394 *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401398 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40139c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4013a0 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x4013a4 *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo57 *  1410) [Q] /\
       eqmod lr_t (r8_to57 *  1410) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo57 *  1410) [Q] /\
       eqmod lr_t (r8_to57 *  1410) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r6, lr                               #! PC = 0x4013a8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x4013ac *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4013b0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4013b4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013b8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4013bc *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4013c0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo57 * -1062) [Q] /\
       eqmod lr_t (r9_to57 * -1062) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo57 * -1062) [Q] /\
       eqmod lr_t (r9_to57 * -1062) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x4013c4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x4013c8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;

assert [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
       prove with [algebra solver isl] && true;
assume [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
    && [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2];


(* CUT 74 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo57, r2_to57] + [r4_bo57, r4_to57] * [ 1410,  1410])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo57, r2_to57] - [r4_bo57, r4_to57] * [ 1410,  1410])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r3_bo57, r3_to57] + [r5_bo57, r5_to57] * [-1062, -1062])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo57, r3_to57] - [r5_bo57, r5_to57] * [-1062, -1062])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo57, r6_to57] + [r8_bo57, r8_to57] * [ 1410,  1410])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo57, r6_to57] - [r8_bo57, r8_to57] * [ 1410,  1410])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r7_bo57, r7_to57] + [r9_bo57, r9_to57] * [-1062, -1062])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r7_bo57, r7_to57] - [r9_bo57, r9_to57] * [-1062, -1062])
          [r9_b, r9_t] [Q, Q] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2];



ghost r2_bo58@int16, r2_to58@int16, r3_bo58@int16, r3_to58@int16,
      r4_bo58@int16, r4_to58@int16, r5_bo58@int16, r5_to58@int16,
      r6_bo58@int16, r6_to58@int16, r7_bo58@int16, r7_to58@int16,
      r8_bo58@int16, r8_to58@int16, r9_bo58@int16, r9_to58@int16:
      r2_bo58 = r2_b /\ r2_to58 = r2_t /\ r3_bo58 = r3_b /\ r3_to58 = r3_t /\
      r4_bo58 = r4_b /\ r4_to58 = r4_t /\ r5_bo58 = r5_b /\ r5_to58 = r5_t /\
      r6_bo58 = r6_b /\ r6_to58 = r6_t /\ r7_bo58 = r7_b /\ r7_to58 = r7_t /\
      r8_bo58 = r8_b /\ r8_to58 = r8_t /\ r9_bo58 = r9_b /\ r9_to58 = r9_t
   && r2_bo58 = r2_b /\ r2_to58 = r2_t /\ r3_bo58 = r3_b /\ r3_to58 = r3_t /\
      r4_bo58 = r4_b /\ r4_to58 = r4_t /\ r5_bo58 = r5_b /\ r5_to58 = r5_t /\
      r6_bo58 = r6_b /\ r6_to58 = r6_t /\ r7_bo58 = r7_b /\ r7_to58 = r7_t /\
      r8_bo58 = r8_b /\ r8_to58 = r8_t /\ r9_bo58 = r9_b /\ r9_to58 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401c08; Value = 0x8e7ee28d; PC = 0x4013cc *)
mov [r10, r11] [L0x401c08, L0x401c0c];
(* smulwb	lr, r10, r6                              #! PC = 0x4013d0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4013d4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013d8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4013dc *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x4013e0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r6_bo58 * -1476) [Q] /\
       eqmod lr_t (r6_to58 * -1476) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r6_bo58 * -1476) [Q] /\
       eqmod lr_t (r6_to58 * -1476) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r6, r2, lr                               #! PC = 0x4013e4 *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4013e8 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4013ec *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4013f0 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013f4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4013f8 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x4013fc *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo58 *   882) [Q] /\
       eqmod lr_t (r7_to58 *   882) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo58 *   882) [Q] /\
       eqmod lr_t (r7_to58 *   882) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x401400 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401404 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401c10; Value = 0x9907ebb3; PC = 0x401408 *)
mov [r10, r11] [L0x401c10, L0x401c14];
(* smulwb	lr, r10, r8                              #! PC = 0x40140c *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401410 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401414 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x401418 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x40141c *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo58 * -1339) [Q] /\
       eqmod lr_t (r8_to58 * -1339) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo58 * -1339) [Q] /\
       eqmod lr_t (r8_to58 * -1339) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r4, lr                               #! PC = 0x401420 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x401424 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x401428 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x40142c *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401430 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401434 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x401438 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo58 *  -296) [Q] /\
       eqmod lr_t (r9_to58 *  -296) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo58 *  -296) [Q] /\
       eqmod lr_t (r9_to58 *  -296) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x40143c *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x401440 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;

assert [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
       [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
       [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
       [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [9*Q2,9*Q2,9*Q2,9*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
       [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
       [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
       [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [9*Q2,9*Q2,9*Q2,9*Q2]
    && [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2] /\
       [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2] /\
       [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2] /\
       [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2];


(* CUT 75 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo58, r2_to58] + [r6_bo58, r6_to58] * [-1476, -1476])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo58, r2_to58] - [r6_bo58, r6_to58] * [-1476, -1476])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r3_bo58, r3_to58] + [r7_bo58, r7_to58] * [  882,   882])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo58, r3_to58] - [r7_bo58, r7_to58] * [  882,   882])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r4_bo58, r4_to58] + [r8_bo58, r8_to58] * [-1339, -1339])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo58, r4_to58] - [r8_bo58, r8_to58] * [-1339, -1339])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r5_bo58, r5_to58] + [r9_bo58, r9_to58] * [ -296,  -296])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r5_bo58, r5_to58] - [r9_bo58, r9_to58] * [ -296,  -296])
          [r9_b, r9_t] [Q, Q] /\
    [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
    [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
    [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [9*Q2,9*Q2,9*Q2,9*Q2] /\
    [9*NQ2,9*NQ2,9*NQ2,9*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [9*Q2,9*Q2,9*Q2,9*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2] /\
    [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2] /\
    [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2] /\
    [9@16*NQ2,9@16*NQ2,9@16*NQ2,9@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[9@16*Q2,9@16*Q2,9@16*Q2,9@16*Q2];



ghost r2_bo59@int16, r2_to59@int16, r3_bo59@int16, r3_to59@int16,
      r4_bo59@int16, r4_to59@int16, r5_bo59@int16, r5_to59@int16,
      r6_bo59@int16, r6_to59@int16, r7_bo59@int16, r7_to59@int16,
      r8_bo59@int16, r8_to59@int16, r9_bo59@int16, r9_to59@int16:
      r2_bo59 = r2_b /\ r2_to59 = r2_t /\ r3_bo59 = r3_b /\ r3_to59 = r3_t /\
      r4_bo59 = r4_b /\ r4_to59 = r4_t /\ r5_bo59 = r5_b /\ r5_to59 = r5_t /\
      r6_bo59 = r6_b /\ r6_to59 = r6_t /\ r7_bo59 = r7_b /\ r7_to59 = r7_t /\
      r8_bo59 = r8_b /\ r8_to59 = r8_t /\ r9_bo59 = r9_b /\ r9_to59 = r9_t
   && r2_bo59 = r2_b /\ r2_to59 = r2_t /\ r3_bo59 = r3_b /\ r3_to59 = r3_t /\
      r4_bo59 = r4_b /\ r4_to59 = r4_t /\ r5_bo59 = r5_b /\ r5_to59 = r5_t /\
      r6_bo59 = r6_b /\ r6_to59 = r6_t /\ r7_bo59 = r7_b /\ r7_to59 = r7_t /\
      r8_bo59 = r8_b /\ r8_to59 = r8_t /\ r9_bo59 = r9_b /\ r9_to59 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401c18; Value = 0x886ba8f4; PC = 0x401444 *)
mov [r10, r11] [L0x401c18, L0x401c1c];
(* smulwb	lr, r10, r2                              #! PC = 0x401448 *)
vpc r2_bW@int32 r2_b; mulj tmp r10 r2_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r2, r10, r2                              #! PC = 0x40144c *)
vpc r2_tW@int32 r2_t; mulj tmp r10 r2_tW; spl r2_w dc tmp 16;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b; vpc r2_t@int16 r2_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401450 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r2, r2, r12, r0                          #! PC = 0x401454 *)
mulj prod r2_b r12_t; add r2_w prod r0;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b;
(* pkhtb	r2, r2, lr, asr #16                       #! PC = 0x401458 *)
mov tmp_b lr_t; mov tmp_t r2_t;
mov r2_b tmp_b; mov r2_t tmp_t;
(* smulwb	lr, r11, r3                              #! PC = 0x40145c *)
vpc r3_bW@int32 r3_b; mulj tmp r11 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r11, r3                              #! PC = 0x401460 *)
vpc r3_tW@int32 r3_t; mulj tmp r11 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401464 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x401468 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x40146c *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov r3_b tmp_b; mov r3_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401c20; Value = 0x50d265f9; PC = 0x401470 *)
mov [r10, r11] [L0x401c20, L0x401c24];
(* smulwb	lr, r10, r4                              #! PC = 0x401474 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401478 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40147c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401480 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x401484 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov r4_b tmp_b; mov r4_t tmp_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401488 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40148c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401490 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401494 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x401498 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov r5_b tmp_b; mov r5_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401c28; Value = 0xa0dfeec7; PC = 0x40149c *)
mov [r10, r11] [L0x401c28, L0x401c2c];
(* smulwb	lr, r10, r6                              #! PC = 0x4014a0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4014a4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014a8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4014ac *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x4014b0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov r6_b tmp_b; mov r6_t tmp_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4014b4 *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4014b8 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014bc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4014c0 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x4014c4 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov r7_b tmp_b; mov r7_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401c30; Value = 0x22fd4efa; PC = 0x4014c8 *)
mov [r10, r11] [L0x401c30, L0x401c34];
(* smulwb	lr, r10, r8                              #! PC = 0x4014cc *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x4014d0 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014d4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4014d8 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x4014dc *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov r8_b tmp_b; mov r8_t tmp_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4014e0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4014e4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014e8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4014ec *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x4014f0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov r9_b tmp_b; mov r9_t tmp_t;

assert eqmod r2_b (r2_bo59 * -1555) [Q] /\
       eqmod r2_t (r2_to59 * -1555) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r2_b (r2_bo59 * -1555) [Q] /\
       eqmod r2_t (r2_to59 * -1555) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r2_b, r2_t] /\ [r2_b, r2_t] <s[Q2, Q2];


assert eqmod r3_b (r3_bo59 *   356) [Q] /\
       eqmod r3_t (r3_to59 *   356) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r3_b (r3_bo59 *   356) [Q] /\
       eqmod r3_t (r3_to59 *   356) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r3_b, r3_t] /\ [r3_b, r3_t] <s[Q2, Q2];


assert eqmod r4_b (r4_bo59 *  1051) [Q] /\
       eqmod r4_t (r4_to59 *  1051) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r4_b (r4_bo59 *  1051) [Q] /\
       eqmod r4_t (r4_to59 *  1051) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r4_b, r4_t] /\ [r4_b, r4_t] <s[Q2, Q2];


assert eqmod r5_b (r5_bo59 *   924) [Q] /\
       eqmod r5_t (r5_to59 *   924) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r5_b (r5_bo59 *   924) [Q] /\
       eqmod r5_t (r5_to59 *   924) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r5_b, r5_t] /\ [r5_b, r5_t] <s[Q2, Q2];


assert eqmod r6_b (r6_bo59 * -1237) [Q] /\
       eqmod r6_t (r6_to59 * -1237) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r6_b (r6_bo59 * -1237) [Q] /\
       eqmod r6_t (r6_to59 * -1237) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r6_b, r6_t] /\ [r6_b, r6_t] <s[Q2, Q2];


assert eqmod r7_b (r7_bo59 *   341) [Q] /\
       eqmod r7_t (r7_to59 *   341) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r7_b (r7_bo59 *   341) [Q] /\
       eqmod r7_t (r7_to59 *   341) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r7_b, r7_t] /\ [r7_b, r7_t] <s[Q2, Q2];


assert eqmod r8_b (r8_bo59 *   455) [Q] /\
       eqmod r8_t (r8_to59 *   455) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r8_b (r8_bo59 *   455) [Q] /\
       eqmod r8_t (r8_to59 *   455) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r8_b, r8_t] /\ [r8_b, r8_t] <s[Q2, Q2];


assert eqmod r9_b (r9_bo59 *   324) [Q] /\
       eqmod r9_t (r9_to59 *   324) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r9_b (r9_bo59 *   324) [Q] /\
       eqmod r9_t (r9_to59 *   324) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r9_b, r9_t] /\ [r9_b, r9_t] <s[Q2, Q2];

(* vmov	r0, s6                                     #! PC = 0x4014f4 *)
mov [r0_b, r0_t] [s6_b, s6_t];
(* str.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2fc; PC = 0x4014f8 *)
mov [L0xbefff2fc, L0xbefff2fe] [r6_b, r6_t];
(* str.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff33c; PC = 0x4014fc *)
mov [L0xbefff33c, L0xbefff33e] [r7_b, r7_t];
(* str.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff37c; PC = 0x401500 *)
mov [L0xbefff37c, L0xbefff37e] [r8_b, r8_t];
(* str.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff3bc; PC = 0x401504 *)
mov [L0xbefff3bc, L0xbefff3be] [r9_b, r9_t];
(* str.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff23c; PC = 0x401508 *)
mov [L0xbefff23c, L0xbefff23e] [r3_b, r3_t];
(* str.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff27c; PC = 0x40150c *)
mov [L0xbefff27c, L0xbefff27e] [r4_b, r4_t];
(* str.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff2bc; PC = 0x401510 *)
mov [L0xbefff2bc, L0xbefff2be] [r5_b, r5_t];
(* str.w	r2, [r0], #4                              #! EA = L0xbefff1fc; PC = 0x401514 *)
mov [L0xbefff1fc, L0xbefff1fe] [r2_b, r2_t];
(* vmov	lr, s14                                    #! PC = 0x401518 *)
mov [lr_b, lr_t] [s14_b, s14_t]; mov lr s14;
(* cmp.w	r0, lr                                    #! PC = 0x40151c *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x4012bc <invntt_fast+1552>              #! PC = 0x401520 *)
#bne.w	0x4012bc <invntt_fast+1552>              #! 0x401520 = 0x401520;

(* CUT 76 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo59, r2_to59] * [-1555, -1555])
          [L0xbefff1fc, L0xbefff1fe] [Q, Q] /\
    eqmod ([r3_bo59, r3_to59] * [  356,   356])
          [L0xbefff23c, L0xbefff23e] [Q, Q] /\
    eqmod ([r4_bo59, r4_to59] * [ 1051,  1051])
          [L0xbefff27c, L0xbefff27e] [Q, Q] /\
    eqmod ([r5_bo59, r5_to59] * [  924,   924])
          [L0xbefff2bc, L0xbefff2be] [Q, Q] /\
    eqmod ([r6_bo59, r6_to59] * [-1237, -1237])
          [L0xbefff2fc, L0xbefff2fe] [Q, Q] /\
    eqmod ([r7_bo59, r7_to59] * [  341,   341])
          [L0xbefff33c, L0xbefff33e] [Q, Q] /\
    eqmod ([r8_bo59, r8_to59] * [  455,   455])
          [L0xbefff37c, L0xbefff37e] [Q, Q] /\
    eqmod ([r9_bo59, r9_to59] * [  324,   324])
          [L0xbefff3bc, L0xbefff3be] [Q, Q] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff1fc,L0xbefff1fe,L0xbefff23c,L0xbefff23e] /\
    [L0xbefff1fc,L0xbefff1fe,L0xbefff23c,L0xbefff23e]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff27c,L0xbefff27e,L0xbefff2bc,L0xbefff2be] /\
    [L0xbefff27c,L0xbefff27e,L0xbefff2bc,L0xbefff2be]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff2fc,L0xbefff2fe,L0xbefff33c,L0xbefff33e] /\
    [L0xbefff2fc,L0xbefff2fe,L0xbefff33c,L0xbefff33e]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff37c,L0xbefff37e,L0xbefff3bc,L0xbefff3be] /\
    [L0xbefff37c,L0xbefff37e,L0xbefff3bc,L0xbefff3be]< [Q2,Q2,Q2,Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1fc,L0xbefff1fe,L0xbefff23c,L0xbefff23e] /\
    [L0xbefff1fc,L0xbefff1fe,L0xbefff23c,L0xbefff23e]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff27c,L0xbefff27e,L0xbefff2bc,L0xbefff2be] /\
    [L0xbefff27c,L0xbefff27e,L0xbefff2bc,L0xbefff2be]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2fc,L0xbefff2fe,L0xbefff33c,L0xbefff33e] /\
    [L0xbefff2fc,L0xbefff2fe,L0xbefff33c,L0xbefff33e]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff37c,L0xbefff37e,L0xbefff3bc,L0xbefff3be] /\
    [L0xbefff37c,L0xbefff37e,L0xbefff3bc,L0xbefff3be]<s[Q2,Q2,Q2,Q2];

(* vmov	s6, r0                                     #! PC = 0x4012bc *)
mov [s6_b, s6_t] [r0_b, r0_t];
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff200; Value = 0x0d2500ba; PC = 0x4012c0 *)
mov [r2_b, r2_t] [L0xbefff200, L0xbefff202];
(* ldr.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff240; Value = 0xfdc90286; PC = 0x4012c4 *)
mov [r3_b, r3_t] [L0xbefff240, L0xbefff242];
(* ldr.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff280; Value = 0xf5b5f235; PC = 0x4012c8 *)
mov [r4_b, r4_t] [L0xbefff280, L0xbefff282];
(* ldr.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff2c0; Value = 0xfa6efe0c; PC = 0x4012cc *)
mov [r5_b, r5_t] [L0xbefff2c0, L0xbefff2c2];
(* ldr.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff300; Value = 0x0942071e; PC = 0x4012d0 *)
mov [r6_b, r6_t] [L0xbefff300, L0xbefff302];
(* ldr.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff340; Value = 0xfb24056b; PC = 0x4012d4 *)
mov [r7_b, r7_t] [L0xbefff340, L0xbefff342];
(* ldr.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff380; Value = 0xf8890c46; PC = 0x4012d8 *)
mov [r8_b, r8_t] [L0xbefff380, L0xbefff382];
(* ldr.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff3c0; Value = 0xf1e7072a; PC = 0x4012dc *)
mov [r9_b, r9_t] [L0xbefff3c0, L0xbefff3c2];

ghost r2_bo60@int16, r2_to60@int16, r3_bo60@int16, r3_to60@int16,
      r4_bo60@int16, r4_to60@int16, r5_bo60@int16, r5_to60@int16,
      r6_bo60@int16, r6_to60@int16, r7_bo60@int16, r7_to60@int16,
      r8_bo60@int16, r8_to60@int16, r9_bo60@int16, r9_to60@int16:
      r2_bo60 = r2_b /\ r2_to60 = r2_t /\ r3_bo60 = r3_b /\ r3_to60 = r3_t /\
      r4_bo60 = r4_b /\ r4_to60 = r4_t /\ r5_bo60 = r5_b /\ r5_to60 = r5_t /\
      r6_bo60 = r6_b /\ r6_to60 = r6_t /\ r7_bo60 = r7_b /\ r7_to60 = r7_t /\
      r8_bo60 = r8_b /\ r8_to60 = r8_t /\ r9_bo60 = r9_b /\ r9_to60 = r9_t
   && r2_bo60 = r2_b /\ r2_to60 = r2_t /\ r3_bo60 = r3_b /\ r3_to60 = r3_t /\
      r4_bo60 = r4_b /\ r4_to60 = r4_t /\ r5_bo60 = r5_b /\ r5_to60 = r5_t /\
      r6_bo60 = r6_b /\ r6_to60 = r6_t /\ r7_bo60 = r7_b /\ r7_to60 = r7_t /\
      r8_bo60 = r8_b /\ r8_to60 = r8_t /\ r9_bo60 = r9_b /\ r9_to60 = r9_t;

(* movw	r0, #26632	; 0x6808                        #! PC = 0x4012e0 *)
mov r0_b 26632@int16; mov r0_t 0@int16; mov r0 26632@int32;
(* ldr.w	r10, [r1], #4                             #! EA = L0x401c38; Value = 0xae550d27; PC = 0x4012e4 *)
mov r10 L0x401c38;
(* smulwb	lr, r10, r3                              #! PC = 0x4012e8 *)
vpc r3_bW@int32 r3_b; mulj tmp r10 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r10, r3                              #! PC = 0x4012ec *)
vpc r3_tW@int32 r3_t; mulj tmp r10 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4012f0 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4012f4 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x4012f8 *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r3_bo60 * -1062) [Q] /\
       eqmod lr_t (r3_to60 * -1062) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r3_bo60 * -1062) [Q] /\
       eqmod lr_t (r3_to60 * -1062) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r3, r2, lr                               #! PC = 0x4012fc *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401300 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x401304 *)
vpc r5_bW@int32 r5_b; mulj tmp r10 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r10, r5                              #! PC = 0x401308 *)
vpc r5_tW@int32 r5_t; mulj tmp r10 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40130c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401310 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401314 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo60 * -1062) [Q] /\
       eqmod lr_t (r5_to60 * -1062) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo60 * -1062) [Q] /\
       eqmod lr_t (r5_to60 * -1062) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r4, lr                               #! PC = 0x401318 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x40131c *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x401320 *)
vpc r7_bW@int32 r7_b; mulj tmp r10 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r10, r7                              #! PC = 0x401324 *)
vpc r7_tW@int32 r7_t; mulj tmp r10 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401328 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x40132c *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x401330 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo60 * -1062) [Q] /\
       eqmod lr_t (r7_to60 * -1062) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo60 * -1062) [Q] /\
       eqmod lr_t (r7_to60 * -1062) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r6, lr                               #! PC = 0x401334 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x401338 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x40133c *)
vpc r9_bW@int32 r9_b; mulj tmp r10 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r10, r9                              #! PC = 0x401340 *)
vpc r9_tW@int32 r9_t; mulj tmp r10 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401344 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401348 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x40134c *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo60 * -1062) [Q] /\
       eqmod lr_t (r9_to60 * -1062) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo60 * -1062) [Q] /\
       eqmod lr_t (r9_to60 * -1062) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r8, lr                               #! PC = 0x401350 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x401354 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

assert [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [6*Q2,6*Q2,6*Q2,6*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
       [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [6*Q2,6*Q2,6*Q2,6*Q2]
    && [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
       [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2];


(* CUT 77 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo60, r2_to60] + [r3_bo60, r3_to60] * [-1062, -1062])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo60, r2_to60] - [r3_bo60, r3_to60] * [-1062, -1062])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r4_bo60, r4_to60] + [r5_bo60, r5_to60] * [-1062, -1062])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo60, r4_to60] - [r5_bo60, r5_to60] * [-1062, -1062])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo60, r6_to60] + [r7_bo60, r7_to60] * [-1062, -1062])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo60, r6_to60] - [r7_bo60, r7_to60] * [-1062, -1062])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r8_bo60, r8_to60] + [r9_bo60, r9_to60] * [-1062, -1062])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r8_bo60, r8_to60] - [r9_bo60, r9_to60] * [-1062, -1062])
          [r9_b, r9_t] [Q, Q] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [6*Q2,6*Q2,6*Q2,6*Q2] /\
    [6*NQ2,6*NQ2,6*NQ2,6*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [6*Q2,6*Q2,6*Q2,6*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2] /\
    [6@16*NQ2,6@16*NQ2,6@16*NQ2,6@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[6@16*Q2,6@16*Q2,6@16*Q2,6@16*Q2]
    prove with [cuts [1, 3, 5, 7, 9, 11, 13, 15]];



ghost r2_bo61@int16, r2_to61@int16, r3_bo61@int16, r3_to61@int16,
      r4_bo61@int16, r4_to61@int16, r5_bo61@int16, r5_to61@int16,
      r6_bo61@int16, r6_to61@int16, r7_bo61@int16, r7_to61@int16,
      r8_bo61@int16, r8_to61@int16, r9_bo61@int16, r9_to61@int16:
      r2_bo61 = r2_b /\ r2_to61 = r2_t /\ r3_bo61 = r3_b /\ r3_to61 = r3_t /\
      r4_bo61 = r4_b /\ r4_to61 = r4_t /\ r5_bo61 = r5_b /\ r5_to61 = r5_t /\
      r6_bo61 = r6_b /\ r6_to61 = r6_t /\ r7_bo61 = r7_b /\ r7_to61 = r7_t /\
      r8_bo61 = r8_b /\ r8_to61 = r8_t /\ r9_bo61 = r9_b /\ r9_to61 = r9_t
   && r2_bo61 = r2_b /\ r2_to61 = r2_t /\ r3_bo61 = r3_b /\ r3_to61 = r3_t /\
      r4_bo61 = r4_b /\ r4_to61 = r4_t /\ r5_bo61 = r5_b /\ r5_to61 = r5_t /\
      r6_bo61 = r6_b /\ r6_to61 = r6_t /\ r7_bo61 = r7_b /\ r7_to61 = r7_t /\
      r8_bo61 = r8_b /\ r8_to61 = r8_t /\ r9_bo61 = r9_b /\ r9_to61 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401c3c; Value = 0x43d365e5; PC = 0x401358 *)
mov [r10, r11] [L0x401c3c, L0x401c40];
(* smulwb	lr, r10, r4                              #! PC = 0x40135c *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401360 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401364 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401368 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x40136c *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r4_bo61 *   882) [Q] /\
       eqmod lr_t (r4_to61 *   882) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r4_bo61 *   882) [Q] /\
       eqmod lr_t (r4_to61 *   882) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r4, r2, lr                               #! PC = 0x401370 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x401374 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401378 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40137c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401380 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401384 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x401388 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r5_bo61 *  -296) [Q] /\
       eqmod lr_t (r5_to61 *  -296) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r5_bo61 *  -296) [Q] /\
       eqmod lr_t (r5_to61 *  -296) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r5, r3, lr                               #! PC = 0x40138c *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401390 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x401394 *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401398 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40139c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4013a0 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x4013a4 *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo61 *   882) [Q] /\
       eqmod lr_t (r8_to61 *   882) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo61 *   882) [Q] /\
       eqmod lr_t (r8_to61 *   882) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r6, lr                               #! PC = 0x4013a8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x4013ac *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4013b0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4013b4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013b8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4013bc *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4013c0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo61 *  -296) [Q] /\
       eqmod lr_t (r9_to61 *  -296) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo61 *  -296) [Q] /\
       eqmod lr_t (r9_to61 *  -296) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r7, lr                               #! PC = 0x4013c4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x4013c8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;

assert [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
       prove with [algebra solver isl] && true;
assume [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
       [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
    && [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
       [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2];


(* CUT 78 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo61, r2_to61] + [r4_bo61, r4_to61] * [  882,   882])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo61, r2_to61] - [r4_bo61, r4_to61] * [  882,   882])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r3_bo61, r3_to61] + [r5_bo61, r5_to61] * [ -296,  -296])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo61, r3_to61] - [r5_bo61, r5_to61] * [ -296,  -296])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r6_bo61, r6_to61] + [r8_bo61, r8_to61] * [  882,   882])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r6_bo61, r6_to61] - [r8_bo61, r8_to61] * [  882,   882])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r7_bo61, r7_to61] + [r9_bo61, r9_to61] * [ -296,  -296])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r7_bo61, r7_to61] - [r9_bo61, r9_to61] * [ -296,  -296])
          [r9_b, r9_t] [Q, Q] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [7*Q2,7*Q2,7*Q2,7*Q2] /\
    [7*NQ2,7*NQ2,7*NQ2,7*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [7*Q2,7*Q2,7*Q2,7*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2] /\
    [7@16*NQ2,7@16*NQ2,7@16*NQ2,7@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[7@16*Q2,7@16*Q2,7@16*Q2,7@16*Q2];



ghost r2_bo62@int16, r2_to62@int16, r3_bo62@int16, r3_to62@int16,
      r4_bo62@int16, r4_to62@int16, r5_bo62@int16, r5_to62@int16,
      r6_bo62@int16, r6_to62@int16, r7_bo62@int16, r7_to62@int16,
      r8_bo62@int16, r8_to62@int16, r9_bo62@int16, r9_to62@int16:
      r2_bo62 = r2_b /\ r2_to62 = r2_t /\ r3_bo62 = r3_b /\ r3_to62 = r3_t /\
      r4_bo62 = r4_b /\ r4_to62 = r4_t /\ r5_bo62 = r5_b /\ r5_to62 = r5_t /\
      r6_bo62 = r6_b /\ r6_to62 = r6_t /\ r7_bo62 = r7_b /\ r7_to62 = r7_t /\
      r8_bo62 = r8_b /\ r8_to62 = r8_t /\ r9_bo62 = r9_b /\ r9_to62 = r9_t
   && r2_bo62 = r2_b /\ r2_to62 = r2_t /\ r3_bo62 = r3_b /\ r3_to62 = r3_t /\
      r4_bo62 = r4_b /\ r4_to62 = r4_t /\ r5_bo62 = r5_b /\ r5_to62 = r5_t /\
      r6_bo62 = r6_b /\ r6_to62 = r6_t /\ r7_bo62 = r7_b /\ r7_to62 = r7_t /\
      r8_bo62 = r8_b /\ r8_to62 = r8_t /\ r9_bo62 = r9_b /\ r9_to62 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401c44; Value = 0x78f6b1f3; PC = 0x4013cc *)
mov [r10, r11] [L0x401c44, L0x401c48];
(* smulwb	lr, r10, r6                              #! PC = 0x4013d0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4013d4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013d8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4013dc *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x4013e0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r6_bo62 *  1573) [Q] /\
       eqmod lr_t (r6_to62 *  1573) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r6_bo62 *  1573) [Q] /\
       eqmod lr_t (r6_to62 *  1573) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r6, r2, lr                               #! PC = 0x4013e4 *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4013e8 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4013ec *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4013f0 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4013f4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4013f8 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x4013fc *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r7_bo62 *  -331) [Q] /\
       eqmod lr_t (r7_to62 *  -331) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r7_bo62 *  -331) [Q] /\
       eqmod lr_t (r7_to62 *  -331) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r7, r3, lr                               #! PC = 0x401400 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x401404 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401c4c; Value = 0x05d82a73; PC = 0x401408 *)
mov [r10, r11] [L0x401c4c, L0x401c50];
(* smulwb	lr, r10, r8                              #! PC = 0x40140c *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x401410 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401414 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x401418 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x40141c *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r8_bo62 *    76) [Q] /\
       eqmod lr_t (r8_to62 *    76) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r8_bo62 *    76) [Q] /\
       eqmod lr_t (r8_to62 *    76) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r8, r4, lr                               #! PC = 0x401420 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x401424 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x401428 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x40142c *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401430 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x401434 *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x401438 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov lr_b tmp_b; mov lr_t tmp_t;

assert eqmod lr_b (r9_bo62 *  -289) [Q] /\
       eqmod lr_t (r9_to62 *  -289) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod lr_b (r9_bo62 *  -289) [Q] /\
       eqmod lr_t (r9_to62 *  -289) [Q] /\
       [NQ2, NQ2] < [lr_b, lr_t] /\ [lr_b, lr_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[lr_b, lr_t] /\ [lr_b, lr_t] <s[Q2, Q2];

(* usub16	r9, r5, lr                               #! PC = 0x40143c *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x401440 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;

assert [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
       prove with [algebra solver isl, cuts [1,3,5,7,9,11,13,15]] && true;
assume [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
       [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
    && [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
       [r2_b,r2_t,r3_b,r3_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
       [r4_b,r4_t,r5_b,r5_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
       [r6_b,r6_t,r7_b,r7_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
       [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
       [r8_b,r8_t,r9_b,r9_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2];


(* CUT 79 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo62, r2_to62] + [r6_bo62, r6_to62] * [ 1573,  1573])
          [r2_b, r2_t] [Q, Q] /\
    eqmod ([r2_bo62, r2_to62] - [r6_bo62, r6_to62] * [ 1573,  1573])
          [r6_b, r6_t] [Q, Q] /\
    eqmod ([r3_bo62, r3_to62] + [r7_bo62, r7_to62] * [ -331,  -331])
          [r3_b, r3_t] [Q, Q] /\
    eqmod ([r3_bo62, r3_to62] - [r7_bo62, r7_to62] * [ -331,  -331])
          [r7_b, r7_t] [Q, Q] /\
    eqmod ([r4_bo62, r4_to62] + [r8_bo62, r8_to62] * [   76,    76])
          [r4_b, r4_t] [Q, Q] /\
    eqmod ([r4_bo62, r4_to62] - [r8_bo62, r8_to62] * [   76,    76])
          [r8_b, r8_t] [Q, Q] /\
    eqmod ([r5_bo62, r5_to62] + [r9_bo62, r9_to62] * [ -289,  -289])
          [r5_b, r5_t] [Q, Q] /\
    eqmod ([r5_bo62, r5_to62] - [r9_bo62, r9_to62] * [ -289,  -289])
          [r9_b, r9_t] [Q, Q] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]< [8*Q2,8*Q2,8*Q2,8*Q2] /\
    [8*NQ2,8*NQ2,8*NQ2,8*NQ2]< [r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]< [8*Q2,8*Q2,8*Q2,8*Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r2_b,r2_t,r3_b,r3_t] /\
    [r2_b,r2_t,r3_b,r3_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r4_b,r4_t,r5_b,r5_t] /\
    [r4_b,r4_t,r5_b,r5_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r6_b,r6_t,r7_b,r7_t] /\
    [r6_b,r6_t,r7_b,r7_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2] /\
    [8@16*NQ2,8@16*NQ2,8@16*NQ2,8@16*NQ2]<s[r8_b,r8_t,r9_b,r9_t] /\
    [r8_b,r8_t,r9_b,r9_t]<s[8@16*Q2,8@16*Q2,8@16*Q2,8@16*Q2];



ghost r2_bo63@int16, r2_to63@int16, r3_bo63@int16, r3_to63@int16,
      r4_bo63@int16, r4_to63@int16, r5_bo63@int16, r5_to63@int16,
      r6_bo63@int16, r6_to63@int16, r7_bo63@int16, r7_to63@int16,
      r8_bo63@int16, r8_to63@int16, r9_bo63@int16, r9_to63@int16:
      r2_bo63 = r2_b /\ r2_to63 = r2_t /\ r3_bo63 = r3_b /\ r3_to63 = r3_t /\
      r4_bo63 = r4_b /\ r4_to63 = r4_t /\ r5_bo63 = r5_b /\ r5_to63 = r5_t /\
      r6_bo63 = r6_b /\ r6_to63 = r6_t /\ r7_bo63 = r7_b /\ r7_to63 = r7_t /\
      r8_bo63 = r8_b /\ r8_to63 = r8_t /\ r9_bo63 = r9_b /\ r9_to63 = r9_t
   && r2_bo63 = r2_b /\ r2_to63 = r2_t /\ r3_bo63 = r3_b /\ r3_to63 = r3_t /\
      r4_bo63 = r4_b /\ r4_to63 = r4_t /\ r5_bo63 = r5_b /\ r5_to63 = r5_t /\
      r6_bo63 = r6_b /\ r6_to63 = r6_t /\ r7_bo63 = r7_b /\ r7_to63 = r7_t /\
      r8_bo63 = r8_b /\ r8_to63 = r8_t /\ r9_bo63 = r9_b /\ r9_to63 = r9_t;

(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401c54; Value = 0x2624735a; PC = 0x401444 *)
mov [r10, r11] [L0x401c54, L0x401c58];
(* smulwb	lr, r10, r2                              #! PC = 0x401448 *)
vpc r2_bW@int32 r2_b; mulj tmp r10 r2_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r2, r10, r2                              #! PC = 0x40144c *)
vpc r2_tW@int32 r2_t; mulj tmp r10 r2_tW; spl r2_w dc tmp 16;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b; vpc r2_t@int16 r2_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401450 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r2, r2, r12, r0                          #! PC = 0x401454 *)
mulj prod r2_b r12_t; add r2_w prod r0;
spl r2_t r2_b r2_w 16; cast r2_b@int16 r2_b;
(* pkhtb	r2, r2, lr, asr #16                       #! PC = 0x401458 *)
mov tmp_b lr_t; mov tmp_t r2_t;
mov r2_b tmp_b; mov r2_t tmp_t;
(* smulwb	lr, r11, r3                              #! PC = 0x40145c *)
vpc r3_bW@int32 r3_b; mulj tmp r11 r3_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r3, r11, r3                              #! PC = 0x401460 *)
vpc r3_tW@int32 r3_t; mulj tmp r11 r3_tW; spl r3_w dc tmp 16;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b; vpc r3_t@int16 r3_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401464 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x401468 *)
mulj prod r3_b r12_t; add r3_w prod r0;
spl r3_t r3_b r3_w 16; cast r3_b@int16 r3_b;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x40146c *)
mov tmp_b lr_t; mov tmp_t r3_t;
mov r3_b tmp_b; mov r3_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401c5c; Value = 0xf5b20600; PC = 0x401470 *)
mov [r10, r11] [L0x401c5c, L0x401c60];
(* smulwb	lr, r10, r4                              #! PC = 0x401474 *)
vpc r4_bW@int32 r4_b; mulj tmp r10 r4_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r4, r10, r4                              #! PC = 0x401478 *)
vpc r4_tW@int32 r4_t; mulj tmp r10 r4_tW; spl r4_w dc tmp 16;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b; vpc r4_t@int16 r4_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40147c *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x401480 *)
mulj prod r4_b r12_t; add r4_w prod r0;
spl r4_t r4_b r4_w 16; cast r4_b@int16 r4_b;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x401484 *)
mov tmp_b lr_t; mov tmp_t r4_t;
mov r4_b tmp_b; mov r4_t tmp_t;
(* smulwb	lr, r11, r5                              #! PC = 0x401488 *)
vpc r5_bW@int32 r5_b; mulj tmp r11 r5_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r5, r11, r5                              #! PC = 0x40148c *)
vpc r5_tW@int32 r5_t; mulj tmp r11 r5_tW; spl r5_w dc tmp 16;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b; vpc r5_t@int16 r5_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x401490 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x401494 *)
mulj prod r5_b r12_t; add r5_w prod r0;
spl r5_t r5_b r5_w 16; cast r5_b@int16 r5_b;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x401498 *)
mov tmp_b lr_t; mov tmp_t r5_t;
mov r5_b tmp_b; mov r5_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401c64; Value = 0x63d0efee; PC = 0x40149c *)
mov [r10, r11] [L0x401c64, L0x401c68];
(* smulwb	lr, r10, r6                              #! PC = 0x4014a0 *)
vpc r6_bW@int32 r6_b; mulj tmp r10 r6_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r6, r10, r6                              #! PC = 0x4014a4 *)
vpc r6_tW@int32 r6_t; mulj tmp r10 r6_tW; spl r6_w dc tmp 16;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b; vpc r6_t@int16 r6_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014a8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4014ac *)
mulj prod r6_b r12_t; add r6_w prod r0;
spl r6_t r6_b r6_w 16; cast r6_b@int16 r6_b;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x4014b0 *)
mov tmp_b lr_t; mov tmp_t r6_t;
mov r6_b tmp_b; mov r6_t tmp_t;
(* smulwb	lr, r11, r7                              #! PC = 0x4014b4 *)
vpc r7_bW@int32 r7_b; mulj tmp r11 r7_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r7, r11, r7                              #! PC = 0x4014b8 *)
vpc r7_tW@int32 r7_t; mulj tmp r11 r7_tW; spl r7_w dc tmp 16;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b; vpc r7_t@int16 r7_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014bc *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4014c0 *)
mulj prod r7_b r12_t; add r7_w prod r0;
spl r7_t r7_b r7_w 16; cast r7_b@int16 r7_b;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x4014c4 *)
mov tmp_b lr_t; mov tmp_t r7_t;
mov r7_b tmp_b; mov r7_t tmp_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401c6c; Value = 0x98a57d1e; PC = 0x4014c8 *)
mov [r10, r11] [L0x401c6c, L0x401c70];
(* smulwb	lr, r10, r8                              #! PC = 0x4014cc *)
vpc r8_bW@int32 r8_b; mulj tmp r10 r8_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r8, r10, r8                              #! PC = 0x4014d0 *)
vpc r8_tW@int32 r8_t; mulj tmp r10 r8_tW; spl r8_w dc tmp 16;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b; vpc r8_t@int16 r8_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014d4 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x4014d8 *)
mulj prod r8_b r12_t; add r8_w prod r0;
spl r8_t r8_b r8_w 16; cast r8_b@int16 r8_b;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x4014dc *)
mov tmp_b lr_t; mov tmp_t r8_t;
mov r8_b tmp_b; mov r8_t tmp_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4014e0 *)
vpc r9_bW@int32 r9_b; mulj tmp r11 r9_bW; spl lr_w dc tmp 16;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b; vpc lr_t@int16 lr_t;
(* smulwt	r9, r11, r9                              #! PC = 0x4014e4 *)
vpc r9_tW@int32 r9_t; mulj tmp r11 r9_tW; spl r9_w dc tmp 16;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b; vpc r9_t@int16 r9_t;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4014e8 *)
mulj prod lr_b r12_t; add lr_w prod r0;
spl lr_t lr_b lr_w 16; cast lr_b@int16 lr_b;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4014ec *)
mulj prod r9_b r12_t; add r9_w prod r0;
spl r9_t r9_b r9_w 16; cast r9_b@int16 r9_b;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x4014f0 *)
mov tmp_b lr_t; mov tmp_t r9_t;
mov r9_b tmp_b; mov r9_t tmp_t;

assert eqmod r2_b (r2_bo63 *   496) [Q] /\
       eqmod r2_t (r2_to63 *   496) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r2_b (r2_bo63 *   496) [Q] /\
       eqmod r2_t (r2_to63 *   496) [Q] /\
       [NQ2, NQ2] < [r2_b, r2_t] /\ [r2_b, r2_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r2_b, r2_t] /\ [r2_b, r2_t] <s[Q2, Q2];


assert eqmod r3_b (r3_bo63 * -1154) [Q] /\
       eqmod r3_t (r3_to63 * -1154) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r3_b (r3_bo63 * -1154) [Q] /\
       eqmod r3_t (r3_to63 * -1154) [Q] /\
       [NQ2, NQ2] < [r3_b, r3_t] /\ [r3_b, r3_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r3_b, r3_t] /\ [r3_b, r3_t] <s[Q2, Q2];


assert eqmod r4_b (r4_bo63 *  -134) [Q] /\
       eqmod r4_t (r4_to63 *  -134) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r4_b (r4_bo63 *  -134) [Q] /\
       eqmod r4_t (r4_to63 *  -134) [Q] /\
       [NQ2, NQ2] < [r4_b, r4_t] /\ [r4_b, r4_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r4_b, r4_t] /\ [r4_b, r4_t] <s[Q2, Q2];


assert eqmod r5_b (r5_bo63 *   446) [Q] /\
       eqmod r5_t (r5_to63 *   446) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r5_b (r5_bo63 *   446) [Q] /\
       eqmod r5_t (r5_to63 *   446) [Q] /\
       [NQ2, NQ2] < [r5_b, r5_t] /\ [r5_b, r5_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r5_b, r5_t] /\ [r5_b, r5_t] <s[Q2, Q2];


assert eqmod r6_b (r6_bo63 *  1298) [Q] /\
       eqmod r6_t (r6_to63 *  1298) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r6_b (r6_bo63 *  1298) [Q] /\
       eqmod r6_t (r6_to63 *  1298) [Q] /\
       [NQ2, NQ2] < [r6_b, r6_t] /\ [r6_b, r6_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r6_b, r6_t] /\ [r6_b, r6_t] <s[Q2, Q2];


assert eqmod r7_b (r7_bo63 *  1195) [Q] /\
       eqmod r7_t (r7_to63 *  1195) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r7_b (r7_bo63 *  1195) [Q] /\
       eqmod r7_t (r7_to63 *  1195) [Q] /\
       [NQ2, NQ2] < [r7_b, r7_t] /\ [r7_b, r7_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r7_b, r7_t] /\ [r7_b, r7_t] <s[Q2, Q2];


assert eqmod r8_b (r8_bo63 * -1344) [Q] /\
       eqmod r8_t (r8_to63 * -1344) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r8_b (r8_bo63 * -1344) [Q] /\
       eqmod r8_t (r8_to63 * -1344) [Q] /\
       [NQ2, NQ2] < [r8_b, r8_t] /\ [r8_b, r8_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r8_b, r8_t] /\ [r8_b, r8_t] <s[Q2, Q2];


assert eqmod r9_b (r9_bo63 *  1194) [Q] /\
       eqmod r9_t (r9_to63 *  1194) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
       prove with [algebra solver isl] && true;
assume eqmod r9_b (r9_bo63 *  1194) [Q] /\
       eqmod r9_t (r9_to63 *  1194) [Q] /\
       [NQ2, NQ2] < [r9_b, r9_t] /\ [r9_b, r9_t] < [Q2, Q2]
    && [NQ2, NQ2] <s[r9_b, r9_t] /\ [r9_b, r9_t] <s[Q2, Q2];

(* vmov	r0, s6                                     #! PC = 0x4014f4 *)
mov [r0_b, r0_t] [s6_b, s6_t];
(* str.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff300; PC = 0x4014f8 *)
mov [L0xbefff300, L0xbefff302] [r6_b, r6_t];
(* str.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff340; PC = 0x4014fc *)
mov [L0xbefff340, L0xbefff342] [r7_b, r7_t];
(* str.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff380; PC = 0x401500 *)
mov [L0xbefff380, L0xbefff382] [r8_b, r8_t];
(* str.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff3c0; PC = 0x401504 *)
mov [L0xbefff3c0, L0xbefff3c2] [r9_b, r9_t];
(* str.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff240; PC = 0x401508 *)
mov [L0xbefff240, L0xbefff242] [r3_b, r3_t];
(* str.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff280; PC = 0x40150c *)
mov [L0xbefff280, L0xbefff282] [r4_b, r4_t];
(* str.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff2c0; PC = 0x401510 *)
mov [L0xbefff2c0, L0xbefff2c2] [r5_b, r5_t];
(* str.w	r2, [r0], #4                              #! EA = L0xbefff200; PC = 0x401514 *)
mov [L0xbefff200, L0xbefff202] [r2_b, r2_t];
(* vmov	lr, s14                                    #! PC = 0x401518 *)
mov [lr_b, lr_t] [s14_b, s14_t]; mov lr s14;
(* cmp.w	r0, lr                                    #! PC = 0x40151c *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x4012bc <invntt_fast+1552>              #! PC = 0x401520 *)
#bne.w	0x4012bc <invntt_fast+1552>              #! 0x401520 = 0x401520;

(* CUT 80 *)
cut Q = 3329 /\ NQ = -3329 /\ Q2 = 1665 /\ NQ2 = -1665 /\
    eqmod ([r2_bo63, r2_to63] * [  496,   496])
          [L0xbefff200, L0xbefff202] [Q, Q] /\
    eqmod ([r3_bo63, r3_to63] * [-1154, -1154])
          [L0xbefff240, L0xbefff242] [Q, Q] /\
    eqmod ([r4_bo63, r4_to63] * [ -134,  -134])
          [L0xbefff280, L0xbefff282] [Q, Q] /\
    eqmod ([r5_bo63, r5_to63] * [  446,   446])
          [L0xbefff2c0, L0xbefff2c2] [Q, Q] /\
    eqmod ([r6_bo63, r6_to63] * [ 1298,  1298])
          [L0xbefff300, L0xbefff302] [Q, Q] /\
    eqmod ([r7_bo63, r7_to63] * [ 1195,  1195])
          [L0xbefff340, L0xbefff342] [Q, Q] /\
    eqmod ([r8_bo63, r8_to63] * [-1344, -1344])
          [L0xbefff380, L0xbefff382] [Q, Q] /\
    eqmod ([r9_bo63, r9_to63] * [ 1194,  1194])
          [L0xbefff3c0, L0xbefff3c2] [Q, Q] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff200,L0xbefff202,L0xbefff240,L0xbefff242] /\
    [L0xbefff200,L0xbefff202,L0xbefff240,L0xbefff242]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff280,L0xbefff282,L0xbefff2c0,L0xbefff2c2] /\
    [L0xbefff280,L0xbefff282,L0xbefff2c0,L0xbefff2c2]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff300,L0xbefff302,L0xbefff340,L0xbefff342] /\
    [L0xbefff300,L0xbefff302,L0xbefff340,L0xbefff342]< [Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]< [L0xbefff380,L0xbefff382,L0xbefff3c0,L0xbefff3c2] /\
    [L0xbefff380,L0xbefff382,L0xbefff3c0,L0xbefff3c2]< [Q2,Q2,Q2,Q2]
 && Q = 3329@16 /\ NQ = (-3329)@16 /\ Q2 = 1665@16 /\ NQ2 = (-1665)@16 /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff200,L0xbefff202,L0xbefff240,L0xbefff242] /\
    [L0xbefff200,L0xbefff202,L0xbefff240,L0xbefff242]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff280,L0xbefff282,L0xbefff2c0,L0xbefff2c2] /\
    [L0xbefff280,L0xbefff282,L0xbefff2c0,L0xbefff2c2]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff300,L0xbefff302,L0xbefff340,L0xbefff342] /\
    [L0xbefff300,L0xbefff302,L0xbefff340,L0xbefff342]<s[Q2,Q2,Q2,Q2] /\
    [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff380,L0xbefff382,L0xbefff3c0,L0xbefff3c2] /\
    [L0xbefff380,L0xbefff382,L0xbefff3c0,L0xbefff3c2]<s[Q2,Q2,Q2,Q2];


{
   (* NOTE: -1353 == 1976 == -2**32 (mod 3329) *)
   eqmod (poly X [L0xbefff1c4,L0xbefff1c6,L0xbefff1c8,L0xbefff1ca,L0xbefff1cc,
                  L0xbefff1ce,L0xbefff1d0,L0xbefff1d2,L0xbefff1d4,L0xbefff1d6,
                  L0xbefff1d8,L0xbefff1da,L0xbefff1dc,L0xbefff1de,L0xbefff1e0,
                  L0xbefff1e2,L0xbefff1e4,L0xbefff1e6,L0xbefff1e8,L0xbefff1ea,
                  L0xbefff1ec,L0xbefff1ee,L0xbefff1f0,L0xbefff1f2,L0xbefff1f4,
                  L0xbefff1f6,L0xbefff1f8,L0xbefff1fa,L0xbefff1fc,L0xbefff1fe,
                  L0xbefff200,L0xbefff202,L0xbefff204,L0xbefff206,L0xbefff208,
                  L0xbefff20a,L0xbefff20c,L0xbefff20e,L0xbefff210,L0xbefff212,
                  L0xbefff214,L0xbefff216,L0xbefff218,L0xbefff21a,L0xbefff21c,
                  L0xbefff21e,L0xbefff220,L0xbefff222,L0xbefff224,L0xbefff226,
                  L0xbefff228,L0xbefff22a,L0xbefff22c,L0xbefff22e,L0xbefff230,
                  L0xbefff232,L0xbefff234,L0xbefff236,L0xbefff238,L0xbefff23a,
                  L0xbefff23c,L0xbefff23e,L0xbefff240,L0xbefff242,L0xbefff244,
                  L0xbefff246,L0xbefff248,L0xbefff24a,L0xbefff24c,L0xbefff24e,
                  L0xbefff250,L0xbefff252,L0xbefff254,L0xbefff256,L0xbefff258,
                  L0xbefff25a,L0xbefff25c,L0xbefff25e,L0xbefff260,L0xbefff262,
                  L0xbefff264,L0xbefff266,L0xbefff268,L0xbefff26a,L0xbefff26c,
                  L0xbefff26e,L0xbefff270,L0xbefff272,L0xbefff274,L0xbefff276,
                  L0xbefff278,L0xbefff27a,L0xbefff27c,L0xbefff27e,L0xbefff280,
                  L0xbefff282,L0xbefff284,L0xbefff286,L0xbefff288,L0xbefff28a,
                  L0xbefff28c,L0xbefff28e,L0xbefff290,L0xbefff292,L0xbefff294,
                  L0xbefff296,L0xbefff298,L0xbefff29a,L0xbefff29c,L0xbefff29e,
                  L0xbefff2a0,L0xbefff2a2,L0xbefff2a4,L0xbefff2a6,L0xbefff2a8,
                  L0xbefff2aa,L0xbefff2ac,L0xbefff2ae,L0xbefff2b0,L0xbefff2b2,
                  L0xbefff2b4,L0xbefff2b6,L0xbefff2b8,L0xbefff2ba,L0xbefff2bc,
                  L0xbefff2be,L0xbefff2c0,L0xbefff2c2,L0xbefff2c4,L0xbefff2c6,
                  L0xbefff2c8,L0xbefff2ca,L0xbefff2cc,L0xbefff2ce,L0xbefff2d0,
                  L0xbefff2d2,L0xbefff2d4,L0xbefff2d6,L0xbefff2d8,L0xbefff2da,
                  L0xbefff2dc,L0xbefff2de,L0xbefff2e0,L0xbefff2e2,L0xbefff2e4,
                  L0xbefff2e6,L0xbefff2e8,L0xbefff2ea,L0xbefff2ec,L0xbefff2ee,
                  L0xbefff2f0,L0xbefff2f2,L0xbefff2f4,L0xbefff2f6,L0xbefff2f8,
                  L0xbefff2fa,L0xbefff2fc,L0xbefff2fe,L0xbefff300,L0xbefff302,
                  L0xbefff304,L0xbefff306,L0xbefff308,L0xbefff30a,L0xbefff30c,
                  L0xbefff30e,L0xbefff310,L0xbefff312,L0xbefff314,L0xbefff316,
                  L0xbefff318,L0xbefff31a,L0xbefff31c,L0xbefff31e,L0xbefff320,
                  L0xbefff322,L0xbefff324,L0xbefff326,L0xbefff328,L0xbefff32a,
                  L0xbefff32c,L0xbefff32e,L0xbefff330,L0xbefff332,L0xbefff334,
                  L0xbefff336,L0xbefff338,L0xbefff33a,L0xbefff33c,L0xbefff33e,
                  L0xbefff340,L0xbefff342,L0xbefff344,L0xbefff346,L0xbefff348,
                  L0xbefff34a,L0xbefff34c,L0xbefff34e,L0xbefff350,L0xbefff352,
                  L0xbefff354,L0xbefff356,L0xbefff358,L0xbefff35a,L0xbefff35c,
                  L0xbefff35e,L0xbefff360,L0xbefff362,L0xbefff364,L0xbefff366,
                  L0xbefff368,L0xbefff36a,L0xbefff36c,L0xbefff36e,L0xbefff370,
                  L0xbefff372,L0xbefff374,L0xbefff376,L0xbefff378,L0xbefff37a,
                  L0xbefff37c,L0xbefff37e,L0xbefff380,L0xbefff382,L0xbefff384,
                  L0xbefff386,L0xbefff388,L0xbefff38a,L0xbefff38c,L0xbefff38e,
                  L0xbefff390,L0xbefff392,L0xbefff394,L0xbefff396,L0xbefff398,
                  L0xbefff39a,L0xbefff39c,L0xbefff39e,L0xbefff3a0,L0xbefff3a2,
                  L0xbefff3a4,L0xbefff3a6,L0xbefff3a8,L0xbefff3aa,L0xbefff3ac,
                  L0xbefff3ae,L0xbefff3b0,L0xbefff3b2,L0xbefff3b4,L0xbefff3b6,
                  L0xbefff3b8,L0xbefff3ba,L0xbefff3bc,L0xbefff3be,L0xbefff3c0,
                  L0xbefff3c2])
         (-1353*F**2) [Q, X**256 - 17**128]
   prove with [all ghosts, all cuts]
&&
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1c4,L0xbefff1c6,L0xbefff1c8,L0xbefff1ca] /\
   [L0xbefff1c4,L0xbefff1c6,L0xbefff1c8,L0xbefff1ca]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1cc,L0xbefff1ce,L0xbefff1d0,L0xbefff1d2] /\
   [L0xbefff1cc,L0xbefff1ce,L0xbefff1d0,L0xbefff1d2]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1d4,L0xbefff1d6,L0xbefff1d8,L0xbefff1da] /\
   [L0xbefff1d4,L0xbefff1d6,L0xbefff1d8,L0xbefff1da]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1dc,L0xbefff1de,L0xbefff1e0,L0xbefff1e2] /\
   [L0xbefff1dc,L0xbefff1de,L0xbefff1e0,L0xbefff1e2]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1e4,L0xbefff1e6,L0xbefff1e8,L0xbefff1ea] /\
   [L0xbefff1e4,L0xbefff1e6,L0xbefff1e8,L0xbefff1ea]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1ec,L0xbefff1ee,L0xbefff1f0,L0xbefff1f2] /\
   [L0xbefff1ec,L0xbefff1ee,L0xbefff1f0,L0xbefff1f2]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1f4,L0xbefff1f6,L0xbefff1f8,L0xbefff1fa] /\
   [L0xbefff1f4,L0xbefff1f6,L0xbefff1f8,L0xbefff1fa]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff1fc,L0xbefff1fe,L0xbefff200,L0xbefff202] /\
   [L0xbefff1fc,L0xbefff1fe,L0xbefff200,L0xbefff202]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff204,L0xbefff206,L0xbefff208,L0xbefff20a] /\
   [L0xbefff204,L0xbefff206,L0xbefff208,L0xbefff20a]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff20c,L0xbefff20e,L0xbefff210,L0xbefff212] /\
   [L0xbefff20c,L0xbefff20e,L0xbefff210,L0xbefff212]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff214,L0xbefff216,L0xbefff218,L0xbefff21a] /\
   [L0xbefff214,L0xbefff216,L0xbefff218,L0xbefff21a]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff21c,L0xbefff21e,L0xbefff220,L0xbefff222] /\
   [L0xbefff21c,L0xbefff21e,L0xbefff220,L0xbefff222]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff224,L0xbefff226,L0xbefff228,L0xbefff22a] /\
   [L0xbefff224,L0xbefff226,L0xbefff228,L0xbefff22a]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff22c,L0xbefff22e,L0xbefff230,L0xbefff232] /\
   [L0xbefff22c,L0xbefff22e,L0xbefff230,L0xbefff232]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff234,L0xbefff236,L0xbefff238,L0xbefff23a] /\
   [L0xbefff234,L0xbefff236,L0xbefff238,L0xbefff23a]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff23c,L0xbefff23e,L0xbefff240,L0xbefff242] /\
   [L0xbefff23c,L0xbefff23e,L0xbefff240,L0xbefff242]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff244,L0xbefff246,L0xbefff248,L0xbefff24a] /\
   [L0xbefff244,L0xbefff246,L0xbefff248,L0xbefff24a]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff24c,L0xbefff24e,L0xbefff250,L0xbefff252] /\
   [L0xbefff24c,L0xbefff24e,L0xbefff250,L0xbefff252]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff254,L0xbefff256,L0xbefff258,L0xbefff25a] /\
   [L0xbefff254,L0xbefff256,L0xbefff258,L0xbefff25a]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff25c,L0xbefff25e,L0xbefff260,L0xbefff262] /\
   [L0xbefff25c,L0xbefff25e,L0xbefff260,L0xbefff262]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff264,L0xbefff266,L0xbefff268,L0xbefff26a] /\
   [L0xbefff264,L0xbefff266,L0xbefff268,L0xbefff26a]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff26c,L0xbefff26e,L0xbefff270,L0xbefff272] /\
   [L0xbefff26c,L0xbefff26e,L0xbefff270,L0xbefff272]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff274,L0xbefff276,L0xbefff278,L0xbefff27a] /\
   [L0xbefff274,L0xbefff276,L0xbefff278,L0xbefff27a]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff27c,L0xbefff27e,L0xbefff280,L0xbefff282] /\
   [L0xbefff27c,L0xbefff27e,L0xbefff280,L0xbefff282]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff284,L0xbefff286,L0xbefff288,L0xbefff28a] /\
   [L0xbefff284,L0xbefff286,L0xbefff288,L0xbefff28a]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff28c,L0xbefff28e,L0xbefff290,L0xbefff292] /\
   [L0xbefff28c,L0xbefff28e,L0xbefff290,L0xbefff292]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff294,L0xbefff296,L0xbefff298,L0xbefff29a] /\
   [L0xbefff294,L0xbefff296,L0xbefff298,L0xbefff29a]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff29c,L0xbefff29e,L0xbefff2a0,L0xbefff2a2] /\
   [L0xbefff29c,L0xbefff29e,L0xbefff2a0,L0xbefff2a2]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2a4,L0xbefff2a6,L0xbefff2a8,L0xbefff2aa] /\
   [L0xbefff2a4,L0xbefff2a6,L0xbefff2a8,L0xbefff2aa]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2ac,L0xbefff2ae,L0xbefff2b0,L0xbefff2b2] /\
   [L0xbefff2ac,L0xbefff2ae,L0xbefff2b0,L0xbefff2b2]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2b4,L0xbefff2b6,L0xbefff2b8,L0xbefff2ba] /\
   [L0xbefff2b4,L0xbefff2b6,L0xbefff2b8,L0xbefff2ba]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2bc,L0xbefff2be,L0xbefff2c0,L0xbefff2c2] /\
   [L0xbefff2bc,L0xbefff2be,L0xbefff2c0,L0xbefff2c2]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2c4,L0xbefff2c6,L0xbefff2c8,L0xbefff2ca] /\
   [L0xbefff2c4,L0xbefff2c6,L0xbefff2c8,L0xbefff2ca]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2cc,L0xbefff2ce,L0xbefff2d0,L0xbefff2d2] /\
   [L0xbefff2cc,L0xbefff2ce,L0xbefff2d0,L0xbefff2d2]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2d4,L0xbefff2d6,L0xbefff2d8,L0xbefff2da] /\
   [L0xbefff2d4,L0xbefff2d6,L0xbefff2d8,L0xbefff2da]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2dc,L0xbefff2de,L0xbefff2e0,L0xbefff2e2] /\
   [L0xbefff2dc,L0xbefff2de,L0xbefff2e0,L0xbefff2e2]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2e4,L0xbefff2e6,L0xbefff2e8,L0xbefff2ea] /\
   [L0xbefff2e4,L0xbefff2e6,L0xbefff2e8,L0xbefff2ea]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2ec,L0xbefff2ee,L0xbefff2f0,L0xbefff2f2] /\
   [L0xbefff2ec,L0xbefff2ee,L0xbefff2f0,L0xbefff2f2]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2f4,L0xbefff2f6,L0xbefff2f8,L0xbefff2fa] /\
   [L0xbefff2f4,L0xbefff2f6,L0xbefff2f8,L0xbefff2fa]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff2fc,L0xbefff2fe,L0xbefff300,L0xbefff302] /\
   [L0xbefff2fc,L0xbefff2fe,L0xbefff300,L0xbefff302]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff304,L0xbefff306,L0xbefff308,L0xbefff30a] /\
   [L0xbefff304,L0xbefff306,L0xbefff308,L0xbefff30a]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff30c,L0xbefff30e,L0xbefff310,L0xbefff312] /\
   [L0xbefff30c,L0xbefff30e,L0xbefff310,L0xbefff312]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff314,L0xbefff316,L0xbefff318,L0xbefff31a] /\
   [L0xbefff314,L0xbefff316,L0xbefff318,L0xbefff31a]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff31c,L0xbefff31e,L0xbefff320,L0xbefff322] /\
   [L0xbefff31c,L0xbefff31e,L0xbefff320,L0xbefff322]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff324,L0xbefff326,L0xbefff328,L0xbefff32a] /\
   [L0xbefff324,L0xbefff326,L0xbefff328,L0xbefff32a]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff32c,L0xbefff32e,L0xbefff330,L0xbefff332] /\
   [L0xbefff32c,L0xbefff32e,L0xbefff330,L0xbefff332]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff334,L0xbefff336,L0xbefff338,L0xbefff33a] /\
   [L0xbefff334,L0xbefff336,L0xbefff338,L0xbefff33a]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff33c,L0xbefff33e,L0xbefff340,L0xbefff342] /\
   [L0xbefff33c,L0xbefff33e,L0xbefff340,L0xbefff342]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff344,L0xbefff346,L0xbefff348,L0xbefff34a] /\
   [L0xbefff344,L0xbefff346,L0xbefff348,L0xbefff34a]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff34c,L0xbefff34e,L0xbefff350,L0xbefff352] /\
   [L0xbefff34c,L0xbefff34e,L0xbefff350,L0xbefff352]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff354,L0xbefff356,L0xbefff358,L0xbefff35a] /\
   [L0xbefff354,L0xbefff356,L0xbefff358,L0xbefff35a]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff35c,L0xbefff35e,L0xbefff360,L0xbefff362] /\
   [L0xbefff35c,L0xbefff35e,L0xbefff360,L0xbefff362]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff364,L0xbefff366,L0xbefff368,L0xbefff36a] /\
   [L0xbefff364,L0xbefff366,L0xbefff368,L0xbefff36a]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff36c,L0xbefff36e,L0xbefff370,L0xbefff372] /\
   [L0xbefff36c,L0xbefff36e,L0xbefff370,L0xbefff372]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff374,L0xbefff376,L0xbefff378,L0xbefff37a] /\
   [L0xbefff374,L0xbefff376,L0xbefff378,L0xbefff37a]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff37c,L0xbefff37e,L0xbefff380,L0xbefff382] /\
   [L0xbefff37c,L0xbefff37e,L0xbefff380,L0xbefff382]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff384,L0xbefff386,L0xbefff388,L0xbefff38a] /\
   [L0xbefff384,L0xbefff386,L0xbefff388,L0xbefff38a]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff38c,L0xbefff38e,L0xbefff390,L0xbefff392] /\
   [L0xbefff38c,L0xbefff38e,L0xbefff390,L0xbefff392]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff394,L0xbefff396,L0xbefff398,L0xbefff39a] /\
   [L0xbefff394,L0xbefff396,L0xbefff398,L0xbefff39a]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff39c,L0xbefff39e,L0xbefff3a0,L0xbefff3a2] /\
   [L0xbefff39c,L0xbefff39e,L0xbefff3a0,L0xbefff3a2]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff3a4,L0xbefff3a6,L0xbefff3a8,L0xbefff3aa] /\
   [L0xbefff3a4,L0xbefff3a6,L0xbefff3a8,L0xbefff3aa]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff3ac,L0xbefff3ae,L0xbefff3b0,L0xbefff3b2] /\
   [L0xbefff3ac,L0xbefff3ae,L0xbefff3b0,L0xbefff3b2]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff3b4,L0xbefff3b6,L0xbefff3b8,L0xbefff3ba] /\
   [L0xbefff3b4,L0xbefff3b6,L0xbefff3b8,L0xbefff3ba]<s[Q2,Q2,Q2,Q2] /\
   [NQ2,NQ2,NQ2,NQ2]<s[L0xbefff3bc,L0xbefff3be,L0xbefff3c0,L0xbefff3c2] /\
   [L0xbefff3bc,L0xbefff3be,L0xbefff3c0,L0xbefff3c2]<s[Q2,Q2,Q2,Q2]
   prove with [all cuts]
}
