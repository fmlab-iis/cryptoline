(* Macbook: cv -v -implicit-const-conversion -slicing -disable_safety -disable_range -debug -o ntt_isl.txt  -jobs 8 ntt_isl.cl
Parsing CryptoLine file:			[OK]		0.1103 seconds
Checking well-formedness:			[OK]		0.0310 seconds

Procedure main
==============
Transforming to SSA form:			[OK]		0.0149 seconds
Normalizing specification:			[OK]		0.0183 seconds
Rewriting assignments:				[OK]		0.0227 seconds
Rewriting value-preserved casting:		[OK]		0.0040 seconds
Verifying algebraic assertions:			[OK]		3461.0038 seconds
Verifying algebraic specification:		[OK]		3348.3106 seconds

Procedure Summary
-----------------
Procedure verification:				[OK]		6809.3865 seconds

Summary
=======
Verification result:				[OK]		6809.5281 seconds
*)


proc main (sint16 x, sint16 f000, sint16 f001, sint16 f002, sint16 f003, sint16 f004, sint16 f005, sint16 f006, sint16 f007, sint16 f008, sint16 f009, sint16 f010, sint16 f011, sint16 f012, sint16 f013, sint16 f014, sint16 f015, sint16 f016, sint16 f017, sint16 f018, sint16 f019, sint16 f020, sint16 f021, sint16 f022, sint16 f023, sint16 f024, sint16 f025, sint16 f026, sint16 f027, sint16 f028, sint16 f029, sint16 f030, sint16 f031, sint16 f032, sint16 f033, sint16 f034, sint16 f035, sint16 f036, sint16 f037, sint16 f038, sint16 f039, sint16 f040, sint16 f041, sint16 f042, sint16 f043, sint16 f044, sint16 f045, sint16 f046, sint16 f047, sint16 f048, sint16 f049, sint16 f050, sint16 f051, sint16 f052, sint16 f053, sint16 f054, sint16 f055, sint16 f056, sint16 f057, sint16 f058, sint16 f059, sint16 f060, sint16 f061, sint16 f062, sint16 f063, sint16 f064, sint16 f065, sint16 f066, sint16 f067, sint16 f068, sint16 f069, sint16 f070, sint16 f071, sint16 f072, sint16 f073, sint16 f074, sint16 f075, sint16 f076, sint16 f077, sint16 f078, sint16 f079, sint16 f080, sint16 f081, sint16 f082, sint16 f083, sint16 f084, sint16 f085, sint16 f086, sint16 f087, sint16 f088, sint16 f089, sint16 f090, sint16 f091, sint16 f092, sint16 f093, sint16 f094, sint16 f095, sint16 f096, sint16 f097, sint16 f098, sint16 f099, sint16 f100, sint16 f101, sint16 f102, sint16 f103, sint16 f104, sint16 f105, sint16 f106, sint16 f107, sint16 f108, sint16 f109, sint16 f110, sint16 f111, sint16 f112, sint16 f113, sint16 f114, sint16 f115, sint16 f116, sint16 f117, sint16 f118, sint16 f119, sint16 f120, sint16 f121, sint16 f122, sint16 f123, sint16 f124, sint16 f125, sint16 f126, sint16 f127, sint16 f128, sint16 f129, sint16 f130, sint16 f131, sint16 f132, sint16 f133, sint16 f134, sint16 f135, sint16 f136, sint16 f137, sint16 f138, sint16 f139, sint16 f140, sint16 f141, sint16 f142, sint16 f143, sint16 f144, sint16 f145, sint16 f146, sint16 f147, sint16 f148, sint16 f149, sint16 f150, sint16 f151, sint16 f152, sint16 f153, sint16 f154, sint16 f155, sint16 f156, sint16 f157, sint16 f158, sint16 f159, sint16 f160, sint16 f161, sint16 f162, sint16 f163, sint16 f164, sint16 f165, sint16 f166, sint16 f167, sint16 f168, sint16 f169, sint16 f170, sint16 f171, sint16 f172, sint16 f173, sint16 f174, sint16 f175, sint16 f176, sint16 f177, sint16 f178, sint16 f179, sint16 f180, sint16 f181, sint16 f182, sint16 f183, sint16 f184, sint16 f185, sint16 f186, sint16 f187, sint16 f188, sint16 f189, sint16 f190, sint16 f191, sint16 f192, sint16 f193, sint16 f194, sint16 f195, sint16 f196, sint16 f197, sint16 f198, sint16 f199, sint16 f200, sint16 f201, sint16 f202, sint16 f203, sint16 f204, sint16 f205, sint16 f206, sint16 f207, sint16 f208, sint16 f209, sint16 f210, sint16 f211, sint16 f212, sint16 f213, sint16 f214, sint16 f215, sint16 f216, sint16 f217, sint16 f218, sint16 f219, sint16 f220, sint16 f221, sint16 f222, sint16 f223, sint16 f224, sint16 f225, sint16 f226, sint16 f227, sint16 f228, sint16 f229, sint16 f230, sint16 f231, sint16 f232, sint16 f233, sint16 f234, sint16 f235, sint16 f236, sint16 f237, sint16 f238, sint16 f239, sint16 f240, sint16 f241, sint16 f242, sint16 f243, sint16 f244, sint16 f245, sint16 f246, sint16 f247, sint16 f248, sint16 f249, sint16 f250, sint16 f251, sint16 f252, sint16 f253, sint16 f254, sint16 f255) =
{
  (* algebraic range *)
  and [(-2) * 1664 <= f000, f000 <= 2 * 1664, (-2) * 1664 <= f001, f001 <= 2 * 1664, (-2) * 1664 <= f002, f002 <= 2 * 1664, (-2) * 1664 <= f003, f003 <= 2 * 1664, (-2) * 1664 <= f004, f004 <= 2 * 1664, (-2) * 1664 <= f005, f005 <= 2 * 1664, (-2) * 1664 <= f006, f006 <= 2 * 1664, (-2) * 1664 <= f007, f007 <= 2 * 1664, (-2) * 1664 <= f008, f008 <= 2 * 1664, (-2) * 1664 <= f009, f009 <= 2 * 1664, (-2) * 1664 <= f010, f010 <= 2 * 1664, (-2) * 1664 <= f011, f011 <= 2 * 1664, (-2) * 1664 <= f012, f012 <= 2 * 1664, (-2) * 1664 <= f013, f013 <= 2 * 1664, (-2) * 1664 <= f014, f014 <= 2 * 1664, (-2) * 1664 <= f015, f015 <= 2 * 1664, (-2) * 1664 <= f016, f016 <= 2 * 1664, (-2) * 1664 <= f017, f017 <= 2 * 1664, (-2) * 1664 <= f018, f018 <= 2 * 1664, (-2) * 1664 <= f019, f019 <= 2 * 1664, (-2) * 1664 <= f020, f020 <= 2 * 1664, (-2) * 1664 <= f021, f021 <= 2 * 1664, (-2) * 1664 <= f022, f022 <= 2 * 1664, (-2) * 1664 <= f023, f023 <= 2 * 1664, (-2) * 1664 <= f024, f024 <= 2 * 1664, (-2) * 1664 <= f025, f025 <= 2 * 1664, (-2) * 1664 <= f026, f026 <= 2 * 1664, (-2) * 1664 <= f027, f027 <= 2 * 1664, (-2) * 1664 <= f028, f028 <= 2 * 1664, (-2) * 1664 <= f029, f029 <= 2 * 1664, (-2) * 1664 <= f030, f030 <= 2 * 1664, (-2) * 1664 <= f031, f031 <= 2 * 1664, (-2) * 1664 <= f032, f032 <= 2 * 1664, (-2) * 1664 <= f033, f033 <= 2 * 1664, (-2) * 1664 <= f034, f034 <= 2 * 1664, (-2) * 1664 <= f035, f035 <= 2 * 1664, (-2) * 1664 <= f036, f036 <= 2 * 1664, (-2) * 1664 <= f037, f037 <= 2 * 1664, (-2) * 1664 <= f038, f038 <= 2 * 1664, (-2) * 1664 <= f039, f039 <= 2 * 1664, (-2) * 1664 <= f040, f040 <= 2 * 1664, (-2) * 1664 <= f041, f041 <= 2 * 1664, (-2) * 1664 <= f042, f042 <= 2 * 1664, (-2) * 1664 <= f043, f043 <= 2 * 1664, (-2) * 1664 <= f044, f044 <= 2 * 1664, (-2) * 1664 <= f045, f045 <= 2 * 1664, (-2) * 1664 <= f046, f046 <= 2 * 1664, (-2) * 1664 <= f047, f047 <= 2 * 1664, (-2) * 1664 <= f048, f048 <= 2 * 1664, (-2) * 1664 <= f049, f049 <= 2 * 1664, (-2) * 1664 <= f050, f050 <= 2 * 1664, (-2) * 1664 <= f051, f051 <= 2 * 1664, (-2) * 1664 <= f052, f052 <= 2 * 1664, (-2) * 1664 <= f053, f053 <= 2 * 1664, (-2) * 1664 <= f054, f054 <= 2 * 1664, (-2) * 1664 <= f055, f055 <= 2 * 1664, (-2) * 1664 <= f056, f056 <= 2 * 1664, (-2) * 1664 <= f057, f057 <= 2 * 1664, (-2) * 1664 <= f058, f058 <= 2 * 1664, (-2) * 1664 <= f059, f059 <= 2 * 1664, (-2) * 1664 <= f060, f060 <= 2 * 1664, (-2) * 1664 <= f061, f061 <= 2 * 1664, (-2) * 1664 <= f062, f062 <= 2 * 1664, (-2) * 1664 <= f063, f063 <= 2 * 1664, (-2) * 1664 <= f064, f064 <= 2 * 1664, (-2) * 1664 <= f065, f065 <= 2 * 1664, (-2) * 1664 <= f066, f066 <= 2 * 1664, (-2) * 1664 <= f067, f067 <= 2 * 1664, (-2) * 1664 <= f068, f068 <= 2 * 1664, (-2) * 1664 <= f069, f069 <= 2 * 1664, (-2) * 1664 <= f070, f070 <= 2 * 1664, (-2) * 1664 <= f071, f071 <= 2 * 1664, (-2) * 1664 <= f072, f072 <= 2 * 1664, (-2) * 1664 <= f073, f073 <= 2 * 1664, (-2) * 1664 <= f074, f074 <= 2 * 1664, (-2) * 1664 <= f075, f075 <= 2 * 1664, (-2) * 1664 <= f076, f076 <= 2 * 1664, (-2) * 1664 <= f077, f077 <= 2 * 1664, (-2) * 1664 <= f078, f078 <= 2 * 1664, (-2) * 1664 <= f079, f079 <= 2 * 1664, (-2) * 1664 <= f080, f080 <= 2 * 1664, (-2) * 1664 <= f081, f081 <= 2 * 1664, (-2) * 1664 <= f082, f082 <= 2 * 1664, (-2) * 1664 <= f083, f083 <= 2 * 1664, (-2) * 1664 <= f084, f084 <= 2 * 1664, (-2) * 1664 <= f085, f085 <= 2 * 1664, (-2) * 1664 <= f086, f086 <= 2 * 1664, (-2) * 1664 <= f087, f087 <= 2 * 1664, (-2) * 1664 <= f088, f088 <= 2 * 1664, (-2) * 1664 <= f089, f089 <= 2 * 1664, (-2) * 1664 <= f090, f090 <= 2 * 1664, (-2) * 1664 <= f091, f091 <= 2 * 1664, (-2) * 1664 <= f092, f092 <= 2 * 1664, (-2) * 1664 <= f093, f093 <= 2 * 1664, (-2) * 1664 <= f094, f094 <= 2 * 1664, (-2) * 1664 <= f095, f095 <= 2 * 1664, (-2) * 1664 <= f096, f096 <= 2 * 1664, (-2) * 1664 <= f097, f097 <= 2 * 1664, (-2) * 1664 <= f098, f098 <= 2 * 1664, (-2) * 1664 <= f099, f099 <= 2 * 1664, (-2) * 1664 <= f100, f100 <= 2 * 1664, (-2) * 1664 <= f101, f101 <= 2 * 1664, (-2) * 1664 <= f102, f102 <= 2 * 1664, (-2) * 1664 <= f103, f103 <= 2 * 1664, (-2) * 1664 <= f104, f104 <= 2 * 1664, (-2) * 1664 <= f105, f105 <= 2 * 1664, (-2) * 1664 <= f106, f106 <= 2 * 1664, (-2) * 1664 <= f107, f107 <= 2 * 1664, (-2) * 1664 <= f108, f108 <= 2 * 1664, (-2) * 1664 <= f109, f109 <= 2 * 1664, (-2) * 1664 <= f110, f110 <= 2 * 1664, (-2) * 1664 <= f111, f111 <= 2 * 1664, (-2) * 1664 <= f112, f112 <= 2 * 1664, (-2) * 1664 <= f113, f113 <= 2 * 1664, (-2) * 1664 <= f114, f114 <= 2 * 1664, (-2) * 1664 <= f115, f115 <= 2 * 1664, (-2) * 1664 <= f116, f116 <= 2 * 1664, (-2) * 1664 <= f117, f117 <= 2 * 1664, (-2) * 1664 <= f118, f118 <= 2 * 1664, (-2) * 1664 <= f119, f119 <= 2 * 1664, (-2) * 1664 <= f120, f120 <= 2 * 1664, (-2) * 1664 <= f121, f121 <= 2 * 1664, (-2) * 1664 <= f122, f122 <= 2 * 1664, (-2) * 1664 <= f123, f123 <= 2 * 1664, (-2) * 1664 <= f124, f124 <= 2 * 1664, (-2) * 1664 <= f125, f125 <= 2 * 1664, (-2) * 1664 <= f126, f126 <= 2 * 1664, (-2) * 1664 <= f127, f127 <= 2 * 1664, (-2) * 1664 <= f128, f128 <= 2 * 1664, (-2) * 1664 <= f129, f129 <= 2 * 1664, (-2) * 1664 <= f130, f130 <= 2 * 1664, (-2) * 1664 <= f131, f131 <= 2 * 1664, (-2) * 1664 <= f132, f132 <= 2 * 1664, (-2) * 1664 <= f133, f133 <= 2 * 1664, (-2) * 1664 <= f134, f134 <= 2 * 1664, (-2) * 1664 <= f135, f135 <= 2 * 1664, (-2) * 1664 <= f136, f136 <= 2 * 1664, (-2) * 1664 <= f137, f137 <= 2 * 1664, (-2) * 1664 <= f138, f138 <= 2 * 1664, (-2) * 1664 <= f139, f139 <= 2 * 1664, (-2) * 1664 <= f140, f140 <= 2 * 1664, (-2) * 1664 <= f141, f141 <= 2 * 1664, (-2) * 1664 <= f142, f142 <= 2 * 1664, (-2) * 1664 <= f143, f143 <= 2 * 1664, (-2) * 1664 <= f144, f144 <= 2 * 1664, (-2) * 1664 <= f145, f145 <= 2 * 1664, (-2) * 1664 <= f146, f146 <= 2 * 1664, (-2) * 1664 <= f147, f147 <= 2 * 1664, (-2) * 1664 <= f148, f148 <= 2 * 1664, (-2) * 1664 <= f149, f149 <= 2 * 1664, (-2) * 1664 <= f150, f150 <= 2 * 1664, (-2) * 1664 <= f151, f151 <= 2 * 1664, (-2) * 1664 <= f152, f152 <= 2 * 1664, (-2) * 1664 <= f153, f153 <= 2 * 1664, (-2) * 1664 <= f154, f154 <= 2 * 1664, (-2) * 1664 <= f155, f155 <= 2 * 1664, (-2) * 1664 <= f156, f156 <= 2 * 1664, (-2) * 1664 <= f157, f157 <= 2 * 1664, (-2) * 1664 <= f158, f158 <= 2 * 1664, (-2) * 1664 <= f159, f159 <= 2 * 1664, (-2) * 1664 <= f160, f160 <= 2 * 1664, (-2) * 1664 <= f161, f161 <= 2 * 1664, (-2) * 1664 <= f162, f162 <= 2 * 1664, (-2) * 1664 <= f163, f163 <= 2 * 1664, (-2) * 1664 <= f164, f164 <= 2 * 1664, (-2) * 1664 <= f165, f165 <= 2 * 1664, (-2) * 1664 <= f166, f166 <= 2 * 1664, (-2) * 1664 <= f167, f167 <= 2 * 1664, (-2) * 1664 <= f168, f168 <= 2 * 1664, (-2) * 1664 <= f169, f169 <= 2 * 1664, (-2) * 1664 <= f170, f170 <= 2 * 1664, (-2) * 1664 <= f171, f171 <= 2 * 1664, (-2) * 1664 <= f172, f172 <= 2 * 1664, (-2) * 1664 <= f173, f173 <= 2 * 1664, (-2) * 1664 <= f174, f174 <= 2 * 1664, (-2) * 1664 <= f175, f175 <= 2 * 1664, (-2) * 1664 <= f176, f176 <= 2 * 1664, (-2) * 1664 <= f177, f177 <= 2 * 1664, (-2) * 1664 <= f178, f178 <= 2 * 1664, (-2) * 1664 <= f179, f179 <= 2 * 1664, (-2) * 1664 <= f180, f180 <= 2 * 1664, (-2) * 1664 <= f181, f181 <= 2 * 1664, (-2) * 1664 <= f182, f182 <= 2 * 1664, (-2) * 1664 <= f183, f183 <= 2 * 1664, (-2) * 1664 <= f184, f184 <= 2 * 1664, (-2) * 1664 <= f185, f185 <= 2 * 1664, (-2) * 1664 <= f186, f186 <= 2 * 1664, (-2) * 1664 <= f187, f187 <= 2 * 1664, (-2) * 1664 <= f188, f188 <= 2 * 1664, (-2) * 1664 <= f189, f189 <= 2 * 1664, (-2) * 1664 <= f190, f190 <= 2 * 1664, (-2) * 1664 <= f191, f191 <= 2 * 1664, (-2) * 1664 <= f192, f192 <= 2 * 1664, (-2) * 1664 <= f193, f193 <= 2 * 1664, (-2) * 1664 <= f194, f194 <= 2 * 1664, (-2) * 1664 <= f195, f195 <= 2 * 1664, (-2) * 1664 <= f196, f196 <= 2 * 1664, (-2) * 1664 <= f197, f197 <= 2 * 1664, (-2) * 1664 <= f198, f198 <= 2 * 1664, (-2) * 1664 <= f199, f199 <= 2 * 1664, (-2) * 1664 <= f200, f200 <= 2 * 1664, (-2) * 1664 <= f201, f201 <= 2 * 1664, (-2) * 1664 <= f202, f202 <= 2 * 1664, (-2) * 1664 <= f203, f203 <= 2 * 1664, (-2) * 1664 <= f204, f204 <= 2 * 1664, (-2) * 1664 <= f205, f205 <= 2 * 1664, (-2) * 1664 <= f206, f206 <= 2 * 1664, (-2) * 1664 <= f207, f207 <= 2 * 1664, (-2) * 1664 <= f208, f208 <= 2 * 1664, (-2) * 1664 <= f209, f209 <= 2 * 1664, (-2) * 1664 <= f210, f210 <= 2 * 1664, (-2) * 1664 <= f211, f211 <= 2 * 1664, (-2) * 1664 <= f212, f212 <= 2 * 1664, (-2) * 1664 <= f213, f213 <= 2 * 1664, (-2) * 1664 <= f214, f214 <= 2 * 1664, (-2) * 1664 <= f215, f215 <= 2 * 1664, (-2) * 1664 <= f216, f216 <= 2 * 1664, (-2) * 1664 <= f217, f217 <= 2 * 1664, (-2) * 1664 <= f218, f218 <= 2 * 1664, (-2) * 1664 <= f219, f219 <= 2 * 1664, (-2) * 1664 <= f220, f220 <= 2 * 1664, (-2) * 1664 <= f221, f221 <= 2 * 1664, (-2) * 1664 <= f222, f222 <= 2 * 1664, (-2) * 1664 <= f223, f223 <= 2 * 1664, (-2) * 1664 <= f224, f224 <= 2 * 1664, (-2) * 1664 <= f225, f225 <= 2 * 1664, (-2) * 1664 <= f226, f226 <= 2 * 1664, (-2) * 1664 <= f227, f227 <= 2 * 1664, (-2) * 1664 <= f228, f228 <= 2 * 1664, (-2) * 1664 <= f229, f229 <= 2 * 1664, (-2) * 1664 <= f230, f230 <= 2 * 1664, (-2) * 1664 <= f231, f231 <= 2 * 1664, (-2) * 1664 <= f232, f232 <= 2 * 1664, (-2) * 1664 <= f233, f233 <= 2 * 1664, (-2) * 1664 <= f234, f234 <= 2 * 1664, (-2) * 1664 <= f235, f235 <= 2 * 1664, (-2) * 1664 <= f236, f236 <= 2 * 1664, (-2) * 1664 <= f237, f237 <= 2 * 1664, (-2) * 1664 <= f238, f238 <= 2 * 1664, (-2) * 1664 <= f239, f239 <= 2 * 1664, (-2) * 1664 <= f240, f240 <= 2 * 1664, (-2) * 1664 <= f241, f241 <= 2 * 1664, (-2) * 1664 <= f242, f242 <= 2 * 1664, (-2) * 1664 <= f243, f243 <= 2 * 1664, (-2) * 1664 <= f244, f244 <= 2 * 1664, (-2) * 1664 <= f245, f245 <= 2 * 1664, (-2) * 1664 <= f246, f246 <= 2 * 1664, (-2) * 1664 <= f247, f247 <= 2 * 1664, (-2) * 1664 <= f248, f248 <= 2 * 1664, (-2) * 1664 <= f249, f249 <= 2 * 1664, (-2) * 1664 <= f250, f250 <= 2 * 1664, (-2) * 1664 <= f251, f251 <= 2 * 1664, (-2) * 1664 <= f252, f252 <= 2 * 1664, (-2) * 1664 <= f253, f253 <= 2 * 1664, (-2) * 1664 <= f254, f254 <= 2 * 1664, (-2) * 1664 <= f255, f255 <= 2 * 1664]

  &&
  and [(-2)@16 * 1664@16 <=s f000, f000 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f001, f001 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f002, f002 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f003, f003 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f004, f004 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f005, f005 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f006, f006 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f007, f007 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f008, f008 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f009, f009 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f010, f010 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f011, f011 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f012, f012 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f013, f013 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f014, f014 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f015, f015 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f016, f016 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f017, f017 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f018, f018 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f019, f019 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f020, f020 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f021, f021 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f022, f022 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f023, f023 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f024, f024 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f025, f025 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f026, f026 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f027, f027 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f028, f028 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f029, f029 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f030, f030 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f031, f031 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f032, f032 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f033, f033 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f034, f034 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f035, f035 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f036, f036 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f037, f037 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f038, f038 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f039, f039 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f040, f040 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f041, f041 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f042, f042 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f043, f043 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f044, f044 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f045, f045 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f046, f046 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f047, f047 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f048, f048 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f049, f049 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f050, f050 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f051, f051 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f052, f052 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f053, f053 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f054, f054 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f055, f055 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f056, f056 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f057, f057 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f058, f058 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f059, f059 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f060, f060 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f061, f061 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f062, f062 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f063, f063 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f064, f064 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f065, f065 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f066, f066 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f067, f067 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f068, f068 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f069, f069 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f070, f070 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f071, f071 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f072, f072 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f073, f073 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f074, f074 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f075, f075 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f076, f076 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f077, f077 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f078, f078 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f079, f079 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f080, f080 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f081, f081 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f082, f082 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f083, f083 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f084, f084 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f085, f085 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f086, f086 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f087, f087 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f088, f088 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f089, f089 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f090, f090 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f091, f091 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f092, f092 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f093, f093 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f094, f094 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f095, f095 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f096, f096 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f097, f097 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f098, f098 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f099, f099 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f100, f100 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f101, f101 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f102, f102 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f103, f103 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f104, f104 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f105, f105 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f106, f106 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f107, f107 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f108, f108 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f109, f109 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f110, f110 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f111, f111 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f112, f112 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f113, f113 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f114, f114 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f115, f115 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f116, f116 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f117, f117 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f118, f118 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f119, f119 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f120, f120 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f121, f121 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f122, f122 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f123, f123 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f124, f124 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f125, f125 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f126, f126 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f127, f127 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f128, f128 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f129, f129 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f130, f130 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f131, f131 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f132, f132 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f133, f133 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f134, f134 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f135, f135 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f136, f136 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f137, f137 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f138, f138 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f139, f139 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f140, f140 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f141, f141 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f142, f142 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f143, f143 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f144, f144 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f145, f145 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f146, f146 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f147, f147 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f148, f148 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f149, f149 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f150, f150 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f151, f151 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f152, f152 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f153, f153 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f154, f154 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f155, f155 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f156, f156 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f157, f157 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f158, f158 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f159, f159 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f160, f160 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f161, f161 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f162, f162 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f163, f163 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f164, f164 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f165, f165 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f166, f166 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f167, f167 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f168, f168 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f169, f169 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f170, f170 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f171, f171 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f172, f172 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f173, f173 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f174, f174 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f175, f175 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f176, f176 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f177, f177 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f178, f178 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f179, f179 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f180, f180 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f181, f181 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f182, f182 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f183, f183 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f184, f184 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f185, f185 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f186, f186 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f187, f187 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f188, f188 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f189, f189 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f190, f190 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f191, f191 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f192, f192 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f193, f193 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f194, f194 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f195, f195 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f196, f196 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f197, f197 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f198, f198 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f199, f199 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f200, f200 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f201, f201 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f202, f202 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f203, f203 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f204, f204 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f205, f205 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f206, f206 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f207, f207 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f208, f208 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f209, f209 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f210, f210 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f211, f211 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f212, f212 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f213, f213 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f214, f214 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f215, f215 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f216, f216 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f217, f217 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f218, f218 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f219, f219 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f220, f220 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f221, f221 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f222, f222 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f223, f223 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f224, f224 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f225, f225 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f226, f226 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f227, f227 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f228, f228 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f229, f229 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f230, f230 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f231, f231 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f232, f232 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f233, f233 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f234, f234 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f235, f235 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f236, f236 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f237, f237 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f238, f238 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f239, f239 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f240, f240 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f241, f241 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f242, f242 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f243, f243 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f244, f244 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f245, f245 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f246, f246 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f247, f247 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f248, f248 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f249, f249 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f250, f250 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f251, f251 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f252, f252 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f253, f253 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f254, f254 <=s 2@16 * 1664@16, (-2)@16 * 1664@16 <=s f255, f255 <=s 2@16 * 1664@16]
}
mov L0xbefff1c4 f000; mov L0xbefff1c6 f001; mov L0xbefff1c8 f002; mov L0xbefff1ca f003; mov L0xbefff1cc f004; mov L0xbefff1ce f005; mov L0xbefff1d0 f006; mov L0xbefff1d2 f007; mov L0xbefff1d4 f008; mov L0xbefff1d6 f009; mov L0xbefff1d8 f010; mov L0xbefff1da f011; mov L0xbefff1dc f012; mov L0xbefff1de f013; mov L0xbefff1e0 f014; mov L0xbefff1e2 f015; mov L0xbefff1e4 f016; mov L0xbefff1e6 f017; mov L0xbefff1e8 f018; mov L0xbefff1ea f019; mov L0xbefff1ec f020; mov L0xbefff1ee f021; mov L0xbefff1f0 f022; mov L0xbefff1f2 f023; mov L0xbefff1f4 f024; mov L0xbefff1f6 f025; mov L0xbefff1f8 f026; mov L0xbefff1fa f027; mov L0xbefff1fc f028; mov L0xbefff1fe f029; mov L0xbefff200 f030; mov L0xbefff202 f031; mov L0xbefff204 f032; mov L0xbefff206 f033; mov L0xbefff208 f034; mov L0xbefff20a f035; mov L0xbefff20c f036; mov L0xbefff20e f037; mov L0xbefff210 f038; mov L0xbefff212 f039; mov L0xbefff214 f040; mov L0xbefff216 f041; mov L0xbefff218 f042; mov L0xbefff21a f043; mov L0xbefff21c f044; mov L0xbefff21e f045; mov L0xbefff220 f046; mov L0xbefff222 f047; mov L0xbefff224 f048; mov L0xbefff226 f049; mov L0xbefff228 f050; mov L0xbefff22a f051; mov L0xbefff22c f052; mov L0xbefff22e f053; mov L0xbefff230 f054; mov L0xbefff232 f055; mov L0xbefff234 f056; mov L0xbefff236 f057; mov L0xbefff238 f058; mov L0xbefff23a f059; mov L0xbefff23c f060; mov L0xbefff23e f061; mov L0xbefff240 f062; mov L0xbefff242 f063; mov L0xbefff244 f064; mov L0xbefff246 f065; mov L0xbefff248 f066; mov L0xbefff24a f067; mov L0xbefff24c f068; mov L0xbefff24e f069; mov L0xbefff250 f070; mov L0xbefff252 f071; mov L0xbefff254 f072; mov L0xbefff256 f073; mov L0xbefff258 f074; mov L0xbefff25a f075; mov L0xbefff25c f076; mov L0xbefff25e f077; mov L0xbefff260 f078; mov L0xbefff262 f079; mov L0xbefff264 f080; mov L0xbefff266 f081; mov L0xbefff268 f082; mov L0xbefff26a f083; mov L0xbefff26c f084; mov L0xbefff26e f085; mov L0xbefff270 f086; mov L0xbefff272 f087; mov L0xbefff274 f088; mov L0xbefff276 f089; mov L0xbefff278 f090; mov L0xbefff27a f091; mov L0xbefff27c f092; mov L0xbefff27e f093; mov L0xbefff280 f094; mov L0xbefff282 f095; mov L0xbefff284 f096; mov L0xbefff286 f097; mov L0xbefff288 f098; mov L0xbefff28a f099; mov L0xbefff28c f100; mov L0xbefff28e f101; mov L0xbefff290 f102; mov L0xbefff292 f103; mov L0xbefff294 f104; mov L0xbefff296 f105; mov L0xbefff298 f106; mov L0xbefff29a f107; mov L0xbefff29c f108; mov L0xbefff29e f109; mov L0xbefff2a0 f110; mov L0xbefff2a2 f111; mov L0xbefff2a4 f112; mov L0xbefff2a6 f113; mov L0xbefff2a8 f114; mov L0xbefff2aa f115; mov L0xbefff2ac f116; mov L0xbefff2ae f117; mov L0xbefff2b0 f118; mov L0xbefff2b2 f119; mov L0xbefff2b4 f120; mov L0xbefff2b6 f121; mov L0xbefff2b8 f122; mov L0xbefff2ba f123; mov L0xbefff2bc f124; mov L0xbefff2be f125; mov L0xbefff2c0 f126; mov L0xbefff2c2 f127; mov L0xbefff2c4 f128; mov L0xbefff2c6 f129; mov L0xbefff2c8 f130; mov L0xbefff2ca f131; mov L0xbefff2cc f132; mov L0xbefff2ce f133; mov L0xbefff2d0 f134; mov L0xbefff2d2 f135; mov L0xbefff2d4 f136; mov L0xbefff2d6 f137; mov L0xbefff2d8 f138; mov L0xbefff2da f139; mov L0xbefff2dc f140; mov L0xbefff2de f141; mov L0xbefff2e0 f142; mov L0xbefff2e2 f143; mov L0xbefff2e4 f144; mov L0xbefff2e6 f145; mov L0xbefff2e8 f146; mov L0xbefff2ea f147; mov L0xbefff2ec f148; mov L0xbefff2ee f149; mov L0xbefff2f0 f150; mov L0xbefff2f2 f151; mov L0xbefff2f4 f152; mov L0xbefff2f6 f153; mov L0xbefff2f8 f154; mov L0xbefff2fa f155; mov L0xbefff2fc f156; mov L0xbefff2fe f157; mov L0xbefff300 f158; mov L0xbefff302 f159; mov L0xbefff304 f160; mov L0xbefff306 f161; mov L0xbefff308 f162; mov L0xbefff30a f163; mov L0xbefff30c f164; mov L0xbefff30e f165; mov L0xbefff310 f166; mov L0xbefff312 f167; mov L0xbefff314 f168; mov L0xbefff316 f169; mov L0xbefff318 f170; mov L0xbefff31a f171; mov L0xbefff31c f172; mov L0xbefff31e f173; mov L0xbefff320 f174; mov L0xbefff322 f175; mov L0xbefff324 f176; mov L0xbefff326 f177; mov L0xbefff328 f178; mov L0xbefff32a f179; mov L0xbefff32c f180; mov L0xbefff32e f181; mov L0xbefff330 f182; mov L0xbefff332 f183; mov L0xbefff334 f184; mov L0xbefff336 f185; mov L0xbefff338 f186; mov L0xbefff33a f187; mov L0xbefff33c f188; mov L0xbefff33e f189; mov L0xbefff340 f190; mov L0xbefff342 f191; mov L0xbefff344 f192; mov L0xbefff346 f193; mov L0xbefff348 f194; mov L0xbefff34a f195; mov L0xbefff34c f196; mov L0xbefff34e f197; mov L0xbefff350 f198; mov L0xbefff352 f199; mov L0xbefff354 f200; mov L0xbefff356 f201; mov L0xbefff358 f202; mov L0xbefff35a f203; mov L0xbefff35c f204; mov L0xbefff35e f205; mov L0xbefff360 f206; mov L0xbefff362 f207; mov L0xbefff364 f208; mov L0xbefff366 f209; mov L0xbefff368 f210; mov L0xbefff36a f211; mov L0xbefff36c f212; mov L0xbefff36e f213; mov L0xbefff370 f214; mov L0xbefff372 f215; mov L0xbefff374 f216; mov L0xbefff376 f217; mov L0xbefff378 f218; mov L0xbefff37a f219; mov L0xbefff37c f220; mov L0xbefff37e f221; mov L0xbefff380 f222; mov L0xbefff382 f223; mov L0xbefff384 f224; mov L0xbefff386 f225; mov L0xbefff388 f226; mov L0xbefff38a f227; mov L0xbefff38c f228; mov L0xbefff38e f229; mov L0xbefff390 f230; mov L0xbefff392 f231; mov L0xbefff394 f232; mov L0xbefff396 f233; mov L0xbefff398 f234; mov L0xbefff39a f235; mov L0xbefff39c f236; mov L0xbefff39e f237; mov L0xbefff3a0 f238; mov L0xbefff3a2 f239; mov L0xbefff3a4 f240; mov L0xbefff3a6 f241; mov L0xbefff3a8 f242; mov L0xbefff3aa f243; mov L0xbefff3ac f244; mov L0xbefff3ae f245; mov L0xbefff3b0 f246; mov L0xbefff3b2 f247; mov L0xbefff3b4 f248; mov L0xbefff3b6 f249; mov L0xbefff3b8 f250; mov L0xbefff3ba f251; mov L0xbefff3bc f252; mov L0xbefff3be f253; mov L0xbefff3c0 f254; mov L0xbefff3c2 f255; 
mov L0x40167c 2230699446@sint32; mov L0x401680 3328631909@sint32; mov L0x401684 4243360600@sint32; mov L0x401688 3408622288@sint32; mov L0x40168c 812805467@sint32; mov L0x401690 2447447570@sint32; mov L0x401694 1094061961@sint32; mov L0x401698 1370157786@sint32; mov L0x40169c 2475831253@sint32; mov L0x4016a0 249002310@sint32; mov L0x4016a4 1028263423@sint32; mov L0x4016a8 3594406395@sint32; mov L0x4016ac 4205945745@sint32; mov L0x4016b0 734105255@sint32; mov L0x4016b4 2252632292@sint32; mov L0x4016b8 381889553@sint32; mov L0x4016bc 372858381@sint32; mov L0x4016c0 427045412@sint32; mov L0x4016c4 21932846@sint32; mov L0x4016c8 3562152210@sint32; mov L0x4016cc 752167598@sint32; mov L0x4016d0 3417653460@sint32; mov L0x4016d4 3157039644@sint32; mov L0x4016d8 4196914574@sint32; mov L0x4016dc 2265533966@sint32; mov L0x4016e0 2112004045@sint32; mov L0x4016e4 932791035@sint32; mov L0x4016e8 2951903026@sint32; mov L0x4016ec 1419184148@sint32; mov L0x4016f0 1727534158@sint32; mov L0x4016f4 1544330386@sint32; mov L0x4016f8 2972545705@sint32; mov L0x4016fc 1817845876@sint32; mov L0x401700 3434425636@sint32; mov L0x401704 4233039261@sint32; mov L0x401708 300609006@sint32; mov L0x40170c 1904287092@sint32; mov L0x401710 2937711185@sint32; mov L0x401714 2651294021@sint32; mov L0x401718 975366560@sint32; mov L0x40171c 2781600929@sint32; mov L0x401720 3889854731@sint32; mov L0x401724 3935010590@sint32; mov L0x401728 3929849920@sint32; mov L0x40172c 838608815@sint32; mov L0x401730 2550660963@sint32; mov L0x401734 2197155094@sint32; mov L0x401738 2130066389@sint32; mov L0x40173c 3598276897@sint32; mov L0x401740 2308109491@sint32; mov L0x401744 72249375@sint32; mov L0x401748 3242190693@sint32; mov L0x40174c 815385801@sint32; mov L0x401750 2382939200@sint32; mov L0x401754 1228239371@sint32; mov L0x401758 1884934581@sint32; mov L0x40175c 3466679822@sint32; mov L0x401760 2889974991@sint32; mov L0x401764 3696329620@sint32; mov L0x401768 42575525@sint32; mov L0x40176c 1211467195@sint32; mov L0x401770 2977706375@sint32; mov L0x401774 3144137970@sint32; mov L0x401778 3080919767@sint32; mov L0x40177c 1719793153@sint32; mov L0x401780 1703020977@sint32; mov L0x401784 2470670584@sint32; mov L0x401788 945692709@sint32; mov L0x40178c 3015121229@sint32; mov L0x401790 345764865@sint32; mov L0x401794 826997308@sint32; mov L0x401798 1839778722@sint32; mov L0x40179c 2991898216@sint32; mov L0x4017a0 1851390229@sint32; mov L0x4017a4 2043625172@sint32; mov L0x4017a8 2964804700@sint32; mov L0x4017ac 2628071007@sint32; mov L0x4017b0 4154339049@sint32; mov L0x4017b4 2701610550@sint32; mov L0x4017b8 1041165097@sint32; mov L0x4017bc 583155668@sint32; mov L0x4017c0 483812778@sint32; mov L0x4017c4 3288636719@sint32; mov L0x4017c8 2696449880@sint32; mov L0x4017cc 2122325384@sint32; mov L0x4017d0 690239563@sint32; mov L0x4017d4 1855260731@sint32; mov L0x4017d8 3700200122@sint32; mov L0x4017dc 1371447954@sint32; mov L0x4017e0 411563403@sint32; mov L0x4017e4 3577634219@sint32; mov L0x4017e8 976656727@sint32; mov L0x4017ec 3718262466@sint32; mov L0x4017f0 1979116802@sint32; mov L0x4017f4 3098982111@sint32; mov L0x4017f8 2708061387@sint32; mov L0x4017fc 723783916@sint32; mov L0x401800 3181552825@sint32; mov L0x401804 3346694253@sint32; mov L0x401808 3087370604@sint32; mov L0x40180c 3415073125@sint32; mov L0x401810 3376368103@sint32; mov L0x401814 3617629408@sint32; mov L0x401818 1408862808@sint32; mov L0x40181c 519937465@sint32; mov L0x401820 1323711759@sint32; mov L0x401824 3714391964@sint32; mov L0x401828 1910737929@sint32; mov L0x40182c 836028480@sint32; mov L0x401830 1474661346@sint32; mov L0x401834 2773859924@sint32; mov L0x401838 3580214553@sint32; mov L0x40183c 1143088323@sint32; mov L0x401840 2546790461@sint32; mov L0x401844 3191874164@sint32; mov L0x401848 4012420634@sint32; mov L0x40184c 2221668274@sint32; mov L0x401850 1563682897@sint32; mov L0x401854 2417773720@sint32; mov L0x401858 1327582262@sint32; mov L0x40185c 1059227441@sint32; mov L0x401860 1583035408@sint32; mov L0x401864 1174052340@sint32; mov L0x401868 2722253228@sint32; mov L0x40186c 3786641338@sint32; mov L0x401870 1141798155@sint32; mov L0x401874 2779020594@sint32; 
nondet lr_b@sint32; nondet lr_t@sint32; nondet r0@sint32; nondet r0_b@sint32; nondet r0_t@sint32; nondet r2@sint32; nondet r3@sint32; nondet r4@sint32; nondet r5@sint32; nondet r6@sint32; nondet r7@sint32; nondet r8@sint32; nondet r9@sint32; nondet s10_b@sint32; nondet s10_t@sint32; nondet s11_b@sint32; nondet s11_t@sint32; nondet s12_b@sint32; nondet s12_t@sint32; nondet s13_b@sint32; nondet s13_t@sint32; nondet s14_b@sint32; nondet s14_t@sint32; nondet s15_b@sint32; nondet s15_t@sint32; nondet s16_b@sint32; nondet s16_t@sint32; nondet s17_b@sint32; nondet s17_t@sint32; nondet s18_b@sint32; nondet s18_t@sint32; nondet s19_b@sint32; nondet s19_t@sint32; nondet s20_b@sint32; nondet s20_t@sint32; nondet s21_b@sint32; nondet s21_t@sint32; nondet s22_b@sint32; nondet s22_t@sint32; nondet s8_b@sint32; nondet s8_t@sint32; nondet s9_b@sint32; nondet s9_t@sint32; nondet zeta_lr@sint32; nondet zeta_r0@sint32; nondet zeta_r2@sint32; nondet zeta_r3@sint32; nondet zeta_r4@sint32; nondet zeta_r5@sint32; nondet zeta_r6@sint32; nondet zeta_r7@sint32; nondet zeta_r8@sint32; nondet zeta_r9@sint32;
ghost inp_poly@bit : inp_poly * inp_poly = f000 * (x**0) + f001 * (x**1) + f002 * (x**2) + f003 * (x**3) + f004 * (x**4) + f005 * (x**5) + f006 * (x**6) + f007 * (x**7) + f008 * (x**8) + f009 * (x**9) + f010 * (x**10) + f011 * (x**11) + f012 * (x**12) + f013 * (x**13) + f014 * (x**14) + f015 * (x**15) + f016 * (x**16) + f017 * (x**17) + f018 * (x**18) + f019 * (x**19) + f020 * (x**20) + f021 * (x**21) + f022 * (x**22) + f023 * (x**23) + f024 * (x**24) + f025 * (x**25) + f026 * (x**26) + f027 * (x**27) + f028 * (x**28) + f029 * (x**29) + f030 * (x**30) + f031 * (x**31) + f032 * (x**32) + f033 * (x**33) + f034 * (x**34) + f035 * (x**35) + f036 * (x**36) + f037 * (x**37) + f038 * (x**38) + f039 * (x**39) + f040 * (x**40) + f041 * (x**41) + f042 * (x**42) + f043 * (x**43) + f044 * (x**44) + f045 * (x**45) + f046 * (x**46) + f047 * (x**47) + f048 * (x**48) + f049 * (x**49) + f050 * (x**50) + f051 * (x**51) + f052 * (x**52) + f053 * (x**53) + f054 * (x**54) + f055 * (x**55) + f056 * (x**56) + f057 * (x**57) + f058 * (x**58) + f059 * (x**59) + f060 * (x**60) + f061 * (x**61) + f062 * (x**62) + f063 * (x**63) + f064 * (x**64) + f065 * (x**65) + f066 * (x**66) + f067 * (x**67) + f068 * (x**68) + f069 * (x**69) + f070 * (x**70) + f071 * (x**71) + f072 * (x**72) + f073 * (x**73) + f074 * (x**74) + f075 * (x**75) + f076 * (x**76) + f077 * (x**77) + f078 * (x**78) + f079 * (x**79) + f080 * (x**80) + f081 * (x**81) + f082 * (x**82) + f083 * (x**83) + f084 * (x**84) + f085 * (x**85) + f086 * (x**86) + f087 * (x**87) + f088 * (x**88) + f089 * (x**89) + f090 * (x**90) + f091 * (x**91) + f092 * (x**92) + f093 * (x**93) + f094 * (x**94) + f095 * (x**95) + f096 * (x**96) + f097 * (x**97) + f098 * (x**98) + f099 * (x**99) + f100 * (x**100) + f101 * (x**101) + f102 * (x**102) + f103 * (x**103) + f104 * (x**104) + f105 * (x**105) + f106 * (x**106) + f107 * (x**107) + f108 * (x**108) + f109 * (x**109) + f110 * (x**110) + f111 * (x**111) + f112 * (x**112) + f113 * (x**113) + f114 * (x**114) + f115 * (x**115) + f116 * (x**116) + f117 * (x**117) + f118 * (x**118) + f119 * (x**119) + f120 * (x**120) + f121 * (x**121) + f122 * (x**122) + f123 * (x**123) + f124 * (x**124) + f125 * (x**125) + f126 * (x**126) + f127 * (x**127) + f128 * (x**128) + f129 * (x**129) + f130 * (x**130) + f131 * (x**131) + f132 * (x**132) + f133 * (x**133) + f134 * (x**134) + f135 * (x**135) + f136 * (x**136) + f137 * (x**137) + f138 * (x**138) + f139 * (x**139) + f140 * (x**140) + f141 * (x**141) + f142 * (x**142) + f143 * (x**143) + f144 * (x**144) + f145 * (x**145) + f146 * (x**146) + f147 * (x**147) + f148 * (x**148) + f149 * (x**149) + f150 * (x**150) + f151 * (x**151) + f152 * (x**152) + f153 * (x**153) + f154 * (x**154) + f155 * (x**155) + f156 * (x**156) + f157 * (x**157) + f158 * (x**158) + f159 * (x**159) + f160 * (x**160) + f161 * (x**161) + f162 * (x**162) + f163 * (x**163) + f164 * (x**164) + f165 * (x**165) + f166 * (x**166) + f167 * (x**167) + f168 * (x**168) + f169 * (x**169) + f170 * (x**170) + f171 * (x**171) + f172 * (x**172) + f173 * (x**173) + f174 * (x**174) + f175 * (x**175) + f176 * (x**176) + f177 * (x**177) + f178 * (x**178) + f179 * (x**179) + f180 * (x**180) + f181 * (x**181) + f182 * (x**182) + f183 * (x**183) + f184 * (x**184) + f185 * (x**185) + f186 * (x**186) + f187 * (x**187) + f188 * (x**188) + f189 * (x**189) + f190 * (x**190) + f191 * (x**191) + f192 * (x**192) + f193 * (x**193) + f194 * (x**194) + f195 * (x**195) + f196 * (x**196) + f197 * (x**197) + f198 * (x**198) + f199 * (x**199) + f200 * (x**200) + f201 * (x**201) + f202 * (x**202) + f203 * (x**203) + f204 * (x**204) + f205 * (x**205) + f206 * (x**206) + f207 * (x**207) + f208 * (x**208) + f209 * (x**209) + f210 * (x**210) + f211 * (x**211) + f212 * (x**212) + f213 * (x**213) + f214 * (x**214) + f215 * (x**215) + f216 * (x**216) + f217 * (x**217) + f218 * (x**218) + f219 * (x**219) + f220 * (x**220) + f221 * (x**221) + f222 * (x**222) + f223 * (x**223) + f224 * (x**224) + f225 * (x**225) + f226 * (x**226) + f227 * (x**227) + f228 * (x**228) + f229 * (x**229) + f230 * (x**230) + f231 * (x**231) + f232 * (x**232) + f233 * (x**233) + f234 * (x**234) + f235 * (x**235) + f236 * (x**236) + f237 * (x**237) + f238 * (x**238) + f239 * (x**239) + f240 * (x**240) + f241 * (x**241) + f242 * (x**242) + f243 * (x**243) + f244 * (x**244) + f245 * (x**245) + f246 * (x**246) + f247 * (x**247) + f248 * (x**248) + f249 * (x**249) + f250 * (x**250) + f251 * (x**251) + f252 * (x**252) + f253 * (x**253) + f254 * (x**254) + f255 * (x**255) && true;
mov zeta_L0x40167c 1729@sint32; mov zeta_L0x401680 2580@sint32; mov zeta_L0x401684 3289@sint32; mov zeta_L0x401688 2642@sint32; mov zeta_L0x40168c 630@sint32; mov zeta_L0x401690 1897@sint32; mov zeta_L0x401694 848@sint32; mov zeta_L0x401698 1062@sint32; mov zeta_L0x40169c 1919@sint32; mov zeta_L0x4016a0 193@sint32; mov zeta_L0x4016a4 797@sint32; mov zeta_L0x4016a8 2786@sint32; mov zeta_L0x4016ac 3260@sint32; mov zeta_L0x4016b0 569@sint32; mov zeta_L0x4016b4 1746@sint32; mov zeta_L0x4016b8 296@sint32; mov zeta_L0x4016bc 289@sint32; mov zeta_L0x4016c0 331@sint32; mov zeta_L0x4016c4 17@sint32; mov zeta_L0x4016c8 2761@sint32; mov zeta_L0x4016cc 583@sint32; mov zeta_L0x4016d0 2649@sint32; mov zeta_L0x4016d4 2447@sint32; mov zeta_L0x4016d8 3253@sint32; mov zeta_L0x4016dc 1756@sint32; mov zeta_L0x4016e0 1637@sint32; mov zeta_L0x4016e4 723@sint32; mov zeta_L0x4016e8 2288@sint32; mov zeta_L0x4016ec 1100@sint32; mov zeta_L0x4016f0 1339@sint32; mov zeta_L0x4016f4 1197@sint32; mov zeta_L0x4016f8 2304@sint32; mov zeta_L0x4016fc 1409@sint32; mov zeta_L0x401700 2662@sint32; mov zeta_L0x401704 3281@sint32; mov zeta_L0x401708 233@sint32; mov zeta_L0x40170c 1476@sint32; mov zeta_L0x401710 2277@sint32; mov zeta_L0x401714 2055@sint32; mov zeta_L0x401718 756@sint32; mov zeta_L0x40171c 2156@sint32; mov zeta_L0x401720 3015@sint32; mov zeta_L0x401724 3050@sint32; mov zeta_L0x401728 3046@sint32; mov zeta_L0x40172c 650@sint32; mov zeta_L0x401730 1977@sint32; mov zeta_L0x401734 1703@sint32; mov zeta_L0x401738 1651@sint32; mov zeta_L0x40173c 2789@sint32; mov zeta_L0x401740 1789@sint32; mov zeta_L0x401744 56@sint32; mov zeta_L0x401748 2513@sint32; mov zeta_L0x40174c 632@sint32; mov zeta_L0x401750 1847@sint32; mov zeta_L0x401754 952@sint32; mov zeta_L0x401758 1461@sint32; mov zeta_L0x40175c 2687@sint32; mov zeta_L0x401760 2240@sint32; mov zeta_L0x401764 2865@sint32; mov zeta_L0x401768 33@sint32; mov zeta_L0x40176c 939@sint32; mov zeta_L0x401770 2308@sint32; mov zeta_L0x401774 2437@sint32; mov zeta_L0x401778 2388@sint32; mov zeta_L0x40177c 1333@sint32; mov zeta_L0x401780 1320@sint32; mov zeta_L0x401784 1915@sint32; mov zeta_L0x401788 733@sint32; mov zeta_L0x40178c 2337@sint32; mov zeta_L0x401790 268@sint32; mov zeta_L0x401794 641@sint32; mov zeta_L0x401798 1426@sint32; mov zeta_L0x40179c 2319@sint32; mov zeta_L0x4017a0 1435@sint32; mov zeta_L0x4017a4 1584@sint32; mov zeta_L0x4017a8 2298@sint32; mov zeta_L0x4017ac 2037@sint32; mov zeta_L0x4017b0 3220@sint32; mov zeta_L0x4017b4 2094@sint32; mov zeta_L0x4017b8 807@sint32; mov zeta_L0x4017bc 452@sint32; mov zeta_L0x4017c0 375@sint32; mov zeta_L0x4017c4 2549@sint32; mov zeta_L0x4017c8 2090@sint32; mov zeta_L0x4017cc 1645@sint32; mov zeta_L0x4017d0 535@sint32; mov zeta_L0x4017d4 1438@sint32; mov zeta_L0x4017d8 2868@sint32; mov zeta_L0x4017dc 1063@sint32; mov zeta_L0x4017e0 319@sint32; mov zeta_L0x4017e4 2773@sint32; mov zeta_L0x4017e8 757@sint32; mov zeta_L0x4017ec 2882@sint32; mov zeta_L0x4017f0 1534@sint32; mov zeta_L0x4017f4 2402@sint32; mov zeta_L0x4017f8 2099@sint32; mov zeta_L0x4017fc 561@sint32; mov zeta_L0x401800 2466@sint32; mov zeta_L0x401804 2594@sint32; mov zeta_L0x401808 2393@sint32; mov zeta_L0x40180c 2647@sint32; mov zeta_L0x401810 2617@sint32; mov zeta_L0x401814 2804@sint32; mov zeta_L0x401818 1092@sint32; mov zeta_L0x40181c 403@sint32; mov zeta_L0x401820 1026@sint32; mov zeta_L0x401824 2879@sint32; mov zeta_L0x401828 1481@sint32; mov zeta_L0x40182c 648@sint32; mov zeta_L0x401830 1143@sint32; mov zeta_L0x401834 2150@sint32; mov zeta_L0x401838 2775@sint32; mov zeta_L0x40183c 886@sint32; mov zeta_L0x401840 1974@sint32; mov zeta_L0x401844 2474@sint32; mov zeta_L0x401848 3110@sint32; mov zeta_L0x40184c 1722@sint32; mov zeta_L0x401850 1212@sint32; mov zeta_L0x401854 1874@sint32; mov zeta_L0x401858 1029@sint32; mov zeta_L0x40185c 821@sint32; mov zeta_L0x401860 1227@sint32; mov zeta_L0x401864 910@sint32; mov zeta_L0x401868 2110@sint32; mov zeta_L0x40186c 2935@sint32; mov zeta_L0x401870 885@sint32; mov zeta_L0x401874 2154@sint32; 

(* #! -> SP = 0xbefff1b0 *)
#! 0xbefff1b0 = 0xbefff1b0;
(* movt	r12, #3329	; 0xd01                         #! PC = 0x400618 *)
mov r12_t 3329@sint16;
(* vldmia	r1!, {s8-s22}                            #! EA = L0x40167c; PC = 0x40061c *)
mov s8 L0x40167c;
mov s9 L0x401680;
mov s10 L0x401684;
mov s11 L0x401688;
mov s12 L0x40168c;
mov s13 L0x401690;
mov s14 L0x401694;
mov s15 L0x401698;
mov s16 L0x40169c;
mov s17 L0x4016a0;
mov s18 L0x4016a4;
mov s19 L0x4016a8;
mov s20 L0x4016ac;
mov s21 L0x4016b0;
mov s22 L0x4016b4;
mov zeta_s8 zeta_L0x40167c;
mov zeta_s9 zeta_L0x401680;
mov zeta_s10 zeta_L0x401684;
mov zeta_s11 zeta_L0x401688;
mov zeta_s12 zeta_L0x40168c;
mov zeta_s13 zeta_L0x401690;
mov zeta_s14 zeta_L0x401694;
mov zeta_s15 zeta_L0x401698;
mov zeta_s16 zeta_L0x40169c;
mov zeta_s17 zeta_L0x4016a0;
mov zeta_s18 zeta_L0x4016a4;
mov zeta_s19 zeta_L0x4016a8;
mov zeta_s20 zeta_L0x4016ac;
mov zeta_s21 zeta_L0x4016b0;
mov zeta_s22 zeta_L0x4016b4;
(* add.w	lr, r0, #32                               #! PC = 0x400620 *)
adds dontcare lr r0 32@sint32;
(* vmov	s24, lr                                    #! PC = 0x400624 *)
mov s24_b lr_b;
mov s24_t lr_t;
mov s24 lr;
mov zeta_s24 zeta_lr;
(* vmov	s23, r0                                    #! PC = 0x400628 *)
mov s23_b r0_b;
mov s23_t r0_t;
mov s23 r0;
mov zeta_s23 zeta_r0;
(* ldr.w	r2, [r0, #32]                             #! EA = L0xbefff1e4; Value = 0x00000000; PC = 0x40062c *)
mov r2_b L0xbefff1e4;
mov r2_t L0xbefff1e6;
(* ldr.w	r3, [r0, #96]	; 0x60                      #! EA = L0xbefff224; Value = 0xb6ffe8f8; PC = 0x400630 *)
mov r3_b L0xbefff224;
mov r3_t L0xbefff226;
(* ldr.w	r4, [r0, #160]	; 0xa0                     #! EA = L0xbefff264; Value = 0xb6fd5000; PC = 0x400634 *)
mov r4_b L0xbefff264;
mov r4_t L0xbefff266;
(* ldr.w	r5, [r0, #224]	; 0xe0                     #! EA = L0xbefff2a4; Value = 0xffffffff; PC = 0x400638 *)
mov r5_b L0xbefff2a4;
mov r5_t L0xbefff2a6;
(* ldr.w	r6, [r0, #288]	; 0x120                    #! EA = L0xbefff2e4; Value = 0xb6fff070; PC = 0x40063c *)
mov r6_b L0xbefff2e4;
mov r6_t L0xbefff2e6;
(* ldr.w	r7, [r0, #352]	; 0x160                    #! EA = L0xbefff324; Value = 0x00000000; PC = 0x400640 *)
mov r7_b L0xbefff324;
mov r7_t L0xbefff326;
(* ldr.w	r8, [r0, #416]	; 0x1a0                    #! EA = L0xbefff364; Value = 0x00000000; PC = 0x400644 *)
mov r8_b L0xbefff364;
mov r8_t L0xbefff366;
(* ldr.w	r9, [r0, #480]	; 0x1e0                    #! EA = L0xbefff3a4; Value = 0xb6ee4a4f; PC = 0x400648 *)
mov r9_b L0xbefff3a4;
mov r9_t L0xbefff3a6;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x40064c *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* vmov	r10, s8                                    #! PC = 0x400650 *)
mov r10_b s8_b;
mov r10_t s8_t;
mov r10 s8;
mov zeta_r10 zeta_s8;
(* smulwb	lr, r10, r6                              #! PC = 0x400654 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400658 *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40065c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400660 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x400664 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x400668 *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x40066c *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x400670 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400674 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400678 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x40067c *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400680 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x400684 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400688 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x40068c *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400690 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400694 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400698 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x40069c *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x4006a0 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x4006a4 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x4006a8 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x4006ac *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4006b0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4006b4 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4006b8 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x4006bc *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x4006c0 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* vmov	r10, s9                                    #! PC = 0x4006c4 *)
mov r10_b s9_b;
mov r10_t s9_t;
mov r10 s9;
mov zeta_r10 zeta_s9;
(* vmov	r11, s10                                   #! PC = 0x4006c8 *)
mov r11_b s10_b;
mov r11_t s10_t;
mov r11 s10;
mov zeta_r11 zeta_s10;
(* smulwb	lr, r10, r4                              #! PC = 0x4006cc *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x4006d0 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4006d4 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x4006d8 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x4006dc *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x4006e0 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4006e4 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x4006e8 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x4006ec *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4006f0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x4006f4 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x4006f8 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x4006fc *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400700 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400704 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x400708 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40070c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400710 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400714 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x400718 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x40071c *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400720 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400724 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400728 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x40072c *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400730 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x400734 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400738 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* vmov	r10, s11                                   #! PC = 0x40073c *)
mov r10_b s11_b;
mov r10_t s11_t;
mov r10 s11;
mov zeta_r10 zeta_s11;
(* vmov	r11, s12                                   #! PC = 0x400740 *)
mov r11_b s12_b;
mov r11_t s12_t;
mov r11 s12;
mov zeta_r11 zeta_s12;
(* smulwb	lr, r10, r3                              #! PC = 0x400744 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x400748 *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40074c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400750 *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x400754 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x400758 *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x40075c *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x400760 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x400764 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400768 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x40076c *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400770 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x400774 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400778 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* vmov	r10, s13                                   #! PC = 0x40077c *)
mov r10_b s13_b;
mov r10_t s13_t;
mov r10 s13;
mov zeta_r10 zeta_s13;
(* vmov	r11, s14                                   #! PC = 0x400780 *)
mov r11_b s14_b;
mov r11_t s14_t;
mov r11 s14;
mov zeta_r11 zeta_s14;
(* smulwb	lr, r10, r7                              #! PC = 0x400784 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400788 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40078c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400790 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400794 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400798 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x40079c *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4007a0 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x4007a4 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007a8 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4007ac *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4007b0 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x4007b4 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x4007b8 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

(*== 8-NTT on a1, a3, ..., a15 ==*)
(*== _3_layer_double_CT_16_plant_fp END ==*)

assert
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

assume
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

(* vmov	r10, s15                                   #! PC = 0x4007bc *)
mov r10_b s15_b;
mov r10_t s15_t;
mov r10 s15;
mov zeta_r10 zeta_s15;
(* vmov	r11, s16                                   #! PC = 0x4007c0 *)
mov r11_b s16_b;
mov r11_t s16_t;
mov r11 s16;
mov zeta_r11 zeta_s16;
(* smulwb	lr, r10, r2                              #! PC = 0x4007c4 *)
cast r2_lsb@sint32 r2_b;
mull tmp_t tmp_b r10 r2_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r2_b;
assert A_lb = r2_b && true;
assume A_lb = r2_b && true;
cast A_lt@sint32 r2_t;
assert A_lt = r2_t && true;
assume A_lt = r2_t && true;
mov zeta zeta_r10;
(* smulwt	r2, r10, r2                              #! PC = 0x4007c8 *)
cast r2_lst@sint32 r2_t;
mull tmp_t tmp_b r10 r2_lst;
spl dontcare r2_t tmp_t 16;
spl r2_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007cc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r2, r2, r12, r0                          #! PC = 0x4007d0 *)
cast r2_sb@sint16 r2_b;
mull tmp_t tmp_b r2_sb r12_t;
uadds carry r2_b tmp_b r0_b;
adc r2_t tmp_t r0_t carry;
(* pkhtb	r2, r2, lr, asr #16                       #! PC = 0x4007d4 *)
mov tmp_b lr_t;
mov tmp_t r2_t;
mov r2_b tmp_b;
mov r2_t tmp_t;
cast C_lb@sint32 r2_b;
assert C_lb = r2_b && true;
assume C_lb = r2_b && true;
cast C_lt@sint32 r2_t;
assert C_lt = r2_t && true;
assume C_lt = r2_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r3                              #! PC = 0x4007d8 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r11 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r11;
(* smulwt	r3, r11, r3                              #! PC = 0x4007dc *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r11 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007e0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4007e4 *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x4007e8 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov r3_b tmp_b;
mov r3_t tmp_t;
cast C_lb@sint32 r3_b;
assert C_lb = r3_b && true;
assume C_lb = r3_b && true;
cast C_lt@sint32 r3_t;
assert C_lt = r3_t && true;
assume C_lt = r3_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	r10, s17                                   #! PC = 0x4007ec *)
mov r10_b s17_b;
mov r10_t s17_t;
mov r10 s17;
mov zeta_r10 zeta_s17;
(* vmov	r11, s18                                   #! PC = 0x4007f0 *)
mov r11_b s18_b;
mov r11_t s18_t;
mov r11 s18;
mov zeta_r11 zeta_s18;
(* smulwb	lr, r10, r4                              #! PC = 0x4007f4 *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x4007f8 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007fc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400800 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x400804 *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov r4_b tmp_b;
mov r4_t tmp_t;
cast C_lb@sint32 r4_b;
assert C_lb = r4_b && true;
assume C_lb = r4_b && true;
cast C_lt@sint32 r4_t;
assert C_lt = r4_t && true;
assume C_lt = r4_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r5                              #! PC = 0x400808 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x40080c *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400810 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400814 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x400818 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov r5_b tmp_b;
mov r5_t tmp_t;
cast C_lb@sint32 r5_b;
assert C_lb = r5_b && true;
assume C_lb = r5_b && true;
cast C_lt@sint32 r5_t;
assert C_lt = r5_t && true;
assume C_lt = r5_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	r10, s19                                   #! PC = 0x40081c *)
mov r10_b s19_b;
mov r10_t s19_t;
mov r10 s19;
mov zeta_r10 zeta_s19;
(* vmov	r11, s20                                   #! PC = 0x400820 *)
mov r11_b s20_b;
mov r11_t s20_t;
mov r11 s20;
mov zeta_r11 zeta_s20;
(* smulwb	lr, r10, r6                              #! PC = 0x400824 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400828 *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40082c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400830 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x400834 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov r6_b tmp_b;
mov r6_t tmp_t;
cast C_lb@sint32 r6_b;
assert C_lb = r6_b && true;
assume C_lb = r6_b && true;
cast C_lt@sint32 r6_t;
assert C_lt = r6_t && true;
assume C_lt = r6_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r7                              #! PC = 0x400838 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r11 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r11;
(* smulwt	r7, r11, r7                              #! PC = 0x40083c *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r11 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400840 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400844 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x400848 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov r7_b tmp_b;
mov r7_t tmp_t;
cast C_lb@sint32 r7_b;
assert C_lb = r7_b && true;
assume C_lb = r7_b && true;
cast C_lt@sint32 r7_t;
assert C_lt = r7_t && true;
assume C_lt = r7_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	r10, s21                                   #! PC = 0x40084c *)
mov r10_b s21_b;
mov r10_t s21_t;
mov r10 s21;
mov zeta_r10 zeta_s21;
(* vmov	r11, s22                                   #! PC = 0x400850 *)
mov r11_b s22_b;
mov r11_t s22_t;
mov r11 s22;
mov zeta_r11 zeta_s22;
(* smulwb	lr, r10, r8                              #! PC = 0x400854 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400858 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40085c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400860 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x400864 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov r8_b tmp_b;
mov r8_t tmp_t;
cast C_lb@sint32 r8_b;
assert C_lb = r8_b && true;
assume C_lb = r8_b && true;
cast C_lt@sint32 r8_t;
assert C_lt = r8_t && true;
assume C_lt = r8_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r9                              #! PC = 0x400868 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x40086c *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400870 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400874 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x400878 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov r9_b tmp_b;
mov r9_t tmp_t;
cast C_lb@sint32 r9_b;
assert C_lb = r9_b && true;
assume C_lb = r9_b && true;
cast C_lt@sint32 r9_t;
assert C_lt = r9_t && true;
assume C_lt = r9_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	s0, r2                                     #! PC = 0x40087c *)
mov s0_b r2_b;
mov s0_t r2_t;
mov s0 r2;
mov zeta_s0 zeta_r2;
(* vmov	s1, r3                                     #! PC = 0x400880 *)
mov s1_b r3_b;
mov s1_t r3_t;
mov s1 r3;
mov zeta_s1 zeta_r3;
(* vmov	s2, r4                                     #! PC = 0x400884 *)
mov s2_b r4_b;
mov s2_t r4_t;
mov s2 r4;
mov zeta_s2 zeta_r4;
(* vmov	s3, r5                                     #! PC = 0x400888 *)
mov s3_b r5_b;
mov s3_t r5_t;
mov s3 r5;
mov zeta_s3 zeta_r5;
(* vmov	s4, r6                                     #! PC = 0x40088c *)
mov s4_b r6_b;
mov s4_t r6_t;
mov s4 r6;
mov zeta_s4 zeta_r6;
(* vmov	s5, r7                                     #! PC = 0x400890 *)
mov s5_b r7_b;
mov s5_t r7_t;
mov s5 r7;
mov zeta_s5 zeta_r7;
(* vmov	s6, r8                                     #! PC = 0x400894 *)
mov s6_b r8_b;
mov s6_t r8_t;
mov s6 r8;
mov zeta_s6 zeta_r8;
(* vmov	s7, r9                                     #! PC = 0x400898 *)
mov s7_b r9_b;
mov s7_t r9_t;
mov s7 r9;
mov zeta_s7 zeta_r9;

(*== coeffs multiplied by twiddles END ==*)

assert
  and [
    (-1) * 1664 <= s0_b, s0_b <= 1 * 1664,
    (-1) * 1664 <= s0_t, s0_t <= 1 * 1664,
    (-1) * 1664 <= s1_b, s1_b <= 1 * 1664,
    (-1) * 1664 <= s1_t, s1_t <= 1 * 1664,
    (-1) * 1664 <= s2_b, s2_b <= 1 * 1664,
    (-1) * 1664 <= s2_t, s2_t <= 1 * 1664,
    (-1) * 1664 <= s3_b, s3_b <= 1 * 1664,
    (-1) * 1664 <= s3_t, s3_t <= 1 * 1664,
    (-1) * 1664 <= s4_b, s4_b <= 1 * 1664,
    (-1) * 1664 <= s4_t, s4_t <= 1 * 1664,
    (-1) * 1664 <= s5_b, s5_b <= 1 * 1664,
    (-1) * 1664 <= s5_t, s5_t <= 1 * 1664,
    (-1) * 1664 <= s6_b, s6_b <= 1 * 1664,
    (-1) * 1664 <= s6_t, s6_t <= 1 * 1664,
    (-1) * 1664 <= s7_b, s7_b <= 1 * 1664,
    (-1) * 1664 <= s7_t, s7_t <= 1 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-1)@16 * 1664@16 <=s s0_b, s0_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s0_t, s0_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_b, s1_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_t, s1_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_b, s2_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_t, s2_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_b, s3_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_t, s3_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_b, s4_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_t, s4_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_b, s5_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_t, s5_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_b, s6_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_t, s6_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_b, s7_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_t, s7_t <=s 1@16 * 1664@16
  ];

assume
  and [
    (-1) * 1664 <= s0_b, s0_b <= 1 * 1664,
    (-1) * 1664 <= s0_t, s0_t <= 1 * 1664,
    (-1) * 1664 <= s1_b, s1_b <= 1 * 1664,
    (-1) * 1664 <= s1_t, s1_t <= 1 * 1664,
    (-1) * 1664 <= s2_b, s2_b <= 1 * 1664,
    (-1) * 1664 <= s2_t, s2_t <= 1 * 1664,
    (-1) * 1664 <= s3_b, s3_b <= 1 * 1664,
    (-1) * 1664 <= s3_t, s3_t <= 1 * 1664,
    (-1) * 1664 <= s4_b, s4_b <= 1 * 1664,
    (-1) * 1664 <= s4_t, s4_t <= 1 * 1664,
    (-1) * 1664 <= s5_b, s5_b <= 1 * 1664,
    (-1) * 1664 <= s5_t, s5_t <= 1 * 1664,
    (-1) * 1664 <= s6_b, s6_b <= 1 * 1664,
    (-1) * 1664 <= s6_t, s6_t <= 1 * 1664,
    (-1) * 1664 <= s7_b, s7_b <= 1 * 1664,
    (-1) * 1664 <= s7_t, s7_t <= 1 * 1664
  ]
  &&
  and [
    (-1)@16 * 1664@16 <=s s0_b, s0_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s0_t, s0_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_b, s1_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_t, s1_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_b, s2_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_t, s2_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_b, s3_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_t, s3_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_b, s4_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_t, s4_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_b, s5_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_t, s5_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_b, s6_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_t, s6_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_b, s7_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_t, s7_t <=s 1@16 * 1664@16
  ];

(* vmov	r0, s23                                    #! PC = 0x40089c *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff1c4; Value = 0xb6ec9408; PC = 0x4008a0 *)
mov r2_b L0xbefff1c4;
mov r2_t L0xbefff1c6;
(* ldr.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff204; Value = 0xb6fde14b; PC = 0x4008a4 *)
mov r3_b L0xbefff204;
mov r3_t L0xbefff206;
(* ldr.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff244; Value = 0x00000000; PC = 0x4008a8 *)
mov r4_b L0xbefff244;
mov r4_t L0xbefff246;
(* ldr.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff284; Value = 0x00000000; PC = 0x4008ac *)
mov r5_b L0xbefff284;
mov r5_t L0xbefff286;
(* ldr.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2c4; Value = 0x00000000; PC = 0x4008b0 *)
mov r6_b L0xbefff2c4;
mov r6_t L0xbefff2c6;
(* ldr.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff304; Value = 0x00000000; PC = 0x4008b4 *)
mov r7_b L0xbefff304;
mov r7_t L0xbefff306;
(* ldr.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff344; Value = 0x00000000; PC = 0x4008b8 *)
mov r8_b L0xbefff344;
mov r8_t L0xbefff346;
(* ldr.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff384; Value = 0x00000000; PC = 0x4008bc *)
mov r9_b L0xbefff384;
mov r9_t L0xbefff386;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x4008c0 *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* vmov	r10, s8                                    #! PC = 0x4008c4 *)
mov r10_b s8_b;
mov r10_t s8_t;
mov r10 s8;
mov zeta_r10 zeta_s8;
(* smulwb	lr, r10, r6                              #! PC = 0x4008c8 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x4008cc *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4008d0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4008d4 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x4008d8 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x4008dc *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4008e0 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x4008e4 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x4008e8 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4008ec *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4008f0 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x4008f4 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x4008f8 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x4008fc *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x400900 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400904 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400908 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x40090c *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400910 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x400914 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400918 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x40091c *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x400920 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400924 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400928 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x40092c *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x400930 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400934 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* vmov	r10, s9                                    #! PC = 0x400938 *)
mov r10_b s9_b;
mov r10_t s9_t;
mov r10 s9;
mov zeta_r10 zeta_s9;
(* vmov	r11, s10                                   #! PC = 0x40093c *)
mov r11_b s10_b;
mov r11_t s10_t;
mov r11 s10;
mov zeta_r11 zeta_s10;
(* smulwb	lr, r10, r4                              #! PC = 0x400940 *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x400944 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400948 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x40094c *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400950 *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x400954 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400958 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x40095c *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x400960 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400964 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400968 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x40096c *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x400970 *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400974 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400978 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x40097c *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400980 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400984 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400988 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x40098c *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400990 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400994 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400998 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40099c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4009a0 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4009a4 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x4009a8 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x4009ac *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* vmov	r10, s11                                   #! PC = 0x4009b0 *)
mov r10_b s11_b;
mov r10_t s11_t;
mov r10 s11;
mov zeta_r10 zeta_s11;
(* vmov	r11, s12                                   #! PC = 0x4009b4 *)
mov r11_b s12_b;
mov r11_t s12_t;
mov r11 s12;
mov zeta_r11 zeta_s12;
(* smulwb	lr, r10, r3                              #! PC = 0x4009b8 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x4009bc *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4009c0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4009c4 *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x4009c8 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x4009cc *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4009d0 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x4009d4 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x4009d8 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4009dc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x4009e0 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x4009e4 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x4009e8 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x4009ec *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* vmov	r10, s13                                   #! PC = 0x4009f0 *)
mov r10_b s13_b;
mov r10_t s13_t;
mov r10 s13;
mov zeta_r10 zeta_s13;
(* vmov	r11, s14                                   #! PC = 0x4009f4 *)
mov r11_b s14_b;
mov r11_t s14_t;
mov r11 s14;
mov zeta_r11 zeta_s14;
(* smulwb	lr, r10, r7                              #! PC = 0x4009f8 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x4009fc *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400a00 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400a04 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400a08 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400a0c *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400a10 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400a14 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400a18 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400a1c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400a20 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400a24 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x400a28 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x400a2c *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

(*== 8-NTT on a0, a2, ..., a14 ==*)
(*== _3_layer_double_CT_16_plant_fp END ==*)

assert
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

assume
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

(* vmov	r0, s23                                    #! PC = 0x400a30 *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* vmov	r10, s1                                    #! PC = 0x400a34 *)
mov r10_b s1_b;
mov r10_t s1_t;
mov r10 s1;
mov zeta_r10 zeta_s1;
(* uadd16	lr, r3, r10                              #! PC = 0x400a38 *)
add lr_b r3_b r10_b;
add lr_t r3_t r10_t;
(* usub16	r3, r3, r10                              #! PC = 0x400a3c *)
sub r3_b r3_b r10_b;
sub r3_t r3_t r10_t;
(* str.w	lr, [r0, #64]	; 0x40                      #! EA = L0xbefff204; PC = 0x400a40 *)
mov L0xbefff204 lr_b;
mov L0xbefff206 lr_t;
(* str.w	r3, [r0, #96]	; 0x60                      #! EA = L0xbefff224; PC = 0x400a44 *)
mov L0xbefff224 r3_b;
mov L0xbefff226 r3_t;
(* vmov	r10, s3                                    #! PC = 0x400a48 *)
mov r10_b s3_b;
mov r10_t s3_t;
mov r10 s3;
mov zeta_r10 zeta_s3;
(* uadd16	lr, r5, r10                              #! PC = 0x400a4c *)
add lr_b r5_b r10_b;
add lr_t r5_t r10_t;
(* usub16	r5, r5, r10                              #! PC = 0x400a50 *)
sub r5_b r5_b r10_b;
sub r5_t r5_t r10_t;
(* str.w	lr, [r0, #192]	; 0xc0                     #! EA = L0xbefff284; PC = 0x400a54 *)
mov L0xbefff284 lr_b;
mov L0xbefff286 lr_t;
(* str.w	r5, [r0, #224]	; 0xe0                     #! EA = L0xbefff2a4; PC = 0x400a58 *)
mov L0xbefff2a4 r5_b;
mov L0xbefff2a6 r5_t;
(* vmov	r10, s5                                    #! PC = 0x400a5c *)
mov r10_b s5_b;
mov r10_t s5_t;
mov r10 s5;
mov zeta_r10 zeta_s5;
(* uadd16	lr, r7, r10                              #! PC = 0x400a60 *)
add lr_b r7_b r10_b;
add lr_t r7_t r10_t;
(* usub16	r7, r7, r10                              #! PC = 0x400a64 *)
sub r7_b r7_b r10_b;
sub r7_t r7_t r10_t;
(* str.w	lr, [r0, #320]	; 0x140                    #! EA = L0xbefff304; PC = 0x400a68 *)
mov L0xbefff304 lr_b;
mov L0xbefff306 lr_t;
(* str.w	r7, [r0, #352]	; 0x160                    #! EA = L0xbefff324; PC = 0x400a6c *)
mov L0xbefff324 r7_b;
mov L0xbefff326 r7_t;
(* vmov	r10, s7                                    #! PC = 0x400a70 *)
mov r10_b s7_b;
mov r10_t s7_t;
mov r10 s7;
mov zeta_r10 zeta_s7;
(* uadd16	lr, r9, r10                              #! PC = 0x400a74 *)
add lr_b r9_b r10_b;
add lr_t r9_t r10_t;
(* usub16	r9, r9, r10                              #! PC = 0x400a78 *)
sub r9_b r9_b r10_b;
sub r9_t r9_t r10_t;
(* str.w	lr, [r0, #448]	; 0x1c0                    #! EA = L0xbefff384; PC = 0x400a7c *)
mov L0xbefff384 lr_b;
mov L0xbefff386 lr_t;
(* str.w	r9, [r0, #480]	; 0x1e0                    #! EA = L0xbefff3a4; PC = 0x400a80 *)
mov L0xbefff3a4 r9_b;
mov L0xbefff3a6 r9_t;
(* vmov	r5, s2                                     #! PC = 0x400a84 *)
mov r5_b s2_b;
mov r5_t s2_t;
mov r5 s2;
mov zeta_r5 zeta_s2;
(* uadd16	lr, r4, r5                               #! PC = 0x400a88 *)
add lr_b r4_b r5_b;
add lr_t r4_t r5_t;
(* usub16	r10, r4, r5                              #! PC = 0x400a8c *)
sub r10_b r4_b r5_b;
sub r10_t r4_t r5_t;
(* str.w	lr, [r0, #128]	; 0x80                     #! EA = L0xbefff244; PC = 0x400a90 *)
mov L0xbefff244 lr_b;
mov L0xbefff246 lr_t;
(* str.w	r10, [r0, #160]	; 0xa0                    #! EA = L0xbefff264; PC = 0x400a94 *)
mov L0xbefff264 r10_b;
mov L0xbefff266 r10_t;
(* vmov	r7, s4                                     #! PC = 0x400a98 *)
mov r7_b s4_b;
mov r7_t s4_t;
mov r7 s4;
mov zeta_r7 zeta_s4;
(* uadd16	lr, r6, r7                               #! PC = 0x400a9c *)
add lr_b r6_b r7_b;
add lr_t r6_t r7_t;
(* usub16	r10, r6, r7                              #! PC = 0x400aa0 *)
sub r10_b r6_b r7_b;
sub r10_t r6_t r7_t;
(* str.w	lr, [r0, #256]	; 0x100                    #! EA = L0xbefff2c4; PC = 0x400aa4 *)
mov L0xbefff2c4 lr_b;
mov L0xbefff2c6 lr_t;
(* str.w	r10, [r0, #288]	; 0x120                   #! EA = L0xbefff2e4; PC = 0x400aa8 *)
mov L0xbefff2e4 r10_b;
mov L0xbefff2e6 r10_t;
(* vmov	r9, s6                                     #! PC = 0x400aac *)
mov r9_b s6_b;
mov r9_t s6_t;
mov r9 s6;
mov zeta_r9 zeta_s6;
(* uadd16	lr, r8, r9                               #! PC = 0x400ab0 *)
add lr_b r8_b r9_b;
add lr_t r8_t r9_t;
(* usub16	r10, r8, r9                              #! PC = 0x400ab4 *)
sub r10_b r8_b r9_b;
sub r10_t r8_t r9_t;
(* str.w	lr, [r0, #384]	; 0x180                    #! EA = L0xbefff344; PC = 0x400ab8 *)
mov L0xbefff344 lr_b;
mov L0xbefff346 lr_t;
(* str.w	r10, [r0, #416]	; 0x1a0                   #! EA = L0xbefff364; PC = 0x400abc *)
mov L0xbefff364 r10_b;
mov L0xbefff366 r10_t;
(* vmov	r3, s0                                     #! PC = 0x400ac0 *)
mov r3_b s0_b;
mov r3_t s0_t;
mov r3 s0;
mov zeta_r3 zeta_s0;
(* uadd16	lr, r2, r3                               #! PC = 0x400ac4 *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* usub16	r10, r2, r3                              #! PC = 0x400ac8 *)
sub r10_b r2_b r3_b;
sub r10_t r2_t r3_t;
(* str.w	r10, [r0, #32]                            #! EA = L0xbefff1e4; PC = 0x400acc *)
mov L0xbefff1e4 r10_b;
mov L0xbefff1e6 r10_t;
(* str.w	lr, [r0], #4                              #! EA = L0xbefff1c4; PC = 0x400ad0 *)
mov L0xbefff1c4 lr_b;
mov L0xbefff1c6 lr_t;
(* vmov	lr, s24                                    #! PC = 0x400ad4 *)
mov lr_b s24_b;
mov lr_t s24_t;
mov lr s24;
mov zeta_lr zeta_s24;
(* #bne.w	0x400628 <ntt_fast+24>                   #! PC = 0x400adc *)
#bne.w	0x400628 <ntt_fast+24>                   #! 0x400adc = 0x400adc;

(*== layer 7+6+5+4 one slice condition ==*)

assert
  and [
    (-6) * 1664 <= L0xbefff1c4, L0xbefff1c4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1c6, L0xbefff1c6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1e4, L0xbefff1e4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1e6, L0xbefff1e6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff204, L0xbefff204 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff206, L0xbefff206 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff224, L0xbefff224 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff226, L0xbefff226 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff244, L0xbefff244 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff246, L0xbefff246 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff264, L0xbefff264 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff266, L0xbefff266 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff284, L0xbefff284 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff286, L0xbefff286 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2a4, L0xbefff2a4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2a6, L0xbefff2a6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2c4, L0xbefff2c4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2c6, L0xbefff2c6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2e4, L0xbefff2e4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2e6, L0xbefff2e6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff304, L0xbefff304 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff306, L0xbefff306 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff324, L0xbefff324 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff326, L0xbefff326 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff344, L0xbefff344 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff346, L0xbefff346 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff364, L0xbefff364 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff366, L0xbefff366 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff384, L0xbefff384 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff386, L0xbefff386 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3a4, L0xbefff3a4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3a6, L0xbefff3a6 <= 6 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-6)@16 * 1664@16 <=s L0xbefff1c4, L0xbefff1c4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1c6, L0xbefff1c6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1e4, L0xbefff1e4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1e6, L0xbefff1e6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff204, L0xbefff204 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff206, L0xbefff206 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff224, L0xbefff224 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff226, L0xbefff226 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff244, L0xbefff244 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff246, L0xbefff246 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff264, L0xbefff264 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff266, L0xbefff266 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff284, L0xbefff284 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff286, L0xbefff286 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2a4, L0xbefff2a4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2a6, L0xbefff2a6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2c4, L0xbefff2c4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2c6, L0xbefff2c6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2e4, L0xbefff2e4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2e6, L0xbefff2e6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff304, L0xbefff304 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff306, L0xbefff306 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff324, L0xbefff324 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff326, L0xbefff326 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff344, L0xbefff344 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff346, L0xbefff346 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff364, L0xbefff364 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff366, L0xbefff366 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff384, L0xbefff384 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff386, L0xbefff386 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3a4, L0xbefff3a4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3a6, L0xbefff3a6 <=s 6@16 * 1664@16
  ]
;

assume
  and [
    (-6) * 1664 <= L0xbefff1c4, L0xbefff1c4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1c6, L0xbefff1c6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1e4, L0xbefff1e4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1e6, L0xbefff1e6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff204, L0xbefff204 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff206, L0xbefff206 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff224, L0xbefff224 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff226, L0xbefff226 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff244, L0xbefff244 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff246, L0xbefff246 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff264, L0xbefff264 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff266, L0xbefff266 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff284, L0xbefff284 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff286, L0xbefff286 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2a4, L0xbefff2a4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2a6, L0xbefff2a6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2c4, L0xbefff2c4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2c6, L0xbefff2c6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2e4, L0xbefff2e4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2e6, L0xbefff2e6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff304, L0xbefff304 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff306, L0xbefff306 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff324, L0xbefff324 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff326, L0xbefff326 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff344, L0xbefff344 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff346, L0xbefff346 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff364, L0xbefff364 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff366, L0xbefff366 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff384, L0xbefff384 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff386, L0xbefff386 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3a4, L0xbefff3a4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3a6, L0xbefff3a6 <= 6 * 1664
  ]
  &&
  and [
    (-6)@16 * 1664@16 <=s L0xbefff1c4, L0xbefff1c4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1c6, L0xbefff1c6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1e4, L0xbefff1e4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1e6, L0xbefff1e6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff204, L0xbefff204 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff206, L0xbefff206 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff224, L0xbefff224 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff226, L0xbefff226 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff244, L0xbefff244 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff246, L0xbefff246 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff264, L0xbefff264 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff266, L0xbefff266 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff284, L0xbefff284 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff286, L0xbefff286 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2a4, L0xbefff2a4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2a6, L0xbefff2a6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2c4, L0xbefff2c4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2c6, L0xbefff2c6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2e4, L0xbefff2e4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2e6, L0xbefff2e6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff304, L0xbefff304 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff306, L0xbefff306 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff324, L0xbefff324 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff326, L0xbefff326 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff344, L0xbefff344 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff346, L0xbefff346 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff364, L0xbefff364 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff366, L0xbefff366 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff384, L0xbefff384 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff386, L0xbefff386 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3a4, L0xbefff3a4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3a6, L0xbefff3a6 <=s 6@16 * 1664@16
  ]
;

(* vmov	s23, r0                                    #! PC = 0x400628 *)
mov s23_b r0_b;
mov s23_t r0_t;
mov s23 r0;
mov zeta_s23 zeta_r0;
(* ldr.w	r2, [r0, #32]                             #! EA = L0xbefff1e8; Value = 0xb6fd53a4; PC = 0x40062c *)
mov r2_b L0xbefff1e8;
mov r2_t L0xbefff1ea;
(* ldr.w	r3, [r0, #96]	; 0x60                      #! EA = L0xbefff228; Value = 0xb6fff5c8; PC = 0x400630 *)
mov r3_b L0xbefff228;
mov r3_t L0xbefff22a;
(* ldr.w	r4, [r0, #160]	; 0xa0                     #! EA = L0xbefff268; Value = 0x00000000; PC = 0x400634 *)
mov r4_b L0xbefff268;
mov r4_t L0xbefff26a;
(* ldr.w	r5, [r0, #224]	; 0xe0                     #! EA = L0xbefff2a8; Value = 0xbefff2d0; PC = 0x400638 *)
mov r5_b L0xbefff2a8;
mov r5_t L0xbefff2aa;
(* ldr.w	r6, [r0, #288]	; 0x120                    #! EA = L0xbefff2e8; Value = 0x00000000; PC = 0x40063c *)
mov r6_b L0xbefff2e8;
mov r6_t L0xbefff2ea;
(* ldr.w	r7, [r0, #352]	; 0x160                    #! EA = L0xbefff328; Value = 0x00000000; PC = 0x400640 *)
mov r7_b L0xbefff328;
mov r7_t L0xbefff32a;
(* ldr.w	r8, [r0, #416]	; 0x1a0                    #! EA = L0xbefff368; Value = 0x00000000; PC = 0x400644 *)
mov r8_b L0xbefff368;
mov r8_t L0xbefff36a;
(* ldr.w	r9, [r0, #480]	; 0x1e0                    #! EA = L0xbefff3a8; Value = 0xb6fb53c4; PC = 0x400648 *)
mov r9_b L0xbefff3a8;
mov r9_t L0xbefff3aa;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x40064c *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* vmov	r10, s8                                    #! PC = 0x400650 *)
mov r10_b s8_b;
mov r10_t s8_t;
mov r10 s8;
mov zeta_r10 zeta_s8;
(* smulwb	lr, r10, r6                              #! PC = 0x400654 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400658 *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40065c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400660 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x400664 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x400668 *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x40066c *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x400670 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400674 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400678 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x40067c *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400680 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x400684 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400688 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x40068c *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400690 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400694 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400698 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x40069c *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x4006a0 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x4006a4 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x4006a8 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x4006ac *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4006b0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4006b4 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4006b8 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x4006bc *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x4006c0 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* vmov	r10, s9                                    #! PC = 0x4006c4 *)
mov r10_b s9_b;
mov r10_t s9_t;
mov r10 s9;
mov zeta_r10 zeta_s9;
(* vmov	r11, s10                                   #! PC = 0x4006c8 *)
mov r11_b s10_b;
mov r11_t s10_t;
mov r11 s10;
mov zeta_r11 zeta_s10;
(* smulwb	lr, r10, r4                              #! PC = 0x4006cc *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x4006d0 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4006d4 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x4006d8 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x4006dc *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x4006e0 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4006e4 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x4006e8 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x4006ec *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4006f0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x4006f4 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x4006f8 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x4006fc *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400700 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400704 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x400708 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40070c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400710 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400714 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x400718 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x40071c *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400720 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400724 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400728 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x40072c *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400730 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x400734 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400738 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* vmov	r10, s11                                   #! PC = 0x40073c *)
mov r10_b s11_b;
mov r10_t s11_t;
mov r10 s11;
mov zeta_r10 zeta_s11;
(* vmov	r11, s12                                   #! PC = 0x400740 *)
mov r11_b s12_b;
mov r11_t s12_t;
mov r11 s12;
mov zeta_r11 zeta_s12;
(* smulwb	lr, r10, r3                              #! PC = 0x400744 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x400748 *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40074c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400750 *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x400754 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x400758 *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x40075c *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x400760 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x400764 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400768 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x40076c *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400770 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x400774 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400778 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* vmov	r10, s13                                   #! PC = 0x40077c *)
mov r10_b s13_b;
mov r10_t s13_t;
mov r10 s13;
mov zeta_r10 zeta_s13;
(* vmov	r11, s14                                   #! PC = 0x400780 *)
mov r11_b s14_b;
mov r11_t s14_t;
mov r11 s14;
mov zeta_r11 zeta_s14;
(* smulwb	lr, r10, r7                              #! PC = 0x400784 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400788 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40078c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400790 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400794 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400798 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x40079c *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4007a0 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x4007a4 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007a8 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4007ac *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4007b0 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x4007b4 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x4007b8 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

(*== 8-NTT on a1, a3, ..., a15 ==*)
(*== _3_layer_double_CT_16_plant_fp END ==*)

assert
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

assume
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

(* vmov	r10, s15                                   #! PC = 0x4007bc *)
mov r10_b s15_b;
mov r10_t s15_t;
mov r10 s15;
mov zeta_r10 zeta_s15;
(* vmov	r11, s16                                   #! PC = 0x4007c0 *)
mov r11_b s16_b;
mov r11_t s16_t;
mov r11 s16;
mov zeta_r11 zeta_s16;
(* smulwb	lr, r10, r2                              #! PC = 0x4007c4 *)
cast r2_lsb@sint32 r2_b;
mull tmp_t tmp_b r10 r2_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r2_b;
assert A_lb = r2_b && true;
assume A_lb = r2_b && true;
cast A_lt@sint32 r2_t;
assert A_lt = r2_t && true;
assume A_lt = r2_t && true;
mov zeta zeta_r10;
(* smulwt	r2, r10, r2                              #! PC = 0x4007c8 *)
cast r2_lst@sint32 r2_t;
mull tmp_t tmp_b r10 r2_lst;
spl dontcare r2_t tmp_t 16;
spl r2_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007cc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r2, r2, r12, r0                          #! PC = 0x4007d0 *)
cast r2_sb@sint16 r2_b;
mull tmp_t tmp_b r2_sb r12_t;
uadds carry r2_b tmp_b r0_b;
adc r2_t tmp_t r0_t carry;
(* pkhtb	r2, r2, lr, asr #16                       #! PC = 0x4007d4 *)
mov tmp_b lr_t;
mov tmp_t r2_t;
mov r2_b tmp_b;
mov r2_t tmp_t;
cast C_lb@sint32 r2_b;
assert C_lb = r2_b && true;
assume C_lb = r2_b && true;
cast C_lt@sint32 r2_t;
assert C_lt = r2_t && true;
assume C_lt = r2_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r3                              #! PC = 0x4007d8 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r11 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r11;
(* smulwt	r3, r11, r3                              #! PC = 0x4007dc *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r11 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007e0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4007e4 *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x4007e8 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov r3_b tmp_b;
mov r3_t tmp_t;
cast C_lb@sint32 r3_b;
assert C_lb = r3_b && true;
assume C_lb = r3_b && true;
cast C_lt@sint32 r3_t;
assert C_lt = r3_t && true;
assume C_lt = r3_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	r10, s17                                   #! PC = 0x4007ec *)
mov r10_b s17_b;
mov r10_t s17_t;
mov r10 s17;
mov zeta_r10 zeta_s17;
(* vmov	r11, s18                                   #! PC = 0x4007f0 *)
mov r11_b s18_b;
mov r11_t s18_t;
mov r11 s18;
mov zeta_r11 zeta_s18;
(* smulwb	lr, r10, r4                              #! PC = 0x4007f4 *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x4007f8 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007fc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400800 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x400804 *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov r4_b tmp_b;
mov r4_t tmp_t;
cast C_lb@sint32 r4_b;
assert C_lb = r4_b && true;
assume C_lb = r4_b && true;
cast C_lt@sint32 r4_t;
assert C_lt = r4_t && true;
assume C_lt = r4_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r5                              #! PC = 0x400808 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x40080c *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400810 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400814 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x400818 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov r5_b tmp_b;
mov r5_t tmp_t;
cast C_lb@sint32 r5_b;
assert C_lb = r5_b && true;
assume C_lb = r5_b && true;
cast C_lt@sint32 r5_t;
assert C_lt = r5_t && true;
assume C_lt = r5_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	r10, s19                                   #! PC = 0x40081c *)
mov r10_b s19_b;
mov r10_t s19_t;
mov r10 s19;
mov zeta_r10 zeta_s19;
(* vmov	r11, s20                                   #! PC = 0x400820 *)
mov r11_b s20_b;
mov r11_t s20_t;
mov r11 s20;
mov zeta_r11 zeta_s20;
(* smulwb	lr, r10, r6                              #! PC = 0x400824 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400828 *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40082c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400830 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x400834 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov r6_b tmp_b;
mov r6_t tmp_t;
cast C_lb@sint32 r6_b;
assert C_lb = r6_b && true;
assume C_lb = r6_b && true;
cast C_lt@sint32 r6_t;
assert C_lt = r6_t && true;
assume C_lt = r6_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r7                              #! PC = 0x400838 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r11 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r11;
(* smulwt	r7, r11, r7                              #! PC = 0x40083c *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r11 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400840 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400844 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x400848 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov r7_b tmp_b;
mov r7_t tmp_t;
cast C_lb@sint32 r7_b;
assert C_lb = r7_b && true;
assume C_lb = r7_b && true;
cast C_lt@sint32 r7_t;
assert C_lt = r7_t && true;
assume C_lt = r7_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	r10, s21                                   #! PC = 0x40084c *)
mov r10_b s21_b;
mov r10_t s21_t;
mov r10 s21;
mov zeta_r10 zeta_s21;
(* vmov	r11, s22                                   #! PC = 0x400850 *)
mov r11_b s22_b;
mov r11_t s22_t;
mov r11 s22;
mov zeta_r11 zeta_s22;
(* smulwb	lr, r10, r8                              #! PC = 0x400854 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400858 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40085c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400860 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x400864 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov r8_b tmp_b;
mov r8_t tmp_t;
cast C_lb@sint32 r8_b;
assert C_lb = r8_b && true;
assume C_lb = r8_b && true;
cast C_lt@sint32 r8_t;
assert C_lt = r8_t && true;
assume C_lt = r8_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r9                              #! PC = 0x400868 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x40086c *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400870 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400874 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x400878 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov r9_b tmp_b;
mov r9_t tmp_t;
cast C_lb@sint32 r9_b;
assert C_lb = r9_b && true;
assume C_lb = r9_b && true;
cast C_lt@sint32 r9_t;
assert C_lt = r9_t && true;
assume C_lt = r9_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	s0, r2                                     #! PC = 0x40087c *)
mov s0_b r2_b;
mov s0_t r2_t;
mov s0 r2;
mov zeta_s0 zeta_r2;
(* vmov	s1, r3                                     #! PC = 0x400880 *)
mov s1_b r3_b;
mov s1_t r3_t;
mov s1 r3;
mov zeta_s1 zeta_r3;
(* vmov	s2, r4                                     #! PC = 0x400884 *)
mov s2_b r4_b;
mov s2_t r4_t;
mov s2 r4;
mov zeta_s2 zeta_r4;
(* vmov	s3, r5                                     #! PC = 0x400888 *)
mov s3_b r5_b;
mov s3_t r5_t;
mov s3 r5;
mov zeta_s3 zeta_r5;
(* vmov	s4, r6                                     #! PC = 0x40088c *)
mov s4_b r6_b;
mov s4_t r6_t;
mov s4 r6;
mov zeta_s4 zeta_r6;
(* vmov	s5, r7                                     #! PC = 0x400890 *)
mov s5_b r7_b;
mov s5_t r7_t;
mov s5 r7;
mov zeta_s5 zeta_r7;
(* vmov	s6, r8                                     #! PC = 0x400894 *)
mov s6_b r8_b;
mov s6_t r8_t;
mov s6 r8;
mov zeta_s6 zeta_r8;
(* vmov	s7, r9                                     #! PC = 0x400898 *)
mov s7_b r9_b;
mov s7_t r9_t;
mov s7 r9;
mov zeta_s7 zeta_r9;

(*== coeffs multiplied by twiddles END ==*)

assert
  and [
    (-1) * 1664 <= s0_b, s0_b <= 1 * 1664,
    (-1) * 1664 <= s0_t, s0_t <= 1 * 1664,
    (-1) * 1664 <= s1_b, s1_b <= 1 * 1664,
    (-1) * 1664 <= s1_t, s1_t <= 1 * 1664,
    (-1) * 1664 <= s2_b, s2_b <= 1 * 1664,
    (-1) * 1664 <= s2_t, s2_t <= 1 * 1664,
    (-1) * 1664 <= s3_b, s3_b <= 1 * 1664,
    (-1) * 1664 <= s3_t, s3_t <= 1 * 1664,
    (-1) * 1664 <= s4_b, s4_b <= 1 * 1664,
    (-1) * 1664 <= s4_t, s4_t <= 1 * 1664,
    (-1) * 1664 <= s5_b, s5_b <= 1 * 1664,
    (-1) * 1664 <= s5_t, s5_t <= 1 * 1664,
    (-1) * 1664 <= s6_b, s6_b <= 1 * 1664,
    (-1) * 1664 <= s6_t, s6_t <= 1 * 1664,
    (-1) * 1664 <= s7_b, s7_b <= 1 * 1664,
    (-1) * 1664 <= s7_t, s7_t <= 1 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-1)@16 * 1664@16 <=s s0_b, s0_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s0_t, s0_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_b, s1_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_t, s1_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_b, s2_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_t, s2_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_b, s3_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_t, s3_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_b, s4_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_t, s4_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_b, s5_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_t, s5_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_b, s6_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_t, s6_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_b, s7_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_t, s7_t <=s 1@16 * 1664@16
  ];

assume
  and [
    (-1) * 1664 <= s0_b, s0_b <= 1 * 1664,
    (-1) * 1664 <= s0_t, s0_t <= 1 * 1664,
    (-1) * 1664 <= s1_b, s1_b <= 1 * 1664,
    (-1) * 1664 <= s1_t, s1_t <= 1 * 1664,
    (-1) * 1664 <= s2_b, s2_b <= 1 * 1664,
    (-1) * 1664 <= s2_t, s2_t <= 1 * 1664,
    (-1) * 1664 <= s3_b, s3_b <= 1 * 1664,
    (-1) * 1664 <= s3_t, s3_t <= 1 * 1664,
    (-1) * 1664 <= s4_b, s4_b <= 1 * 1664,
    (-1) * 1664 <= s4_t, s4_t <= 1 * 1664,
    (-1) * 1664 <= s5_b, s5_b <= 1 * 1664,
    (-1) * 1664 <= s5_t, s5_t <= 1 * 1664,
    (-1) * 1664 <= s6_b, s6_b <= 1 * 1664,
    (-1) * 1664 <= s6_t, s6_t <= 1 * 1664,
    (-1) * 1664 <= s7_b, s7_b <= 1 * 1664,
    (-1) * 1664 <= s7_t, s7_t <= 1 * 1664
  ]
  &&
  and [
    (-1)@16 * 1664@16 <=s s0_b, s0_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s0_t, s0_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_b, s1_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_t, s1_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_b, s2_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_t, s2_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_b, s3_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_t, s3_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_b, s4_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_t, s4_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_b, s5_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_t, s5_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_b, s6_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_t, s6_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_b, s7_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_t, s7_t <=s 1@16 * 1664@16
  ];

(* vmov	r0, s23                                    #! PC = 0x40089c *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff1c8; Value = 0xb6ffb000; PC = 0x4008a0 *)
mov r2_b L0xbefff1c8;
mov r2_t L0xbefff1ca;
(* ldr.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff208; Value = 0xb6ffb510; PC = 0x4008a4 *)
mov r3_b L0xbefff208;
mov r3_t L0xbefff20a;
(* ldr.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff248; Value = 0x00000000; PC = 0x4008a8 *)
mov r4_b L0xbefff248;
mov r4_t L0xbefff24a;
(* ldr.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff288; Value = 0x00000000; PC = 0x4008ac *)
mov r5_b L0xbefff288;
mov r5_t L0xbefff28a;
(* ldr.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2c8; Value = 0x00000000; PC = 0x4008b0 *)
mov r6_b L0xbefff2c8;
mov r6_t L0xbefff2ca;
(* ldr.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff308; Value = 0x00000000; PC = 0x4008b4 *)
mov r7_b L0xbefff308;
mov r7_t L0xbefff30a;
(* ldr.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff348; Value = 0x00000000; PC = 0x4008b8 *)
mov r8_b L0xbefff348;
mov r8_t L0xbefff34a;
(* ldr.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff388; Value = 0x00000000; PC = 0x4008bc *)
mov r9_b L0xbefff388;
mov r9_t L0xbefff38a;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x4008c0 *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* vmov	r10, s8                                    #! PC = 0x4008c4 *)
mov r10_b s8_b;
mov r10_t s8_t;
mov r10 s8;
mov zeta_r10 zeta_s8;
(* smulwb	lr, r10, r6                              #! PC = 0x4008c8 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x4008cc *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4008d0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4008d4 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x4008d8 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x4008dc *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4008e0 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x4008e4 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x4008e8 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4008ec *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4008f0 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x4008f4 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x4008f8 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x4008fc *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x400900 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400904 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400908 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x40090c *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400910 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x400914 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400918 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x40091c *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x400920 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400924 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400928 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x40092c *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x400930 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400934 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* vmov	r10, s9                                    #! PC = 0x400938 *)
mov r10_b s9_b;
mov r10_t s9_t;
mov r10 s9;
mov zeta_r10 zeta_s9;
(* vmov	r11, s10                                   #! PC = 0x40093c *)
mov r11_b s10_b;
mov r11_t s10_t;
mov r11 s10;
mov zeta_r11 zeta_s10;
(* smulwb	lr, r10, r4                              #! PC = 0x400940 *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x400944 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400948 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x40094c *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400950 *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x400954 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400958 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x40095c *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x400960 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400964 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400968 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x40096c *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x400970 *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400974 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400978 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x40097c *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400980 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400984 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400988 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x40098c *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400990 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400994 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400998 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40099c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4009a0 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4009a4 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x4009a8 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x4009ac *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* vmov	r10, s11                                   #! PC = 0x4009b0 *)
mov r10_b s11_b;
mov r10_t s11_t;
mov r10 s11;
mov zeta_r10 zeta_s11;
(* vmov	r11, s12                                   #! PC = 0x4009b4 *)
mov r11_b s12_b;
mov r11_t s12_t;
mov r11 s12;
mov zeta_r11 zeta_s12;
(* smulwb	lr, r10, r3                              #! PC = 0x4009b8 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x4009bc *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4009c0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4009c4 *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x4009c8 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x4009cc *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4009d0 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x4009d4 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x4009d8 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4009dc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x4009e0 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x4009e4 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x4009e8 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x4009ec *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* vmov	r10, s13                                   #! PC = 0x4009f0 *)
mov r10_b s13_b;
mov r10_t s13_t;
mov r10 s13;
mov zeta_r10 zeta_s13;
(* vmov	r11, s14                                   #! PC = 0x4009f4 *)
mov r11_b s14_b;
mov r11_t s14_t;
mov r11 s14;
mov zeta_r11 zeta_s14;
(* smulwb	lr, r10, r7                              #! PC = 0x4009f8 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x4009fc *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400a00 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400a04 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400a08 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400a0c *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400a10 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400a14 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400a18 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400a1c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400a20 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400a24 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x400a28 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x400a2c *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

(*== 8-NTT on a0, a2, ..., a14 ==*)
(*== _3_layer_double_CT_16_plant_fp END ==*)

assert
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

assume
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

(* vmov	r0, s23                                    #! PC = 0x400a30 *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* vmov	r10, s1                                    #! PC = 0x400a34 *)
mov r10_b s1_b;
mov r10_t s1_t;
mov r10 s1;
mov zeta_r10 zeta_s1;
(* uadd16	lr, r3, r10                              #! PC = 0x400a38 *)
add lr_b r3_b r10_b;
add lr_t r3_t r10_t;
(* usub16	r3, r3, r10                              #! PC = 0x400a3c *)
sub r3_b r3_b r10_b;
sub r3_t r3_t r10_t;
(* str.w	lr, [r0, #64]	; 0x40                      #! EA = L0xbefff208; PC = 0x400a40 *)
mov L0xbefff208 lr_b;
mov L0xbefff20a lr_t;
(* str.w	r3, [r0, #96]	; 0x60                      #! EA = L0xbefff228; PC = 0x400a44 *)
mov L0xbefff228 r3_b;
mov L0xbefff22a r3_t;
(* vmov	r10, s3                                    #! PC = 0x400a48 *)
mov r10_b s3_b;
mov r10_t s3_t;
mov r10 s3;
mov zeta_r10 zeta_s3;
(* uadd16	lr, r5, r10                              #! PC = 0x400a4c *)
add lr_b r5_b r10_b;
add lr_t r5_t r10_t;
(* usub16	r5, r5, r10                              #! PC = 0x400a50 *)
sub r5_b r5_b r10_b;
sub r5_t r5_t r10_t;
(* str.w	lr, [r0, #192]	; 0xc0                     #! EA = L0xbefff288; PC = 0x400a54 *)
mov L0xbefff288 lr_b;
mov L0xbefff28a lr_t;
(* str.w	r5, [r0, #224]	; 0xe0                     #! EA = L0xbefff2a8; PC = 0x400a58 *)
mov L0xbefff2a8 r5_b;
mov L0xbefff2aa r5_t;
(* vmov	r10, s5                                    #! PC = 0x400a5c *)
mov r10_b s5_b;
mov r10_t s5_t;
mov r10 s5;
mov zeta_r10 zeta_s5;
(* uadd16	lr, r7, r10                              #! PC = 0x400a60 *)
add lr_b r7_b r10_b;
add lr_t r7_t r10_t;
(* usub16	r7, r7, r10                              #! PC = 0x400a64 *)
sub r7_b r7_b r10_b;
sub r7_t r7_t r10_t;
(* str.w	lr, [r0, #320]	; 0x140                    #! EA = L0xbefff308; PC = 0x400a68 *)
mov L0xbefff308 lr_b;
mov L0xbefff30a lr_t;
(* str.w	r7, [r0, #352]	; 0x160                    #! EA = L0xbefff328; PC = 0x400a6c *)
mov L0xbefff328 r7_b;
mov L0xbefff32a r7_t;
(* vmov	r10, s7                                    #! PC = 0x400a70 *)
mov r10_b s7_b;
mov r10_t s7_t;
mov r10 s7;
mov zeta_r10 zeta_s7;
(* uadd16	lr, r9, r10                              #! PC = 0x400a74 *)
add lr_b r9_b r10_b;
add lr_t r9_t r10_t;
(* usub16	r9, r9, r10                              #! PC = 0x400a78 *)
sub r9_b r9_b r10_b;
sub r9_t r9_t r10_t;
(* str.w	lr, [r0, #448]	; 0x1c0                    #! EA = L0xbefff388; PC = 0x400a7c *)
mov L0xbefff388 lr_b;
mov L0xbefff38a lr_t;
(* str.w	r9, [r0, #480]	; 0x1e0                    #! EA = L0xbefff3a8; PC = 0x400a80 *)
mov L0xbefff3a8 r9_b;
mov L0xbefff3aa r9_t;
(* vmov	r5, s2                                     #! PC = 0x400a84 *)
mov r5_b s2_b;
mov r5_t s2_t;
mov r5 s2;
mov zeta_r5 zeta_s2;
(* uadd16	lr, r4, r5                               #! PC = 0x400a88 *)
add lr_b r4_b r5_b;
add lr_t r4_t r5_t;
(* usub16	r10, r4, r5                              #! PC = 0x400a8c *)
sub r10_b r4_b r5_b;
sub r10_t r4_t r5_t;
(* str.w	lr, [r0, #128]	; 0x80                     #! EA = L0xbefff248; PC = 0x400a90 *)
mov L0xbefff248 lr_b;
mov L0xbefff24a lr_t;
(* str.w	r10, [r0, #160]	; 0xa0                    #! EA = L0xbefff268; PC = 0x400a94 *)
mov L0xbefff268 r10_b;
mov L0xbefff26a r10_t;
(* vmov	r7, s4                                     #! PC = 0x400a98 *)
mov r7_b s4_b;
mov r7_t s4_t;
mov r7 s4;
mov zeta_r7 zeta_s4;
(* uadd16	lr, r6, r7                               #! PC = 0x400a9c *)
add lr_b r6_b r7_b;
add lr_t r6_t r7_t;
(* usub16	r10, r6, r7                              #! PC = 0x400aa0 *)
sub r10_b r6_b r7_b;
sub r10_t r6_t r7_t;
(* str.w	lr, [r0, #256]	; 0x100                    #! EA = L0xbefff2c8; PC = 0x400aa4 *)
mov L0xbefff2c8 lr_b;
mov L0xbefff2ca lr_t;
(* str.w	r10, [r0, #288]	; 0x120                   #! EA = L0xbefff2e8; PC = 0x400aa8 *)
mov L0xbefff2e8 r10_b;
mov L0xbefff2ea r10_t;
(* vmov	r9, s6                                     #! PC = 0x400aac *)
mov r9_b s6_b;
mov r9_t s6_t;
mov r9 s6;
mov zeta_r9 zeta_s6;
(* uadd16	lr, r8, r9                               #! PC = 0x400ab0 *)
add lr_b r8_b r9_b;
add lr_t r8_t r9_t;
(* usub16	r10, r8, r9                              #! PC = 0x400ab4 *)
sub r10_b r8_b r9_b;
sub r10_t r8_t r9_t;
(* str.w	lr, [r0, #384]	; 0x180                    #! EA = L0xbefff348; PC = 0x400ab8 *)
mov L0xbefff348 lr_b;
mov L0xbefff34a lr_t;
(* str.w	r10, [r0, #416]	; 0x1a0                   #! EA = L0xbefff368; PC = 0x400abc *)
mov L0xbefff368 r10_b;
mov L0xbefff36a r10_t;
(* vmov	r3, s0                                     #! PC = 0x400ac0 *)
mov r3_b s0_b;
mov r3_t s0_t;
mov r3 s0;
mov zeta_r3 zeta_s0;
(* uadd16	lr, r2, r3                               #! PC = 0x400ac4 *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* usub16	r10, r2, r3                              #! PC = 0x400ac8 *)
sub r10_b r2_b r3_b;
sub r10_t r2_t r3_t;
(* str.w	r10, [r0, #32]                            #! EA = L0xbefff1e8; PC = 0x400acc *)
mov L0xbefff1e8 r10_b;
mov L0xbefff1ea r10_t;
(* str.w	lr, [r0], #4                              #! EA = L0xbefff1c8; PC = 0x400ad0 *)
mov L0xbefff1c8 lr_b;
mov L0xbefff1ca lr_t;
(* vmov	lr, s24                                    #! PC = 0x400ad4 *)
mov lr_b s24_b;
mov lr_t s24_t;
mov lr s24;
mov zeta_lr zeta_s24;
(* #bne.w	0x400628 <ntt_fast+24>                   #! PC = 0x400adc *)
#bne.w	0x400628 <ntt_fast+24>                   #! 0x400adc = 0x400adc;

(*== layer 7+6+5+4 one slice condition ==*)

assert
  and [
    (-6) * 1664 <= L0xbefff1c8, L0xbefff1c8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1ca, L0xbefff1ca <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1e8, L0xbefff1e8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1ea, L0xbefff1ea <= 6 * 1664,
    (-6) * 1664 <= L0xbefff208, L0xbefff208 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff20a, L0xbefff20a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff228, L0xbefff228 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff22a, L0xbefff22a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff248, L0xbefff248 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff24a, L0xbefff24a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff268, L0xbefff268 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff26a, L0xbefff26a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff288, L0xbefff288 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff28a, L0xbefff28a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2a8, L0xbefff2a8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2aa, L0xbefff2aa <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2c8, L0xbefff2c8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2ca, L0xbefff2ca <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2e8, L0xbefff2e8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2ea, L0xbefff2ea <= 6 * 1664,
    (-6) * 1664 <= L0xbefff308, L0xbefff308 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff30a, L0xbefff30a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff328, L0xbefff328 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff32a, L0xbefff32a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff348, L0xbefff348 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff34a, L0xbefff34a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff368, L0xbefff368 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff36a, L0xbefff36a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff388, L0xbefff388 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff38a, L0xbefff38a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3a8, L0xbefff3a8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3aa, L0xbefff3aa <= 6 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-6)@16 * 1664@16 <=s L0xbefff1c8, L0xbefff1c8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1ca, L0xbefff1ca <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1e8, L0xbefff1e8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1ea, L0xbefff1ea <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff208, L0xbefff208 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff20a, L0xbefff20a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff228, L0xbefff228 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff22a, L0xbefff22a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff248, L0xbefff248 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff24a, L0xbefff24a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff268, L0xbefff268 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff26a, L0xbefff26a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff288, L0xbefff288 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff28a, L0xbefff28a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2a8, L0xbefff2a8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2aa, L0xbefff2aa <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2c8, L0xbefff2c8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2ca, L0xbefff2ca <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2e8, L0xbefff2e8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2ea, L0xbefff2ea <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff308, L0xbefff308 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff30a, L0xbefff30a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff328, L0xbefff328 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff32a, L0xbefff32a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff348, L0xbefff348 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff34a, L0xbefff34a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff368, L0xbefff368 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff36a, L0xbefff36a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff388, L0xbefff388 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff38a, L0xbefff38a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3a8, L0xbefff3a8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3aa, L0xbefff3aa <=s 6@16 * 1664@16
  ]
;

assume
  and [
    (-6) * 1664 <= L0xbefff1c8, L0xbefff1c8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1ca, L0xbefff1ca <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1e8, L0xbefff1e8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1ea, L0xbefff1ea <= 6 * 1664,
    (-6) * 1664 <= L0xbefff208, L0xbefff208 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff20a, L0xbefff20a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff228, L0xbefff228 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff22a, L0xbefff22a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff248, L0xbefff248 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff24a, L0xbefff24a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff268, L0xbefff268 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff26a, L0xbefff26a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff288, L0xbefff288 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff28a, L0xbefff28a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2a8, L0xbefff2a8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2aa, L0xbefff2aa <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2c8, L0xbefff2c8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2ca, L0xbefff2ca <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2e8, L0xbefff2e8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2ea, L0xbefff2ea <= 6 * 1664,
    (-6) * 1664 <= L0xbefff308, L0xbefff308 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff30a, L0xbefff30a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff328, L0xbefff328 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff32a, L0xbefff32a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff348, L0xbefff348 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff34a, L0xbefff34a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff368, L0xbefff368 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff36a, L0xbefff36a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff388, L0xbefff388 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff38a, L0xbefff38a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3a8, L0xbefff3a8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3aa, L0xbefff3aa <= 6 * 1664
  ]
  &&
  and [
    (-6)@16 * 1664@16 <=s L0xbefff1c8, L0xbefff1c8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1ca, L0xbefff1ca <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1e8, L0xbefff1e8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1ea, L0xbefff1ea <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff208, L0xbefff208 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff20a, L0xbefff20a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff228, L0xbefff228 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff22a, L0xbefff22a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff248, L0xbefff248 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff24a, L0xbefff24a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff268, L0xbefff268 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff26a, L0xbefff26a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff288, L0xbefff288 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff28a, L0xbefff28a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2a8, L0xbefff2a8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2aa, L0xbefff2aa <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2c8, L0xbefff2c8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2ca, L0xbefff2ca <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2e8, L0xbefff2e8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2ea, L0xbefff2ea <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff308, L0xbefff308 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff30a, L0xbefff30a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff328, L0xbefff328 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff32a, L0xbefff32a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff348, L0xbefff348 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff34a, L0xbefff34a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff368, L0xbefff368 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff36a, L0xbefff36a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff388, L0xbefff388 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff38a, L0xbefff38a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3a8, L0xbefff3a8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3aa, L0xbefff3aa <=s 6@16 * 1664@16
  ]
;

(* vmov	s23, r0                                    #! PC = 0x400628 *)
mov s23_b r0_b;
mov s23_t r0_t;
mov s23 r0;
mov zeta_s23 zeta_r0;
(* ldr.w	r2, [r0, #32]                             #! EA = L0xbefff1ec; Value = 0x0002a02c; PC = 0x40062c *)
mov r2_b L0xbefff1ec;
mov r2_t L0xbefff1ee;
(* ldr.w	r3, [r0, #96]	; 0x60                      #! EA = L0xbefff22c; Value = 0xb6fff8e8; PC = 0x400630 *)
mov r3_b L0xbefff22c;
mov r3_t L0xbefff22e;
(* ldr.w	r4, [r0, #160]	; 0xa0                     #! EA = L0xbefff26c; Value = 0xb6ec9408; PC = 0x400634 *)
mov r4_b L0xbefff26c;
mov r4_t L0xbefff26e;
(* ldr.w	r5, [r0, #224]	; 0xe0                     #! EA = L0xbefff2ac; Value = 0xb6fff8e8; PC = 0x400638 *)
mov r5_b L0xbefff2ac;
mov r5_t L0xbefff2ae;
(* ldr.w	r6, [r0, #288]	; 0x120                    #! EA = L0xbefff2ec; Value = 0x00000001; PC = 0x40063c *)
mov r6_b L0xbefff2ec;
mov r6_t L0xbefff2ee;
(* ldr.w	r7, [r0, #352]	; 0x160                    #! EA = L0xbefff32c; Value = 0x00000000; PC = 0x400640 *)
mov r7_b L0xbefff32c;
mov r7_t L0xbefff32e;
(* ldr.w	r8, [r0, #416]	; 0x1a0                    #! EA = L0xbefff36c; Value = 0x00000000; PC = 0x400644 *)
mov r8_b L0xbefff36c;
mov r8_t L0xbefff36e;
(* ldr.w	r9, [r0, #480]	; 0x1e0                    #! EA = L0xbefff3ac; Value = 0x00401541; PC = 0x400648 *)
mov r9_b L0xbefff3ac;
mov r9_t L0xbefff3ae;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x40064c *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* vmov	r10, s8                                    #! PC = 0x400650 *)
mov r10_b s8_b;
mov r10_t s8_t;
mov r10 s8;
mov zeta_r10 zeta_s8;
(* smulwb	lr, r10, r6                              #! PC = 0x400654 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400658 *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40065c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400660 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x400664 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x400668 *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x40066c *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x400670 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400674 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400678 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x40067c *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400680 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x400684 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400688 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x40068c *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400690 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400694 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400698 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x40069c *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x4006a0 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x4006a4 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x4006a8 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x4006ac *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4006b0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4006b4 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4006b8 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x4006bc *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x4006c0 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* vmov	r10, s9                                    #! PC = 0x4006c4 *)
mov r10_b s9_b;
mov r10_t s9_t;
mov r10 s9;
mov zeta_r10 zeta_s9;
(* vmov	r11, s10                                   #! PC = 0x4006c8 *)
mov r11_b s10_b;
mov r11_t s10_t;
mov r11 s10;
mov zeta_r11 zeta_s10;
(* smulwb	lr, r10, r4                              #! PC = 0x4006cc *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x4006d0 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4006d4 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x4006d8 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x4006dc *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x4006e0 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4006e4 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x4006e8 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x4006ec *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4006f0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x4006f4 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x4006f8 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x4006fc *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400700 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400704 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x400708 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40070c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400710 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400714 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x400718 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x40071c *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400720 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400724 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400728 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x40072c *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400730 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x400734 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400738 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* vmov	r10, s11                                   #! PC = 0x40073c *)
mov r10_b s11_b;
mov r10_t s11_t;
mov r10 s11;
mov zeta_r10 zeta_s11;
(* vmov	r11, s12                                   #! PC = 0x400740 *)
mov r11_b s12_b;
mov r11_t s12_t;
mov r11 s12;
mov zeta_r11 zeta_s12;
(* smulwb	lr, r10, r3                              #! PC = 0x400744 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x400748 *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40074c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400750 *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x400754 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x400758 *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x40075c *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x400760 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x400764 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400768 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x40076c *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400770 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x400774 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400778 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* vmov	r10, s13                                   #! PC = 0x40077c *)
mov r10_b s13_b;
mov r10_t s13_t;
mov r10 s13;
mov zeta_r10 zeta_s13;
(* vmov	r11, s14                                   #! PC = 0x400780 *)
mov r11_b s14_b;
mov r11_t s14_t;
mov r11 s14;
mov zeta_r11 zeta_s14;
(* smulwb	lr, r10, r7                              #! PC = 0x400784 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400788 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40078c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400790 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400794 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400798 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x40079c *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4007a0 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x4007a4 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007a8 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4007ac *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4007b0 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x4007b4 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x4007b8 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

(*== 8-NTT on a1, a3, ..., a15 ==*)
(*== _3_layer_double_CT_16_plant_fp END ==*)

assert
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

assume
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

(* vmov	r10, s15                                   #! PC = 0x4007bc *)
mov r10_b s15_b;
mov r10_t s15_t;
mov r10 s15;
mov zeta_r10 zeta_s15;
(* vmov	r11, s16                                   #! PC = 0x4007c0 *)
mov r11_b s16_b;
mov r11_t s16_t;
mov r11 s16;
mov zeta_r11 zeta_s16;
(* smulwb	lr, r10, r2                              #! PC = 0x4007c4 *)
cast r2_lsb@sint32 r2_b;
mull tmp_t tmp_b r10 r2_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r2_b;
assert A_lb = r2_b && true;
assume A_lb = r2_b && true;
cast A_lt@sint32 r2_t;
assert A_lt = r2_t && true;
assume A_lt = r2_t && true;
mov zeta zeta_r10;
(* smulwt	r2, r10, r2                              #! PC = 0x4007c8 *)
cast r2_lst@sint32 r2_t;
mull tmp_t tmp_b r10 r2_lst;
spl dontcare r2_t tmp_t 16;
spl r2_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007cc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r2, r2, r12, r0                          #! PC = 0x4007d0 *)
cast r2_sb@sint16 r2_b;
mull tmp_t tmp_b r2_sb r12_t;
uadds carry r2_b tmp_b r0_b;
adc r2_t tmp_t r0_t carry;
(* pkhtb	r2, r2, lr, asr #16                       #! PC = 0x4007d4 *)
mov tmp_b lr_t;
mov tmp_t r2_t;
mov r2_b tmp_b;
mov r2_t tmp_t;
cast C_lb@sint32 r2_b;
assert C_lb = r2_b && true;
assume C_lb = r2_b && true;
cast C_lt@sint32 r2_t;
assert C_lt = r2_t && true;
assume C_lt = r2_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r3                              #! PC = 0x4007d8 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r11 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r11;
(* smulwt	r3, r11, r3                              #! PC = 0x4007dc *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r11 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007e0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4007e4 *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x4007e8 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov r3_b tmp_b;
mov r3_t tmp_t;
cast C_lb@sint32 r3_b;
assert C_lb = r3_b && true;
assume C_lb = r3_b && true;
cast C_lt@sint32 r3_t;
assert C_lt = r3_t && true;
assume C_lt = r3_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	r10, s17                                   #! PC = 0x4007ec *)
mov r10_b s17_b;
mov r10_t s17_t;
mov r10 s17;
mov zeta_r10 zeta_s17;
(* vmov	r11, s18                                   #! PC = 0x4007f0 *)
mov r11_b s18_b;
mov r11_t s18_t;
mov r11 s18;
mov zeta_r11 zeta_s18;
(* smulwb	lr, r10, r4                              #! PC = 0x4007f4 *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x4007f8 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007fc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400800 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x400804 *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov r4_b tmp_b;
mov r4_t tmp_t;
cast C_lb@sint32 r4_b;
assert C_lb = r4_b && true;
assume C_lb = r4_b && true;
cast C_lt@sint32 r4_t;
assert C_lt = r4_t && true;
assume C_lt = r4_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r5                              #! PC = 0x400808 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x40080c *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400810 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400814 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x400818 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov r5_b tmp_b;
mov r5_t tmp_t;
cast C_lb@sint32 r5_b;
assert C_lb = r5_b && true;
assume C_lb = r5_b && true;
cast C_lt@sint32 r5_t;
assert C_lt = r5_t && true;
assume C_lt = r5_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	r10, s19                                   #! PC = 0x40081c *)
mov r10_b s19_b;
mov r10_t s19_t;
mov r10 s19;
mov zeta_r10 zeta_s19;
(* vmov	r11, s20                                   #! PC = 0x400820 *)
mov r11_b s20_b;
mov r11_t s20_t;
mov r11 s20;
mov zeta_r11 zeta_s20;
(* smulwb	lr, r10, r6                              #! PC = 0x400824 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400828 *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40082c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400830 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x400834 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov r6_b tmp_b;
mov r6_t tmp_t;
cast C_lb@sint32 r6_b;
assert C_lb = r6_b && true;
assume C_lb = r6_b && true;
cast C_lt@sint32 r6_t;
assert C_lt = r6_t && true;
assume C_lt = r6_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r7                              #! PC = 0x400838 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r11 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r11;
(* smulwt	r7, r11, r7                              #! PC = 0x40083c *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r11 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400840 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400844 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x400848 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov r7_b tmp_b;
mov r7_t tmp_t;
cast C_lb@sint32 r7_b;
assert C_lb = r7_b && true;
assume C_lb = r7_b && true;
cast C_lt@sint32 r7_t;
assert C_lt = r7_t && true;
assume C_lt = r7_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	r10, s21                                   #! PC = 0x40084c *)
mov r10_b s21_b;
mov r10_t s21_t;
mov r10 s21;
mov zeta_r10 zeta_s21;
(* vmov	r11, s22                                   #! PC = 0x400850 *)
mov r11_b s22_b;
mov r11_t s22_t;
mov r11 s22;
mov zeta_r11 zeta_s22;
(* smulwb	lr, r10, r8                              #! PC = 0x400854 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400858 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40085c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400860 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x400864 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov r8_b tmp_b;
mov r8_t tmp_t;
cast C_lb@sint32 r8_b;
assert C_lb = r8_b && true;
assume C_lb = r8_b && true;
cast C_lt@sint32 r8_t;
assert C_lt = r8_t && true;
assume C_lt = r8_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r9                              #! PC = 0x400868 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x40086c *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400870 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400874 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x400878 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov r9_b tmp_b;
mov r9_t tmp_t;
cast C_lb@sint32 r9_b;
assert C_lb = r9_b && true;
assume C_lb = r9_b && true;
cast C_lt@sint32 r9_t;
assert C_lt = r9_t && true;
assume C_lt = r9_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	s0, r2                                     #! PC = 0x40087c *)
mov s0_b r2_b;
mov s0_t r2_t;
mov s0 r2;
mov zeta_s0 zeta_r2;
(* vmov	s1, r3                                     #! PC = 0x400880 *)
mov s1_b r3_b;
mov s1_t r3_t;
mov s1 r3;
mov zeta_s1 zeta_r3;
(* vmov	s2, r4                                     #! PC = 0x400884 *)
mov s2_b r4_b;
mov s2_t r4_t;
mov s2 r4;
mov zeta_s2 zeta_r4;
(* vmov	s3, r5                                     #! PC = 0x400888 *)
mov s3_b r5_b;
mov s3_t r5_t;
mov s3 r5;
mov zeta_s3 zeta_r5;
(* vmov	s4, r6                                     #! PC = 0x40088c *)
mov s4_b r6_b;
mov s4_t r6_t;
mov s4 r6;
mov zeta_s4 zeta_r6;
(* vmov	s5, r7                                     #! PC = 0x400890 *)
mov s5_b r7_b;
mov s5_t r7_t;
mov s5 r7;
mov zeta_s5 zeta_r7;
(* vmov	s6, r8                                     #! PC = 0x400894 *)
mov s6_b r8_b;
mov s6_t r8_t;
mov s6 r8;
mov zeta_s6 zeta_r8;
(* vmov	s7, r9                                     #! PC = 0x400898 *)
mov s7_b r9_b;
mov s7_t r9_t;
mov s7 r9;
mov zeta_s7 zeta_r9;

(*== coeffs multiplied by twiddles END ==*)

assert
  and [
    (-1) * 1664 <= s0_b, s0_b <= 1 * 1664,
    (-1) * 1664 <= s0_t, s0_t <= 1 * 1664,
    (-1) * 1664 <= s1_b, s1_b <= 1 * 1664,
    (-1) * 1664 <= s1_t, s1_t <= 1 * 1664,
    (-1) * 1664 <= s2_b, s2_b <= 1 * 1664,
    (-1) * 1664 <= s2_t, s2_t <= 1 * 1664,
    (-1) * 1664 <= s3_b, s3_b <= 1 * 1664,
    (-1) * 1664 <= s3_t, s3_t <= 1 * 1664,
    (-1) * 1664 <= s4_b, s4_b <= 1 * 1664,
    (-1) * 1664 <= s4_t, s4_t <= 1 * 1664,
    (-1) * 1664 <= s5_b, s5_b <= 1 * 1664,
    (-1) * 1664 <= s5_t, s5_t <= 1 * 1664,
    (-1) * 1664 <= s6_b, s6_b <= 1 * 1664,
    (-1) * 1664 <= s6_t, s6_t <= 1 * 1664,
    (-1) * 1664 <= s7_b, s7_b <= 1 * 1664,
    (-1) * 1664 <= s7_t, s7_t <= 1 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-1)@16 * 1664@16 <=s s0_b, s0_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s0_t, s0_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_b, s1_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_t, s1_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_b, s2_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_t, s2_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_b, s3_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_t, s3_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_b, s4_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_t, s4_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_b, s5_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_t, s5_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_b, s6_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_t, s6_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_b, s7_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_t, s7_t <=s 1@16 * 1664@16
  ];

assume
  and [
    (-1) * 1664 <= s0_b, s0_b <= 1 * 1664,
    (-1) * 1664 <= s0_t, s0_t <= 1 * 1664,
    (-1) * 1664 <= s1_b, s1_b <= 1 * 1664,
    (-1) * 1664 <= s1_t, s1_t <= 1 * 1664,
    (-1) * 1664 <= s2_b, s2_b <= 1 * 1664,
    (-1) * 1664 <= s2_t, s2_t <= 1 * 1664,
    (-1) * 1664 <= s3_b, s3_b <= 1 * 1664,
    (-1) * 1664 <= s3_t, s3_t <= 1 * 1664,
    (-1) * 1664 <= s4_b, s4_b <= 1 * 1664,
    (-1) * 1664 <= s4_t, s4_t <= 1 * 1664,
    (-1) * 1664 <= s5_b, s5_b <= 1 * 1664,
    (-1) * 1664 <= s5_t, s5_t <= 1 * 1664,
    (-1) * 1664 <= s6_b, s6_b <= 1 * 1664,
    (-1) * 1664 <= s6_t, s6_t <= 1 * 1664,
    (-1) * 1664 <= s7_b, s7_b <= 1 * 1664,
    (-1) * 1664 <= s7_t, s7_t <= 1 * 1664
  ]
  &&
  and [
    (-1)@16 * 1664@16 <=s s0_b, s0_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s0_t, s0_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_b, s1_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_t, s1_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_b, s2_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_t, s2_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_b, s3_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_t, s3_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_b, s4_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_t, s4_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_b, s5_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_t, s5_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_b, s6_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_t, s6_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_b, s7_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_t, s7_t <=s 1@16 * 1664@16
  ];

(* vmov	r0, s23                                    #! PC = 0x40089c *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff1cc; Value = 0x0000000d; PC = 0x4008a0 *)
mov r2_b L0xbefff1cc;
mov r2_t L0xbefff1ce;
(* ldr.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff20c; Value = 0x00000001; PC = 0x4008a4 *)
mov r3_b L0xbefff20c;
mov r3_t L0xbefff20e;
(* ldr.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff24c; Value = 0xb6fd5534; PC = 0x4008a8 *)
mov r4_b L0xbefff24c;
mov r4_t L0xbefff24e;
(* ldr.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff28c; Value = 0x00000000; PC = 0x4008ac *)
mov r5_b L0xbefff28c;
mov r5_t L0xbefff28e;
(* ldr.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2cc; Value = 0x00000000; PC = 0x4008b0 *)
mov r6_b L0xbefff2cc;
mov r6_t L0xbefff2ce;
(* ldr.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff30c; Value = 0x00000000; PC = 0x4008b4 *)
mov r7_b L0xbefff30c;
mov r7_t L0xbefff30e;
(* ldr.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff34c; Value = 0x00000000; PC = 0x4008b8 *)
mov r8_b L0xbefff34c;
mov r8_t L0xbefff34e;
(* ldr.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff38c; Value = 0x00000000; PC = 0x4008bc *)
mov r9_b L0xbefff38c;
mov r9_t L0xbefff38e;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x4008c0 *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* vmov	r10, s8                                    #! PC = 0x4008c4 *)
mov r10_b s8_b;
mov r10_t s8_t;
mov r10 s8;
mov zeta_r10 zeta_s8;
(* smulwb	lr, r10, r6                              #! PC = 0x4008c8 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x4008cc *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4008d0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4008d4 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x4008d8 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x4008dc *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4008e0 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x4008e4 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x4008e8 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4008ec *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4008f0 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x4008f4 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x4008f8 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x4008fc *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x400900 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400904 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400908 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x40090c *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400910 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x400914 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400918 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x40091c *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x400920 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400924 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400928 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x40092c *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x400930 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400934 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* vmov	r10, s9                                    #! PC = 0x400938 *)
mov r10_b s9_b;
mov r10_t s9_t;
mov r10 s9;
mov zeta_r10 zeta_s9;
(* vmov	r11, s10                                   #! PC = 0x40093c *)
mov r11_b s10_b;
mov r11_t s10_t;
mov r11 s10;
mov zeta_r11 zeta_s10;
(* smulwb	lr, r10, r4                              #! PC = 0x400940 *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x400944 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400948 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x40094c *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400950 *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x400954 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400958 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x40095c *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x400960 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400964 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400968 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x40096c *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x400970 *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400974 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400978 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x40097c *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400980 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400984 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400988 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x40098c *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400990 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400994 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400998 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40099c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4009a0 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4009a4 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x4009a8 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x4009ac *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* vmov	r10, s11                                   #! PC = 0x4009b0 *)
mov r10_b s11_b;
mov r10_t s11_t;
mov r10 s11;
mov zeta_r10 zeta_s11;
(* vmov	r11, s12                                   #! PC = 0x4009b4 *)
mov r11_b s12_b;
mov r11_t s12_t;
mov r11 s12;
mov zeta_r11 zeta_s12;
(* smulwb	lr, r10, r3                              #! PC = 0x4009b8 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x4009bc *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4009c0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4009c4 *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x4009c8 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x4009cc *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4009d0 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x4009d4 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x4009d8 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4009dc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x4009e0 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x4009e4 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x4009e8 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x4009ec *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* vmov	r10, s13                                   #! PC = 0x4009f0 *)
mov r10_b s13_b;
mov r10_t s13_t;
mov r10 s13;
mov zeta_r10 zeta_s13;
(* vmov	r11, s14                                   #! PC = 0x4009f4 *)
mov r11_b s14_b;
mov r11_t s14_t;
mov r11 s14;
mov zeta_r11 zeta_s14;
(* smulwb	lr, r10, r7                              #! PC = 0x4009f8 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x4009fc *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400a00 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400a04 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400a08 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400a0c *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400a10 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400a14 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400a18 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400a1c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400a20 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400a24 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x400a28 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x400a2c *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

(*== 8-NTT on a0, a2, ..., a14 ==*)
(*== _3_layer_double_CT_16_plant_fp END ==*)

assert
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

assume
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

(* vmov	r0, s23                                    #! PC = 0x400a30 *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* vmov	r10, s1                                    #! PC = 0x400a34 *)
mov r10_b s1_b;
mov r10_t s1_t;
mov r10 s1;
mov zeta_r10 zeta_s1;
(* uadd16	lr, r3, r10                              #! PC = 0x400a38 *)
add lr_b r3_b r10_b;
add lr_t r3_t r10_t;
(* usub16	r3, r3, r10                              #! PC = 0x400a3c *)
sub r3_b r3_b r10_b;
sub r3_t r3_t r10_t;
(* str.w	lr, [r0, #64]	; 0x40                      #! EA = L0xbefff20c; PC = 0x400a40 *)
mov L0xbefff20c lr_b;
mov L0xbefff20e lr_t;
(* str.w	r3, [r0, #96]	; 0x60                      #! EA = L0xbefff22c; PC = 0x400a44 *)
mov L0xbefff22c r3_b;
mov L0xbefff22e r3_t;
(* vmov	r10, s3                                    #! PC = 0x400a48 *)
mov r10_b s3_b;
mov r10_t s3_t;
mov r10 s3;
mov zeta_r10 zeta_s3;
(* uadd16	lr, r5, r10                              #! PC = 0x400a4c *)
add lr_b r5_b r10_b;
add lr_t r5_t r10_t;
(* usub16	r5, r5, r10                              #! PC = 0x400a50 *)
sub r5_b r5_b r10_b;
sub r5_t r5_t r10_t;
(* str.w	lr, [r0, #192]	; 0xc0                     #! EA = L0xbefff28c; PC = 0x400a54 *)
mov L0xbefff28c lr_b;
mov L0xbefff28e lr_t;
(* str.w	r5, [r0, #224]	; 0xe0                     #! EA = L0xbefff2ac; PC = 0x400a58 *)
mov L0xbefff2ac r5_b;
mov L0xbefff2ae r5_t;
(* vmov	r10, s5                                    #! PC = 0x400a5c *)
mov r10_b s5_b;
mov r10_t s5_t;
mov r10 s5;
mov zeta_r10 zeta_s5;
(* uadd16	lr, r7, r10                              #! PC = 0x400a60 *)
add lr_b r7_b r10_b;
add lr_t r7_t r10_t;
(* usub16	r7, r7, r10                              #! PC = 0x400a64 *)
sub r7_b r7_b r10_b;
sub r7_t r7_t r10_t;
(* str.w	lr, [r0, #320]	; 0x140                    #! EA = L0xbefff30c; PC = 0x400a68 *)
mov L0xbefff30c lr_b;
mov L0xbefff30e lr_t;
(* str.w	r7, [r0, #352]	; 0x160                    #! EA = L0xbefff32c; PC = 0x400a6c *)
mov L0xbefff32c r7_b;
mov L0xbefff32e r7_t;
(* vmov	r10, s7                                    #! PC = 0x400a70 *)
mov r10_b s7_b;
mov r10_t s7_t;
mov r10 s7;
mov zeta_r10 zeta_s7;
(* uadd16	lr, r9, r10                              #! PC = 0x400a74 *)
add lr_b r9_b r10_b;
add lr_t r9_t r10_t;
(* usub16	r9, r9, r10                              #! PC = 0x400a78 *)
sub r9_b r9_b r10_b;
sub r9_t r9_t r10_t;
(* str.w	lr, [r0, #448]	; 0x1c0                    #! EA = L0xbefff38c; PC = 0x400a7c *)
mov L0xbefff38c lr_b;
mov L0xbefff38e lr_t;
(* str.w	r9, [r0, #480]	; 0x1e0                    #! EA = L0xbefff3ac; PC = 0x400a80 *)
mov L0xbefff3ac r9_b;
mov L0xbefff3ae r9_t;
(* vmov	r5, s2                                     #! PC = 0x400a84 *)
mov r5_b s2_b;
mov r5_t s2_t;
mov r5 s2;
mov zeta_r5 zeta_s2;
(* uadd16	lr, r4, r5                               #! PC = 0x400a88 *)
add lr_b r4_b r5_b;
add lr_t r4_t r5_t;
(* usub16	r10, r4, r5                              #! PC = 0x400a8c *)
sub r10_b r4_b r5_b;
sub r10_t r4_t r5_t;
(* str.w	lr, [r0, #128]	; 0x80                     #! EA = L0xbefff24c; PC = 0x400a90 *)
mov L0xbefff24c lr_b;
mov L0xbefff24e lr_t;
(* str.w	r10, [r0, #160]	; 0xa0                    #! EA = L0xbefff26c; PC = 0x400a94 *)
mov L0xbefff26c r10_b;
mov L0xbefff26e r10_t;
(* vmov	r7, s4                                     #! PC = 0x400a98 *)
mov r7_b s4_b;
mov r7_t s4_t;
mov r7 s4;
mov zeta_r7 zeta_s4;
(* uadd16	lr, r6, r7                               #! PC = 0x400a9c *)
add lr_b r6_b r7_b;
add lr_t r6_t r7_t;
(* usub16	r10, r6, r7                              #! PC = 0x400aa0 *)
sub r10_b r6_b r7_b;
sub r10_t r6_t r7_t;
(* str.w	lr, [r0, #256]	; 0x100                    #! EA = L0xbefff2cc; PC = 0x400aa4 *)
mov L0xbefff2cc lr_b;
mov L0xbefff2ce lr_t;
(* str.w	r10, [r0, #288]	; 0x120                   #! EA = L0xbefff2ec; PC = 0x400aa8 *)
mov L0xbefff2ec r10_b;
mov L0xbefff2ee r10_t;
(* vmov	r9, s6                                     #! PC = 0x400aac *)
mov r9_b s6_b;
mov r9_t s6_t;
mov r9 s6;
mov zeta_r9 zeta_s6;
(* uadd16	lr, r8, r9                               #! PC = 0x400ab0 *)
add lr_b r8_b r9_b;
add lr_t r8_t r9_t;
(* usub16	r10, r8, r9                              #! PC = 0x400ab4 *)
sub r10_b r8_b r9_b;
sub r10_t r8_t r9_t;
(* str.w	lr, [r0, #384]	; 0x180                    #! EA = L0xbefff34c; PC = 0x400ab8 *)
mov L0xbefff34c lr_b;
mov L0xbefff34e lr_t;
(* str.w	r10, [r0, #416]	; 0x1a0                   #! EA = L0xbefff36c; PC = 0x400abc *)
mov L0xbefff36c r10_b;
mov L0xbefff36e r10_t;
(* vmov	r3, s0                                     #! PC = 0x400ac0 *)
mov r3_b s0_b;
mov r3_t s0_t;
mov r3 s0;
mov zeta_r3 zeta_s0;
(* uadd16	lr, r2, r3                               #! PC = 0x400ac4 *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* usub16	r10, r2, r3                              #! PC = 0x400ac8 *)
sub r10_b r2_b r3_b;
sub r10_t r2_t r3_t;
(* str.w	r10, [r0, #32]                            #! EA = L0xbefff1ec; PC = 0x400acc *)
mov L0xbefff1ec r10_b;
mov L0xbefff1ee r10_t;
(* str.w	lr, [r0], #4                              #! EA = L0xbefff1cc; PC = 0x400ad0 *)
mov L0xbefff1cc lr_b;
mov L0xbefff1ce lr_t;
(* vmov	lr, s24                                    #! PC = 0x400ad4 *)
mov lr_b s24_b;
mov lr_t s24_t;
mov lr s24;
mov zeta_lr zeta_s24;
(* #bne.w	0x400628 <ntt_fast+24>                   #! PC = 0x400adc *)
#bne.w	0x400628 <ntt_fast+24>                   #! 0x400adc = 0x400adc;

(*== layer 7+6+5+4 one slice condition ==*)

assert
  and [
    (-6) * 1664 <= L0xbefff1cc, L0xbefff1cc <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1ce, L0xbefff1ce <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1ec, L0xbefff1ec <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1ee, L0xbefff1ee <= 6 * 1664,
    (-6) * 1664 <= L0xbefff20c, L0xbefff20c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff20e, L0xbefff20e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff22c, L0xbefff22c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff22e, L0xbefff22e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff24c, L0xbefff24c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff24e, L0xbefff24e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff26c, L0xbefff26c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff26e, L0xbefff26e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff28c, L0xbefff28c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff28e, L0xbefff28e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2ac, L0xbefff2ac <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2ae, L0xbefff2ae <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2cc, L0xbefff2cc <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2ce, L0xbefff2ce <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2ec, L0xbefff2ec <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2ee, L0xbefff2ee <= 6 * 1664,
    (-6) * 1664 <= L0xbefff30c, L0xbefff30c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff30e, L0xbefff30e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff32c, L0xbefff32c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff32e, L0xbefff32e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff34c, L0xbefff34c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff34e, L0xbefff34e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff36c, L0xbefff36c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff36e, L0xbefff36e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff38c, L0xbefff38c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff38e, L0xbefff38e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3ac, L0xbefff3ac <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3ae, L0xbefff3ae <= 6 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-6)@16 * 1664@16 <=s L0xbefff1cc, L0xbefff1cc <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1ce, L0xbefff1ce <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1ec, L0xbefff1ec <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1ee, L0xbefff1ee <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff20c, L0xbefff20c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff20e, L0xbefff20e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff22c, L0xbefff22c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff22e, L0xbefff22e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff24c, L0xbefff24c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff24e, L0xbefff24e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff26c, L0xbefff26c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff26e, L0xbefff26e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff28c, L0xbefff28c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff28e, L0xbefff28e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2ac, L0xbefff2ac <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2ae, L0xbefff2ae <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2cc, L0xbefff2cc <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2ce, L0xbefff2ce <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2ec, L0xbefff2ec <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2ee, L0xbefff2ee <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff30c, L0xbefff30c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff30e, L0xbefff30e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff32c, L0xbefff32c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff32e, L0xbefff32e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff34c, L0xbefff34c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff34e, L0xbefff34e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff36c, L0xbefff36c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff36e, L0xbefff36e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff38c, L0xbefff38c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff38e, L0xbefff38e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3ac, L0xbefff3ac <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3ae, L0xbefff3ae <=s 6@16 * 1664@16
  ]
;

assume
  and [
    (-6) * 1664 <= L0xbefff1cc, L0xbefff1cc <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1ce, L0xbefff1ce <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1ec, L0xbefff1ec <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1ee, L0xbefff1ee <= 6 * 1664,
    (-6) * 1664 <= L0xbefff20c, L0xbefff20c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff20e, L0xbefff20e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff22c, L0xbefff22c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff22e, L0xbefff22e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff24c, L0xbefff24c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff24e, L0xbefff24e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff26c, L0xbefff26c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff26e, L0xbefff26e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff28c, L0xbefff28c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff28e, L0xbefff28e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2ac, L0xbefff2ac <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2ae, L0xbefff2ae <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2cc, L0xbefff2cc <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2ce, L0xbefff2ce <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2ec, L0xbefff2ec <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2ee, L0xbefff2ee <= 6 * 1664,
    (-6) * 1664 <= L0xbefff30c, L0xbefff30c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff30e, L0xbefff30e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff32c, L0xbefff32c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff32e, L0xbefff32e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff34c, L0xbefff34c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff34e, L0xbefff34e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff36c, L0xbefff36c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff36e, L0xbefff36e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff38c, L0xbefff38c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff38e, L0xbefff38e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3ac, L0xbefff3ac <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3ae, L0xbefff3ae <= 6 * 1664
  ]
  &&
  and [
    (-6)@16 * 1664@16 <=s L0xbefff1cc, L0xbefff1cc <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1ce, L0xbefff1ce <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1ec, L0xbefff1ec <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1ee, L0xbefff1ee <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff20c, L0xbefff20c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff20e, L0xbefff20e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff22c, L0xbefff22c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff22e, L0xbefff22e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff24c, L0xbefff24c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff24e, L0xbefff24e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff26c, L0xbefff26c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff26e, L0xbefff26e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff28c, L0xbefff28c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff28e, L0xbefff28e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2ac, L0xbefff2ac <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2ae, L0xbefff2ae <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2cc, L0xbefff2cc <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2ce, L0xbefff2ce <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2ec, L0xbefff2ec <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2ee, L0xbefff2ee <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff30c, L0xbefff30c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff30e, L0xbefff30e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff32c, L0xbefff32c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff32e, L0xbefff32e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff34c, L0xbefff34c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff34e, L0xbefff34e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff36c, L0xbefff36c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff36e, L0xbefff36e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff38c, L0xbefff38c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff38e, L0xbefff38e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3ac, L0xbefff3ac <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3ae, L0xbefff3ae <=s 6@16 * 1664@16
  ]
;

(* vmov	s23, r0                                    #! PC = 0x400628 *)
mov s23_b r0_b;
mov s23_t r0_t;
mov s23 r0;
mov zeta_s23 zeta_r0;
(* ldr.w	r2, [r0, #32]                             #! EA = L0xbefff1f0; Value = 0xbefff218; PC = 0x40062c *)
mov r2_b L0xbefff1f0;
mov r2_t L0xbefff1f2;
(* ldr.w	r3, [r0, #96]	; 0x60                      #! EA = L0xbefff230; Value = 0x00000000; PC = 0x400630 *)
mov r3_b L0xbefff230;
mov r3_t L0xbefff232;
(* ldr.w	r4, [r0, #160]	; 0xa0                     #! EA = L0xbefff270; Value = 0x00000000; PC = 0x400634 *)
mov r4_b L0xbefff270;
mov r4_t L0xbefff272;
(* ldr.w	r5, [r0, #224]	; 0xe0                     #! EA = L0xbefff2b0; Value = 0xb6ffe8f8; PC = 0x400638 *)
mov r5_b L0xbefff2b0;
mov r5_t L0xbefff2b2;
(* ldr.w	r6, [r0, #288]	; 0x120                    #! EA = L0xbefff2f0; Value = 0x00000000; PC = 0x40063c *)
mov r6_b L0xbefff2f0;
mov r6_t L0xbefff2f2;
(* ldr.w	r7, [r0, #352]	; 0x160                    #! EA = L0xbefff330; Value = 0x00000001; PC = 0x400640 *)
mov r7_b L0xbefff330;
mov r7_t L0xbefff332;
(* ldr.w	r8, [r0, #416]	; 0x1a0                    #! EA = L0xbefff370; Value = 0x00000000; PC = 0x400644 *)
mov r8_b L0xbefff370;
mov r8_t L0xbefff372;
(* ldr.w	r9, [r0, #480]	; 0x1e0                    #! EA = L0xbefff3b0; Value = 0xb6fb53c4; PC = 0x400648 *)
mov r9_b L0xbefff3b0;
mov r9_t L0xbefff3b2;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x40064c *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* vmov	r10, s8                                    #! PC = 0x400650 *)
mov r10_b s8_b;
mov r10_t s8_t;
mov r10 s8;
mov zeta_r10 zeta_s8;
(* smulwb	lr, r10, r6                              #! PC = 0x400654 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400658 *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40065c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400660 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x400664 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x400668 *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x40066c *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x400670 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400674 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400678 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x40067c *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400680 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x400684 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400688 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x40068c *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400690 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400694 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400698 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x40069c *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x4006a0 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x4006a4 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x4006a8 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x4006ac *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4006b0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4006b4 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4006b8 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x4006bc *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x4006c0 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* vmov	r10, s9                                    #! PC = 0x4006c4 *)
mov r10_b s9_b;
mov r10_t s9_t;
mov r10 s9;
mov zeta_r10 zeta_s9;
(* vmov	r11, s10                                   #! PC = 0x4006c8 *)
mov r11_b s10_b;
mov r11_t s10_t;
mov r11 s10;
mov zeta_r11 zeta_s10;
(* smulwb	lr, r10, r4                              #! PC = 0x4006cc *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x4006d0 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4006d4 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x4006d8 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x4006dc *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x4006e0 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4006e4 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x4006e8 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x4006ec *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4006f0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x4006f4 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x4006f8 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x4006fc *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400700 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400704 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x400708 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40070c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400710 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400714 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x400718 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x40071c *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400720 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400724 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400728 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x40072c *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400730 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x400734 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400738 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* vmov	r10, s11                                   #! PC = 0x40073c *)
mov r10_b s11_b;
mov r10_t s11_t;
mov r10 s11;
mov zeta_r10 zeta_s11;
(* vmov	r11, s12                                   #! PC = 0x400740 *)
mov r11_b s12_b;
mov r11_t s12_t;
mov r11 s12;
mov zeta_r11 zeta_s12;
(* smulwb	lr, r10, r3                              #! PC = 0x400744 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x400748 *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40074c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400750 *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x400754 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x400758 *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x40075c *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x400760 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x400764 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400768 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x40076c *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400770 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x400774 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400778 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* vmov	r10, s13                                   #! PC = 0x40077c *)
mov r10_b s13_b;
mov r10_t s13_t;
mov r10 s13;
mov zeta_r10 zeta_s13;
(* vmov	r11, s14                                   #! PC = 0x400780 *)
mov r11_b s14_b;
mov r11_t s14_t;
mov r11 s14;
mov zeta_r11 zeta_s14;
(* smulwb	lr, r10, r7                              #! PC = 0x400784 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400788 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40078c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400790 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400794 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400798 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x40079c *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4007a0 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x4007a4 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007a8 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4007ac *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4007b0 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x4007b4 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x4007b8 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

(*== 8-NTT on a1, a3, ..., a15 ==*)
(*== _3_layer_double_CT_16_plant_fp END ==*)

assert
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

assume
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

(* vmov	r10, s15                                   #! PC = 0x4007bc *)
mov r10_b s15_b;
mov r10_t s15_t;
mov r10 s15;
mov zeta_r10 zeta_s15;
(* vmov	r11, s16                                   #! PC = 0x4007c0 *)
mov r11_b s16_b;
mov r11_t s16_t;
mov r11 s16;
mov zeta_r11 zeta_s16;
(* smulwb	lr, r10, r2                              #! PC = 0x4007c4 *)
cast r2_lsb@sint32 r2_b;
mull tmp_t tmp_b r10 r2_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r2_b;
assert A_lb = r2_b && true;
assume A_lb = r2_b && true;
cast A_lt@sint32 r2_t;
assert A_lt = r2_t && true;
assume A_lt = r2_t && true;
mov zeta zeta_r10;
(* smulwt	r2, r10, r2                              #! PC = 0x4007c8 *)
cast r2_lst@sint32 r2_t;
mull tmp_t tmp_b r10 r2_lst;
spl dontcare r2_t tmp_t 16;
spl r2_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007cc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r2, r2, r12, r0                          #! PC = 0x4007d0 *)
cast r2_sb@sint16 r2_b;
mull tmp_t tmp_b r2_sb r12_t;
uadds carry r2_b tmp_b r0_b;
adc r2_t tmp_t r0_t carry;
(* pkhtb	r2, r2, lr, asr #16                       #! PC = 0x4007d4 *)
mov tmp_b lr_t;
mov tmp_t r2_t;
mov r2_b tmp_b;
mov r2_t tmp_t;
cast C_lb@sint32 r2_b;
assert C_lb = r2_b && true;
assume C_lb = r2_b && true;
cast C_lt@sint32 r2_t;
assert C_lt = r2_t && true;
assume C_lt = r2_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r3                              #! PC = 0x4007d8 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r11 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r11;
(* smulwt	r3, r11, r3                              #! PC = 0x4007dc *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r11 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007e0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4007e4 *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x4007e8 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov r3_b tmp_b;
mov r3_t tmp_t;
cast C_lb@sint32 r3_b;
assert C_lb = r3_b && true;
assume C_lb = r3_b && true;
cast C_lt@sint32 r3_t;
assert C_lt = r3_t && true;
assume C_lt = r3_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	r10, s17                                   #! PC = 0x4007ec *)
mov r10_b s17_b;
mov r10_t s17_t;
mov r10 s17;
mov zeta_r10 zeta_s17;
(* vmov	r11, s18                                   #! PC = 0x4007f0 *)
mov r11_b s18_b;
mov r11_t s18_t;
mov r11 s18;
mov zeta_r11 zeta_s18;
(* smulwb	lr, r10, r4                              #! PC = 0x4007f4 *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x4007f8 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007fc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400800 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x400804 *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov r4_b tmp_b;
mov r4_t tmp_t;
cast C_lb@sint32 r4_b;
assert C_lb = r4_b && true;
assume C_lb = r4_b && true;
cast C_lt@sint32 r4_t;
assert C_lt = r4_t && true;
assume C_lt = r4_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r5                              #! PC = 0x400808 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x40080c *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400810 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400814 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x400818 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov r5_b tmp_b;
mov r5_t tmp_t;
cast C_lb@sint32 r5_b;
assert C_lb = r5_b && true;
assume C_lb = r5_b && true;
cast C_lt@sint32 r5_t;
assert C_lt = r5_t && true;
assume C_lt = r5_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	r10, s19                                   #! PC = 0x40081c *)
mov r10_b s19_b;
mov r10_t s19_t;
mov r10 s19;
mov zeta_r10 zeta_s19;
(* vmov	r11, s20                                   #! PC = 0x400820 *)
mov r11_b s20_b;
mov r11_t s20_t;
mov r11 s20;
mov zeta_r11 zeta_s20;
(* smulwb	lr, r10, r6                              #! PC = 0x400824 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400828 *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40082c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400830 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x400834 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov r6_b tmp_b;
mov r6_t tmp_t;
cast C_lb@sint32 r6_b;
assert C_lb = r6_b && true;
assume C_lb = r6_b && true;
cast C_lt@sint32 r6_t;
assert C_lt = r6_t && true;
assume C_lt = r6_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r7                              #! PC = 0x400838 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r11 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r11;
(* smulwt	r7, r11, r7                              #! PC = 0x40083c *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r11 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400840 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400844 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x400848 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov r7_b tmp_b;
mov r7_t tmp_t;
cast C_lb@sint32 r7_b;
assert C_lb = r7_b && true;
assume C_lb = r7_b && true;
cast C_lt@sint32 r7_t;
assert C_lt = r7_t && true;
assume C_lt = r7_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	r10, s21                                   #! PC = 0x40084c *)
mov r10_b s21_b;
mov r10_t s21_t;
mov r10 s21;
mov zeta_r10 zeta_s21;
(* vmov	r11, s22                                   #! PC = 0x400850 *)
mov r11_b s22_b;
mov r11_t s22_t;
mov r11 s22;
mov zeta_r11 zeta_s22;
(* smulwb	lr, r10, r8                              #! PC = 0x400854 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400858 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40085c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400860 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x400864 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov r8_b tmp_b;
mov r8_t tmp_t;
cast C_lb@sint32 r8_b;
assert C_lb = r8_b && true;
assume C_lb = r8_b && true;
cast C_lt@sint32 r8_t;
assert C_lt = r8_t && true;
assume C_lt = r8_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r9                              #! PC = 0x400868 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x40086c *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400870 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400874 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x400878 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov r9_b tmp_b;
mov r9_t tmp_t;
cast C_lb@sint32 r9_b;
assert C_lb = r9_b && true;
assume C_lb = r9_b && true;
cast C_lt@sint32 r9_t;
assert C_lt = r9_t && true;
assume C_lt = r9_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	s0, r2                                     #! PC = 0x40087c *)
mov s0_b r2_b;
mov s0_t r2_t;
mov s0 r2;
mov zeta_s0 zeta_r2;
(* vmov	s1, r3                                     #! PC = 0x400880 *)
mov s1_b r3_b;
mov s1_t r3_t;
mov s1 r3;
mov zeta_s1 zeta_r3;
(* vmov	s2, r4                                     #! PC = 0x400884 *)
mov s2_b r4_b;
mov s2_t r4_t;
mov s2 r4;
mov zeta_s2 zeta_r4;
(* vmov	s3, r5                                     #! PC = 0x400888 *)
mov s3_b r5_b;
mov s3_t r5_t;
mov s3 r5;
mov zeta_s3 zeta_r5;
(* vmov	s4, r6                                     #! PC = 0x40088c *)
mov s4_b r6_b;
mov s4_t r6_t;
mov s4 r6;
mov zeta_s4 zeta_r6;
(* vmov	s5, r7                                     #! PC = 0x400890 *)
mov s5_b r7_b;
mov s5_t r7_t;
mov s5 r7;
mov zeta_s5 zeta_r7;
(* vmov	s6, r8                                     #! PC = 0x400894 *)
mov s6_b r8_b;
mov s6_t r8_t;
mov s6 r8;
mov zeta_s6 zeta_r8;
(* vmov	s7, r9                                     #! PC = 0x400898 *)
mov s7_b r9_b;
mov s7_t r9_t;
mov s7 r9;
mov zeta_s7 zeta_r9;

(*== coeffs multiplied by twiddles END ==*)

assert
  and [
    (-1) * 1664 <= s0_b, s0_b <= 1 * 1664,
    (-1) * 1664 <= s0_t, s0_t <= 1 * 1664,
    (-1) * 1664 <= s1_b, s1_b <= 1 * 1664,
    (-1) * 1664 <= s1_t, s1_t <= 1 * 1664,
    (-1) * 1664 <= s2_b, s2_b <= 1 * 1664,
    (-1) * 1664 <= s2_t, s2_t <= 1 * 1664,
    (-1) * 1664 <= s3_b, s3_b <= 1 * 1664,
    (-1) * 1664 <= s3_t, s3_t <= 1 * 1664,
    (-1) * 1664 <= s4_b, s4_b <= 1 * 1664,
    (-1) * 1664 <= s4_t, s4_t <= 1 * 1664,
    (-1) * 1664 <= s5_b, s5_b <= 1 * 1664,
    (-1) * 1664 <= s5_t, s5_t <= 1 * 1664,
    (-1) * 1664 <= s6_b, s6_b <= 1 * 1664,
    (-1) * 1664 <= s6_t, s6_t <= 1 * 1664,
    (-1) * 1664 <= s7_b, s7_b <= 1 * 1664,
    (-1) * 1664 <= s7_t, s7_t <= 1 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-1)@16 * 1664@16 <=s s0_b, s0_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s0_t, s0_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_b, s1_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_t, s1_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_b, s2_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_t, s2_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_b, s3_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_t, s3_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_b, s4_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_t, s4_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_b, s5_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_t, s5_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_b, s6_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_t, s6_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_b, s7_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_t, s7_t <=s 1@16 * 1664@16
  ];

assume
  and [
    (-1) * 1664 <= s0_b, s0_b <= 1 * 1664,
    (-1) * 1664 <= s0_t, s0_t <= 1 * 1664,
    (-1) * 1664 <= s1_b, s1_b <= 1 * 1664,
    (-1) * 1664 <= s1_t, s1_t <= 1 * 1664,
    (-1) * 1664 <= s2_b, s2_b <= 1 * 1664,
    (-1) * 1664 <= s2_t, s2_t <= 1 * 1664,
    (-1) * 1664 <= s3_b, s3_b <= 1 * 1664,
    (-1) * 1664 <= s3_t, s3_t <= 1 * 1664,
    (-1) * 1664 <= s4_b, s4_b <= 1 * 1664,
    (-1) * 1664 <= s4_t, s4_t <= 1 * 1664,
    (-1) * 1664 <= s5_b, s5_b <= 1 * 1664,
    (-1) * 1664 <= s5_t, s5_t <= 1 * 1664,
    (-1) * 1664 <= s6_b, s6_b <= 1 * 1664,
    (-1) * 1664 <= s6_t, s6_t <= 1 * 1664,
    (-1) * 1664 <= s7_b, s7_b <= 1 * 1664,
    (-1) * 1664 <= s7_t, s7_t <= 1 * 1664
  ]
  &&
  and [
    (-1)@16 * 1664@16 <=s s0_b, s0_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s0_t, s0_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_b, s1_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_t, s1_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_b, s2_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_t, s2_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_b, s3_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_t, s3_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_b, s4_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_t, s4_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_b, s5_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_t, s5_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_b, s6_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_t, s6_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_b, s7_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_t, s7_t <=s 1@16 * 1664@16
  ];

(* vmov	r0, s23                                    #! PC = 0x40089c *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff1d0; Value = 0x00000000; PC = 0x4008a0 *)
mov r2_b L0xbefff1d0;
mov r2_t L0xbefff1d2;
(* ldr.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff210; Value = 0x00000009; PC = 0x4008a4 *)
mov r3_b L0xbefff210;
mov r3_t L0xbefff212;
(* ldr.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff250; Value = 0xb6fd5754; PC = 0x4008a8 *)
mov r4_b L0xbefff250;
mov r4_t L0xbefff252;
(* ldr.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff290; Value = 0xb6ffb530; PC = 0x4008ac *)
mov r5_b L0xbefff290;
mov r5_t L0xbefff292;
(* ldr.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2d0; Value = 0x00000000; PC = 0x4008b0 *)
mov r6_b L0xbefff2d0;
mov r6_t L0xbefff2d2;
(* ldr.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff310; Value = 0x00000000; PC = 0x4008b4 *)
mov r7_b L0xbefff310;
mov r7_t L0xbefff312;
(* ldr.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff350; Value = 0x00000000; PC = 0x4008b8 *)
mov r8_b L0xbefff350;
mov r8_t L0xbefff352;
(* ldr.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff390; Value = 0x0040152d; PC = 0x4008bc *)
mov r9_b L0xbefff390;
mov r9_t L0xbefff392;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x4008c0 *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* vmov	r10, s8                                    #! PC = 0x4008c4 *)
mov r10_b s8_b;
mov r10_t s8_t;
mov r10 s8;
mov zeta_r10 zeta_s8;
(* smulwb	lr, r10, r6                              #! PC = 0x4008c8 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x4008cc *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4008d0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4008d4 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x4008d8 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x4008dc *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4008e0 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x4008e4 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x4008e8 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4008ec *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4008f0 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x4008f4 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x4008f8 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x4008fc *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x400900 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400904 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400908 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x40090c *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400910 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x400914 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400918 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x40091c *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x400920 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400924 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400928 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x40092c *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x400930 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400934 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* vmov	r10, s9                                    #! PC = 0x400938 *)
mov r10_b s9_b;
mov r10_t s9_t;
mov r10 s9;
mov zeta_r10 zeta_s9;
(* vmov	r11, s10                                   #! PC = 0x40093c *)
mov r11_b s10_b;
mov r11_t s10_t;
mov r11 s10;
mov zeta_r11 zeta_s10;
(* smulwb	lr, r10, r4                              #! PC = 0x400940 *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x400944 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400948 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x40094c *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400950 *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x400954 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400958 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x40095c *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x400960 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400964 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400968 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x40096c *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x400970 *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400974 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400978 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x40097c *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400980 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400984 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400988 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x40098c *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400990 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400994 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400998 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40099c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4009a0 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4009a4 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x4009a8 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x4009ac *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* vmov	r10, s11                                   #! PC = 0x4009b0 *)
mov r10_b s11_b;
mov r10_t s11_t;
mov r10 s11;
mov zeta_r10 zeta_s11;
(* vmov	r11, s12                                   #! PC = 0x4009b4 *)
mov r11_b s12_b;
mov r11_t s12_t;
mov r11 s12;
mov zeta_r11 zeta_s12;
(* smulwb	lr, r10, r3                              #! PC = 0x4009b8 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x4009bc *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4009c0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4009c4 *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x4009c8 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x4009cc *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4009d0 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x4009d4 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x4009d8 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4009dc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x4009e0 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x4009e4 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x4009e8 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x4009ec *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* vmov	r10, s13                                   #! PC = 0x4009f0 *)
mov r10_b s13_b;
mov r10_t s13_t;
mov r10 s13;
mov zeta_r10 zeta_s13;
(* vmov	r11, s14                                   #! PC = 0x4009f4 *)
mov r11_b s14_b;
mov r11_t s14_t;
mov r11 s14;
mov zeta_r11 zeta_s14;
(* smulwb	lr, r10, r7                              #! PC = 0x4009f8 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x4009fc *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400a00 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400a04 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400a08 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400a0c *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400a10 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400a14 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400a18 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400a1c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400a20 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400a24 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x400a28 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x400a2c *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

(*== 8-NTT on a0, a2, ..., a14 ==*)
(*== _3_layer_double_CT_16_plant_fp END ==*)

assert
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

assume
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

(* vmov	r0, s23                                    #! PC = 0x400a30 *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* vmov	r10, s1                                    #! PC = 0x400a34 *)
mov r10_b s1_b;
mov r10_t s1_t;
mov r10 s1;
mov zeta_r10 zeta_s1;
(* uadd16	lr, r3, r10                              #! PC = 0x400a38 *)
add lr_b r3_b r10_b;
add lr_t r3_t r10_t;
(* usub16	r3, r3, r10                              #! PC = 0x400a3c *)
sub r3_b r3_b r10_b;
sub r3_t r3_t r10_t;
(* str.w	lr, [r0, #64]	; 0x40                      #! EA = L0xbefff210; PC = 0x400a40 *)
mov L0xbefff210 lr_b;
mov L0xbefff212 lr_t;
(* str.w	r3, [r0, #96]	; 0x60                      #! EA = L0xbefff230; PC = 0x400a44 *)
mov L0xbefff230 r3_b;
mov L0xbefff232 r3_t;
(* vmov	r10, s3                                    #! PC = 0x400a48 *)
mov r10_b s3_b;
mov r10_t s3_t;
mov r10 s3;
mov zeta_r10 zeta_s3;
(* uadd16	lr, r5, r10                              #! PC = 0x400a4c *)
add lr_b r5_b r10_b;
add lr_t r5_t r10_t;
(* usub16	r5, r5, r10                              #! PC = 0x400a50 *)
sub r5_b r5_b r10_b;
sub r5_t r5_t r10_t;
(* str.w	lr, [r0, #192]	; 0xc0                     #! EA = L0xbefff290; PC = 0x400a54 *)
mov L0xbefff290 lr_b;
mov L0xbefff292 lr_t;
(* str.w	r5, [r0, #224]	; 0xe0                     #! EA = L0xbefff2b0; PC = 0x400a58 *)
mov L0xbefff2b0 r5_b;
mov L0xbefff2b2 r5_t;
(* vmov	r10, s5                                    #! PC = 0x400a5c *)
mov r10_b s5_b;
mov r10_t s5_t;
mov r10 s5;
mov zeta_r10 zeta_s5;
(* uadd16	lr, r7, r10                              #! PC = 0x400a60 *)
add lr_b r7_b r10_b;
add lr_t r7_t r10_t;
(* usub16	r7, r7, r10                              #! PC = 0x400a64 *)
sub r7_b r7_b r10_b;
sub r7_t r7_t r10_t;
(* str.w	lr, [r0, #320]	; 0x140                    #! EA = L0xbefff310; PC = 0x400a68 *)
mov L0xbefff310 lr_b;
mov L0xbefff312 lr_t;
(* str.w	r7, [r0, #352]	; 0x160                    #! EA = L0xbefff330; PC = 0x400a6c *)
mov L0xbefff330 r7_b;
mov L0xbefff332 r7_t;
(* vmov	r10, s7                                    #! PC = 0x400a70 *)
mov r10_b s7_b;
mov r10_t s7_t;
mov r10 s7;
mov zeta_r10 zeta_s7;
(* uadd16	lr, r9, r10                              #! PC = 0x400a74 *)
add lr_b r9_b r10_b;
add lr_t r9_t r10_t;
(* usub16	r9, r9, r10                              #! PC = 0x400a78 *)
sub r9_b r9_b r10_b;
sub r9_t r9_t r10_t;
(* str.w	lr, [r0, #448]	; 0x1c0                    #! EA = L0xbefff390; PC = 0x400a7c *)
mov L0xbefff390 lr_b;
mov L0xbefff392 lr_t;
(* str.w	r9, [r0, #480]	; 0x1e0                    #! EA = L0xbefff3b0; PC = 0x400a80 *)
mov L0xbefff3b0 r9_b;
mov L0xbefff3b2 r9_t;
(* vmov	r5, s2                                     #! PC = 0x400a84 *)
mov r5_b s2_b;
mov r5_t s2_t;
mov r5 s2;
mov zeta_r5 zeta_s2;
(* uadd16	lr, r4, r5                               #! PC = 0x400a88 *)
add lr_b r4_b r5_b;
add lr_t r4_t r5_t;
(* usub16	r10, r4, r5                              #! PC = 0x400a8c *)
sub r10_b r4_b r5_b;
sub r10_t r4_t r5_t;
(* str.w	lr, [r0, #128]	; 0x80                     #! EA = L0xbefff250; PC = 0x400a90 *)
mov L0xbefff250 lr_b;
mov L0xbefff252 lr_t;
(* str.w	r10, [r0, #160]	; 0xa0                    #! EA = L0xbefff270; PC = 0x400a94 *)
mov L0xbefff270 r10_b;
mov L0xbefff272 r10_t;
(* vmov	r7, s4                                     #! PC = 0x400a98 *)
mov r7_b s4_b;
mov r7_t s4_t;
mov r7 s4;
mov zeta_r7 zeta_s4;
(* uadd16	lr, r6, r7                               #! PC = 0x400a9c *)
add lr_b r6_b r7_b;
add lr_t r6_t r7_t;
(* usub16	r10, r6, r7                              #! PC = 0x400aa0 *)
sub r10_b r6_b r7_b;
sub r10_t r6_t r7_t;
(* str.w	lr, [r0, #256]	; 0x100                    #! EA = L0xbefff2d0; PC = 0x400aa4 *)
mov L0xbefff2d0 lr_b;
mov L0xbefff2d2 lr_t;
(* str.w	r10, [r0, #288]	; 0x120                   #! EA = L0xbefff2f0; PC = 0x400aa8 *)
mov L0xbefff2f0 r10_b;
mov L0xbefff2f2 r10_t;
(* vmov	r9, s6                                     #! PC = 0x400aac *)
mov r9_b s6_b;
mov r9_t s6_t;
mov r9 s6;
mov zeta_r9 zeta_s6;
(* uadd16	lr, r8, r9                               #! PC = 0x400ab0 *)
add lr_b r8_b r9_b;
add lr_t r8_t r9_t;
(* usub16	r10, r8, r9                              #! PC = 0x400ab4 *)
sub r10_b r8_b r9_b;
sub r10_t r8_t r9_t;
(* str.w	lr, [r0, #384]	; 0x180                    #! EA = L0xbefff350; PC = 0x400ab8 *)
mov L0xbefff350 lr_b;
mov L0xbefff352 lr_t;
(* str.w	r10, [r0, #416]	; 0x1a0                   #! EA = L0xbefff370; PC = 0x400abc *)
mov L0xbefff370 r10_b;
mov L0xbefff372 r10_t;
(* vmov	r3, s0                                     #! PC = 0x400ac0 *)
mov r3_b s0_b;
mov r3_t s0_t;
mov r3 s0;
mov zeta_r3 zeta_s0;
(* uadd16	lr, r2, r3                               #! PC = 0x400ac4 *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* usub16	r10, r2, r3                              #! PC = 0x400ac8 *)
sub r10_b r2_b r3_b;
sub r10_t r2_t r3_t;
(* str.w	r10, [r0, #32]                            #! EA = L0xbefff1f0; PC = 0x400acc *)
mov L0xbefff1f0 r10_b;
mov L0xbefff1f2 r10_t;
(* str.w	lr, [r0], #4                              #! EA = L0xbefff1d0; PC = 0x400ad0 *)
mov L0xbefff1d0 lr_b;
mov L0xbefff1d2 lr_t;
(* vmov	lr, s24                                    #! PC = 0x400ad4 *)
mov lr_b s24_b;
mov lr_t s24_t;
mov lr s24;
mov zeta_lr zeta_s24;
(* #bne.w	0x400628 <ntt_fast+24>                   #! PC = 0x400adc *)
#bne.w	0x400628 <ntt_fast+24>                   #! 0x400adc = 0x400adc;

(*== layer 7+6+5+4 one slice condition ==*)

assert
  and [
    (-6) * 1664 <= L0xbefff1d0, L0xbefff1d0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1d2, L0xbefff1d2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1f0, L0xbefff1f0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1f2, L0xbefff1f2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff210, L0xbefff210 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff212, L0xbefff212 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff230, L0xbefff230 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff232, L0xbefff232 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff250, L0xbefff250 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff252, L0xbefff252 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff270, L0xbefff270 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff272, L0xbefff272 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff290, L0xbefff290 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff292, L0xbefff292 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2b0, L0xbefff2b0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2b2, L0xbefff2b2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2d0, L0xbefff2d0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2d2, L0xbefff2d2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2f0, L0xbefff2f0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2f2, L0xbefff2f2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff310, L0xbefff310 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff312, L0xbefff312 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff330, L0xbefff330 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff332, L0xbefff332 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff350, L0xbefff350 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff352, L0xbefff352 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff370, L0xbefff370 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff372, L0xbefff372 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff390, L0xbefff390 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff392, L0xbefff392 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3b0, L0xbefff3b0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3b2, L0xbefff3b2 <= 6 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-6)@16 * 1664@16 <=s L0xbefff1d0, L0xbefff1d0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1d2, L0xbefff1d2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1f0, L0xbefff1f0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1f2, L0xbefff1f2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff210, L0xbefff210 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff212, L0xbefff212 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff230, L0xbefff230 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff232, L0xbefff232 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff250, L0xbefff250 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff252, L0xbefff252 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff270, L0xbefff270 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff272, L0xbefff272 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff290, L0xbefff290 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff292, L0xbefff292 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2b0, L0xbefff2b0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2b2, L0xbefff2b2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2d0, L0xbefff2d0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2d2, L0xbefff2d2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2f0, L0xbefff2f0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2f2, L0xbefff2f2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff310, L0xbefff310 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff312, L0xbefff312 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff330, L0xbefff330 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff332, L0xbefff332 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff350, L0xbefff350 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff352, L0xbefff352 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff370, L0xbefff370 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff372, L0xbefff372 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff390, L0xbefff390 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff392, L0xbefff392 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3b0, L0xbefff3b0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3b2, L0xbefff3b2 <=s 6@16 * 1664@16
  ]
;

assume
  and [
    (-6) * 1664 <= L0xbefff1d0, L0xbefff1d0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1d2, L0xbefff1d2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1f0, L0xbefff1f0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1f2, L0xbefff1f2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff210, L0xbefff210 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff212, L0xbefff212 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff230, L0xbefff230 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff232, L0xbefff232 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff250, L0xbefff250 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff252, L0xbefff252 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff270, L0xbefff270 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff272, L0xbefff272 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff290, L0xbefff290 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff292, L0xbefff292 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2b0, L0xbefff2b0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2b2, L0xbefff2b2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2d0, L0xbefff2d0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2d2, L0xbefff2d2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2f0, L0xbefff2f0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2f2, L0xbefff2f2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff310, L0xbefff310 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff312, L0xbefff312 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff330, L0xbefff330 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff332, L0xbefff332 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff350, L0xbefff350 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff352, L0xbefff352 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff370, L0xbefff370 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff372, L0xbefff372 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff390, L0xbefff390 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff392, L0xbefff392 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3b0, L0xbefff3b0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3b2, L0xbefff3b2 <= 6 * 1664
  ]
  &&
  and [
    (-6)@16 * 1664@16 <=s L0xbefff1d0, L0xbefff1d0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1d2, L0xbefff1d2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1f0, L0xbefff1f0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1f2, L0xbefff1f2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff210, L0xbefff210 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff212, L0xbefff212 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff230, L0xbefff230 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff232, L0xbefff232 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff250, L0xbefff250 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff252, L0xbefff252 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff270, L0xbefff270 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff272, L0xbefff272 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff290, L0xbefff290 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff292, L0xbefff292 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2b0, L0xbefff2b0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2b2, L0xbefff2b2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2d0, L0xbefff2d0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2d2, L0xbefff2d2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2f0, L0xbefff2f0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2f2, L0xbefff2f2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff310, L0xbefff310 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff312, L0xbefff312 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff330, L0xbefff330 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff332, L0xbefff332 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff350, L0xbefff350 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff352, L0xbefff352 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff370, L0xbefff370 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff372, L0xbefff372 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff390, L0xbefff390 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff392, L0xbefff392 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3b0, L0xbefff3b0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3b2, L0xbefff3b2 <=s 6@16 * 1664@16
  ]
;

(* vmov	s23, r0                                    #! PC = 0x400628 *)
mov s23_b r0_b;
mov s23_t r0_t;
mov s23 r0;
mov zeta_s23 zeta_r0;
(* ldr.w	r2, [r0, #32]                             #! EA = L0xbefff1f4; Value = 0xbefff218; PC = 0x40062c *)
mov r2_b L0xbefff1f4;
mov r2_t L0xbefff1f6;
(* ldr.w	r3, [r0, #96]	; 0x60                      #! EA = L0xbefff234; Value = 0xb6fff5c8; PC = 0x400630 *)
mov r3_b L0xbefff234;
mov r3_t L0xbefff236;
(* ldr.w	r4, [r0, #160]	; 0xa0                     #! EA = L0xbefff274; Value = 0x00000000; PC = 0x400634 *)
mov r4_b L0xbefff274;
mov r4_t L0xbefff276;
(* ldr.w	r5, [r0, #224]	; 0xe0                     #! EA = L0xbefff2b4; Value = 0xbefff2d0; PC = 0x400638 *)
mov r5_b L0xbefff2b4;
mov r5_t L0xbefff2b6;
(* ldr.w	r6, [r0, #288]	; 0x120                    #! EA = L0xbefff2f4; Value = 0x00000001; PC = 0x40063c *)
mov r6_b L0xbefff2f4;
mov r6_t L0xbefff2f6;
(* ldr.w	r7, [r0, #352]	; 0x160                    #! EA = L0xbefff334; Value = 0x00000000; PC = 0x400640 *)
mov r7_b L0xbefff334;
mov r7_t L0xbefff336;
(* ldr.w	r8, [r0, #416]	; 0x1a0                    #! EA = L0xbefff374; Value = 0x00000000; PC = 0x400644 *)
mov r8_b L0xbefff374;
mov r8_t L0xbefff376;
(* ldr.w	r9, [r0, #480]	; 0x1e0                    #! EA = L0xbefff3b4; Value = 0x0040152d; PC = 0x400648 *)
mov r9_b L0xbefff3b4;
mov r9_t L0xbefff3b6;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x40064c *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* vmov	r10, s8                                    #! PC = 0x400650 *)
mov r10_b s8_b;
mov r10_t s8_t;
mov r10 s8;
mov zeta_r10 zeta_s8;
(* smulwb	lr, r10, r6                              #! PC = 0x400654 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400658 *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40065c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400660 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x400664 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x400668 *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x40066c *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x400670 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400674 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400678 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x40067c *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400680 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x400684 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400688 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x40068c *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400690 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400694 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400698 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x40069c *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x4006a0 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x4006a4 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x4006a8 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x4006ac *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4006b0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4006b4 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4006b8 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x4006bc *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x4006c0 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* vmov	r10, s9                                    #! PC = 0x4006c4 *)
mov r10_b s9_b;
mov r10_t s9_t;
mov r10 s9;
mov zeta_r10 zeta_s9;
(* vmov	r11, s10                                   #! PC = 0x4006c8 *)
mov r11_b s10_b;
mov r11_t s10_t;
mov r11 s10;
mov zeta_r11 zeta_s10;
(* smulwb	lr, r10, r4                              #! PC = 0x4006cc *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x4006d0 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4006d4 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x4006d8 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x4006dc *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x4006e0 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4006e4 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x4006e8 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x4006ec *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4006f0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x4006f4 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x4006f8 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x4006fc *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400700 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400704 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x400708 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40070c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400710 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400714 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x400718 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x40071c *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400720 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400724 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400728 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x40072c *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400730 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x400734 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400738 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* vmov	r10, s11                                   #! PC = 0x40073c *)
mov r10_b s11_b;
mov r10_t s11_t;
mov r10 s11;
mov zeta_r10 zeta_s11;
(* vmov	r11, s12                                   #! PC = 0x400740 *)
mov r11_b s12_b;
mov r11_t s12_t;
mov r11 s12;
mov zeta_r11 zeta_s12;
(* smulwb	lr, r10, r3                              #! PC = 0x400744 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x400748 *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40074c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400750 *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x400754 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x400758 *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x40075c *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x400760 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x400764 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400768 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x40076c *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400770 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x400774 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400778 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* vmov	r10, s13                                   #! PC = 0x40077c *)
mov r10_b s13_b;
mov r10_t s13_t;
mov r10 s13;
mov zeta_r10 zeta_s13;
(* vmov	r11, s14                                   #! PC = 0x400780 *)
mov r11_b s14_b;
mov r11_t s14_t;
mov r11 s14;
mov zeta_r11 zeta_s14;
(* smulwb	lr, r10, r7                              #! PC = 0x400784 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400788 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40078c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400790 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400794 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400798 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x40079c *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4007a0 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x4007a4 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007a8 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4007ac *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4007b0 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x4007b4 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x4007b8 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

(*== 8-NTT on a1, a3, ..., a15 ==*)
(*== _3_layer_double_CT_16_plant_fp END ==*)

assert
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

assume
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

(* vmov	r10, s15                                   #! PC = 0x4007bc *)
mov r10_b s15_b;
mov r10_t s15_t;
mov r10 s15;
mov zeta_r10 zeta_s15;
(* vmov	r11, s16                                   #! PC = 0x4007c0 *)
mov r11_b s16_b;
mov r11_t s16_t;
mov r11 s16;
mov zeta_r11 zeta_s16;
(* smulwb	lr, r10, r2                              #! PC = 0x4007c4 *)
cast r2_lsb@sint32 r2_b;
mull tmp_t tmp_b r10 r2_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r2_b;
assert A_lb = r2_b && true;
assume A_lb = r2_b && true;
cast A_lt@sint32 r2_t;
assert A_lt = r2_t && true;
assume A_lt = r2_t && true;
mov zeta zeta_r10;
(* smulwt	r2, r10, r2                              #! PC = 0x4007c8 *)
cast r2_lst@sint32 r2_t;
mull tmp_t tmp_b r10 r2_lst;
spl dontcare r2_t tmp_t 16;
spl r2_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007cc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r2, r2, r12, r0                          #! PC = 0x4007d0 *)
cast r2_sb@sint16 r2_b;
mull tmp_t tmp_b r2_sb r12_t;
uadds carry r2_b tmp_b r0_b;
adc r2_t tmp_t r0_t carry;
(* pkhtb	r2, r2, lr, asr #16                       #! PC = 0x4007d4 *)
mov tmp_b lr_t;
mov tmp_t r2_t;
mov r2_b tmp_b;
mov r2_t tmp_t;
cast C_lb@sint32 r2_b;
assert C_lb = r2_b && true;
assume C_lb = r2_b && true;
cast C_lt@sint32 r2_t;
assert C_lt = r2_t && true;
assume C_lt = r2_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r3                              #! PC = 0x4007d8 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r11 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r11;
(* smulwt	r3, r11, r3                              #! PC = 0x4007dc *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r11 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007e0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4007e4 *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x4007e8 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov r3_b tmp_b;
mov r3_t tmp_t;
cast C_lb@sint32 r3_b;
assert C_lb = r3_b && true;
assume C_lb = r3_b && true;
cast C_lt@sint32 r3_t;
assert C_lt = r3_t && true;
assume C_lt = r3_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	r10, s17                                   #! PC = 0x4007ec *)
mov r10_b s17_b;
mov r10_t s17_t;
mov r10 s17;
mov zeta_r10 zeta_s17;
(* vmov	r11, s18                                   #! PC = 0x4007f0 *)
mov r11_b s18_b;
mov r11_t s18_t;
mov r11 s18;
mov zeta_r11 zeta_s18;
(* smulwb	lr, r10, r4                              #! PC = 0x4007f4 *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x4007f8 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007fc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400800 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x400804 *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov r4_b tmp_b;
mov r4_t tmp_t;
cast C_lb@sint32 r4_b;
assert C_lb = r4_b && true;
assume C_lb = r4_b && true;
cast C_lt@sint32 r4_t;
assert C_lt = r4_t && true;
assume C_lt = r4_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r5                              #! PC = 0x400808 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x40080c *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400810 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400814 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x400818 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov r5_b tmp_b;
mov r5_t tmp_t;
cast C_lb@sint32 r5_b;
assert C_lb = r5_b && true;
assume C_lb = r5_b && true;
cast C_lt@sint32 r5_t;
assert C_lt = r5_t && true;
assume C_lt = r5_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	r10, s19                                   #! PC = 0x40081c *)
mov r10_b s19_b;
mov r10_t s19_t;
mov r10 s19;
mov zeta_r10 zeta_s19;
(* vmov	r11, s20                                   #! PC = 0x400820 *)
mov r11_b s20_b;
mov r11_t s20_t;
mov r11 s20;
mov zeta_r11 zeta_s20;
(* smulwb	lr, r10, r6                              #! PC = 0x400824 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400828 *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40082c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400830 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x400834 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov r6_b tmp_b;
mov r6_t tmp_t;
cast C_lb@sint32 r6_b;
assert C_lb = r6_b && true;
assume C_lb = r6_b && true;
cast C_lt@sint32 r6_t;
assert C_lt = r6_t && true;
assume C_lt = r6_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r7                              #! PC = 0x400838 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r11 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r11;
(* smulwt	r7, r11, r7                              #! PC = 0x40083c *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r11 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400840 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400844 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x400848 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov r7_b tmp_b;
mov r7_t tmp_t;
cast C_lb@sint32 r7_b;
assert C_lb = r7_b && true;
assume C_lb = r7_b && true;
cast C_lt@sint32 r7_t;
assert C_lt = r7_t && true;
assume C_lt = r7_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	r10, s21                                   #! PC = 0x40084c *)
mov r10_b s21_b;
mov r10_t s21_t;
mov r10 s21;
mov zeta_r10 zeta_s21;
(* vmov	r11, s22                                   #! PC = 0x400850 *)
mov r11_b s22_b;
mov r11_t s22_t;
mov r11 s22;
mov zeta_r11 zeta_s22;
(* smulwb	lr, r10, r8                              #! PC = 0x400854 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400858 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40085c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400860 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x400864 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov r8_b tmp_b;
mov r8_t tmp_t;
cast C_lb@sint32 r8_b;
assert C_lb = r8_b && true;
assume C_lb = r8_b && true;
cast C_lt@sint32 r8_t;
assert C_lt = r8_t && true;
assume C_lt = r8_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r9                              #! PC = 0x400868 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x40086c *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400870 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400874 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x400878 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov r9_b tmp_b;
mov r9_t tmp_t;
cast C_lb@sint32 r9_b;
assert C_lb = r9_b && true;
assume C_lb = r9_b && true;
cast C_lt@sint32 r9_t;
assert C_lt = r9_t && true;
assume C_lt = r9_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	s0, r2                                     #! PC = 0x40087c *)
mov s0_b r2_b;
mov s0_t r2_t;
mov s0 r2;
mov zeta_s0 zeta_r2;
(* vmov	s1, r3                                     #! PC = 0x400880 *)
mov s1_b r3_b;
mov s1_t r3_t;
mov s1 r3;
mov zeta_s1 zeta_r3;
(* vmov	s2, r4                                     #! PC = 0x400884 *)
mov s2_b r4_b;
mov s2_t r4_t;
mov s2 r4;
mov zeta_s2 zeta_r4;
(* vmov	s3, r5                                     #! PC = 0x400888 *)
mov s3_b r5_b;
mov s3_t r5_t;
mov s3 r5;
mov zeta_s3 zeta_r5;
(* vmov	s4, r6                                     #! PC = 0x40088c *)
mov s4_b r6_b;
mov s4_t r6_t;
mov s4 r6;
mov zeta_s4 zeta_r6;
(* vmov	s5, r7                                     #! PC = 0x400890 *)
mov s5_b r7_b;
mov s5_t r7_t;
mov s5 r7;
mov zeta_s5 zeta_r7;
(* vmov	s6, r8                                     #! PC = 0x400894 *)
mov s6_b r8_b;
mov s6_t r8_t;
mov s6 r8;
mov zeta_s6 zeta_r8;
(* vmov	s7, r9                                     #! PC = 0x400898 *)
mov s7_b r9_b;
mov s7_t r9_t;
mov s7 r9;
mov zeta_s7 zeta_r9;

(*== coeffs multiplied by twiddles END ==*)

assert
  and [
    (-1) * 1664 <= s0_b, s0_b <= 1 * 1664,
    (-1) * 1664 <= s0_t, s0_t <= 1 * 1664,
    (-1) * 1664 <= s1_b, s1_b <= 1 * 1664,
    (-1) * 1664 <= s1_t, s1_t <= 1 * 1664,
    (-1) * 1664 <= s2_b, s2_b <= 1 * 1664,
    (-1) * 1664 <= s2_t, s2_t <= 1 * 1664,
    (-1) * 1664 <= s3_b, s3_b <= 1 * 1664,
    (-1) * 1664 <= s3_t, s3_t <= 1 * 1664,
    (-1) * 1664 <= s4_b, s4_b <= 1 * 1664,
    (-1) * 1664 <= s4_t, s4_t <= 1 * 1664,
    (-1) * 1664 <= s5_b, s5_b <= 1 * 1664,
    (-1) * 1664 <= s5_t, s5_t <= 1 * 1664,
    (-1) * 1664 <= s6_b, s6_b <= 1 * 1664,
    (-1) * 1664 <= s6_t, s6_t <= 1 * 1664,
    (-1) * 1664 <= s7_b, s7_b <= 1 * 1664,
    (-1) * 1664 <= s7_t, s7_t <= 1 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-1)@16 * 1664@16 <=s s0_b, s0_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s0_t, s0_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_b, s1_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_t, s1_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_b, s2_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_t, s2_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_b, s3_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_t, s3_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_b, s4_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_t, s4_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_b, s5_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_t, s5_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_b, s6_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_t, s6_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_b, s7_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_t, s7_t <=s 1@16 * 1664@16
  ];

assume
  and [
    (-1) * 1664 <= s0_b, s0_b <= 1 * 1664,
    (-1) * 1664 <= s0_t, s0_t <= 1 * 1664,
    (-1) * 1664 <= s1_b, s1_b <= 1 * 1664,
    (-1) * 1664 <= s1_t, s1_t <= 1 * 1664,
    (-1) * 1664 <= s2_b, s2_b <= 1 * 1664,
    (-1) * 1664 <= s2_t, s2_t <= 1 * 1664,
    (-1) * 1664 <= s3_b, s3_b <= 1 * 1664,
    (-1) * 1664 <= s3_t, s3_t <= 1 * 1664,
    (-1) * 1664 <= s4_b, s4_b <= 1 * 1664,
    (-1) * 1664 <= s4_t, s4_t <= 1 * 1664,
    (-1) * 1664 <= s5_b, s5_b <= 1 * 1664,
    (-1) * 1664 <= s5_t, s5_t <= 1 * 1664,
    (-1) * 1664 <= s6_b, s6_b <= 1 * 1664,
    (-1) * 1664 <= s6_t, s6_t <= 1 * 1664,
    (-1) * 1664 <= s7_b, s7_b <= 1 * 1664,
    (-1) * 1664 <= s7_t, s7_t <= 1 * 1664
  ]
  &&
  and [
    (-1)@16 * 1664@16 <=s s0_b, s0_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s0_t, s0_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_b, s1_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_t, s1_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_b, s2_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_t, s2_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_b, s3_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_t, s3_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_b, s4_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_t, s4_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_b, s5_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_t, s5_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_b, s6_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_t, s6_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_b, s7_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_t, s7_t <=s 1@16 * 1664@16
  ];

(* vmov	r0, s23                                    #! PC = 0x40089c *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff1d4; Value = 0xb6fe059d; PC = 0x4008a0 *)
mov r2_b L0xbefff1d4;
mov r2_t L0xbefff1d6;
(* ldr.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff214; Value = 0x00000000; PC = 0x4008a4 *)
mov r3_b L0xbefff214;
mov r3_t L0xbefff216;
(* ldr.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff254; Value = 0xb6fff02c; PC = 0x4008a8 *)
mov r4_b L0xbefff254;
mov r4_t L0xbefff256;
(* ldr.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff294; Value = 0xb6feab70; PC = 0x4008ac *)
mov r5_b L0xbefff294;
mov r5_t L0xbefff296;
(* ldr.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2d4; Value = 0xb6fff908; PC = 0x4008b0 *)
mov r6_b L0xbefff2d4;
mov r6_t L0xbefff2d6;
(* ldr.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff314; Value = 0x00000000; PC = 0x4008b4 *)
mov r7_b L0xbefff314;
mov r7_t L0xbefff316;
(* ldr.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff354; Value = 0x00000000; PC = 0x4008b8 *)
mov r8_b L0xbefff354;
mov r8_t L0xbefff356;
(* ldr.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff394; Value = 0xb6fe0f91; PC = 0x4008bc *)
mov r9_b L0xbefff394;
mov r9_t L0xbefff396;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x4008c0 *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* vmov	r10, s8                                    #! PC = 0x4008c4 *)
mov r10_b s8_b;
mov r10_t s8_t;
mov r10 s8;
mov zeta_r10 zeta_s8;
(* smulwb	lr, r10, r6                              #! PC = 0x4008c8 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x4008cc *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4008d0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4008d4 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x4008d8 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x4008dc *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4008e0 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x4008e4 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x4008e8 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4008ec *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4008f0 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x4008f4 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x4008f8 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x4008fc *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x400900 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400904 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400908 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x40090c *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400910 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x400914 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400918 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x40091c *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x400920 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400924 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400928 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x40092c *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x400930 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400934 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* vmov	r10, s9                                    #! PC = 0x400938 *)
mov r10_b s9_b;
mov r10_t s9_t;
mov r10 s9;
mov zeta_r10 zeta_s9;
(* vmov	r11, s10                                   #! PC = 0x40093c *)
mov r11_b s10_b;
mov r11_t s10_t;
mov r11 s10;
mov zeta_r11 zeta_s10;
(* smulwb	lr, r10, r4                              #! PC = 0x400940 *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x400944 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400948 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x40094c *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400950 *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x400954 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400958 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x40095c *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x400960 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400964 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400968 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x40096c *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x400970 *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400974 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400978 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x40097c *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400980 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400984 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400988 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x40098c *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400990 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400994 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400998 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40099c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4009a0 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4009a4 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x4009a8 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x4009ac *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* vmov	r10, s11                                   #! PC = 0x4009b0 *)
mov r10_b s11_b;
mov r10_t s11_t;
mov r10 s11;
mov zeta_r10 zeta_s11;
(* vmov	r11, s12                                   #! PC = 0x4009b4 *)
mov r11_b s12_b;
mov r11_t s12_t;
mov r11 s12;
mov zeta_r11 zeta_s12;
(* smulwb	lr, r10, r3                              #! PC = 0x4009b8 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x4009bc *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4009c0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4009c4 *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x4009c8 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x4009cc *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4009d0 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x4009d4 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x4009d8 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4009dc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x4009e0 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x4009e4 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x4009e8 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x4009ec *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* vmov	r10, s13                                   #! PC = 0x4009f0 *)
mov r10_b s13_b;
mov r10_t s13_t;
mov r10 s13;
mov zeta_r10 zeta_s13;
(* vmov	r11, s14                                   #! PC = 0x4009f4 *)
mov r11_b s14_b;
mov r11_t s14_t;
mov r11 s14;
mov zeta_r11 zeta_s14;
(* smulwb	lr, r10, r7                              #! PC = 0x4009f8 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x4009fc *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400a00 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400a04 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400a08 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400a0c *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400a10 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400a14 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400a18 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400a1c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400a20 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400a24 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x400a28 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x400a2c *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

(*== 8-NTT on a0, a2, ..., a14 ==*)
(*== _3_layer_double_CT_16_plant_fp END ==*)

assert
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

assume
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

(* vmov	r0, s23                                    #! PC = 0x400a30 *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* vmov	r10, s1                                    #! PC = 0x400a34 *)
mov r10_b s1_b;
mov r10_t s1_t;
mov r10 s1;
mov zeta_r10 zeta_s1;
(* uadd16	lr, r3, r10                              #! PC = 0x400a38 *)
add lr_b r3_b r10_b;
add lr_t r3_t r10_t;
(* usub16	r3, r3, r10                              #! PC = 0x400a3c *)
sub r3_b r3_b r10_b;
sub r3_t r3_t r10_t;
(* str.w	lr, [r0, #64]	; 0x40                      #! EA = L0xbefff214; PC = 0x400a40 *)
mov L0xbefff214 lr_b;
mov L0xbefff216 lr_t;
(* str.w	r3, [r0, #96]	; 0x60                      #! EA = L0xbefff234; PC = 0x400a44 *)
mov L0xbefff234 r3_b;
mov L0xbefff236 r3_t;
(* vmov	r10, s3                                    #! PC = 0x400a48 *)
mov r10_b s3_b;
mov r10_t s3_t;
mov r10 s3;
mov zeta_r10 zeta_s3;
(* uadd16	lr, r5, r10                              #! PC = 0x400a4c *)
add lr_b r5_b r10_b;
add lr_t r5_t r10_t;
(* usub16	r5, r5, r10                              #! PC = 0x400a50 *)
sub r5_b r5_b r10_b;
sub r5_t r5_t r10_t;
(* str.w	lr, [r0, #192]	; 0xc0                     #! EA = L0xbefff294; PC = 0x400a54 *)
mov L0xbefff294 lr_b;
mov L0xbefff296 lr_t;
(* str.w	r5, [r0, #224]	; 0xe0                     #! EA = L0xbefff2b4; PC = 0x400a58 *)
mov L0xbefff2b4 r5_b;
mov L0xbefff2b6 r5_t;
(* vmov	r10, s5                                    #! PC = 0x400a5c *)
mov r10_b s5_b;
mov r10_t s5_t;
mov r10 s5;
mov zeta_r10 zeta_s5;
(* uadd16	lr, r7, r10                              #! PC = 0x400a60 *)
add lr_b r7_b r10_b;
add lr_t r7_t r10_t;
(* usub16	r7, r7, r10                              #! PC = 0x400a64 *)
sub r7_b r7_b r10_b;
sub r7_t r7_t r10_t;
(* str.w	lr, [r0, #320]	; 0x140                    #! EA = L0xbefff314; PC = 0x400a68 *)
mov L0xbefff314 lr_b;
mov L0xbefff316 lr_t;
(* str.w	r7, [r0, #352]	; 0x160                    #! EA = L0xbefff334; PC = 0x400a6c *)
mov L0xbefff334 r7_b;
mov L0xbefff336 r7_t;
(* vmov	r10, s7                                    #! PC = 0x400a70 *)
mov r10_b s7_b;
mov r10_t s7_t;
mov r10 s7;
mov zeta_r10 zeta_s7;
(* uadd16	lr, r9, r10                              #! PC = 0x400a74 *)
add lr_b r9_b r10_b;
add lr_t r9_t r10_t;
(* usub16	r9, r9, r10                              #! PC = 0x400a78 *)
sub r9_b r9_b r10_b;
sub r9_t r9_t r10_t;
(* str.w	lr, [r0, #448]	; 0x1c0                    #! EA = L0xbefff394; PC = 0x400a7c *)
mov L0xbefff394 lr_b;
mov L0xbefff396 lr_t;
(* str.w	r9, [r0, #480]	; 0x1e0                    #! EA = L0xbefff3b4; PC = 0x400a80 *)
mov L0xbefff3b4 r9_b;
mov L0xbefff3b6 r9_t;
(* vmov	r5, s2                                     #! PC = 0x400a84 *)
mov r5_b s2_b;
mov r5_t s2_t;
mov r5 s2;
mov zeta_r5 zeta_s2;
(* uadd16	lr, r4, r5                               #! PC = 0x400a88 *)
add lr_b r4_b r5_b;
add lr_t r4_t r5_t;
(* usub16	r10, r4, r5                              #! PC = 0x400a8c *)
sub r10_b r4_b r5_b;
sub r10_t r4_t r5_t;
(* str.w	lr, [r0, #128]	; 0x80                     #! EA = L0xbefff254; PC = 0x400a90 *)
mov L0xbefff254 lr_b;
mov L0xbefff256 lr_t;
(* str.w	r10, [r0, #160]	; 0xa0                    #! EA = L0xbefff274; PC = 0x400a94 *)
mov L0xbefff274 r10_b;
mov L0xbefff276 r10_t;
(* vmov	r7, s4                                     #! PC = 0x400a98 *)
mov r7_b s4_b;
mov r7_t s4_t;
mov r7 s4;
mov zeta_r7 zeta_s4;
(* uadd16	lr, r6, r7                               #! PC = 0x400a9c *)
add lr_b r6_b r7_b;
add lr_t r6_t r7_t;
(* usub16	r10, r6, r7                              #! PC = 0x400aa0 *)
sub r10_b r6_b r7_b;
sub r10_t r6_t r7_t;
(* str.w	lr, [r0, #256]	; 0x100                    #! EA = L0xbefff2d4; PC = 0x400aa4 *)
mov L0xbefff2d4 lr_b;
mov L0xbefff2d6 lr_t;
(* str.w	r10, [r0, #288]	; 0x120                   #! EA = L0xbefff2f4; PC = 0x400aa8 *)
mov L0xbefff2f4 r10_b;
mov L0xbefff2f6 r10_t;
(* vmov	r9, s6                                     #! PC = 0x400aac *)
mov r9_b s6_b;
mov r9_t s6_t;
mov r9 s6;
mov zeta_r9 zeta_s6;
(* uadd16	lr, r8, r9                               #! PC = 0x400ab0 *)
add lr_b r8_b r9_b;
add lr_t r8_t r9_t;
(* usub16	r10, r8, r9                              #! PC = 0x400ab4 *)
sub r10_b r8_b r9_b;
sub r10_t r8_t r9_t;
(* str.w	lr, [r0, #384]	; 0x180                    #! EA = L0xbefff354; PC = 0x400ab8 *)
mov L0xbefff354 lr_b;
mov L0xbefff356 lr_t;
(* str.w	r10, [r0, #416]	; 0x1a0                   #! EA = L0xbefff374; PC = 0x400abc *)
mov L0xbefff374 r10_b;
mov L0xbefff376 r10_t;
(* vmov	r3, s0                                     #! PC = 0x400ac0 *)
mov r3_b s0_b;
mov r3_t s0_t;
mov r3 s0;
mov zeta_r3 zeta_s0;
(* uadd16	lr, r2, r3                               #! PC = 0x400ac4 *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* usub16	r10, r2, r3                              #! PC = 0x400ac8 *)
sub r10_b r2_b r3_b;
sub r10_t r2_t r3_t;
(* str.w	r10, [r0, #32]                            #! EA = L0xbefff1f4; PC = 0x400acc *)
mov L0xbefff1f4 r10_b;
mov L0xbefff1f6 r10_t;
(* str.w	lr, [r0], #4                              #! EA = L0xbefff1d4; PC = 0x400ad0 *)
mov L0xbefff1d4 lr_b;
mov L0xbefff1d6 lr_t;
(* vmov	lr, s24                                    #! PC = 0x400ad4 *)
mov lr_b s24_b;
mov lr_t s24_t;
mov lr s24;
mov zeta_lr zeta_s24;
(* #bne.w	0x400628 <ntt_fast+24>                   #! PC = 0x400adc *)
#bne.w	0x400628 <ntt_fast+24>                   #! 0x400adc = 0x400adc;

(*== layer 7+6+5+4 one slice condition ==*)

assert
  and [
    (-6) * 1664 <= L0xbefff1d4, L0xbefff1d4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1d6, L0xbefff1d6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1f4, L0xbefff1f4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1f6, L0xbefff1f6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff214, L0xbefff214 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff216, L0xbefff216 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff234, L0xbefff234 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff236, L0xbefff236 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff254, L0xbefff254 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff256, L0xbefff256 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff274, L0xbefff274 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff276, L0xbefff276 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff294, L0xbefff294 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff296, L0xbefff296 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2b4, L0xbefff2b4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2b6, L0xbefff2b6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2d4, L0xbefff2d4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2d6, L0xbefff2d6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2f4, L0xbefff2f4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2f6, L0xbefff2f6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff314, L0xbefff314 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff316, L0xbefff316 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff334, L0xbefff334 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff336, L0xbefff336 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff354, L0xbefff354 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff356, L0xbefff356 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff374, L0xbefff374 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff376, L0xbefff376 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff394, L0xbefff394 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff396, L0xbefff396 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3b4, L0xbefff3b4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3b6, L0xbefff3b6 <= 6 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-6)@16 * 1664@16 <=s L0xbefff1d4, L0xbefff1d4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1d6, L0xbefff1d6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1f4, L0xbefff1f4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1f6, L0xbefff1f6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff214, L0xbefff214 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff216, L0xbefff216 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff234, L0xbefff234 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff236, L0xbefff236 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff254, L0xbefff254 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff256, L0xbefff256 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff274, L0xbefff274 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff276, L0xbefff276 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff294, L0xbefff294 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff296, L0xbefff296 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2b4, L0xbefff2b4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2b6, L0xbefff2b6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2d4, L0xbefff2d4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2d6, L0xbefff2d6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2f4, L0xbefff2f4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2f6, L0xbefff2f6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff314, L0xbefff314 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff316, L0xbefff316 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff334, L0xbefff334 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff336, L0xbefff336 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff354, L0xbefff354 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff356, L0xbefff356 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff374, L0xbefff374 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff376, L0xbefff376 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff394, L0xbefff394 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff396, L0xbefff396 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3b4, L0xbefff3b4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3b6, L0xbefff3b6 <=s 6@16 * 1664@16
  ]
;

assume
  and [
    (-6) * 1664 <= L0xbefff1d4, L0xbefff1d4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1d6, L0xbefff1d6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1f4, L0xbefff1f4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1f6, L0xbefff1f6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff214, L0xbefff214 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff216, L0xbefff216 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff234, L0xbefff234 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff236, L0xbefff236 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff254, L0xbefff254 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff256, L0xbefff256 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff274, L0xbefff274 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff276, L0xbefff276 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff294, L0xbefff294 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff296, L0xbefff296 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2b4, L0xbefff2b4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2b6, L0xbefff2b6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2d4, L0xbefff2d4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2d6, L0xbefff2d6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2f4, L0xbefff2f4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2f6, L0xbefff2f6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff314, L0xbefff314 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff316, L0xbefff316 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff334, L0xbefff334 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff336, L0xbefff336 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff354, L0xbefff354 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff356, L0xbefff356 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff374, L0xbefff374 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff376, L0xbefff376 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff394, L0xbefff394 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff396, L0xbefff396 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3b4, L0xbefff3b4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3b6, L0xbefff3b6 <= 6 * 1664
  ]
  &&
  and [
    (-6)@16 * 1664@16 <=s L0xbefff1d4, L0xbefff1d4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1d6, L0xbefff1d6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1f4, L0xbefff1f4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1f6, L0xbefff1f6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff214, L0xbefff214 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff216, L0xbefff216 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff234, L0xbefff234 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff236, L0xbefff236 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff254, L0xbefff254 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff256, L0xbefff256 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff274, L0xbefff274 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff276, L0xbefff276 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff294, L0xbefff294 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff296, L0xbefff296 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2b4, L0xbefff2b4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2b6, L0xbefff2b6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2d4, L0xbefff2d4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2d6, L0xbefff2d6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2f4, L0xbefff2f4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2f6, L0xbefff2f6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff314, L0xbefff314 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff316, L0xbefff316 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff334, L0xbefff334 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff336, L0xbefff336 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff354, L0xbefff354 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff356, L0xbefff356 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff374, L0xbefff374 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff376, L0xbefff376 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff394, L0xbefff394 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff396, L0xbefff396 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3b4, L0xbefff3b4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3b6, L0xbefff3b6 <=s 6@16 * 1664@16
  ]
;

(* vmov	s23, r0                                    #! PC = 0x400628 *)
mov s23_b r0_b;
mov s23_t r0_t;
mov s23 r0;
mov zeta_s23 zeta_r0;
(* ldr.w	r2, [r0, #32]                             #! EA = L0xbefff1f8; Value = 0xb6fd5000; PC = 0x40062c *)
mov r2_b L0xbefff1f8;
mov r2_t L0xbefff1fa;
(* ldr.w	r3, [r0, #96]	; 0x60                      #! EA = L0xbefff238; Value = 0xb6ffe8f8; PC = 0x400630 *)
mov r3_b L0xbefff238;
mov r3_t L0xbefff23a;
(* ldr.w	r4, [r0, #160]	; 0xa0                     #! EA = L0xbefff278; Value = 0x00000000; PC = 0x400634 *)
mov r4_b L0xbefff278;
mov r4_t L0xbefff27a;
(* ldr.w	r5, [r0, #224]	; 0xe0                     #! EA = L0xbefff2b8; Value = 0xb6fff9a8; PC = 0x400638 *)
mov r5_b L0xbefff2b8;
mov r5_t L0xbefff2ba;
(* ldr.w	r6, [r0, #288]	; 0x120                    #! EA = L0xbefff2f8; Value = 0xb6ffbc00; PC = 0x40063c *)
mov r6_b L0xbefff2f8;
mov r6_t L0xbefff2fa;
(* ldr.w	r7, [r0, #352]	; 0x160                    #! EA = L0xbefff338; Value = 0x00000000; PC = 0x400640 *)
mov r7_b L0xbefff338;
mov r7_t L0xbefff33a;
(* ldr.w	r8, [r0, #416]	; 0x1a0                    #! EA = L0xbefff378; Value = 0x00000000; PC = 0x400644 *)
mov r8_b L0xbefff378;
mov r8_t L0xbefff37a;
(* ldr.w	r9, [r0, #480]	; 0x1e0                    #! EA = L0xbefff3b8; Value = 0x00000000; PC = 0x400648 *)
mov r9_b L0xbefff3b8;
mov r9_t L0xbefff3ba;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x40064c *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* vmov	r10, s8                                    #! PC = 0x400650 *)
mov r10_b s8_b;
mov r10_t s8_t;
mov r10 s8;
mov zeta_r10 zeta_s8;
(* smulwb	lr, r10, r6                              #! PC = 0x400654 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400658 *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40065c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400660 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x400664 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x400668 *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x40066c *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x400670 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400674 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400678 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x40067c *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400680 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x400684 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400688 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x40068c *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400690 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400694 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400698 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x40069c *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x4006a0 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x4006a4 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x4006a8 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x4006ac *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4006b0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4006b4 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4006b8 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x4006bc *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x4006c0 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* vmov	r10, s9                                    #! PC = 0x4006c4 *)
mov r10_b s9_b;
mov r10_t s9_t;
mov r10 s9;
mov zeta_r10 zeta_s9;
(* vmov	r11, s10                                   #! PC = 0x4006c8 *)
mov r11_b s10_b;
mov r11_t s10_t;
mov r11 s10;
mov zeta_r11 zeta_s10;
(* smulwb	lr, r10, r4                              #! PC = 0x4006cc *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x4006d0 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4006d4 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x4006d8 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x4006dc *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x4006e0 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4006e4 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x4006e8 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x4006ec *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4006f0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x4006f4 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x4006f8 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x4006fc *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400700 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400704 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x400708 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40070c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400710 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400714 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x400718 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x40071c *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400720 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400724 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400728 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x40072c *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400730 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x400734 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400738 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* vmov	r10, s11                                   #! PC = 0x40073c *)
mov r10_b s11_b;
mov r10_t s11_t;
mov r10 s11;
mov zeta_r10 zeta_s11;
(* vmov	r11, s12                                   #! PC = 0x400740 *)
mov r11_b s12_b;
mov r11_t s12_t;
mov r11 s12;
mov zeta_r11 zeta_s12;
(* smulwb	lr, r10, r3                              #! PC = 0x400744 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x400748 *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40074c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400750 *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x400754 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x400758 *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x40075c *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x400760 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x400764 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400768 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x40076c *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400770 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x400774 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400778 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* vmov	r10, s13                                   #! PC = 0x40077c *)
mov r10_b s13_b;
mov r10_t s13_t;
mov r10 s13;
mov zeta_r10 zeta_s13;
(* vmov	r11, s14                                   #! PC = 0x400780 *)
mov r11_b s14_b;
mov r11_t s14_t;
mov r11 s14;
mov zeta_r11 zeta_s14;
(* smulwb	lr, r10, r7                              #! PC = 0x400784 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400788 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40078c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400790 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400794 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400798 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x40079c *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4007a0 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x4007a4 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007a8 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4007ac *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4007b0 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x4007b4 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x4007b8 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

(*== 8-NTT on a1, a3, ..., a15 ==*)
(*== _3_layer_double_CT_16_plant_fp END ==*)

assert
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

assume
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

(* vmov	r10, s15                                   #! PC = 0x4007bc *)
mov r10_b s15_b;
mov r10_t s15_t;
mov r10 s15;
mov zeta_r10 zeta_s15;
(* vmov	r11, s16                                   #! PC = 0x4007c0 *)
mov r11_b s16_b;
mov r11_t s16_t;
mov r11 s16;
mov zeta_r11 zeta_s16;
(* smulwb	lr, r10, r2                              #! PC = 0x4007c4 *)
cast r2_lsb@sint32 r2_b;
mull tmp_t tmp_b r10 r2_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r2_b;
assert A_lb = r2_b && true;
assume A_lb = r2_b && true;
cast A_lt@sint32 r2_t;
assert A_lt = r2_t && true;
assume A_lt = r2_t && true;
mov zeta zeta_r10;
(* smulwt	r2, r10, r2                              #! PC = 0x4007c8 *)
cast r2_lst@sint32 r2_t;
mull tmp_t tmp_b r10 r2_lst;
spl dontcare r2_t tmp_t 16;
spl r2_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007cc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r2, r2, r12, r0                          #! PC = 0x4007d0 *)
cast r2_sb@sint16 r2_b;
mull tmp_t tmp_b r2_sb r12_t;
uadds carry r2_b tmp_b r0_b;
adc r2_t tmp_t r0_t carry;
(* pkhtb	r2, r2, lr, asr #16                       #! PC = 0x4007d4 *)
mov tmp_b lr_t;
mov tmp_t r2_t;
mov r2_b tmp_b;
mov r2_t tmp_t;
cast C_lb@sint32 r2_b;
assert C_lb = r2_b && true;
assume C_lb = r2_b && true;
cast C_lt@sint32 r2_t;
assert C_lt = r2_t && true;
assume C_lt = r2_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r3                              #! PC = 0x4007d8 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r11 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r11;
(* smulwt	r3, r11, r3                              #! PC = 0x4007dc *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r11 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007e0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4007e4 *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x4007e8 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov r3_b tmp_b;
mov r3_t tmp_t;
cast C_lb@sint32 r3_b;
assert C_lb = r3_b && true;
assume C_lb = r3_b && true;
cast C_lt@sint32 r3_t;
assert C_lt = r3_t && true;
assume C_lt = r3_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	r10, s17                                   #! PC = 0x4007ec *)
mov r10_b s17_b;
mov r10_t s17_t;
mov r10 s17;
mov zeta_r10 zeta_s17;
(* vmov	r11, s18                                   #! PC = 0x4007f0 *)
mov r11_b s18_b;
mov r11_t s18_t;
mov r11 s18;
mov zeta_r11 zeta_s18;
(* smulwb	lr, r10, r4                              #! PC = 0x4007f4 *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x4007f8 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007fc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400800 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x400804 *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov r4_b tmp_b;
mov r4_t tmp_t;
cast C_lb@sint32 r4_b;
assert C_lb = r4_b && true;
assume C_lb = r4_b && true;
cast C_lt@sint32 r4_t;
assert C_lt = r4_t && true;
assume C_lt = r4_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r5                              #! PC = 0x400808 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x40080c *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400810 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400814 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x400818 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov r5_b tmp_b;
mov r5_t tmp_t;
cast C_lb@sint32 r5_b;
assert C_lb = r5_b && true;
assume C_lb = r5_b && true;
cast C_lt@sint32 r5_t;
assert C_lt = r5_t && true;
assume C_lt = r5_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	r10, s19                                   #! PC = 0x40081c *)
mov r10_b s19_b;
mov r10_t s19_t;
mov r10 s19;
mov zeta_r10 zeta_s19;
(* vmov	r11, s20                                   #! PC = 0x400820 *)
mov r11_b s20_b;
mov r11_t s20_t;
mov r11 s20;
mov zeta_r11 zeta_s20;
(* smulwb	lr, r10, r6                              #! PC = 0x400824 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400828 *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40082c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400830 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x400834 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov r6_b tmp_b;
mov r6_t tmp_t;
cast C_lb@sint32 r6_b;
assert C_lb = r6_b && true;
assume C_lb = r6_b && true;
cast C_lt@sint32 r6_t;
assert C_lt = r6_t && true;
assume C_lt = r6_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r7                              #! PC = 0x400838 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r11 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r11;
(* smulwt	r7, r11, r7                              #! PC = 0x40083c *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r11 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400840 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400844 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x400848 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov r7_b tmp_b;
mov r7_t tmp_t;
cast C_lb@sint32 r7_b;
assert C_lb = r7_b && true;
assume C_lb = r7_b && true;
cast C_lt@sint32 r7_t;
assert C_lt = r7_t && true;
assume C_lt = r7_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	r10, s21                                   #! PC = 0x40084c *)
mov r10_b s21_b;
mov r10_t s21_t;
mov r10 s21;
mov zeta_r10 zeta_s21;
(* vmov	r11, s22                                   #! PC = 0x400850 *)
mov r11_b s22_b;
mov r11_t s22_t;
mov r11 s22;
mov zeta_r11 zeta_s22;
(* smulwb	lr, r10, r8                              #! PC = 0x400854 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400858 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40085c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400860 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x400864 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov r8_b tmp_b;
mov r8_t tmp_t;
cast C_lb@sint32 r8_b;
assert C_lb = r8_b && true;
assume C_lb = r8_b && true;
cast C_lt@sint32 r8_t;
assert C_lt = r8_t && true;
assume C_lt = r8_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r9                              #! PC = 0x400868 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x40086c *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400870 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400874 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x400878 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov r9_b tmp_b;
mov r9_t tmp_t;
cast C_lb@sint32 r9_b;
assert C_lb = r9_b && true;
assume C_lb = r9_b && true;
cast C_lt@sint32 r9_t;
assert C_lt = r9_t && true;
assume C_lt = r9_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	s0, r2                                     #! PC = 0x40087c *)
mov s0_b r2_b;
mov s0_t r2_t;
mov s0 r2;
mov zeta_s0 zeta_r2;
(* vmov	s1, r3                                     #! PC = 0x400880 *)
mov s1_b r3_b;
mov s1_t r3_t;
mov s1 r3;
mov zeta_s1 zeta_r3;
(* vmov	s2, r4                                     #! PC = 0x400884 *)
mov s2_b r4_b;
mov s2_t r4_t;
mov s2 r4;
mov zeta_s2 zeta_r4;
(* vmov	s3, r5                                     #! PC = 0x400888 *)
mov s3_b r5_b;
mov s3_t r5_t;
mov s3 r5;
mov zeta_s3 zeta_r5;
(* vmov	s4, r6                                     #! PC = 0x40088c *)
mov s4_b r6_b;
mov s4_t r6_t;
mov s4 r6;
mov zeta_s4 zeta_r6;
(* vmov	s5, r7                                     #! PC = 0x400890 *)
mov s5_b r7_b;
mov s5_t r7_t;
mov s5 r7;
mov zeta_s5 zeta_r7;
(* vmov	s6, r8                                     #! PC = 0x400894 *)
mov s6_b r8_b;
mov s6_t r8_t;
mov s6 r8;
mov zeta_s6 zeta_r8;
(* vmov	s7, r9                                     #! PC = 0x400898 *)
mov s7_b r9_b;
mov s7_t r9_t;
mov s7 r9;
mov zeta_s7 zeta_r9;

(*== coeffs multiplied by twiddles END ==*)

assert
  and [
    (-1) * 1664 <= s0_b, s0_b <= 1 * 1664,
    (-1) * 1664 <= s0_t, s0_t <= 1 * 1664,
    (-1) * 1664 <= s1_b, s1_b <= 1 * 1664,
    (-1) * 1664 <= s1_t, s1_t <= 1 * 1664,
    (-1) * 1664 <= s2_b, s2_b <= 1 * 1664,
    (-1) * 1664 <= s2_t, s2_t <= 1 * 1664,
    (-1) * 1664 <= s3_b, s3_b <= 1 * 1664,
    (-1) * 1664 <= s3_t, s3_t <= 1 * 1664,
    (-1) * 1664 <= s4_b, s4_b <= 1 * 1664,
    (-1) * 1664 <= s4_t, s4_t <= 1 * 1664,
    (-1) * 1664 <= s5_b, s5_b <= 1 * 1664,
    (-1) * 1664 <= s5_t, s5_t <= 1 * 1664,
    (-1) * 1664 <= s6_b, s6_b <= 1 * 1664,
    (-1) * 1664 <= s6_t, s6_t <= 1 * 1664,
    (-1) * 1664 <= s7_b, s7_b <= 1 * 1664,
    (-1) * 1664 <= s7_t, s7_t <= 1 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-1)@16 * 1664@16 <=s s0_b, s0_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s0_t, s0_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_b, s1_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_t, s1_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_b, s2_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_t, s2_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_b, s3_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_t, s3_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_b, s4_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_t, s4_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_b, s5_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_t, s5_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_b, s6_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_t, s6_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_b, s7_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_t, s7_t <=s 1@16 * 1664@16
  ];

assume
  and [
    (-1) * 1664 <= s0_b, s0_b <= 1 * 1664,
    (-1) * 1664 <= s0_t, s0_t <= 1 * 1664,
    (-1) * 1664 <= s1_b, s1_b <= 1 * 1664,
    (-1) * 1664 <= s1_t, s1_t <= 1 * 1664,
    (-1) * 1664 <= s2_b, s2_b <= 1 * 1664,
    (-1) * 1664 <= s2_t, s2_t <= 1 * 1664,
    (-1) * 1664 <= s3_b, s3_b <= 1 * 1664,
    (-1) * 1664 <= s3_t, s3_t <= 1 * 1664,
    (-1) * 1664 <= s4_b, s4_b <= 1 * 1664,
    (-1) * 1664 <= s4_t, s4_t <= 1 * 1664,
    (-1) * 1664 <= s5_b, s5_b <= 1 * 1664,
    (-1) * 1664 <= s5_t, s5_t <= 1 * 1664,
    (-1) * 1664 <= s6_b, s6_b <= 1 * 1664,
    (-1) * 1664 <= s6_t, s6_t <= 1 * 1664,
    (-1) * 1664 <= s7_b, s7_b <= 1 * 1664,
    (-1) * 1664 <= s7_t, s7_t <= 1 * 1664
  ]
  &&
  and [
    (-1)@16 * 1664@16 <=s s0_b, s0_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s0_t, s0_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_b, s1_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_t, s1_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_b, s2_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_t, s2_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_b, s3_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_t, s3_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_b, s4_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_t, s4_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_b, s5_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_t, s5_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_b, s6_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_t, s6_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_b, s7_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_t, s7_t <=s 1@16 * 1664@16
  ];

(* vmov	r0, s23                                    #! PC = 0x40089c *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff1d8; Value = 0xb6ffe980; PC = 0x4008a0 *)
mov r2_b L0xbefff1d8;
mov r2_t L0xbefff1da;
(* ldr.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff218; Value = 0xb6ffff18; PC = 0x4008a4 *)
mov r3_b L0xbefff218;
mov r3_t L0xbefff21a;
(* ldr.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff258; Value = 0x000000a0; PC = 0x4008a8 *)
mov r4_b L0xbefff258;
mov r4_t L0xbefff25a;
(* ldr.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff298; Value = 0xb6feab5c; PC = 0x4008ac *)
mov r5_b L0xbefff298;
mov r5_t L0xbefff29a;
(* ldr.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2d8; Value = 0xb6fff8fc; PC = 0x4008b0 *)
mov r6_b L0xbefff2d8;
mov r6_t L0xbefff2da;
(* ldr.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff318; Value = 0x00000000; PC = 0x4008b4 *)
mov r7_b L0xbefff318;
mov r7_t L0xbefff31a;
(* ldr.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff358; Value = 0x00000000; PC = 0x4008b8 *)
mov r8_b L0xbefff358;
mov r8_t L0xbefff35a;
(* ldr.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff398; Value = 0xb6fb6630; PC = 0x4008bc *)
mov r9_b L0xbefff398;
mov r9_t L0xbefff39a;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x4008c0 *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* vmov	r10, s8                                    #! PC = 0x4008c4 *)
mov r10_b s8_b;
mov r10_t s8_t;
mov r10 s8;
mov zeta_r10 zeta_s8;
(* smulwb	lr, r10, r6                              #! PC = 0x4008c8 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x4008cc *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4008d0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4008d4 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x4008d8 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x4008dc *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4008e0 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x4008e4 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x4008e8 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4008ec *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4008f0 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x4008f4 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x4008f8 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x4008fc *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x400900 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400904 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400908 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x40090c *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400910 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x400914 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400918 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x40091c *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x400920 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400924 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400928 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x40092c *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x400930 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400934 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* vmov	r10, s9                                    #! PC = 0x400938 *)
mov r10_b s9_b;
mov r10_t s9_t;
mov r10 s9;
mov zeta_r10 zeta_s9;
(* vmov	r11, s10                                   #! PC = 0x40093c *)
mov r11_b s10_b;
mov r11_t s10_t;
mov r11 s10;
mov zeta_r11 zeta_s10;
(* smulwb	lr, r10, r4                              #! PC = 0x400940 *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x400944 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400948 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x40094c *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400950 *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x400954 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400958 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x40095c *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x400960 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400964 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400968 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x40096c *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x400970 *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400974 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400978 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x40097c *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400980 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400984 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400988 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x40098c *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400990 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400994 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400998 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40099c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4009a0 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4009a4 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x4009a8 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x4009ac *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* vmov	r10, s11                                   #! PC = 0x4009b0 *)
mov r10_b s11_b;
mov r10_t s11_t;
mov r10 s11;
mov zeta_r10 zeta_s11;
(* vmov	r11, s12                                   #! PC = 0x4009b4 *)
mov r11_b s12_b;
mov r11_t s12_t;
mov r11 s12;
mov zeta_r11 zeta_s12;
(* smulwb	lr, r10, r3                              #! PC = 0x4009b8 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x4009bc *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4009c0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4009c4 *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x4009c8 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x4009cc *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4009d0 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x4009d4 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x4009d8 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4009dc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x4009e0 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x4009e4 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x4009e8 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x4009ec *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* vmov	r10, s13                                   #! PC = 0x4009f0 *)
mov r10_b s13_b;
mov r10_t s13_t;
mov r10 s13;
mov zeta_r10 zeta_s13;
(* vmov	r11, s14                                   #! PC = 0x4009f4 *)
mov r11_b s14_b;
mov r11_t s14_t;
mov r11 s14;
mov zeta_r11 zeta_s14;
(* smulwb	lr, r10, r7                              #! PC = 0x4009f8 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x4009fc *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400a00 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400a04 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400a08 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400a0c *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400a10 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400a14 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400a18 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400a1c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400a20 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400a24 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x400a28 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x400a2c *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

(*== 8-NTT on a0, a2, ..., a14 ==*)
(*== _3_layer_double_CT_16_plant_fp END ==*)

assert
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

assume
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

(* vmov	r0, s23                                    #! PC = 0x400a30 *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* vmov	r10, s1                                    #! PC = 0x400a34 *)
mov r10_b s1_b;
mov r10_t s1_t;
mov r10 s1;
mov zeta_r10 zeta_s1;
(* uadd16	lr, r3, r10                              #! PC = 0x400a38 *)
add lr_b r3_b r10_b;
add lr_t r3_t r10_t;
(* usub16	r3, r3, r10                              #! PC = 0x400a3c *)
sub r3_b r3_b r10_b;
sub r3_t r3_t r10_t;
(* str.w	lr, [r0, #64]	; 0x40                      #! EA = L0xbefff218; PC = 0x400a40 *)
mov L0xbefff218 lr_b;
mov L0xbefff21a lr_t;
(* str.w	r3, [r0, #96]	; 0x60                      #! EA = L0xbefff238; PC = 0x400a44 *)
mov L0xbefff238 r3_b;
mov L0xbefff23a r3_t;
(* vmov	r10, s3                                    #! PC = 0x400a48 *)
mov r10_b s3_b;
mov r10_t s3_t;
mov r10 s3;
mov zeta_r10 zeta_s3;
(* uadd16	lr, r5, r10                              #! PC = 0x400a4c *)
add lr_b r5_b r10_b;
add lr_t r5_t r10_t;
(* usub16	r5, r5, r10                              #! PC = 0x400a50 *)
sub r5_b r5_b r10_b;
sub r5_t r5_t r10_t;
(* str.w	lr, [r0, #192]	; 0xc0                     #! EA = L0xbefff298; PC = 0x400a54 *)
mov L0xbefff298 lr_b;
mov L0xbefff29a lr_t;
(* str.w	r5, [r0, #224]	; 0xe0                     #! EA = L0xbefff2b8; PC = 0x400a58 *)
mov L0xbefff2b8 r5_b;
mov L0xbefff2ba r5_t;
(* vmov	r10, s5                                    #! PC = 0x400a5c *)
mov r10_b s5_b;
mov r10_t s5_t;
mov r10 s5;
mov zeta_r10 zeta_s5;
(* uadd16	lr, r7, r10                              #! PC = 0x400a60 *)
add lr_b r7_b r10_b;
add lr_t r7_t r10_t;
(* usub16	r7, r7, r10                              #! PC = 0x400a64 *)
sub r7_b r7_b r10_b;
sub r7_t r7_t r10_t;
(* str.w	lr, [r0, #320]	; 0x140                    #! EA = L0xbefff318; PC = 0x400a68 *)
mov L0xbefff318 lr_b;
mov L0xbefff31a lr_t;
(* str.w	r7, [r0, #352]	; 0x160                    #! EA = L0xbefff338; PC = 0x400a6c *)
mov L0xbefff338 r7_b;
mov L0xbefff33a r7_t;
(* vmov	r10, s7                                    #! PC = 0x400a70 *)
mov r10_b s7_b;
mov r10_t s7_t;
mov r10 s7;
mov zeta_r10 zeta_s7;
(* uadd16	lr, r9, r10                              #! PC = 0x400a74 *)
add lr_b r9_b r10_b;
add lr_t r9_t r10_t;
(* usub16	r9, r9, r10                              #! PC = 0x400a78 *)
sub r9_b r9_b r10_b;
sub r9_t r9_t r10_t;
(* str.w	lr, [r0, #448]	; 0x1c0                    #! EA = L0xbefff398; PC = 0x400a7c *)
mov L0xbefff398 lr_b;
mov L0xbefff39a lr_t;
(* str.w	r9, [r0, #480]	; 0x1e0                    #! EA = L0xbefff3b8; PC = 0x400a80 *)
mov L0xbefff3b8 r9_b;
mov L0xbefff3ba r9_t;
(* vmov	r5, s2                                     #! PC = 0x400a84 *)
mov r5_b s2_b;
mov r5_t s2_t;
mov r5 s2;
mov zeta_r5 zeta_s2;
(* uadd16	lr, r4, r5                               #! PC = 0x400a88 *)
add lr_b r4_b r5_b;
add lr_t r4_t r5_t;
(* usub16	r10, r4, r5                              #! PC = 0x400a8c *)
sub r10_b r4_b r5_b;
sub r10_t r4_t r5_t;
(* str.w	lr, [r0, #128]	; 0x80                     #! EA = L0xbefff258; PC = 0x400a90 *)
mov L0xbefff258 lr_b;
mov L0xbefff25a lr_t;
(* str.w	r10, [r0, #160]	; 0xa0                    #! EA = L0xbefff278; PC = 0x400a94 *)
mov L0xbefff278 r10_b;
mov L0xbefff27a r10_t;
(* vmov	r7, s4                                     #! PC = 0x400a98 *)
mov r7_b s4_b;
mov r7_t s4_t;
mov r7 s4;
mov zeta_r7 zeta_s4;
(* uadd16	lr, r6, r7                               #! PC = 0x400a9c *)
add lr_b r6_b r7_b;
add lr_t r6_t r7_t;
(* usub16	r10, r6, r7                              #! PC = 0x400aa0 *)
sub r10_b r6_b r7_b;
sub r10_t r6_t r7_t;
(* str.w	lr, [r0, #256]	; 0x100                    #! EA = L0xbefff2d8; PC = 0x400aa4 *)
mov L0xbefff2d8 lr_b;
mov L0xbefff2da lr_t;
(* str.w	r10, [r0, #288]	; 0x120                   #! EA = L0xbefff2f8; PC = 0x400aa8 *)
mov L0xbefff2f8 r10_b;
mov L0xbefff2fa r10_t;
(* vmov	r9, s6                                     #! PC = 0x400aac *)
mov r9_b s6_b;
mov r9_t s6_t;
mov r9 s6;
mov zeta_r9 zeta_s6;
(* uadd16	lr, r8, r9                               #! PC = 0x400ab0 *)
add lr_b r8_b r9_b;
add lr_t r8_t r9_t;
(* usub16	r10, r8, r9                              #! PC = 0x400ab4 *)
sub r10_b r8_b r9_b;
sub r10_t r8_t r9_t;
(* str.w	lr, [r0, #384]	; 0x180                    #! EA = L0xbefff358; PC = 0x400ab8 *)
mov L0xbefff358 lr_b;
mov L0xbefff35a lr_t;
(* str.w	r10, [r0, #416]	; 0x1a0                   #! EA = L0xbefff378; PC = 0x400abc *)
mov L0xbefff378 r10_b;
mov L0xbefff37a r10_t;
(* vmov	r3, s0                                     #! PC = 0x400ac0 *)
mov r3_b s0_b;
mov r3_t s0_t;
mov r3 s0;
mov zeta_r3 zeta_s0;
(* uadd16	lr, r2, r3                               #! PC = 0x400ac4 *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* usub16	r10, r2, r3                              #! PC = 0x400ac8 *)
sub r10_b r2_b r3_b;
sub r10_t r2_t r3_t;
(* str.w	r10, [r0, #32]                            #! EA = L0xbefff1f8; PC = 0x400acc *)
mov L0xbefff1f8 r10_b;
mov L0xbefff1fa r10_t;
(* str.w	lr, [r0], #4                              #! EA = L0xbefff1d8; PC = 0x400ad0 *)
mov L0xbefff1d8 lr_b;
mov L0xbefff1da lr_t;
(* vmov	lr, s24                                    #! PC = 0x400ad4 *)
mov lr_b s24_b;
mov lr_t s24_t;
mov lr s24;
mov zeta_lr zeta_s24;
(* #bne.w	0x400628 <ntt_fast+24>                   #! PC = 0x400adc *)
#bne.w	0x400628 <ntt_fast+24>                   #! 0x400adc = 0x400adc;

(*== layer 7+6+5+4 one slice condition ==*)

assert
  and [
    (-6) * 1664 <= L0xbefff1d8, L0xbefff1d8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1da, L0xbefff1da <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1f8, L0xbefff1f8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1fa, L0xbefff1fa <= 6 * 1664,
    (-6) * 1664 <= L0xbefff218, L0xbefff218 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff21a, L0xbefff21a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff238, L0xbefff238 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff23a, L0xbefff23a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff258, L0xbefff258 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff25a, L0xbefff25a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff278, L0xbefff278 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff27a, L0xbefff27a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff298, L0xbefff298 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff29a, L0xbefff29a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2b8, L0xbefff2b8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2ba, L0xbefff2ba <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2d8, L0xbefff2d8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2da, L0xbefff2da <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2f8, L0xbefff2f8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2fa, L0xbefff2fa <= 6 * 1664,
    (-6) * 1664 <= L0xbefff318, L0xbefff318 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff31a, L0xbefff31a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff338, L0xbefff338 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff33a, L0xbefff33a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff358, L0xbefff358 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff35a, L0xbefff35a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff378, L0xbefff378 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff37a, L0xbefff37a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff398, L0xbefff398 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff39a, L0xbefff39a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3b8, L0xbefff3b8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3ba, L0xbefff3ba <= 6 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-6)@16 * 1664@16 <=s L0xbefff1d8, L0xbefff1d8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1da, L0xbefff1da <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1f8, L0xbefff1f8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1fa, L0xbefff1fa <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff218, L0xbefff218 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff21a, L0xbefff21a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff238, L0xbefff238 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff23a, L0xbefff23a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff258, L0xbefff258 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff25a, L0xbefff25a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff278, L0xbefff278 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff27a, L0xbefff27a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff298, L0xbefff298 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff29a, L0xbefff29a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2b8, L0xbefff2b8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2ba, L0xbefff2ba <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2d8, L0xbefff2d8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2da, L0xbefff2da <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2f8, L0xbefff2f8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2fa, L0xbefff2fa <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff318, L0xbefff318 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff31a, L0xbefff31a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff338, L0xbefff338 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff33a, L0xbefff33a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff358, L0xbefff358 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff35a, L0xbefff35a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff378, L0xbefff378 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff37a, L0xbefff37a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff398, L0xbefff398 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff39a, L0xbefff39a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3b8, L0xbefff3b8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3ba, L0xbefff3ba <=s 6@16 * 1664@16
  ]
;

assume
  and [
    (-6) * 1664 <= L0xbefff1d8, L0xbefff1d8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1da, L0xbefff1da <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1f8, L0xbefff1f8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1fa, L0xbefff1fa <= 6 * 1664,
    (-6) * 1664 <= L0xbefff218, L0xbefff218 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff21a, L0xbefff21a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff238, L0xbefff238 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff23a, L0xbefff23a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff258, L0xbefff258 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff25a, L0xbefff25a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff278, L0xbefff278 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff27a, L0xbefff27a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff298, L0xbefff298 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff29a, L0xbefff29a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2b8, L0xbefff2b8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2ba, L0xbefff2ba <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2d8, L0xbefff2d8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2da, L0xbefff2da <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2f8, L0xbefff2f8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2fa, L0xbefff2fa <= 6 * 1664,
    (-6) * 1664 <= L0xbefff318, L0xbefff318 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff31a, L0xbefff31a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff338, L0xbefff338 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff33a, L0xbefff33a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff358, L0xbefff358 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff35a, L0xbefff35a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff378, L0xbefff378 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff37a, L0xbefff37a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff398, L0xbefff398 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff39a, L0xbefff39a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3b8, L0xbefff3b8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3ba, L0xbefff3ba <= 6 * 1664
  ]
  &&
  and [
    (-6)@16 * 1664@16 <=s L0xbefff1d8, L0xbefff1d8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1da, L0xbefff1da <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1f8, L0xbefff1f8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1fa, L0xbefff1fa <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff218, L0xbefff218 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff21a, L0xbefff21a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff238, L0xbefff238 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff23a, L0xbefff23a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff258, L0xbefff258 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff25a, L0xbefff25a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff278, L0xbefff278 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff27a, L0xbefff27a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff298, L0xbefff298 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff29a, L0xbefff29a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2b8, L0xbefff2b8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2ba, L0xbefff2ba <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2d8, L0xbefff2d8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2da, L0xbefff2da <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2f8, L0xbefff2f8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2fa, L0xbefff2fa <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff318, L0xbefff318 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff31a, L0xbefff31a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff338, L0xbefff338 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff33a, L0xbefff33a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff358, L0xbefff358 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff35a, L0xbefff35a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff378, L0xbefff378 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff37a, L0xbefff37a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff398, L0xbefff398 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff39a, L0xbefff39a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3b8, L0xbefff3b8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3ba, L0xbefff3ba <=s 6@16 * 1664@16
  ]
;

(* vmov	s23, r0                                    #! PC = 0x400628 *)
mov s23_b r0_b;
mov s23_t r0_t;
mov s23 r0;
mov zeta_s23 zeta_r0;
(* ldr.w	r2, [r0, #32]                             #! EA = L0xbefff1fc; Value = 0xb6fff5c8; PC = 0x40062c *)
mov r2_b L0xbefff1fc;
mov r2_t L0xbefff1fe;
(* ldr.w	r3, [r0, #96]	; 0x60                      #! EA = L0xbefff23c; Value = 0xbefff280; PC = 0x400630 *)
mov r3_b L0xbefff23c;
mov r3_t L0xbefff23e;
(* ldr.w	r4, [r0, #160]	; 0xa0                     #! EA = L0xbefff27c; Value = 0x00000000; PC = 0x400634 *)
mov r4_b L0xbefff27c;
mov r4_t L0xbefff27e;
(* ldr.w	r5, [r0, #224]	; 0xe0                     #! EA = L0xbefff2bc; Value = 0xb6fd81f5; PC = 0x400638 *)
mov r5_b L0xbefff2bc;
mov r5_t L0xbefff2be;
(* ldr.w	r6, [r0, #288]	; 0x120                    #! EA = L0xbefff2fc; Value = 0xb6fff000; PC = 0x40063c *)
mov r6_b L0xbefff2fc;
mov r6_t L0xbefff2fe;
(* ldr.w	r7, [r0, #352]	; 0x160                    #! EA = L0xbefff33c; Value = 0x00000000; PC = 0x400640 *)
mov r7_b L0xbefff33c;
mov r7_t L0xbefff33e;
(* ldr.w	r8, [r0, #416]	; 0x1a0                    #! EA = L0xbefff37c; Value = 0x00000000; PC = 0x400644 *)
mov r8_b L0xbefff37c;
mov r8_t L0xbefff37e;
(* ldr.w	r9, [r0, #480]	; 0x1e0                    #! EA = L0xbefff3bc; Value = 0x00400469; PC = 0x400648 *)
mov r9_b L0xbefff3bc;
mov r9_t L0xbefff3be;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x40064c *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* vmov	r10, s8                                    #! PC = 0x400650 *)
mov r10_b s8_b;
mov r10_t s8_t;
mov r10 s8;
mov zeta_r10 zeta_s8;
(* smulwb	lr, r10, r6                              #! PC = 0x400654 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400658 *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40065c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400660 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x400664 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x400668 *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x40066c *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x400670 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400674 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400678 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x40067c *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400680 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x400684 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400688 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x40068c *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400690 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400694 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400698 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x40069c *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x4006a0 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x4006a4 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x4006a8 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x4006ac *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4006b0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4006b4 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4006b8 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x4006bc *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x4006c0 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* vmov	r10, s9                                    #! PC = 0x4006c4 *)
mov r10_b s9_b;
mov r10_t s9_t;
mov r10 s9;
mov zeta_r10 zeta_s9;
(* vmov	r11, s10                                   #! PC = 0x4006c8 *)
mov r11_b s10_b;
mov r11_t s10_t;
mov r11 s10;
mov zeta_r11 zeta_s10;
(* smulwb	lr, r10, r4                              #! PC = 0x4006cc *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x4006d0 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4006d4 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x4006d8 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x4006dc *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x4006e0 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4006e4 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x4006e8 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x4006ec *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4006f0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x4006f4 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x4006f8 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x4006fc *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400700 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400704 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x400708 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40070c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400710 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400714 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x400718 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x40071c *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400720 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400724 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400728 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x40072c *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400730 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x400734 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400738 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* vmov	r10, s11                                   #! PC = 0x40073c *)
mov r10_b s11_b;
mov r10_t s11_t;
mov r10 s11;
mov zeta_r10 zeta_s11;
(* vmov	r11, s12                                   #! PC = 0x400740 *)
mov r11_b s12_b;
mov r11_t s12_t;
mov r11 s12;
mov zeta_r11 zeta_s12;
(* smulwb	lr, r10, r3                              #! PC = 0x400744 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x400748 *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40074c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400750 *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x400754 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x400758 *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x40075c *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x400760 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x400764 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400768 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x40076c *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400770 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x400774 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400778 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* vmov	r10, s13                                   #! PC = 0x40077c *)
mov r10_b s13_b;
mov r10_t s13_t;
mov r10 s13;
mov zeta_r10 zeta_s13;
(* vmov	r11, s14                                   #! PC = 0x400780 *)
mov r11_b s14_b;
mov r11_t s14_t;
mov r11 s14;
mov zeta_r11 zeta_s14;
(* smulwb	lr, r10, r7                              #! PC = 0x400784 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400788 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40078c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400790 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400794 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400798 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x40079c *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4007a0 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x4007a4 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007a8 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4007ac *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4007b0 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x4007b4 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x4007b8 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

(*== 8-NTT on a1, a3, ..., a15 ==*)
(*== _3_layer_double_CT_16_plant_fp END ==*)

assert
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

assume
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

(* vmov	r10, s15                                   #! PC = 0x4007bc *)
mov r10_b s15_b;
mov r10_t s15_t;
mov r10 s15;
mov zeta_r10 zeta_s15;
(* vmov	r11, s16                                   #! PC = 0x4007c0 *)
mov r11_b s16_b;
mov r11_t s16_t;
mov r11 s16;
mov zeta_r11 zeta_s16;
(* smulwb	lr, r10, r2                              #! PC = 0x4007c4 *)
cast r2_lsb@sint32 r2_b;
mull tmp_t tmp_b r10 r2_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r2_b;
assert A_lb = r2_b && true;
assume A_lb = r2_b && true;
cast A_lt@sint32 r2_t;
assert A_lt = r2_t && true;
assume A_lt = r2_t && true;
mov zeta zeta_r10;
(* smulwt	r2, r10, r2                              #! PC = 0x4007c8 *)
cast r2_lst@sint32 r2_t;
mull tmp_t tmp_b r10 r2_lst;
spl dontcare r2_t tmp_t 16;
spl r2_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007cc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r2, r2, r12, r0                          #! PC = 0x4007d0 *)
cast r2_sb@sint16 r2_b;
mull tmp_t tmp_b r2_sb r12_t;
uadds carry r2_b tmp_b r0_b;
adc r2_t tmp_t r0_t carry;
(* pkhtb	r2, r2, lr, asr #16                       #! PC = 0x4007d4 *)
mov tmp_b lr_t;
mov tmp_t r2_t;
mov r2_b tmp_b;
mov r2_t tmp_t;
cast C_lb@sint32 r2_b;
assert C_lb = r2_b && true;
assume C_lb = r2_b && true;
cast C_lt@sint32 r2_t;
assert C_lt = r2_t && true;
assume C_lt = r2_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r3                              #! PC = 0x4007d8 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r11 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r11;
(* smulwt	r3, r11, r3                              #! PC = 0x4007dc *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r11 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007e0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4007e4 *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x4007e8 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov r3_b tmp_b;
mov r3_t tmp_t;
cast C_lb@sint32 r3_b;
assert C_lb = r3_b && true;
assume C_lb = r3_b && true;
cast C_lt@sint32 r3_t;
assert C_lt = r3_t && true;
assume C_lt = r3_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	r10, s17                                   #! PC = 0x4007ec *)
mov r10_b s17_b;
mov r10_t s17_t;
mov r10 s17;
mov zeta_r10 zeta_s17;
(* vmov	r11, s18                                   #! PC = 0x4007f0 *)
mov r11_b s18_b;
mov r11_t s18_t;
mov r11 s18;
mov zeta_r11 zeta_s18;
(* smulwb	lr, r10, r4                              #! PC = 0x4007f4 *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x4007f8 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007fc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400800 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x400804 *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov r4_b tmp_b;
mov r4_t tmp_t;
cast C_lb@sint32 r4_b;
assert C_lb = r4_b && true;
assume C_lb = r4_b && true;
cast C_lt@sint32 r4_t;
assert C_lt = r4_t && true;
assume C_lt = r4_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r5                              #! PC = 0x400808 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x40080c *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400810 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400814 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x400818 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov r5_b tmp_b;
mov r5_t tmp_t;
cast C_lb@sint32 r5_b;
assert C_lb = r5_b && true;
assume C_lb = r5_b && true;
cast C_lt@sint32 r5_t;
assert C_lt = r5_t && true;
assume C_lt = r5_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	r10, s19                                   #! PC = 0x40081c *)
mov r10_b s19_b;
mov r10_t s19_t;
mov r10 s19;
mov zeta_r10 zeta_s19;
(* vmov	r11, s20                                   #! PC = 0x400820 *)
mov r11_b s20_b;
mov r11_t s20_t;
mov r11 s20;
mov zeta_r11 zeta_s20;
(* smulwb	lr, r10, r6                              #! PC = 0x400824 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400828 *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40082c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400830 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x400834 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov r6_b tmp_b;
mov r6_t tmp_t;
cast C_lb@sint32 r6_b;
assert C_lb = r6_b && true;
assume C_lb = r6_b && true;
cast C_lt@sint32 r6_t;
assert C_lt = r6_t && true;
assume C_lt = r6_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r7                              #! PC = 0x400838 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r11 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r11;
(* smulwt	r7, r11, r7                              #! PC = 0x40083c *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r11 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400840 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400844 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x400848 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov r7_b tmp_b;
mov r7_t tmp_t;
cast C_lb@sint32 r7_b;
assert C_lb = r7_b && true;
assume C_lb = r7_b && true;
cast C_lt@sint32 r7_t;
assert C_lt = r7_t && true;
assume C_lt = r7_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	r10, s21                                   #! PC = 0x40084c *)
mov r10_b s21_b;
mov r10_t s21_t;
mov r10 s21;
mov zeta_r10 zeta_s21;
(* vmov	r11, s22                                   #! PC = 0x400850 *)
mov r11_b s22_b;
mov r11_t s22_t;
mov r11 s22;
mov zeta_r11 zeta_s22;
(* smulwb	lr, r10, r8                              #! PC = 0x400854 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400858 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40085c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400860 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x400864 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov r8_b tmp_b;
mov r8_t tmp_t;
cast C_lb@sint32 r8_b;
assert C_lb = r8_b && true;
assume C_lb = r8_b && true;
cast C_lt@sint32 r8_t;
assert C_lt = r8_t && true;
assume C_lt = r8_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r9                              #! PC = 0x400868 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x40086c *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400870 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400874 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x400878 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov r9_b tmp_b;
mov r9_t tmp_t;
cast C_lb@sint32 r9_b;
assert C_lb = r9_b && true;
assume C_lb = r9_b && true;
cast C_lt@sint32 r9_t;
assert C_lt = r9_t && true;
assume C_lt = r9_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	s0, r2                                     #! PC = 0x40087c *)
mov s0_b r2_b;
mov s0_t r2_t;
mov s0 r2;
mov zeta_s0 zeta_r2;
(* vmov	s1, r3                                     #! PC = 0x400880 *)
mov s1_b r3_b;
mov s1_t r3_t;
mov s1 r3;
mov zeta_s1 zeta_r3;
(* vmov	s2, r4                                     #! PC = 0x400884 *)
mov s2_b r4_b;
mov s2_t r4_t;
mov s2 r4;
mov zeta_s2 zeta_r4;
(* vmov	s3, r5                                     #! PC = 0x400888 *)
mov s3_b r5_b;
mov s3_t r5_t;
mov s3 r5;
mov zeta_s3 zeta_r5;
(* vmov	s4, r6                                     #! PC = 0x40088c *)
mov s4_b r6_b;
mov s4_t r6_t;
mov s4 r6;
mov zeta_s4 zeta_r6;
(* vmov	s5, r7                                     #! PC = 0x400890 *)
mov s5_b r7_b;
mov s5_t r7_t;
mov s5 r7;
mov zeta_s5 zeta_r7;
(* vmov	s6, r8                                     #! PC = 0x400894 *)
mov s6_b r8_b;
mov s6_t r8_t;
mov s6 r8;
mov zeta_s6 zeta_r8;
(* vmov	s7, r9                                     #! PC = 0x400898 *)
mov s7_b r9_b;
mov s7_t r9_t;
mov s7 r9;
mov zeta_s7 zeta_r9;

(*== coeffs multiplied by twiddles END ==*)

assert
  and [
    (-1) * 1664 <= s0_b, s0_b <= 1 * 1664,
    (-1) * 1664 <= s0_t, s0_t <= 1 * 1664,
    (-1) * 1664 <= s1_b, s1_b <= 1 * 1664,
    (-1) * 1664 <= s1_t, s1_t <= 1 * 1664,
    (-1) * 1664 <= s2_b, s2_b <= 1 * 1664,
    (-1) * 1664 <= s2_t, s2_t <= 1 * 1664,
    (-1) * 1664 <= s3_b, s3_b <= 1 * 1664,
    (-1) * 1664 <= s3_t, s3_t <= 1 * 1664,
    (-1) * 1664 <= s4_b, s4_b <= 1 * 1664,
    (-1) * 1664 <= s4_t, s4_t <= 1 * 1664,
    (-1) * 1664 <= s5_b, s5_b <= 1 * 1664,
    (-1) * 1664 <= s5_t, s5_t <= 1 * 1664,
    (-1) * 1664 <= s6_b, s6_b <= 1 * 1664,
    (-1) * 1664 <= s6_t, s6_t <= 1 * 1664,
    (-1) * 1664 <= s7_b, s7_b <= 1 * 1664,
    (-1) * 1664 <= s7_t, s7_t <= 1 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-1)@16 * 1664@16 <=s s0_b, s0_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s0_t, s0_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_b, s1_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_t, s1_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_b, s2_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_t, s2_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_b, s3_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_t, s3_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_b, s4_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_t, s4_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_b, s5_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_t, s5_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_b, s6_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_t, s6_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_b, s7_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_t, s7_t <=s 1@16 * 1664@16
  ];

assume
  and [
    (-1) * 1664 <= s0_b, s0_b <= 1 * 1664,
    (-1) * 1664 <= s0_t, s0_t <= 1 * 1664,
    (-1) * 1664 <= s1_b, s1_b <= 1 * 1664,
    (-1) * 1664 <= s1_t, s1_t <= 1 * 1664,
    (-1) * 1664 <= s2_b, s2_b <= 1 * 1664,
    (-1) * 1664 <= s2_t, s2_t <= 1 * 1664,
    (-1) * 1664 <= s3_b, s3_b <= 1 * 1664,
    (-1) * 1664 <= s3_t, s3_t <= 1 * 1664,
    (-1) * 1664 <= s4_b, s4_b <= 1 * 1664,
    (-1) * 1664 <= s4_t, s4_t <= 1 * 1664,
    (-1) * 1664 <= s5_b, s5_b <= 1 * 1664,
    (-1) * 1664 <= s5_t, s5_t <= 1 * 1664,
    (-1) * 1664 <= s6_b, s6_b <= 1 * 1664,
    (-1) * 1664 <= s6_t, s6_t <= 1 * 1664,
    (-1) * 1664 <= s7_b, s7_b <= 1 * 1664,
    (-1) * 1664 <= s7_t, s7_t <= 1 * 1664
  ]
  &&
  and [
    (-1)@16 * 1664@16 <=s s0_b, s0_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s0_t, s0_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_b, s1_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_t, s1_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_b, s2_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_t, s2_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_b, s3_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_t, s3_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_b, s4_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_t, s4_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_b, s5_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_t, s5_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_b, s6_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_t, s6_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_b, s7_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_t, s7_t <=s 1@16 * 1664@16
  ];

(* vmov	r0, s23                                    #! PC = 0x40089c *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff1dc; Value = 0x00000003; PC = 0x4008a0 *)
mov r2_b L0xbefff1dc;
mov r2_t L0xbefff1de;
(* ldr.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff21c; Value = 0xb6fff070; PC = 0x4008a4 *)
mov r3_b L0xbefff21c;
mov r3_t L0xbefff21e;
(* ldr.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff25c; Value = 0x00000000; PC = 0x4008a8 *)
mov r4_b L0xbefff25c;
mov r4_t L0xbefff25e;
(* ldr.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff29c; Value = 0x00000011; PC = 0x4008ac *)
mov r5_b L0xbefff29c;
mov r5_t L0xbefff29e;
(* ldr.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2dc; Value = 0xb6fff070; PC = 0x4008b0 *)
mov r6_b L0xbefff2dc;
mov r6_t L0xbefff2de;
(* ldr.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff31c; Value = 0x00000000; PC = 0x4008b4 *)
mov r7_b L0xbefff31c;
mov r7_t L0xbefff31e;
(* ldr.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff35c; Value = 0x00000000; PC = 0x4008b8 *)
mov r8_b L0xbefff35c;
mov r8_t L0xbefff35e;
(* ldr.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff39c; Value = 0x00000000; PC = 0x4008bc *)
mov r9_b L0xbefff39c;
mov r9_t L0xbefff39e;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x4008c0 *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* vmov	r10, s8                                    #! PC = 0x4008c4 *)
mov r10_b s8_b;
mov r10_t s8_t;
mov r10 s8;
mov zeta_r10 zeta_s8;
(* smulwb	lr, r10, r6                              #! PC = 0x4008c8 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x4008cc *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4008d0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4008d4 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x4008d8 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x4008dc *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4008e0 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x4008e4 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x4008e8 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4008ec *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4008f0 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x4008f4 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x4008f8 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x4008fc *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x400900 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400904 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400908 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x40090c *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400910 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x400914 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400918 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x40091c *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x400920 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400924 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400928 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x40092c *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x400930 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400934 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* vmov	r10, s9                                    #! PC = 0x400938 *)
mov r10_b s9_b;
mov r10_t s9_t;
mov r10 s9;
mov zeta_r10 zeta_s9;
(* vmov	r11, s10                                   #! PC = 0x40093c *)
mov r11_b s10_b;
mov r11_t s10_t;
mov r11 s10;
mov zeta_r11 zeta_s10;
(* smulwb	lr, r10, r4                              #! PC = 0x400940 *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x400944 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400948 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x40094c *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400950 *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x400954 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400958 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x40095c *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x400960 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400964 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400968 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x40096c *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x400970 *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400974 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400978 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x40097c *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400980 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400984 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400988 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x40098c *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400990 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400994 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400998 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40099c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4009a0 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4009a4 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x4009a8 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x4009ac *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* vmov	r10, s11                                   #! PC = 0x4009b0 *)
mov r10_b s11_b;
mov r10_t s11_t;
mov r10 s11;
mov zeta_r10 zeta_s11;
(* vmov	r11, s12                                   #! PC = 0x4009b4 *)
mov r11_b s12_b;
mov r11_t s12_t;
mov r11 s12;
mov zeta_r11 zeta_s12;
(* smulwb	lr, r10, r3                              #! PC = 0x4009b8 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x4009bc *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4009c0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4009c4 *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x4009c8 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x4009cc *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4009d0 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x4009d4 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x4009d8 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4009dc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x4009e0 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x4009e4 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x4009e8 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x4009ec *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* vmov	r10, s13                                   #! PC = 0x4009f0 *)
mov r10_b s13_b;
mov r10_t s13_t;
mov r10 s13;
mov zeta_r10 zeta_s13;
(* vmov	r11, s14                                   #! PC = 0x4009f4 *)
mov r11_b s14_b;
mov r11_t s14_t;
mov r11 s14;
mov zeta_r11 zeta_s14;
(* smulwb	lr, r10, r7                              #! PC = 0x4009f8 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x4009fc *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400a00 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400a04 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400a08 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400a0c *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400a10 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400a14 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400a18 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400a1c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400a20 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400a24 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x400a28 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x400a2c *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

(*== 8-NTT on a0, a2, ..., a14 ==*)
(*== _3_layer_double_CT_16_plant_fp END ==*)

assert
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

assume
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

(* vmov	r0, s23                                    #! PC = 0x400a30 *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* vmov	r10, s1                                    #! PC = 0x400a34 *)
mov r10_b s1_b;
mov r10_t s1_t;
mov r10 s1;
mov zeta_r10 zeta_s1;
(* uadd16	lr, r3, r10                              #! PC = 0x400a38 *)
add lr_b r3_b r10_b;
add lr_t r3_t r10_t;
(* usub16	r3, r3, r10                              #! PC = 0x400a3c *)
sub r3_b r3_b r10_b;
sub r3_t r3_t r10_t;
(* str.w	lr, [r0, #64]	; 0x40                      #! EA = L0xbefff21c; PC = 0x400a40 *)
mov L0xbefff21c lr_b;
mov L0xbefff21e lr_t;
(* str.w	r3, [r0, #96]	; 0x60                      #! EA = L0xbefff23c; PC = 0x400a44 *)
mov L0xbefff23c r3_b;
mov L0xbefff23e r3_t;
(* vmov	r10, s3                                    #! PC = 0x400a48 *)
mov r10_b s3_b;
mov r10_t s3_t;
mov r10 s3;
mov zeta_r10 zeta_s3;
(* uadd16	lr, r5, r10                              #! PC = 0x400a4c *)
add lr_b r5_b r10_b;
add lr_t r5_t r10_t;
(* usub16	r5, r5, r10                              #! PC = 0x400a50 *)
sub r5_b r5_b r10_b;
sub r5_t r5_t r10_t;
(* str.w	lr, [r0, #192]	; 0xc0                     #! EA = L0xbefff29c; PC = 0x400a54 *)
mov L0xbefff29c lr_b;
mov L0xbefff29e lr_t;
(* str.w	r5, [r0, #224]	; 0xe0                     #! EA = L0xbefff2bc; PC = 0x400a58 *)
mov L0xbefff2bc r5_b;
mov L0xbefff2be r5_t;
(* vmov	r10, s5                                    #! PC = 0x400a5c *)
mov r10_b s5_b;
mov r10_t s5_t;
mov r10 s5;
mov zeta_r10 zeta_s5;
(* uadd16	lr, r7, r10                              #! PC = 0x400a60 *)
add lr_b r7_b r10_b;
add lr_t r7_t r10_t;
(* usub16	r7, r7, r10                              #! PC = 0x400a64 *)
sub r7_b r7_b r10_b;
sub r7_t r7_t r10_t;
(* str.w	lr, [r0, #320]	; 0x140                    #! EA = L0xbefff31c; PC = 0x400a68 *)
mov L0xbefff31c lr_b;
mov L0xbefff31e lr_t;
(* str.w	r7, [r0, #352]	; 0x160                    #! EA = L0xbefff33c; PC = 0x400a6c *)
mov L0xbefff33c r7_b;
mov L0xbefff33e r7_t;
(* vmov	r10, s7                                    #! PC = 0x400a70 *)
mov r10_b s7_b;
mov r10_t s7_t;
mov r10 s7;
mov zeta_r10 zeta_s7;
(* uadd16	lr, r9, r10                              #! PC = 0x400a74 *)
add lr_b r9_b r10_b;
add lr_t r9_t r10_t;
(* usub16	r9, r9, r10                              #! PC = 0x400a78 *)
sub r9_b r9_b r10_b;
sub r9_t r9_t r10_t;
(* str.w	lr, [r0, #448]	; 0x1c0                    #! EA = L0xbefff39c; PC = 0x400a7c *)
mov L0xbefff39c lr_b;
mov L0xbefff39e lr_t;
(* str.w	r9, [r0, #480]	; 0x1e0                    #! EA = L0xbefff3bc; PC = 0x400a80 *)
mov L0xbefff3bc r9_b;
mov L0xbefff3be r9_t;
(* vmov	r5, s2                                     #! PC = 0x400a84 *)
mov r5_b s2_b;
mov r5_t s2_t;
mov r5 s2;
mov zeta_r5 zeta_s2;
(* uadd16	lr, r4, r5                               #! PC = 0x400a88 *)
add lr_b r4_b r5_b;
add lr_t r4_t r5_t;
(* usub16	r10, r4, r5                              #! PC = 0x400a8c *)
sub r10_b r4_b r5_b;
sub r10_t r4_t r5_t;
(* str.w	lr, [r0, #128]	; 0x80                     #! EA = L0xbefff25c; PC = 0x400a90 *)
mov L0xbefff25c lr_b;
mov L0xbefff25e lr_t;
(* str.w	r10, [r0, #160]	; 0xa0                    #! EA = L0xbefff27c; PC = 0x400a94 *)
mov L0xbefff27c r10_b;
mov L0xbefff27e r10_t;
(* vmov	r7, s4                                     #! PC = 0x400a98 *)
mov r7_b s4_b;
mov r7_t s4_t;
mov r7 s4;
mov zeta_r7 zeta_s4;
(* uadd16	lr, r6, r7                               #! PC = 0x400a9c *)
add lr_b r6_b r7_b;
add lr_t r6_t r7_t;
(* usub16	r10, r6, r7                              #! PC = 0x400aa0 *)
sub r10_b r6_b r7_b;
sub r10_t r6_t r7_t;
(* str.w	lr, [r0, #256]	; 0x100                    #! EA = L0xbefff2dc; PC = 0x400aa4 *)
mov L0xbefff2dc lr_b;
mov L0xbefff2de lr_t;
(* str.w	r10, [r0, #288]	; 0x120                   #! EA = L0xbefff2fc; PC = 0x400aa8 *)
mov L0xbefff2fc r10_b;
mov L0xbefff2fe r10_t;
(* vmov	r9, s6                                     #! PC = 0x400aac *)
mov r9_b s6_b;
mov r9_t s6_t;
mov r9 s6;
mov zeta_r9 zeta_s6;
(* uadd16	lr, r8, r9                               #! PC = 0x400ab0 *)
add lr_b r8_b r9_b;
add lr_t r8_t r9_t;
(* usub16	r10, r8, r9                              #! PC = 0x400ab4 *)
sub r10_b r8_b r9_b;
sub r10_t r8_t r9_t;
(* str.w	lr, [r0, #384]	; 0x180                    #! EA = L0xbefff35c; PC = 0x400ab8 *)
mov L0xbefff35c lr_b;
mov L0xbefff35e lr_t;
(* str.w	r10, [r0, #416]	; 0x1a0                   #! EA = L0xbefff37c; PC = 0x400abc *)
mov L0xbefff37c r10_b;
mov L0xbefff37e r10_t;
(* vmov	r3, s0                                     #! PC = 0x400ac0 *)
mov r3_b s0_b;
mov r3_t s0_t;
mov r3 s0;
mov zeta_r3 zeta_s0;
(* uadd16	lr, r2, r3                               #! PC = 0x400ac4 *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* usub16	r10, r2, r3                              #! PC = 0x400ac8 *)
sub r10_b r2_b r3_b;
sub r10_t r2_t r3_t;
(* str.w	r10, [r0, #32]                            #! EA = L0xbefff1fc; PC = 0x400acc *)
mov L0xbefff1fc r10_b;
mov L0xbefff1fe r10_t;
(* str.w	lr, [r0], #4                              #! EA = L0xbefff1dc; PC = 0x400ad0 *)
mov L0xbefff1dc lr_b;
mov L0xbefff1de lr_t;
(* vmov	lr, s24                                    #! PC = 0x400ad4 *)
mov lr_b s24_b;
mov lr_t s24_t;
mov lr s24;
mov zeta_lr zeta_s24;
(* #bne.w	0x400628 <ntt_fast+24>                   #! PC = 0x400adc *)
#bne.w	0x400628 <ntt_fast+24>                   #! 0x400adc = 0x400adc;

(*== layer 7+6+5+4 one slice condition ==*)

assert
  and [
    (-6) * 1664 <= L0xbefff1dc, L0xbefff1dc <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1de, L0xbefff1de <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1fc, L0xbefff1fc <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1fe, L0xbefff1fe <= 6 * 1664,
    (-6) * 1664 <= L0xbefff21c, L0xbefff21c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff21e, L0xbefff21e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff23c, L0xbefff23c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff23e, L0xbefff23e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff25c, L0xbefff25c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff25e, L0xbefff25e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff27c, L0xbefff27c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff27e, L0xbefff27e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff29c, L0xbefff29c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff29e, L0xbefff29e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2bc, L0xbefff2bc <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2be, L0xbefff2be <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2dc, L0xbefff2dc <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2de, L0xbefff2de <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2fc, L0xbefff2fc <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2fe, L0xbefff2fe <= 6 * 1664,
    (-6) * 1664 <= L0xbefff31c, L0xbefff31c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff31e, L0xbefff31e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff33c, L0xbefff33c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff33e, L0xbefff33e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff35c, L0xbefff35c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff35e, L0xbefff35e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff37c, L0xbefff37c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff37e, L0xbefff37e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff39c, L0xbefff39c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff39e, L0xbefff39e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3bc, L0xbefff3bc <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3be, L0xbefff3be <= 6 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-6)@16 * 1664@16 <=s L0xbefff1dc, L0xbefff1dc <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1de, L0xbefff1de <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1fc, L0xbefff1fc <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1fe, L0xbefff1fe <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff21c, L0xbefff21c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff21e, L0xbefff21e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff23c, L0xbefff23c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff23e, L0xbefff23e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff25c, L0xbefff25c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff25e, L0xbefff25e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff27c, L0xbefff27c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff27e, L0xbefff27e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff29c, L0xbefff29c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff29e, L0xbefff29e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2bc, L0xbefff2bc <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2be, L0xbefff2be <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2dc, L0xbefff2dc <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2de, L0xbefff2de <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2fc, L0xbefff2fc <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2fe, L0xbefff2fe <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff31c, L0xbefff31c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff31e, L0xbefff31e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff33c, L0xbefff33c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff33e, L0xbefff33e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff35c, L0xbefff35c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff35e, L0xbefff35e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff37c, L0xbefff37c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff37e, L0xbefff37e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff39c, L0xbefff39c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff39e, L0xbefff39e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3bc, L0xbefff3bc <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3be, L0xbefff3be <=s 6@16 * 1664@16
  ]
;

assume
  and [
    (-6) * 1664 <= L0xbefff1dc, L0xbefff1dc <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1de, L0xbefff1de <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1fc, L0xbefff1fc <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1fe, L0xbefff1fe <= 6 * 1664,
    (-6) * 1664 <= L0xbefff21c, L0xbefff21c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff21e, L0xbefff21e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff23c, L0xbefff23c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff23e, L0xbefff23e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff25c, L0xbefff25c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff25e, L0xbefff25e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff27c, L0xbefff27c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff27e, L0xbefff27e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff29c, L0xbefff29c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff29e, L0xbefff29e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2bc, L0xbefff2bc <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2be, L0xbefff2be <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2dc, L0xbefff2dc <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2de, L0xbefff2de <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2fc, L0xbefff2fc <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2fe, L0xbefff2fe <= 6 * 1664,
    (-6) * 1664 <= L0xbefff31c, L0xbefff31c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff31e, L0xbefff31e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff33c, L0xbefff33c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff33e, L0xbefff33e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff35c, L0xbefff35c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff35e, L0xbefff35e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff37c, L0xbefff37c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff37e, L0xbefff37e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff39c, L0xbefff39c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff39e, L0xbefff39e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3bc, L0xbefff3bc <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3be, L0xbefff3be <= 6 * 1664
  ]
  &&
  and [
    (-6)@16 * 1664@16 <=s L0xbefff1dc, L0xbefff1dc <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1de, L0xbefff1de <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1fc, L0xbefff1fc <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1fe, L0xbefff1fe <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff21c, L0xbefff21c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff21e, L0xbefff21e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff23c, L0xbefff23c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff23e, L0xbefff23e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff25c, L0xbefff25c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff25e, L0xbefff25e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff27c, L0xbefff27c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff27e, L0xbefff27e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff29c, L0xbefff29c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff29e, L0xbefff29e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2bc, L0xbefff2bc <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2be, L0xbefff2be <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2dc, L0xbefff2dc <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2de, L0xbefff2de <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2fc, L0xbefff2fc <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2fe, L0xbefff2fe <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff31c, L0xbefff31c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff31e, L0xbefff31e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff33c, L0xbefff33c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff33e, L0xbefff33e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff35c, L0xbefff35c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff35e, L0xbefff35e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff37c, L0xbefff37c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff37e, L0xbefff37e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff39c, L0xbefff39c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff39e, L0xbefff39e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3bc, L0xbefff3bc <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3be, L0xbefff3be <=s 6@16 * 1664@16
  ]
;

(* vmov	s23, r0                                    #! PC = 0x400628 *)
mov s23_b r0_b;
mov s23_t r0_t;
mov s23 r0;
mov zeta_s23 zeta_r0;
(* ldr.w	r2, [r0, #32]                             #! EA = L0xbefff200; Value = 0x000f0002; PC = 0x40062c *)
mov r2_b L0xbefff200;
mov r2_t L0xbefff202;
(* ldr.w	r3, [r0, #96]	; 0x60                      #! EA = L0xbefff240; Value = 0xbefff280; PC = 0x400630 *)
mov r3_b L0xbefff240;
mov r3_t L0xbefff242;
(* ldr.w	r4, [r0, #160]	; 0xa0                     #! EA = L0xbefff280; Value = 0x00000000; PC = 0x400634 *)
mov r4_b L0xbefff280;
mov r4_t L0xbefff282;
(* ldr.w	r5, [r0, #224]	; 0xe0                     #! EA = L0xbefff2c0; Value = 0x00000000; PC = 0x400638 *)
mov r5_b L0xbefff2c0;
mov r5_t L0xbefff2c2;
(* ldr.w	r6, [r0, #288]	; 0x120                    #! EA = L0xbefff300; Value = 0xbefff330; PC = 0x40063c *)
mov r6_b L0xbefff300;
mov r6_t L0xbefff302;
(* ldr.w	r7, [r0, #352]	; 0x160                    #! EA = L0xbefff340; Value = 0x00000000; PC = 0x400640 *)
mov r7_b L0xbefff340;
mov r7_t L0xbefff342;
(* ldr.w	r8, [r0, #416]	; 0x1a0                    #! EA = L0xbefff380; Value = 0x00000000; PC = 0x400644 *)
mov r8_b L0xbefff380;
mov r8_t L0xbefff382;
(* ldr.w	r9, [r0, #480]	; 0x1e0                    #! EA = L0xbefff3c0; Value = 0x00000000; PC = 0x400648 *)
mov r9_b L0xbefff3c0;
mov r9_t L0xbefff3c2;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x40064c *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* vmov	r10, s8                                    #! PC = 0x400650 *)
mov r10_b s8_b;
mov r10_t s8_t;
mov r10 s8;
mov zeta_r10 zeta_s8;
(* smulwb	lr, r10, r6                              #! PC = 0x400654 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400658 *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40065c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400660 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x400664 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x400668 *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x40066c *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x400670 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400674 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400678 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x40067c *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400680 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x400684 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400688 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x40068c *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400690 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400694 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400698 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x40069c *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x4006a0 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x4006a4 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x4006a8 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x4006ac *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4006b0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4006b4 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4006b8 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x4006bc *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x4006c0 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* vmov	r10, s9                                    #! PC = 0x4006c4 *)
mov r10_b s9_b;
mov r10_t s9_t;
mov r10 s9;
mov zeta_r10 zeta_s9;
(* vmov	r11, s10                                   #! PC = 0x4006c8 *)
mov r11_b s10_b;
mov r11_t s10_t;
mov r11 s10;
mov zeta_r11 zeta_s10;
(* smulwb	lr, r10, r4                              #! PC = 0x4006cc *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x4006d0 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4006d4 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x4006d8 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x4006dc *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x4006e0 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4006e4 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x4006e8 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x4006ec *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4006f0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x4006f4 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x4006f8 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x4006fc *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400700 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400704 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x400708 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40070c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400710 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400714 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x400718 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x40071c *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400720 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400724 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400728 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x40072c *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400730 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x400734 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400738 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* vmov	r10, s11                                   #! PC = 0x40073c *)
mov r10_b s11_b;
mov r10_t s11_t;
mov r10 s11;
mov zeta_r10 zeta_s11;
(* vmov	r11, s12                                   #! PC = 0x400740 *)
mov r11_b s12_b;
mov r11_t s12_t;
mov r11 s12;
mov zeta_r11 zeta_s12;
(* smulwb	lr, r10, r3                              #! PC = 0x400744 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x400748 *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40074c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400750 *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x400754 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x400758 *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x40075c *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x400760 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x400764 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400768 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x40076c *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400770 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x400774 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400778 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* vmov	r10, s13                                   #! PC = 0x40077c *)
mov r10_b s13_b;
mov r10_t s13_t;
mov r10 s13;
mov zeta_r10 zeta_s13;
(* vmov	r11, s14                                   #! PC = 0x400780 *)
mov r11_b s14_b;
mov r11_t s14_t;
mov r11 s14;
mov zeta_r11 zeta_s14;
(* smulwb	lr, r10, r7                              #! PC = 0x400784 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400788 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40078c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400790 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400794 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400798 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x40079c *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x4007a0 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x4007a4 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007a8 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4007ac *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4007b0 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x4007b4 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x4007b8 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

(*== 8-NTT on a1, a3, ..., a15 ==*)
(*== _3_layer_double_CT_16_plant_fp END ==*)

assert
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

assume
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

(* vmov	r10, s15                                   #! PC = 0x4007bc *)
mov r10_b s15_b;
mov r10_t s15_t;
mov r10 s15;
mov zeta_r10 zeta_s15;
(* vmov	r11, s16                                   #! PC = 0x4007c0 *)
mov r11_b s16_b;
mov r11_t s16_t;
mov r11 s16;
mov zeta_r11 zeta_s16;
(* smulwb	lr, r10, r2                              #! PC = 0x4007c4 *)
cast r2_lsb@sint32 r2_b;
mull tmp_t tmp_b r10 r2_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r2_b;
assert A_lb = r2_b && true;
assume A_lb = r2_b && true;
cast A_lt@sint32 r2_t;
assert A_lt = r2_t && true;
assume A_lt = r2_t && true;
mov zeta zeta_r10;
(* smulwt	r2, r10, r2                              #! PC = 0x4007c8 *)
cast r2_lst@sint32 r2_t;
mull tmp_t tmp_b r10 r2_lst;
spl dontcare r2_t tmp_t 16;
spl r2_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007cc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r2, r2, r12, r0                          #! PC = 0x4007d0 *)
cast r2_sb@sint16 r2_b;
mull tmp_t tmp_b r2_sb r12_t;
uadds carry r2_b tmp_b r0_b;
adc r2_t tmp_t r0_t carry;
(* pkhtb	r2, r2, lr, asr #16                       #! PC = 0x4007d4 *)
mov tmp_b lr_t;
mov tmp_t r2_t;
mov r2_b tmp_b;
mov r2_t tmp_t;
cast C_lb@sint32 r2_b;
assert C_lb = r2_b && true;
assume C_lb = r2_b && true;
cast C_lt@sint32 r2_t;
assert C_lt = r2_t && true;
assume C_lt = r2_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r3                              #! PC = 0x4007d8 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r11 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r11;
(* smulwt	r3, r11, r3                              #! PC = 0x4007dc *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r11 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007e0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4007e4 *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	r3, r3, lr, asr #16                       #! PC = 0x4007e8 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov r3_b tmp_b;
mov r3_t tmp_t;
cast C_lb@sint32 r3_b;
assert C_lb = r3_b && true;
assume C_lb = r3_b && true;
cast C_lt@sint32 r3_t;
assert C_lt = r3_t && true;
assume C_lt = r3_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	r10, s17                                   #! PC = 0x4007ec *)
mov r10_b s17_b;
mov r10_t s17_t;
mov r10 s17;
mov zeta_r10 zeta_s17;
(* vmov	r11, s18                                   #! PC = 0x4007f0 *)
mov r11_b s18_b;
mov r11_t s18_t;
mov r11 s18;
mov zeta_r11 zeta_s18;
(* smulwb	lr, r10, r4                              #! PC = 0x4007f4 *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x4007f8 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4007fc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400800 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	r4, r4, lr, asr #16                       #! PC = 0x400804 *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov r4_b tmp_b;
mov r4_t tmp_t;
cast C_lb@sint32 r4_b;
assert C_lb = r4_b && true;
assume C_lb = r4_b && true;
cast C_lt@sint32 r4_t;
assert C_lt = r4_t && true;
assume C_lt = r4_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r5                              #! PC = 0x400808 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x40080c *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400810 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400814 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	r5, r5, lr, asr #16                       #! PC = 0x400818 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov r5_b tmp_b;
mov r5_t tmp_t;
cast C_lb@sint32 r5_b;
assert C_lb = r5_b && true;
assume C_lb = r5_b && true;
cast C_lt@sint32 r5_t;
assert C_lt = r5_t && true;
assume C_lt = r5_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	r10, s19                                   #! PC = 0x40081c *)
mov r10_b s19_b;
mov r10_t s19_t;
mov r10 s19;
mov zeta_r10 zeta_s19;
(* vmov	r11, s20                                   #! PC = 0x400820 *)
mov r11_b s20_b;
mov r11_t s20_t;
mov r11 s20;
mov zeta_r11 zeta_s20;
(* smulwb	lr, r10, r6                              #! PC = 0x400824 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400828 *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40082c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400830 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	r6, r6, lr, asr #16                       #! PC = 0x400834 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov r6_b tmp_b;
mov r6_t tmp_t;
cast C_lb@sint32 r6_b;
assert C_lb = r6_b && true;
assume C_lb = r6_b && true;
cast C_lt@sint32 r6_t;
assert C_lt = r6_t && true;
assume C_lt = r6_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r7                              #! PC = 0x400838 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r11 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r11;
(* smulwt	r7, r11, r7                              #! PC = 0x40083c *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r11 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400840 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400844 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	r7, r7, lr, asr #16                       #! PC = 0x400848 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov r7_b tmp_b;
mov r7_t tmp_t;
cast C_lb@sint32 r7_b;
assert C_lb = r7_b && true;
assume C_lb = r7_b && true;
cast C_lt@sint32 r7_t;
assert C_lt = r7_t && true;
assume C_lt = r7_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	r10, s21                                   #! PC = 0x40084c *)
mov r10_b s21_b;
mov r10_t s21_t;
mov r10 s21;
mov zeta_r10 zeta_s21;
(* vmov	r11, s22                                   #! PC = 0x400850 *)
mov r11_b s22_b;
mov r11_t s22_t;
mov r11 s22;
mov zeta_r11 zeta_s22;
(* smulwb	lr, r10, r8                              #! PC = 0x400854 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400858 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40085c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400860 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	r8, r8, lr, asr #16                       #! PC = 0x400864 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov r8_b tmp_b;
mov r8_t tmp_t;
cast C_lb@sint32 r8_b;
assert C_lb = r8_b && true;
assume C_lb = r8_b && true;
cast C_lt@sint32 r8_t;
assert C_lt = r8_t && true;
assume C_lt = r8_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* smulwb	lr, r11, r9                              #! PC = 0x400868 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x40086c *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400870 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400874 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	r9, r9, lr, asr #16                       #! PC = 0x400878 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov r9_b tmp_b;
mov r9_t tmp_t;
cast C_lb@sint32 r9_b;
assert C_lb = r9_b && true;
assume C_lb = r9_b && true;
cast C_lt@sint32 r9_t;
assert C_lt = r9_t && true;
assume C_lt = r9_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* vmov	s0, r2                                     #! PC = 0x40087c *)
mov s0_b r2_b;
mov s0_t r2_t;
mov s0 r2;
mov zeta_s0 zeta_r2;
(* vmov	s1, r3                                     #! PC = 0x400880 *)
mov s1_b r3_b;
mov s1_t r3_t;
mov s1 r3;
mov zeta_s1 zeta_r3;
(* vmov	s2, r4                                     #! PC = 0x400884 *)
mov s2_b r4_b;
mov s2_t r4_t;
mov s2 r4;
mov zeta_s2 zeta_r4;
(* vmov	s3, r5                                     #! PC = 0x400888 *)
mov s3_b r5_b;
mov s3_t r5_t;
mov s3 r5;
mov zeta_s3 zeta_r5;
(* vmov	s4, r6                                     #! PC = 0x40088c *)
mov s4_b r6_b;
mov s4_t r6_t;
mov s4 r6;
mov zeta_s4 zeta_r6;
(* vmov	s5, r7                                     #! PC = 0x400890 *)
mov s5_b r7_b;
mov s5_t r7_t;
mov s5 r7;
mov zeta_s5 zeta_r7;
(* vmov	s6, r8                                     #! PC = 0x400894 *)
mov s6_b r8_b;
mov s6_t r8_t;
mov s6 r8;
mov zeta_s6 zeta_r8;
(* vmov	s7, r9                                     #! PC = 0x400898 *)
mov s7_b r9_b;
mov s7_t r9_t;
mov s7 r9;
mov zeta_s7 zeta_r9;

(*== coeffs multiplied by twiddles END ==*)

assert
  and [
    (-1) * 1664 <= s0_b, s0_b <= 1 * 1664,
    (-1) * 1664 <= s0_t, s0_t <= 1 * 1664,
    (-1) * 1664 <= s1_b, s1_b <= 1 * 1664,
    (-1) * 1664 <= s1_t, s1_t <= 1 * 1664,
    (-1) * 1664 <= s2_b, s2_b <= 1 * 1664,
    (-1) * 1664 <= s2_t, s2_t <= 1 * 1664,
    (-1) * 1664 <= s3_b, s3_b <= 1 * 1664,
    (-1) * 1664 <= s3_t, s3_t <= 1 * 1664,
    (-1) * 1664 <= s4_b, s4_b <= 1 * 1664,
    (-1) * 1664 <= s4_t, s4_t <= 1 * 1664,
    (-1) * 1664 <= s5_b, s5_b <= 1 * 1664,
    (-1) * 1664 <= s5_t, s5_t <= 1 * 1664,
    (-1) * 1664 <= s6_b, s6_b <= 1 * 1664,
    (-1) * 1664 <= s6_t, s6_t <= 1 * 1664,
    (-1) * 1664 <= s7_b, s7_b <= 1 * 1664,
    (-1) * 1664 <= s7_t, s7_t <= 1 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-1)@16 * 1664@16 <=s s0_b, s0_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s0_t, s0_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_b, s1_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_t, s1_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_b, s2_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_t, s2_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_b, s3_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_t, s3_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_b, s4_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_t, s4_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_b, s5_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_t, s5_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_b, s6_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_t, s6_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_b, s7_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_t, s7_t <=s 1@16 * 1664@16
  ];

assume
  and [
    (-1) * 1664 <= s0_b, s0_b <= 1 * 1664,
    (-1) * 1664 <= s0_t, s0_t <= 1 * 1664,
    (-1) * 1664 <= s1_b, s1_b <= 1 * 1664,
    (-1) * 1664 <= s1_t, s1_t <= 1 * 1664,
    (-1) * 1664 <= s2_b, s2_b <= 1 * 1664,
    (-1) * 1664 <= s2_t, s2_t <= 1 * 1664,
    (-1) * 1664 <= s3_b, s3_b <= 1 * 1664,
    (-1) * 1664 <= s3_t, s3_t <= 1 * 1664,
    (-1) * 1664 <= s4_b, s4_b <= 1 * 1664,
    (-1) * 1664 <= s4_t, s4_t <= 1 * 1664,
    (-1) * 1664 <= s5_b, s5_b <= 1 * 1664,
    (-1) * 1664 <= s5_t, s5_t <= 1 * 1664,
    (-1) * 1664 <= s6_b, s6_b <= 1 * 1664,
    (-1) * 1664 <= s6_t, s6_t <= 1 * 1664,
    (-1) * 1664 <= s7_b, s7_b <= 1 * 1664,
    (-1) * 1664 <= s7_t, s7_t <= 1 * 1664
  ]
  &&
  and [
    (-1)@16 * 1664@16 <=s s0_b, s0_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s0_t, s0_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_b, s1_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s1_t, s1_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_b, s2_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s2_t, s2_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_b, s3_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s3_t, s3_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_b, s4_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s4_t, s4_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_b, s5_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s5_t, s5_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_b, s6_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s6_t, s6_t <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_b, s7_b <=s 1@16 * 1664@16,
    (-1)@16 * 1664@16 <=s s7_t, s7_t <=s 1@16 * 1664@16
  ];

(* vmov	r0, s23                                    #! PC = 0x40089c *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff1e0; Value = 0xb6fff914; PC = 0x4008a0 *)
mov r2_b L0xbefff1e0;
mov r2_t L0xbefff1e2;
(* ldr.w	r3, [r0, #64]	; 0x40                      #! EA = L0xbefff220; Value = 0xb6fff070; PC = 0x4008a4 *)
mov r3_b L0xbefff220;
mov r3_t L0xbefff222;
(* ldr.w	r4, [r0, #128]	; 0x80                     #! EA = L0xbefff260; Value = 0xb6fd5304; PC = 0x4008a8 *)
mov r4_b L0xbefff260;
mov r4_t L0xbefff262;
(* ldr.w	r5, [r0, #192]	; 0xc0                     #! EA = L0xbefff2a0; Value = 0xb6ffbc00; PC = 0x4008ac *)
mov r5_b L0xbefff2a0;
mov r5_t L0xbefff2a2;
(* ldr.w	r6, [r0, #256]	; 0x100                    #! EA = L0xbefff2e0; Value = 0xb6fff8e8; PC = 0x4008b0 *)
mov r6_b L0xbefff2e0;
mov r6_t L0xbefff2e2;
(* ldr.w	r7, [r0, #320]	; 0x140                    #! EA = L0xbefff320; Value = 0xbefff52c; PC = 0x4008b4 *)
mov r7_b L0xbefff320;
mov r7_t L0xbefff322;
(* ldr.w	r8, [r0, #384]	; 0x180                    #! EA = L0xbefff360; Value = 0x00000000; PC = 0x4008b8 *)
mov r8_b L0xbefff360;
mov r8_t L0xbefff362;
(* ldr.w	r9, [r0, #448]	; 0x1c0                    #! EA = L0xbefff3a0; Value = 0x00000000; PC = 0x4008bc *)
mov r9_b L0xbefff3a0;
mov r9_t L0xbefff3a2;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x4008c0 *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* vmov	r10, s8                                    #! PC = 0x4008c4 *)
mov r10_b s8_b;
mov r10_t s8_t;
mov r10 s8;
mov zeta_r10 zeta_s8;
(* smulwb	lr, r10, r6                              #! PC = 0x4008c8 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x4008cc *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4008d0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x4008d4 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x4008d8 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x4008dc *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4008e0 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x4008e4 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x4008e8 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4008ec *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x4008f0 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x4008f4 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x4008f8 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x4008fc *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x400900 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400904 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400908 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x40090c *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400910 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x400914 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400918 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x40091c *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x400920 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400924 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400928 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x40092c *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x400930 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400934 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* vmov	r10, s9                                    #! PC = 0x400938 *)
mov r10_b s9_b;
mov r10_t s9_t;
mov r10 s9;
mov zeta_r10 zeta_s9;
(* vmov	r11, s10                                   #! PC = 0x40093c *)
mov r11_b s10_b;
mov r11_t s10_t;
mov r11 s10;
mov zeta_r11 zeta_s10;
(* smulwb	lr, r10, r4                              #! PC = 0x400940 *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x400944 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400948 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x40094c *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400950 *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x400954 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400958 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x40095c *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x400960 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400964 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400968 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x40096c *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x400970 *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400974 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400978 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x40097c *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400980 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400984 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400988 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x40098c *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400990 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400994 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400998 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x40099c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x4009a0 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x4009a4 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x4009a8 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x4009ac *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* vmov	r10, s11                                   #! PC = 0x4009b0 *)
mov r10_b s11_b;
mov r10_t s11_t;
mov r10 s11;
mov zeta_r10 zeta_s11;
(* vmov	r11, s12                                   #! PC = 0x4009b4 *)
mov r11_b s12_b;
mov r11_t s12_t;
mov r11 s12;
mov zeta_r11 zeta_s12;
(* smulwb	lr, r10, r3                              #! PC = 0x4009b8 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x4009bc *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4009c0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x4009c4 *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x4009c8 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x4009cc *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x4009d0 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x4009d4 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x4009d8 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x4009dc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x4009e0 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x4009e4 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x4009e8 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x4009ec *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* vmov	r10, s13                                   #! PC = 0x4009f0 *)
mov r10_b s13_b;
mov r10_t s13_t;
mov r10 s13;
mov zeta_r10 zeta_s13;
(* vmov	r11, s14                                   #! PC = 0x4009f4 *)
mov r11_b s14_b;
mov r11_t s14_t;
mov r11 s14;
mov zeta_r11 zeta_s14;
(* smulwb	lr, r10, r7                              #! PC = 0x4009f8 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x4009fc *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400a00 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400a04 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400a08 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400a0c *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400a10 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400a14 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400a18 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400a1c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400a20 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400a24 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x400a28 *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x400a2c *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;

(*== 8-NTT on a0, a2, ..., a14 ==*)
(*== _3_layer_double_CT_16_plant_fp END ==*)

assert
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

assume
  and [
    (-5) * 1664 <= r2_b, r2_b <= 5 * 1664,
    (-5) * 1664 <= r2_t, r2_t <= 5 * 1664,
    (-5) * 1664 <= r3_b, r3_b <= 5 * 1664,
    (-5) * 1664 <= r3_t, r3_t <= 5 * 1664,
    (-5) * 1664 <= r4_b, r4_b <= 5 * 1664,
    (-5) * 1664 <= r4_t, r4_t <= 5 * 1664,
    (-5) * 1664 <= r5_b, r5_b <= 5 * 1664,
    (-5) * 1664 <= r5_t, r5_t <= 5 * 1664,
    (-5) * 1664 <= r6_b, r6_b <= 5 * 1664,
    (-5) * 1664 <= r6_t, r6_t <= 5 * 1664,
    (-5) * 1664 <= r7_b, r7_b <= 5 * 1664,
    (-5) * 1664 <= r7_t, r7_t <= 5 * 1664,
    (-5) * 1664 <= r8_b, r8_b <= 5 * 1664,
    (-5) * 1664 <= r8_t, r8_t <= 5 * 1664,
    (-5) * 1664 <= r9_b, r9_b <= 5 * 1664,
    (-5) * 1664 <= r9_t, r9_t <= 5 * 1664
  ]
  &&
  and [
    (-5)@16 * 1664@16 <=s r2_b, r2_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r2_t, r2_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_b, r3_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r3_t, r3_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_b, r4_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r4_t, r4_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_b, r5_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r5_t, r5_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_b, r6_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r6_t, r6_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_b, r7_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r7_t, r7_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_b, r8_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r8_t, r8_t <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_b, r9_b <=s 5@16 * 1664@16,
    (-5)@16 * 1664@16 <=s r9_t, r9_t <=s 5@16 * 1664@16
  ];

(* vmov	r0, s23                                    #! PC = 0x400a30 *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* vmov	r10, s1                                    #! PC = 0x400a34 *)
mov r10_b s1_b;
mov r10_t s1_t;
mov r10 s1;
mov zeta_r10 zeta_s1;
(* uadd16	lr, r3, r10                              #! PC = 0x400a38 *)
add lr_b r3_b r10_b;
add lr_t r3_t r10_t;
(* usub16	r3, r3, r10                              #! PC = 0x400a3c *)
sub r3_b r3_b r10_b;
sub r3_t r3_t r10_t;
(* str.w	lr, [r0, #64]	; 0x40                      #! EA = L0xbefff220; PC = 0x400a40 *)
mov L0xbefff220 lr_b;
mov L0xbefff222 lr_t;
(* str.w	r3, [r0, #96]	; 0x60                      #! EA = L0xbefff240; PC = 0x400a44 *)
mov L0xbefff240 r3_b;
mov L0xbefff242 r3_t;
(* vmov	r10, s3                                    #! PC = 0x400a48 *)
mov r10_b s3_b;
mov r10_t s3_t;
mov r10 s3;
mov zeta_r10 zeta_s3;
(* uadd16	lr, r5, r10                              #! PC = 0x400a4c *)
add lr_b r5_b r10_b;
add lr_t r5_t r10_t;
(* usub16	r5, r5, r10                              #! PC = 0x400a50 *)
sub r5_b r5_b r10_b;
sub r5_t r5_t r10_t;
(* str.w	lr, [r0, #192]	; 0xc0                     #! EA = L0xbefff2a0; PC = 0x400a54 *)
mov L0xbefff2a0 lr_b;
mov L0xbefff2a2 lr_t;
(* str.w	r5, [r0, #224]	; 0xe0                     #! EA = L0xbefff2c0; PC = 0x400a58 *)
mov L0xbefff2c0 r5_b;
mov L0xbefff2c2 r5_t;
(* vmov	r10, s5                                    #! PC = 0x400a5c *)
mov r10_b s5_b;
mov r10_t s5_t;
mov r10 s5;
mov zeta_r10 zeta_s5;
(* uadd16	lr, r7, r10                              #! PC = 0x400a60 *)
add lr_b r7_b r10_b;
add lr_t r7_t r10_t;
(* usub16	r7, r7, r10                              #! PC = 0x400a64 *)
sub r7_b r7_b r10_b;
sub r7_t r7_t r10_t;
(* str.w	lr, [r0, #320]	; 0x140                    #! EA = L0xbefff320; PC = 0x400a68 *)
mov L0xbefff320 lr_b;
mov L0xbefff322 lr_t;
(* str.w	r7, [r0, #352]	; 0x160                    #! EA = L0xbefff340; PC = 0x400a6c *)
mov L0xbefff340 r7_b;
mov L0xbefff342 r7_t;
(* vmov	r10, s7                                    #! PC = 0x400a70 *)
mov r10_b s7_b;
mov r10_t s7_t;
mov r10 s7;
mov zeta_r10 zeta_s7;
(* uadd16	lr, r9, r10                              #! PC = 0x400a74 *)
add lr_b r9_b r10_b;
add lr_t r9_t r10_t;
(* usub16	r9, r9, r10                              #! PC = 0x400a78 *)
sub r9_b r9_b r10_b;
sub r9_t r9_t r10_t;
(* str.w	lr, [r0, #448]	; 0x1c0                    #! EA = L0xbefff3a0; PC = 0x400a7c *)
mov L0xbefff3a0 lr_b;
mov L0xbefff3a2 lr_t;
(* str.w	r9, [r0, #480]	; 0x1e0                    #! EA = L0xbefff3c0; PC = 0x400a80 *)
mov L0xbefff3c0 r9_b;
mov L0xbefff3c2 r9_t;
(* vmov	r5, s2                                     #! PC = 0x400a84 *)
mov r5_b s2_b;
mov r5_t s2_t;
mov r5 s2;
mov zeta_r5 zeta_s2;
(* uadd16	lr, r4, r5                               #! PC = 0x400a88 *)
add lr_b r4_b r5_b;
add lr_t r4_t r5_t;
(* usub16	r10, r4, r5                              #! PC = 0x400a8c *)
sub r10_b r4_b r5_b;
sub r10_t r4_t r5_t;
(* str.w	lr, [r0, #128]	; 0x80                     #! EA = L0xbefff260; PC = 0x400a90 *)
mov L0xbefff260 lr_b;
mov L0xbefff262 lr_t;
(* str.w	r10, [r0, #160]	; 0xa0                    #! EA = L0xbefff280; PC = 0x400a94 *)
mov L0xbefff280 r10_b;
mov L0xbefff282 r10_t;
(* vmov	r7, s4                                     #! PC = 0x400a98 *)
mov r7_b s4_b;
mov r7_t s4_t;
mov r7 s4;
mov zeta_r7 zeta_s4;
(* uadd16	lr, r6, r7                               #! PC = 0x400a9c *)
add lr_b r6_b r7_b;
add lr_t r6_t r7_t;
(* usub16	r10, r6, r7                              #! PC = 0x400aa0 *)
sub r10_b r6_b r7_b;
sub r10_t r6_t r7_t;
(* str.w	lr, [r0, #256]	; 0x100                    #! EA = L0xbefff2e0; PC = 0x400aa4 *)
mov L0xbefff2e0 lr_b;
mov L0xbefff2e2 lr_t;
(* str.w	r10, [r0, #288]	; 0x120                   #! EA = L0xbefff300; PC = 0x400aa8 *)
mov L0xbefff300 r10_b;
mov L0xbefff302 r10_t;
(* vmov	r9, s6                                     #! PC = 0x400aac *)
mov r9_b s6_b;
mov r9_t s6_t;
mov r9 s6;
mov zeta_r9 zeta_s6;
(* uadd16	lr, r8, r9                               #! PC = 0x400ab0 *)
add lr_b r8_b r9_b;
add lr_t r8_t r9_t;
(* usub16	r10, r8, r9                              #! PC = 0x400ab4 *)
sub r10_b r8_b r9_b;
sub r10_t r8_t r9_t;
(* str.w	lr, [r0, #384]	; 0x180                    #! EA = L0xbefff360; PC = 0x400ab8 *)
mov L0xbefff360 lr_b;
mov L0xbefff362 lr_t;
(* str.w	r10, [r0, #416]	; 0x1a0                   #! EA = L0xbefff380; PC = 0x400abc *)
mov L0xbefff380 r10_b;
mov L0xbefff382 r10_t;
(* vmov	r3, s0                                     #! PC = 0x400ac0 *)
mov r3_b s0_b;
mov r3_t s0_t;
mov r3 s0;
mov zeta_r3 zeta_s0;
(* uadd16	lr, r2, r3                               #! PC = 0x400ac4 *)
add lr_b r2_b r3_b;
add lr_t r2_t r3_t;
(* usub16	r10, r2, r3                              #! PC = 0x400ac8 *)
sub r10_b r2_b r3_b;
sub r10_t r2_t r3_t;
(* str.w	r10, [r0, #32]                            #! EA = L0xbefff200; PC = 0x400acc *)
mov L0xbefff200 r10_b;
mov L0xbefff202 r10_t;
(* str.w	lr, [r0], #4                              #! EA = L0xbefff1e0; PC = 0x400ad0 *)
mov L0xbefff1e0 lr_b;
mov L0xbefff1e2 lr_t;
(* vmov	lr, s24                                    #! PC = 0x400ad4 *)
mov lr_b s24_b;
mov lr_t s24_t;
mov lr s24;
mov zeta_lr zeta_s24;
(* #bne.w	0x400628 <ntt_fast+24>                   #! PC = 0x400adc *)
#bne.w	0x400628 <ntt_fast+24>                   #! 0x400adc = 0x400adc;

(*== layer 7+6+5+4 one slice condition ==*)

assert
  and [
    (-6) * 1664 <= L0xbefff1e0, L0xbefff1e0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1e2, L0xbefff1e2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff200, L0xbefff200 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff202, L0xbefff202 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff220, L0xbefff220 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff222, L0xbefff222 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff240, L0xbefff240 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff242, L0xbefff242 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff260, L0xbefff260 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff262, L0xbefff262 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff280, L0xbefff280 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff282, L0xbefff282 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2a0, L0xbefff2a0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2a2, L0xbefff2a2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2c0, L0xbefff2c0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2c2, L0xbefff2c2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2e0, L0xbefff2e0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2e2, L0xbefff2e2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff300, L0xbefff300 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff302, L0xbefff302 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff320, L0xbefff320 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff322, L0xbefff322 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff340, L0xbefff340 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff342, L0xbefff342 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff360, L0xbefff360 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff362, L0xbefff362 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff380, L0xbefff380 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff382, L0xbefff382 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3a0, L0xbefff3a0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3a2, L0xbefff3a2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3c0, L0xbefff3c0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3c2, L0xbefff3c2 <= 6 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-6)@16 * 1664@16 <=s L0xbefff1e0, L0xbefff1e0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1e2, L0xbefff1e2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff200, L0xbefff200 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff202, L0xbefff202 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff220, L0xbefff220 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff222, L0xbefff222 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff240, L0xbefff240 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff242, L0xbefff242 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff260, L0xbefff260 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff262, L0xbefff262 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff280, L0xbefff280 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff282, L0xbefff282 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2a0, L0xbefff2a0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2a2, L0xbefff2a2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2c0, L0xbefff2c0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2c2, L0xbefff2c2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2e0, L0xbefff2e0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2e2, L0xbefff2e2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff300, L0xbefff300 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff302, L0xbefff302 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff320, L0xbefff320 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff322, L0xbefff322 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff340, L0xbefff340 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff342, L0xbefff342 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff360, L0xbefff360 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff362, L0xbefff362 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff380, L0xbefff380 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff382, L0xbefff382 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3a0, L0xbefff3a0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3a2, L0xbefff3a2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3c0, L0xbefff3c0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3c2, L0xbefff3c2 <=s 6@16 * 1664@16
  ]
;

assume
  and [
    (-6) * 1664 <= L0xbefff1e0, L0xbefff1e0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1e2, L0xbefff1e2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff200, L0xbefff200 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff202, L0xbefff202 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff220, L0xbefff220 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff222, L0xbefff222 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff240, L0xbefff240 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff242, L0xbefff242 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff260, L0xbefff260 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff262, L0xbefff262 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff280, L0xbefff280 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff282, L0xbefff282 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2a0, L0xbefff2a0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2a2, L0xbefff2a2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2c0, L0xbefff2c0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2c2, L0xbefff2c2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2e0, L0xbefff2e0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2e2, L0xbefff2e2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff300, L0xbefff300 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff302, L0xbefff302 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff320, L0xbefff320 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff322, L0xbefff322 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff340, L0xbefff340 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff342, L0xbefff342 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff360, L0xbefff360 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff362, L0xbefff362 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff380, L0xbefff380 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff382, L0xbefff382 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3a0, L0xbefff3a0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3a2, L0xbefff3a2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3c0, L0xbefff3c0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3c2, L0xbefff3c2 <= 6 * 1664
  ]
  &&
  and [
    (-6)@16 * 1664@16 <=s L0xbefff1e0, L0xbefff1e0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1e2, L0xbefff1e2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff200, L0xbefff200 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff202, L0xbefff202 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff220, L0xbefff220 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff222, L0xbefff222 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff240, L0xbefff240 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff242, L0xbefff242 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff260, L0xbefff260 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff262, L0xbefff262 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff280, L0xbefff280 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff282, L0xbefff282 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2a0, L0xbefff2a0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2a2, L0xbefff2a2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2c0, L0xbefff2c0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2c2, L0xbefff2c2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2e0, L0xbefff2e0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2e2, L0xbefff2e2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff300, L0xbefff300 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff302, L0xbefff302 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff320, L0xbefff320 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff322, L0xbefff322 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff340, L0xbefff340 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff342, L0xbefff342 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff360, L0xbefff360 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff362, L0xbefff362 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff380, L0xbefff380 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff382, L0xbefff382 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3a0, L0xbefff3a0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3a2, L0xbefff3a2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3c0, L0xbefff3c0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3c2, L0xbefff3c2 <=s 6@16 * 1664@16
  ]
;


(*== layer 7+6+5+4 END ==*)

cut (* 0 *)
  and [
    (-6) * 1664 <= L0xbefff1c4, L0xbefff1c4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1c6, L0xbefff1c6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1c8, L0xbefff1c8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1ca, L0xbefff1ca <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1cc, L0xbefff1cc <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1ce, L0xbefff1ce <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1d0, L0xbefff1d0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1d2, L0xbefff1d2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1d4, L0xbefff1d4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1d6, L0xbefff1d6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1d8, L0xbefff1d8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1da, L0xbefff1da <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1dc, L0xbefff1dc <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1de, L0xbefff1de <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1e0, L0xbefff1e0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1e2, L0xbefff1e2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1e4, L0xbefff1e4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1e6, L0xbefff1e6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1e8, L0xbefff1e8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1ea, L0xbefff1ea <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1ec, L0xbefff1ec <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1ee, L0xbefff1ee <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1f0, L0xbefff1f0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1f2, L0xbefff1f2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1f4, L0xbefff1f4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1f6, L0xbefff1f6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1f8, L0xbefff1f8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1fa, L0xbefff1fa <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1fc, L0xbefff1fc <= 6 * 1664,
    (-6) * 1664 <= L0xbefff1fe, L0xbefff1fe <= 6 * 1664,
    (-6) * 1664 <= L0xbefff200, L0xbefff200 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff202, L0xbefff202 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff204, L0xbefff204 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff206, L0xbefff206 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff208, L0xbefff208 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff20a, L0xbefff20a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff20c, L0xbefff20c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff20e, L0xbefff20e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff210, L0xbefff210 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff212, L0xbefff212 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff214, L0xbefff214 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff216, L0xbefff216 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff218, L0xbefff218 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff21a, L0xbefff21a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff21c, L0xbefff21c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff21e, L0xbefff21e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff220, L0xbefff220 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff222, L0xbefff222 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff224, L0xbefff224 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff226, L0xbefff226 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff228, L0xbefff228 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff22a, L0xbefff22a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff22c, L0xbefff22c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff22e, L0xbefff22e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff230, L0xbefff230 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff232, L0xbefff232 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff234, L0xbefff234 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff236, L0xbefff236 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff238, L0xbefff238 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff23a, L0xbefff23a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff23c, L0xbefff23c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff23e, L0xbefff23e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff240, L0xbefff240 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff242, L0xbefff242 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff244, L0xbefff244 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff246, L0xbefff246 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff248, L0xbefff248 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff24a, L0xbefff24a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff24c, L0xbefff24c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff24e, L0xbefff24e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff250, L0xbefff250 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff252, L0xbefff252 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff254, L0xbefff254 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff256, L0xbefff256 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff258, L0xbefff258 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff25a, L0xbefff25a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff25c, L0xbefff25c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff25e, L0xbefff25e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff260, L0xbefff260 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff262, L0xbefff262 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff264, L0xbefff264 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff266, L0xbefff266 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff268, L0xbefff268 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff26a, L0xbefff26a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff26c, L0xbefff26c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff26e, L0xbefff26e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff270, L0xbefff270 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff272, L0xbefff272 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff274, L0xbefff274 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff276, L0xbefff276 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff278, L0xbefff278 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff27a, L0xbefff27a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff27c, L0xbefff27c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff27e, L0xbefff27e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff280, L0xbefff280 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff282, L0xbefff282 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff284, L0xbefff284 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff286, L0xbefff286 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff288, L0xbefff288 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff28a, L0xbefff28a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff28c, L0xbefff28c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff28e, L0xbefff28e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff290, L0xbefff290 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff292, L0xbefff292 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff294, L0xbefff294 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff296, L0xbefff296 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff298, L0xbefff298 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff29a, L0xbefff29a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff29c, L0xbefff29c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff29e, L0xbefff29e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2a0, L0xbefff2a0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2a2, L0xbefff2a2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2a4, L0xbefff2a4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2a6, L0xbefff2a6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2a8, L0xbefff2a8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2aa, L0xbefff2aa <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2ac, L0xbefff2ac <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2ae, L0xbefff2ae <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2b0, L0xbefff2b0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2b2, L0xbefff2b2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2b4, L0xbefff2b4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2b6, L0xbefff2b6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2b8, L0xbefff2b8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2ba, L0xbefff2ba <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2bc, L0xbefff2bc <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2be, L0xbefff2be <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2c0, L0xbefff2c0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2c2, L0xbefff2c2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2c4, L0xbefff2c4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2c6, L0xbefff2c6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2c8, L0xbefff2c8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2ca, L0xbefff2ca <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2cc, L0xbefff2cc <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2ce, L0xbefff2ce <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2d0, L0xbefff2d0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2d2, L0xbefff2d2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2d4, L0xbefff2d4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2d6, L0xbefff2d6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2d8, L0xbefff2d8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2da, L0xbefff2da <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2dc, L0xbefff2dc <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2de, L0xbefff2de <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2e0, L0xbefff2e0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2e2, L0xbefff2e2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2e4, L0xbefff2e4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2e6, L0xbefff2e6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2e8, L0xbefff2e8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2ea, L0xbefff2ea <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2ec, L0xbefff2ec <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2ee, L0xbefff2ee <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2f0, L0xbefff2f0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2f2, L0xbefff2f2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2f4, L0xbefff2f4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2f6, L0xbefff2f6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2f8, L0xbefff2f8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2fa, L0xbefff2fa <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2fc, L0xbefff2fc <= 6 * 1664,
    (-6) * 1664 <= L0xbefff2fe, L0xbefff2fe <= 6 * 1664,
    (-6) * 1664 <= L0xbefff300, L0xbefff300 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff302, L0xbefff302 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff304, L0xbefff304 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff306, L0xbefff306 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff308, L0xbefff308 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff30a, L0xbefff30a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff30c, L0xbefff30c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff30e, L0xbefff30e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff310, L0xbefff310 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff312, L0xbefff312 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff314, L0xbefff314 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff316, L0xbefff316 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff318, L0xbefff318 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff31a, L0xbefff31a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff31c, L0xbefff31c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff31e, L0xbefff31e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff320, L0xbefff320 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff322, L0xbefff322 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff324, L0xbefff324 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff326, L0xbefff326 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff328, L0xbefff328 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff32a, L0xbefff32a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff32c, L0xbefff32c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff32e, L0xbefff32e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff330, L0xbefff330 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff332, L0xbefff332 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff334, L0xbefff334 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff336, L0xbefff336 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff338, L0xbefff338 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff33a, L0xbefff33a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff33c, L0xbefff33c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff33e, L0xbefff33e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff340, L0xbefff340 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff342, L0xbefff342 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff344, L0xbefff344 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff346, L0xbefff346 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff348, L0xbefff348 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff34a, L0xbefff34a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff34c, L0xbefff34c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff34e, L0xbefff34e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff350, L0xbefff350 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff352, L0xbefff352 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff354, L0xbefff354 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff356, L0xbefff356 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff358, L0xbefff358 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff35a, L0xbefff35a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff35c, L0xbefff35c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff35e, L0xbefff35e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff360, L0xbefff360 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff362, L0xbefff362 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff364, L0xbefff364 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff366, L0xbefff366 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff368, L0xbefff368 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff36a, L0xbefff36a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff36c, L0xbefff36c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff36e, L0xbefff36e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff370, L0xbefff370 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff372, L0xbefff372 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff374, L0xbefff374 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff376, L0xbefff376 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff378, L0xbefff378 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff37a, L0xbefff37a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff37c, L0xbefff37c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff37e, L0xbefff37e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff380, L0xbefff380 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff382, L0xbefff382 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff384, L0xbefff384 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff386, L0xbefff386 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff388, L0xbefff388 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff38a, L0xbefff38a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff38c, L0xbefff38c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff38e, L0xbefff38e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff390, L0xbefff390 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff392, L0xbefff392 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff394, L0xbefff394 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff396, L0xbefff396 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff398, L0xbefff398 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff39a, L0xbefff39a <= 6 * 1664,
    (-6) * 1664 <= L0xbefff39c, L0xbefff39c <= 6 * 1664,
    (-6) * 1664 <= L0xbefff39e, L0xbefff39e <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3a0, L0xbefff3a0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3a2, L0xbefff3a2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3a4, L0xbefff3a4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3a6, L0xbefff3a6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3a8, L0xbefff3a8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3aa, L0xbefff3aa <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3ac, L0xbefff3ac <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3ae, L0xbefff3ae <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3b0, L0xbefff3b0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3b2, L0xbefff3b2 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3b4, L0xbefff3b4 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3b6, L0xbefff3b6 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3b8, L0xbefff3b8 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3ba, L0xbefff3ba <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3bc, L0xbefff3bc <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3be, L0xbefff3be <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3c0, L0xbefff3c0 <= 6 * 1664,
    (-6) * 1664 <= L0xbefff3c2, L0xbefff3c2 <= 6 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-6)@16 * 1664@16 <=s L0xbefff1c4, L0xbefff1c4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1c6, L0xbefff1c6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1c8, L0xbefff1c8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1ca, L0xbefff1ca <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1cc, L0xbefff1cc <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1ce, L0xbefff1ce <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1d0, L0xbefff1d0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1d2, L0xbefff1d2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1d4, L0xbefff1d4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1d6, L0xbefff1d6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1d8, L0xbefff1d8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1da, L0xbefff1da <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1dc, L0xbefff1dc <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1de, L0xbefff1de <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1e0, L0xbefff1e0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1e2, L0xbefff1e2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1e4, L0xbefff1e4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1e6, L0xbefff1e6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1e8, L0xbefff1e8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1ea, L0xbefff1ea <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1ec, L0xbefff1ec <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1ee, L0xbefff1ee <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1f0, L0xbefff1f0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1f2, L0xbefff1f2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1f4, L0xbefff1f4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1f6, L0xbefff1f6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1f8, L0xbefff1f8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1fa, L0xbefff1fa <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1fc, L0xbefff1fc <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff1fe, L0xbefff1fe <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff200, L0xbefff200 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff202, L0xbefff202 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff204, L0xbefff204 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff206, L0xbefff206 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff208, L0xbefff208 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff20a, L0xbefff20a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff20c, L0xbefff20c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff20e, L0xbefff20e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff210, L0xbefff210 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff212, L0xbefff212 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff214, L0xbefff214 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff216, L0xbefff216 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff218, L0xbefff218 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff21a, L0xbefff21a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff21c, L0xbefff21c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff21e, L0xbefff21e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff220, L0xbefff220 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff222, L0xbefff222 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff224, L0xbefff224 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff226, L0xbefff226 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff228, L0xbefff228 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff22a, L0xbefff22a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff22c, L0xbefff22c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff22e, L0xbefff22e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff230, L0xbefff230 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff232, L0xbefff232 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff234, L0xbefff234 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff236, L0xbefff236 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff238, L0xbefff238 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff23a, L0xbefff23a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff23c, L0xbefff23c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff23e, L0xbefff23e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff240, L0xbefff240 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff242, L0xbefff242 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff244, L0xbefff244 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff246, L0xbefff246 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff248, L0xbefff248 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff24a, L0xbefff24a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff24c, L0xbefff24c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff24e, L0xbefff24e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff250, L0xbefff250 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff252, L0xbefff252 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff254, L0xbefff254 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff256, L0xbefff256 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff258, L0xbefff258 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff25a, L0xbefff25a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff25c, L0xbefff25c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff25e, L0xbefff25e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff260, L0xbefff260 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff262, L0xbefff262 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff264, L0xbefff264 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff266, L0xbefff266 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff268, L0xbefff268 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff26a, L0xbefff26a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff26c, L0xbefff26c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff26e, L0xbefff26e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff270, L0xbefff270 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff272, L0xbefff272 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff274, L0xbefff274 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff276, L0xbefff276 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff278, L0xbefff278 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff27a, L0xbefff27a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff27c, L0xbefff27c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff27e, L0xbefff27e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff280, L0xbefff280 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff282, L0xbefff282 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff284, L0xbefff284 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff286, L0xbefff286 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff288, L0xbefff288 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff28a, L0xbefff28a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff28c, L0xbefff28c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff28e, L0xbefff28e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff290, L0xbefff290 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff292, L0xbefff292 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff294, L0xbefff294 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff296, L0xbefff296 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff298, L0xbefff298 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff29a, L0xbefff29a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff29c, L0xbefff29c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff29e, L0xbefff29e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2a0, L0xbefff2a0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2a2, L0xbefff2a2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2a4, L0xbefff2a4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2a6, L0xbefff2a6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2a8, L0xbefff2a8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2aa, L0xbefff2aa <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2ac, L0xbefff2ac <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2ae, L0xbefff2ae <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2b0, L0xbefff2b0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2b2, L0xbefff2b2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2b4, L0xbefff2b4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2b6, L0xbefff2b6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2b8, L0xbefff2b8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2ba, L0xbefff2ba <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2bc, L0xbefff2bc <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2be, L0xbefff2be <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2c0, L0xbefff2c0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2c2, L0xbefff2c2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2c4, L0xbefff2c4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2c6, L0xbefff2c6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2c8, L0xbefff2c8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2ca, L0xbefff2ca <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2cc, L0xbefff2cc <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2ce, L0xbefff2ce <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2d0, L0xbefff2d0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2d2, L0xbefff2d2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2d4, L0xbefff2d4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2d6, L0xbefff2d6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2d8, L0xbefff2d8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2da, L0xbefff2da <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2dc, L0xbefff2dc <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2de, L0xbefff2de <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2e0, L0xbefff2e0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2e2, L0xbefff2e2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2e4, L0xbefff2e4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2e6, L0xbefff2e6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2e8, L0xbefff2e8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2ea, L0xbefff2ea <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2ec, L0xbefff2ec <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2ee, L0xbefff2ee <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2f0, L0xbefff2f0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2f2, L0xbefff2f2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2f4, L0xbefff2f4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2f6, L0xbefff2f6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2f8, L0xbefff2f8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2fa, L0xbefff2fa <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2fc, L0xbefff2fc <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff2fe, L0xbefff2fe <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff300, L0xbefff300 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff302, L0xbefff302 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff304, L0xbefff304 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff306, L0xbefff306 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff308, L0xbefff308 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff30a, L0xbefff30a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff30c, L0xbefff30c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff30e, L0xbefff30e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff310, L0xbefff310 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff312, L0xbefff312 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff314, L0xbefff314 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff316, L0xbefff316 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff318, L0xbefff318 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff31a, L0xbefff31a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff31c, L0xbefff31c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff31e, L0xbefff31e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff320, L0xbefff320 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff322, L0xbefff322 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff324, L0xbefff324 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff326, L0xbefff326 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff328, L0xbefff328 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff32a, L0xbefff32a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff32c, L0xbefff32c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff32e, L0xbefff32e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff330, L0xbefff330 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff332, L0xbefff332 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff334, L0xbefff334 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff336, L0xbefff336 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff338, L0xbefff338 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff33a, L0xbefff33a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff33c, L0xbefff33c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff33e, L0xbefff33e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff340, L0xbefff340 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff342, L0xbefff342 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff344, L0xbefff344 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff346, L0xbefff346 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff348, L0xbefff348 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff34a, L0xbefff34a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff34c, L0xbefff34c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff34e, L0xbefff34e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff350, L0xbefff350 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff352, L0xbefff352 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff354, L0xbefff354 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff356, L0xbefff356 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff358, L0xbefff358 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff35a, L0xbefff35a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff35c, L0xbefff35c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff35e, L0xbefff35e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff360, L0xbefff360 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff362, L0xbefff362 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff364, L0xbefff364 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff366, L0xbefff366 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff368, L0xbefff368 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff36a, L0xbefff36a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff36c, L0xbefff36c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff36e, L0xbefff36e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff370, L0xbefff370 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff372, L0xbefff372 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff374, L0xbefff374 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff376, L0xbefff376 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff378, L0xbefff378 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff37a, L0xbefff37a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff37c, L0xbefff37c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff37e, L0xbefff37e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff380, L0xbefff380 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff382, L0xbefff382 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff384, L0xbefff384 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff386, L0xbefff386 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff388, L0xbefff388 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff38a, L0xbefff38a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff38c, L0xbefff38c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff38e, L0xbefff38e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff390, L0xbefff390 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff392, L0xbefff392 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff394, L0xbefff394 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff396, L0xbefff396 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff398, L0xbefff398 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff39a, L0xbefff39a <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff39c, L0xbefff39c <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff39e, L0xbefff39e <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3a0, L0xbefff3a0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3a2, L0xbefff3a2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3a4, L0xbefff3a4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3a6, L0xbefff3a6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3a8, L0xbefff3a8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3aa, L0xbefff3aa <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3ac, L0xbefff3ac <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3ae, L0xbefff3ae <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3b0, L0xbefff3b0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3b2, L0xbefff3b2 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3b4, L0xbefff3b4 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3b6, L0xbefff3b6 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3b8, L0xbefff3b8 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3ba, L0xbefff3ba <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3bc, L0xbefff3bc <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3be, L0xbefff3be <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3c0, L0xbefff3c0 <=s 6@16 * 1664@16,
    (-6)@16 * 1664@16 <=s L0xbefff3c2, L0xbefff3c2 <=s 6@16 * 1664@16
  ]
;


(* cut *)
(* algebraic condition *)
(*
  and [eqmod (inp_poly * inp_poly) (L0xbefff1c4 * (x**0) + L0xbefff1c6 * (x**1) + L0xbefff1c8 * (x**2) + L0xbefff1ca * (x**3) + L0xbefff1cc * (x**4) + L0xbefff1ce * (x**5) + L0xbefff1d0 * (x**6) + L0xbefff1d2 * (x**7) + L0xbefff1d4 * (x**8) + L0xbefff1d6 * (x**9) + L0xbefff1d8 * (x**10) + L0xbefff1da * (x**11) + L0xbefff1dc * (x**12) + L0xbefff1de * (x**13) + L0xbefff1e0 * (x**14) + L0xbefff1e2 * (x**15))[3329, x**16 - 1062], eqmod (inp_poly * inp_poly) (L0xbefff1e4 * (x**0) + L0xbefff1e6 * (x**1) + L0xbefff1e8 * (x**2) + L0xbefff1ea * (x**3) + L0xbefff1ec * (x**4) + L0xbefff1ee * (x**5) + L0xbefff1f0 * (x**6) + L0xbefff1f2 * (x**7) + L0xbefff1f4 * (x**8) + L0xbefff1f6 * (x**9) + L0xbefff1f8 * (x**10) + L0xbefff1fa * (x**11) + L0xbefff1fc * (x**12) + L0xbefff1fe * (x**13) + L0xbefff200 * (x**14) + L0xbefff202 * (x**15))[3329, x**16 - 2267], eqmod (inp_poly * inp_poly) (L0xbefff204 * (x**0) + L0xbefff206 * (x**1) + L0xbefff208 * (x**2) + L0xbefff20a * (x**3) + L0xbefff20c * (x**4) + L0xbefff20e * (x**5) + L0xbefff210 * (x**6) + L0xbefff212 * (x**7) + L0xbefff214 * (x**8) + L0xbefff216 * (x**9) + L0xbefff218 * (x**10) + L0xbefff21a * (x**11) + L0xbefff21c * (x**12) + L0xbefff21e * (x**13) + L0xbefff220 * (x**14) + L0xbefff222 * (x**15))[3329, x**16 - 1919], eqmod (inp_poly * inp_poly) (L0xbefff224 * (x**0) + L0xbefff226 * (x**1) + L0xbefff228 * (x**2) + L0xbefff22a * (x**3) + L0xbefff22c * (x**4) + L0xbefff22e * (x**5) + L0xbefff230 * (x**6) + L0xbefff232 * (x**7) + L0xbefff234 * (x**8) + L0xbefff236 * (x**9) + L0xbefff238 * (x**10) + L0xbefff23a * (x**11) + L0xbefff23c * (x**12) + L0xbefff23e * (x**13) + L0xbefff240 * (x**14) + L0xbefff242 * (x**15))[3329, x**16 - 1410], eqmod (inp_poly * inp_poly) (L0xbefff244 * (x**0) + L0xbefff246 * (x**1) + L0xbefff248 * (x**2) + L0xbefff24a * (x**3) + L0xbefff24c * (x**4) + L0xbefff24e * (x**5) + L0xbefff250 * (x**6) + L0xbefff252 * (x**7) + L0xbefff254 * (x**8) + L0xbefff256 * (x**9) + L0xbefff258 * (x**10) + L0xbefff25a * (x**11) + L0xbefff25c * (x**12) + L0xbefff25e * (x**13) + L0xbefff260 * (x**14) + L0xbefff262 * (x**15))[3329, x**16 - 193], eqmod (inp_poly * inp_poly) (L0xbefff264 * (x**0) + L0xbefff266 * (x**1) + L0xbefff268 * (x**2) + L0xbefff26a * (x**3) + L0xbefff26c * (x**4) + L0xbefff26e * (x**5) + L0xbefff270 * (x**6) + L0xbefff272 * (x**7) + L0xbefff274 * (x**8) + L0xbefff276 * (x**9) + L0xbefff278 * (x**10) + L0xbefff27a * (x**11) + L0xbefff27c * (x**12) + L0xbefff27e * (x**13) + L0xbefff280 * (x**14) + L0xbefff282 * (x**15))[3329, x**16 - 3136], eqmod (inp_poly * inp_poly) (L0xbefff284 * (x**0) + L0xbefff286 * (x**1) + L0xbefff288 * (x**2) + L0xbefff28a * (x**3) + L0xbefff28c * (x**4) + L0xbefff28e * (x**5) + L0xbefff290 * (x**6) + L0xbefff292 * (x**7) + L0xbefff294 * (x**8) + L0xbefff296 * (x**9) + L0xbefff298 * (x**10) + L0xbefff29a * (x**11) + L0xbefff29c * (x**12) + L0xbefff29e * (x**13) + L0xbefff2a0 * (x**14) + L0xbefff2a2 * (x**15))[3329, x**16 - 797], eqmod (inp_poly * inp_poly) (L0xbefff2a4 * (x**0) + L0xbefff2a6 * (x**1) + L0xbefff2a8 * (x**2) + L0xbefff2aa * (x**3) + L0xbefff2ac * (x**4) + L0xbefff2ae * (x**5) + L0xbefff2b0 * (x**6) + L0xbefff2b2 * (x**7) + L0xbefff2b4 * (x**8) + L0xbefff2b6 * (x**9) + L0xbefff2b8 * (x**10) + L0xbefff2ba * (x**11) + L0xbefff2bc * (x**12) + L0xbefff2be * (x**13) + L0xbefff2c0 * (x**14) + L0xbefff2c2 * (x**15))[3329, x**16 - 2532], eqmod (inp_poly * inp_poly) (L0xbefff2c4 * (x**0) + L0xbefff2c6 * (x**1) + L0xbefff2c8 * (x**2) + L0xbefff2ca * (x**3) + L0xbefff2cc * (x**4) + L0xbefff2ce * (x**5) + L0xbefff2d0 * (x**6) + L0xbefff2d2 * (x**7) + L0xbefff2d4 * (x**8) + L0xbefff2d6 * (x**9) + L0xbefff2d8 * (x**10) + L0xbefff2da * (x**11) + L0xbefff2dc * (x**12) + L0xbefff2de * (x**13) + L0xbefff2e0 * (x**14) + L0xbefff2e2 * (x**15))[3329, x**16 - 2786], eqmod (inp_poly * inp_poly) (L0xbefff2e4 * (x**0) + L0xbefff2e6 * (x**1) + L0xbefff2e8 * (x**2) + L0xbefff2ea * (x**3) + L0xbefff2ec * (x**4) + L0xbefff2ee * (x**5) + L0xbefff2f0 * (x**6) + L0xbefff2f2 * (x**7) + L0xbefff2f4 * (x**8) + L0xbefff2f6 * (x**9) + L0xbefff2f8 * (x**10) + L0xbefff2fa * (x**11) + L0xbefff2fc * (x**12) + L0xbefff2fe * (x**13) + L0xbefff300 * (x**14) + L0xbefff302 * (x**15))[3329, x**16 - 543], eqmod (inp_poly * inp_poly) (L0xbefff304 * (x**0) + L0xbefff306 * (x**1) + L0xbefff308 * (x**2) + L0xbefff30a * (x**3) + L0xbefff30c * (x**4) + L0xbefff30e * (x**5) + L0xbefff310 * (x**6) + L0xbefff312 * (x**7) + L0xbefff314 * (x**8) + L0xbefff316 * (x**9) + L0xbefff318 * (x**10) + L0xbefff31a * (x**11) + L0xbefff31c * (x**12) + L0xbefff31e * (x**13) + L0xbefff320 * (x**14) + L0xbefff322 * (x**15))[3329, x**16 - 3260], eqmod (inp_poly * inp_poly) (L0xbefff324 * (x**0) + L0xbefff326 * (x**1) + L0xbefff328 * (x**2) + L0xbefff32a * (x**3) + L0xbefff32c * (x**4) + L0xbefff32e * (x**5) + L0xbefff330 * (x**6) + L0xbefff332 * (x**7) + L0xbefff334 * (x**8) + L0xbefff336 * (x**9) + L0xbefff338 * (x**10) + L0xbefff33a * (x**11) + L0xbefff33c * (x**12) + L0xbefff33e * (x**13) + L0xbefff340 * (x**14) + L0xbefff342 * (x**15))[3329, x**16 - 69], eqmod (inp_poly * inp_poly) (L0xbefff344 * (x**0) + L0xbefff346 * (x**1) + L0xbefff348 * (x**2) + L0xbefff34a * (x**3) + L0xbefff34c * (x**4) + L0xbefff34e * (x**5) + L0xbefff350 * (x**6) + L0xbefff352 * (x**7) + L0xbefff354 * (x**8) + L0xbefff356 * (x**9) + L0xbefff358 * (x**10) + L0xbefff35a * (x**11) + L0xbefff35c * (x**12) + L0xbefff35e * (x**13) + L0xbefff360 * (x**14) + L0xbefff362 * (x**15))[3329, x**16 - 569], eqmod (inp_poly * inp_poly) (L0xbefff364 * (x**0) + L0xbefff366 * (x**1) + L0xbefff368 * (x**2) + L0xbefff36a * (x**3) + L0xbefff36c * (x**4) + L0xbefff36e * (x**5) + L0xbefff370 * (x**6) + L0xbefff372 * (x**7) + L0xbefff374 * (x**8) + L0xbefff376 * (x**9) + L0xbefff378 * (x**10) + L0xbefff37a * (x**11) + L0xbefff37c * (x**12) + L0xbefff37e * (x**13) + L0xbefff380 * (x**14) + L0xbefff382 * (x**15))[3329, x**16 - 2760], eqmod (inp_poly * inp_poly) (L0xbefff384 * (x**0) + L0xbefff386 * (x**1) + L0xbefff388 * (x**2) + L0xbefff38a * (x**3) + L0xbefff38c * (x**4) + L0xbefff38e * (x**5) + L0xbefff390 * (x**6) + L0xbefff392 * (x**7) + L0xbefff394 * (x**8) + L0xbefff396 * (x**9) + L0xbefff398 * (x**10) + L0xbefff39a * (x**11) + L0xbefff39c * (x**12) + L0xbefff39e * (x**13) + L0xbefff3a0 * (x**14) + L0xbefff3a2 * (x**15))[3329, x**16 - 1746], eqmod (inp_poly * inp_poly) (L0xbefff3a4 * (x**0) + L0xbefff3a6 * (x**1) + L0xbefff3a8 * (x**2) + L0xbefff3aa * (x**3) + L0xbefff3ac * (x**4) + L0xbefff3ae * (x**5) + L0xbefff3b0 * (x**6) + L0xbefff3b2 * (x**7) + L0xbefff3b4 * (x**8) + L0xbefff3b6 * (x**9) + L0xbefff3b8 * (x**10) + L0xbefff3ba * (x**11) + L0xbefff3bc * (x**12) + L0xbefff3be * (x**13) + L0xbefff3c0 * (x**14) + L0xbefff3c2 * (x**15))[3329, x**16 - 1583]]
  &&
  and [(-6)@16 * 1664@16 <=s L0xbefff1c4, L0xbefff1c4 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1c6, L0xbefff1c6 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1c8, L0xbefff1c8 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1ca, L0xbefff1ca <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1cc, L0xbefff1cc <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1ce, L0xbefff1ce <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1d0, L0xbefff1d0 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1d2, L0xbefff1d2 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1d4, L0xbefff1d4 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1d6, L0xbefff1d6 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1d8, L0xbefff1d8 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1da, L0xbefff1da <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1dc, L0xbefff1dc <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1de, L0xbefff1de <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1e0, L0xbefff1e0 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1e2, L0xbefff1e2 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1e4, L0xbefff1e4 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1e6, L0xbefff1e6 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1e8, L0xbefff1e8 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1ea, L0xbefff1ea <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1ec, L0xbefff1ec <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1ee, L0xbefff1ee <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1f0, L0xbefff1f0 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1f2, L0xbefff1f2 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1f4, L0xbefff1f4 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1f6, L0xbefff1f6 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1f8, L0xbefff1f8 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1fa, L0xbefff1fa <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1fc, L0xbefff1fc <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff1fe, L0xbefff1fe <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff200, L0xbefff200 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff202, L0xbefff202 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff204, L0xbefff204 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff206, L0xbefff206 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff208, L0xbefff208 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff20a, L0xbefff20a <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff20c, L0xbefff20c <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff20e, L0xbefff20e <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff210, L0xbefff210 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff212, L0xbefff212 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff214, L0xbefff214 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff216, L0xbefff216 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff218, L0xbefff218 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff21a, L0xbefff21a <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff21c, L0xbefff21c <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff21e, L0xbefff21e <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff220, L0xbefff220 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff222, L0xbefff222 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff224, L0xbefff224 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff226, L0xbefff226 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff228, L0xbefff228 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff22a, L0xbefff22a <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff22c, L0xbefff22c <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff22e, L0xbefff22e <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff230, L0xbefff230 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff232, L0xbefff232 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff234, L0xbefff234 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff236, L0xbefff236 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff238, L0xbefff238 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff23a, L0xbefff23a <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff23c, L0xbefff23c <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff23e, L0xbefff23e <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff240, L0xbefff240 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff242, L0xbefff242 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff244, L0xbefff244 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff246, L0xbefff246 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff248, L0xbefff248 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff24a, L0xbefff24a <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff24c, L0xbefff24c <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff24e, L0xbefff24e <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff250, L0xbefff250 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff252, L0xbefff252 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff254, L0xbefff254 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff256, L0xbefff256 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff258, L0xbefff258 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff25a, L0xbefff25a <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff25c, L0xbefff25c <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff25e, L0xbefff25e <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff260, L0xbefff260 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff262, L0xbefff262 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff264, L0xbefff264 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff266, L0xbefff266 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff268, L0xbefff268 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff26a, L0xbefff26a <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff26c, L0xbefff26c <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff26e, L0xbefff26e <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff270, L0xbefff270 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff272, L0xbefff272 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff274, L0xbefff274 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff276, L0xbefff276 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff278, L0xbefff278 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff27a, L0xbefff27a <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff27c, L0xbefff27c <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff27e, L0xbefff27e <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff280, L0xbefff280 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff282, L0xbefff282 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff284, L0xbefff284 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff286, L0xbefff286 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff288, L0xbefff288 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff28a, L0xbefff28a <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff28c, L0xbefff28c <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff28e, L0xbefff28e <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff290, L0xbefff290 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff292, L0xbefff292 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff294, L0xbefff294 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff296, L0xbefff296 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff298, L0xbefff298 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff29a, L0xbefff29a <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff29c, L0xbefff29c <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff29e, L0xbefff29e <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2a0, L0xbefff2a0 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2a2, L0xbefff2a2 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2a4, L0xbefff2a4 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2a6, L0xbefff2a6 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2a8, L0xbefff2a8 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2aa, L0xbefff2aa <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2ac, L0xbefff2ac <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2ae, L0xbefff2ae <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2b0, L0xbefff2b0 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2b2, L0xbefff2b2 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2b4, L0xbefff2b4 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2b6, L0xbefff2b6 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2b8, L0xbefff2b8 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2ba, L0xbefff2ba <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2bc, L0xbefff2bc <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2be, L0xbefff2be <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2c0, L0xbefff2c0 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2c2, L0xbefff2c2 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2c4, L0xbefff2c4 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2c6, L0xbefff2c6 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2c8, L0xbefff2c8 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2ca, L0xbefff2ca <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2cc, L0xbefff2cc <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2ce, L0xbefff2ce <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2d0, L0xbefff2d0 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2d2, L0xbefff2d2 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2d4, L0xbefff2d4 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2d6, L0xbefff2d6 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2d8, L0xbefff2d8 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2da, L0xbefff2da <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2dc, L0xbefff2dc <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2de, L0xbefff2de <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2e0, L0xbefff2e0 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2e2, L0xbefff2e2 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2e4, L0xbefff2e4 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2e6, L0xbefff2e6 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2e8, L0xbefff2e8 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2ea, L0xbefff2ea <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2ec, L0xbefff2ec <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2ee, L0xbefff2ee <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2f0, L0xbefff2f0 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2f2, L0xbefff2f2 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2f4, L0xbefff2f4 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2f6, L0xbefff2f6 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2f8, L0xbefff2f8 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2fa, L0xbefff2fa <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2fc, L0xbefff2fc <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff2fe, L0xbefff2fe <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff300, L0xbefff300 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff302, L0xbefff302 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff304, L0xbefff304 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff306, L0xbefff306 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff308, L0xbefff308 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff30a, L0xbefff30a <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff30c, L0xbefff30c <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff30e, L0xbefff30e <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff310, L0xbefff310 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff312, L0xbefff312 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff314, L0xbefff314 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff316, L0xbefff316 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff318, L0xbefff318 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff31a, L0xbefff31a <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff31c, L0xbefff31c <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff31e, L0xbefff31e <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff320, L0xbefff320 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff322, L0xbefff322 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff324, L0xbefff324 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff326, L0xbefff326 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff328, L0xbefff328 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff32a, L0xbefff32a <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff32c, L0xbefff32c <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff32e, L0xbefff32e <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff330, L0xbefff330 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff332, L0xbefff332 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff334, L0xbefff334 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff336, L0xbefff336 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff338, L0xbefff338 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff33a, L0xbefff33a <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff33c, L0xbefff33c <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff33e, L0xbefff33e <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff340, L0xbefff340 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff342, L0xbefff342 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff344, L0xbefff344 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff346, L0xbefff346 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff348, L0xbefff348 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff34a, L0xbefff34a <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff34c, L0xbefff34c <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff34e, L0xbefff34e <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff350, L0xbefff350 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff352, L0xbefff352 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff354, L0xbefff354 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff356, L0xbefff356 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff358, L0xbefff358 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff35a, L0xbefff35a <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff35c, L0xbefff35c <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff35e, L0xbefff35e <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff360, L0xbefff360 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff362, L0xbefff362 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff364, L0xbefff364 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff366, L0xbefff366 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff368, L0xbefff368 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff36a, L0xbefff36a <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff36c, L0xbefff36c <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff36e, L0xbefff36e <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff370, L0xbefff370 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff372, L0xbefff372 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff374, L0xbefff374 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff376, L0xbefff376 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff378, L0xbefff378 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff37a, L0xbefff37a <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff37c, L0xbefff37c <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff37e, L0xbefff37e <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff380, L0xbefff380 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff382, L0xbefff382 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff384, L0xbefff384 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff386, L0xbefff386 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff388, L0xbefff388 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff38a, L0xbefff38a <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff38c, L0xbefff38c <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff38e, L0xbefff38e <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff390, L0xbefff390 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff392, L0xbefff392 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff394, L0xbefff394 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff396, L0xbefff396 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff398, L0xbefff398 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff39a, L0xbefff39a <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff39c, L0xbefff39c <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff39e, L0xbefff39e <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff3a0, L0xbefff3a0 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff3a2, L0xbefff3a2 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff3a4, L0xbefff3a4 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff3a6, L0xbefff3a6 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff3a8, L0xbefff3a8 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff3aa, L0xbefff3aa <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff3ac, L0xbefff3ac <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff3ae, L0xbefff3ae <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff3b0, L0xbefff3b0 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff3b2, L0xbefff3b2 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff3b4, L0xbefff3b4 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff3b6, L0xbefff3b6 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff3b8, L0xbefff3b8 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff3ba, L0xbefff3ba <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff3bc, L0xbefff3bc <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff3be, L0xbefff3be <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff3c0, L0xbefff3c0 <=s 6@16 * 1664@16, (-6)@16 * 1664@16 <=s L0xbefff3c2, L0xbefff3c2 <=s 6@16 * 1664@16];
*)

(* sub.w	r0, r0, #32                               #! PC = 0x400ae0 *)
subs dontcare r0 r0 32@sint32;
(* add.w	lr, r0, #512	; 0x200                      #! PC = 0x400ae4 *)
adds dontcare lr r0 512@sint32;
(* vmov	s13, lr                                    #! PC = 0x400ae8 *)
mov s13_b lr_b;
mov s13_t lr_t;
mov s13 lr;
mov zeta_s13 zeta_lr;
(* vmov	s23, r0                                    #! PC = 0x400aec *)
mov s23_b r0_b;
mov s23_t r0_t;
mov s23 r0;
mov zeta_s23 zeta_r0;
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff1c4; Value = 0xbf049124; PC = 0x400af0 *)
mov r2_b L0xbefff1c4;
mov r2_t L0xbefff1c6;
(* ldr.w	r3, [r0, #4]                              #! EA = L0xbefff1c8; Value = 0xb7c2b4ca; PC = 0x400af4 *)
mov r3_b L0xbefff1c8;
mov r3_t L0xbefff1ca;
(* ldr.w	r4, [r0, #8]                              #! EA = L0xbefff1cc; Value = 0xffa4fe13; PC = 0x400af8 *)
mov r4_b L0xbefff1cc;
mov r4_t L0xbefff1ce;
(* ldr.w	r5, [r0, #12]                             #! EA = L0xbefff1d0; Value = 0xfb840075; PC = 0x400afc *)
mov r5_b L0xbefff1d0;
mov r5_t L0xbefff1d2;
(* ldr.w	r6, [r0, #16]                             #! EA = L0xbefff1d4; Value = 0xb678066c; PC = 0x400b00 *)
mov r6_b L0xbefff1d4;
mov r6_t L0xbefff1d6;
(* ldr.w	r7, [r0, #20]                             #! EA = L0xbefff1d8; Value = 0xba80ecde; PC = 0x400b04 *)
mov r7_b L0xbefff1d8;
mov r7_t L0xbefff1da;
(* ldr.w	r8, [r0, #24]                             #! EA = L0xbefff1dc; Value = 0x0135fc13; PC = 0x400b08 *)
mov r8_b L0xbefff1dc;
mov r8_t L0xbefff1de;
(* ldr.w	r9, [r0, #28]                             #! EA = L0xbefff1e0; Value = 0xc3ddf4bd; PC = 0x400b0c *)
mov r9_b L0xbefff1e0;
mov r9_t L0xbefff1e2;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400b10 *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* ldr.w	r10, [r1], #4                             #! EA = L0x4016b8; Value = 0x16c32c11; PC = 0x400b14 *)
mov r10 L0x4016b8;
mov zeta_r10 zeta_L0x4016b8;
(* smulwb	lr, r10, r6                              #! PC = 0x400b18 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400b1c *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b20 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400b24 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x400b28 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x400b2c *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400b30 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x400b34 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400b38 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b3c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400b40 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400b44 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x400b48 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400b4c *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x400b50 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400b54 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b58 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400b5c *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400b60 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x400b64 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400b68 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x400b6c *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x400b70 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b74 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400b78 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400b7c *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x400b80 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400b84 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4016bc; Value = 0x16395e0d; PC = 0x400b88 *)
mov r10 L0x4016bc;
mov r11 L0x4016c0;
mov zeta_r10 zeta_L0x4016bc;
mov zeta_r11 zeta_L0x4016c0;
(* smulwb	lr, r10, r4                              #! PC = 0x400b8c *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x400b90 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b94 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400b98 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400b9c *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x400ba0 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400ba4 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x400ba8 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x400bac *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bb0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400bb4 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400bb8 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x400bbc *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400bc0 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400bc4 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x400bc8 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bcc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400bd0 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400bd4 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x400bd8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400bdc *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400be0 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400be4 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400be8 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400bec *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400bf0 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x400bf4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400bf8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4016c4; Value = 0x014eab2e; PC = 0x400bfc *)
mov r10 L0x4016c4;
mov r11 L0x4016c8;
mov zeta_r10 zeta_L0x4016c4;
mov zeta_r11 zeta_L0x4016c8;
(* smulwb	lr, r10, r3                              #! PC = 0x400c00 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x400c04 *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c08 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400c0c *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x400c10 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x400c14 *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400c18 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x400c1c *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x400c20 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c24 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400c28 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400c2c *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x400c30 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400c34 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4016cc; Value = 0x2cd52aae; PC = 0x400c38 *)
mov r10 L0x4016cc;
mov r11 L0x4016d0;
mov zeta_r10 zeta_L0x4016cc;
mov zeta_r11 zeta_L0x4016d0;
(* smulwb	lr, r10, r7                              #! PC = 0x400c3c *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400c40 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c44 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400c48 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400c4c *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400c50 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400c54 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400c58 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400c5c *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c60 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400c64 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400c68 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x400c6c *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x400c70 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;
(* vmov	r0, s23                                    #! PC = 0x400c74 *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* str.w	r6, [r0, #16]                             #! EA = L0xbefff1d4; PC = 0x400c78 *)
mov L0xbefff1d4 r6_b;
mov L0xbefff1d6 r6_t;
(* str.w	r7, [r0, #20]                             #! EA = L0xbefff1d8; PC = 0x400c7c *)
mov L0xbefff1d8 r7_b;
mov L0xbefff1da r7_t;
(* str.w	r8, [r0, #24]                             #! EA = L0xbefff1dc; PC = 0x400c80 *)
mov L0xbefff1dc r8_b;
mov L0xbefff1de r8_t;
(* str.w	r9, [r0, #28]                             #! EA = L0xbefff1e0; PC = 0x400c84 *)
mov L0xbefff1e0 r9_b;
mov L0xbefff1e2 r9_t;
(* str.w	r3, [r0, #4]                              #! EA = L0xbefff1c8; PC = 0x400c88 *)
mov L0xbefff1c8 r3_b;
mov L0xbefff1ca r3_t;
(* str.w	r4, [r0, #8]                              #! EA = L0xbefff1cc; PC = 0x400c8c *)
mov L0xbefff1cc r4_b;
mov L0xbefff1ce r4_t;
(* str.w	r5, [r0, #12]                             #! EA = L0xbefff1d0; PC = 0x400c90 *)
mov L0xbefff1d0 r5_b;
mov L0xbefff1d2 r5_t;
(* str.w	r2, [r0], #32                             #! EA = L0xbefff1c4; PC = 0x400c94 *)
mov L0xbefff1c4 r2_b;
mov L0xbefff1c6 r2_t;
(* vmov	lr, s13                                    #! PC = 0x400c98 *)
mov lr_b s13_b;
mov lr_t s13_t;
mov lr s13;
mov zeta_lr zeta_s13;
(* #bne.w	0x400aec <ntt_fast+1244>                 #! PC = 0x400ca0 *)
#bne.w	0x400aec <ntt_fast+1244>                 #! 0x400ca0 = 0x400ca0;
(* vmov	s23, r0                                    #! PC = 0x400aec *)
mov s23_b r0_b;
mov s23_t r0_t;
mov s23 r0;
mov zeta_s23 zeta_r0;
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff1e4; Value = 0xb4cc9db6; PC = 0x400af0 *)
mov r2_b L0xbefff1e4;
mov r2_t L0xbefff1e6;
(* ldr.w	r3, [r0, #4]                              #! EA = L0xbefff1e8; Value = 0xb178aa68; PC = 0x400af4 *)
mov r3_b L0xbefff1e8;
mov r3_t L0xbefff1ea;
(* ldr.w	r4, [r0, #8]                              #! EA = L0xbefff1ec; Value = 0x091008b9; PC = 0x400af8 *)
mov r4_b L0xbefff1ec;
mov r4_t L0xbefff1ee;
(* ldr.w	r5, [r0, #12]                             #! EA = L0xbefff1f0; Value = 0x01700cff; PC = 0x400afc *)
mov r5_b L0xbefff1f0;
mov r5_t L0xbefff1f2;
(* ldr.w	r6, [r0, #16]                             #! EA = L0xbefff1f4; Value = 0xb85a0c76; PC = 0x400b00 *)
mov r6_b L0xbefff1f4;
mov r6_t L0xbefff1f6;
(* ldr.w	r7, [r0, #20]                             #! EA = L0xbefff1f8; Value = 0xb8b2f41c; PC = 0x400b04 *)
mov r7_b L0xbefff1f8;
mov r7_t L0xbefff1fa;
(* ldr.w	r8, [r0, #24]                             #! EA = L0xbefff1fc; Value = 0x045bfb4f; PC = 0x400b08 *)
mov r8_b L0xbefff1fc;
mov r8_t L0xbefff1fe;
(* ldr.w	r9, [r0, #28]                             #! EA = L0xbefff200; Value = 0xb875ff29; PC = 0x400b0c *)
mov r9_b L0xbefff200;
mov r9_t L0xbefff202;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400b10 *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* ldr.w	r10, [r1], #4                             #! EA = L0x4016d4; Value = 0xbc2c9a1c; PC = 0x400b14 *)
mov r10 L0x4016d4;
mov zeta_r10 zeta_L0x4016d4;
(* smulwb	lr, r10, r6                              #! PC = 0x400b18 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400b1c *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b20 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400b24 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x400b28 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x400b2c *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400b30 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x400b34 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400b38 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b3c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400b40 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400b44 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x400b48 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400b4c *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x400b50 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400b54 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b58 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400b5c *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400b60 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x400b64 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400b68 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x400b6c *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x400b70 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b74 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400b78 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400b7c *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x400b80 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400b84 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4016d8; Value = 0xfa27d58e; PC = 0x400b88 *)
mov r10 L0x4016d8;
mov r11 L0x4016dc;
mov zeta_r10 zeta_L0x4016d8;
mov zeta_r11 zeta_L0x4016dc;
(* smulwb	lr, r10, r4                              #! PC = 0x400b8c *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x400b90 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b94 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400b98 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400b9c *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x400ba0 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400ba4 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x400ba8 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x400bac *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bb0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400bb4 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400bb8 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x400bbc *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400bc0 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400bc4 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x400bc8 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bcc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400bd0 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400bd4 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x400bd8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400bdc *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400be0 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400be4 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400be8 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400bec *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400bf0 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x400bf4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400bf8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4016e0; Value = 0x7de29fcd; PC = 0x400bfc *)
mov r10 L0x4016e0;
mov r11 L0x4016e4;
mov zeta_r10 zeta_L0x4016e0;
mov zeta_r11 zeta_L0x4016e4;
(* smulwb	lr, r10, r3                              #! PC = 0x400c00 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x400c04 *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c08 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400c0c *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x400c10 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x400c14 *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400c18 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x400c1c *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x400c20 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c24 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400c28 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400c2c *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x400c30 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400c34 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4016e8; Value = 0xaff27732; PC = 0x400c38 *)
mov r10 L0x4016e8;
mov r11 L0x4016ec;
mov zeta_r10 zeta_L0x4016e8;
mov zeta_r11 zeta_L0x4016ec;
(* smulwb	lr, r10, r7                              #! PC = 0x400c3c *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400c40 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c44 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400c48 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400c4c *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400c50 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400c54 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400c58 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400c5c *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c60 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400c64 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400c68 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x400c6c *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x400c70 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;
(* vmov	r0, s23                                    #! PC = 0x400c74 *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* str.w	r6, [r0, #16]                             #! EA = L0xbefff1f4; PC = 0x400c78 *)
mov L0xbefff1f4 r6_b;
mov L0xbefff1f6 r6_t;
(* str.w	r7, [r0, #20]                             #! EA = L0xbefff1f8; PC = 0x400c7c *)
mov L0xbefff1f8 r7_b;
mov L0xbefff1fa r7_t;
(* str.w	r8, [r0, #24]                             #! EA = L0xbefff1fc; PC = 0x400c80 *)
mov L0xbefff1fc r8_b;
mov L0xbefff1fe r8_t;
(* str.w	r9, [r0, #28]                             #! EA = L0xbefff200; PC = 0x400c84 *)
mov L0xbefff200 r9_b;
mov L0xbefff202 r9_t;
(* str.w	r3, [r0, #4]                              #! EA = L0xbefff1e8; PC = 0x400c88 *)
mov L0xbefff1e8 r3_b;
mov L0xbefff1ea r3_t;
(* str.w	r4, [r0, #8]                              #! EA = L0xbefff1ec; PC = 0x400c8c *)
mov L0xbefff1ec r4_b;
mov L0xbefff1ee r4_t;
(* str.w	r5, [r0, #12]                             #! EA = L0xbefff1f0; PC = 0x400c90 *)
mov L0xbefff1f0 r5_b;
mov L0xbefff1f2 r5_t;
(* str.w	r2, [r0], #32                             #! EA = L0xbefff1e4; PC = 0x400c94 *)
mov L0xbefff1e4 r2_b;
mov L0xbefff1e6 r2_t;
(* vmov	lr, s13                                    #! PC = 0x400c98 *)
mov lr_b s13_b;
mov lr_t s13_t;
mov lr s13;
mov zeta_lr zeta_s13;
(* #bne.w	0x400aec <ntt_fast+1244>                 #! PC = 0x400ca0 *)
#bne.w	0x400aec <ntt_fast+1244>                 #! 0x400ca0 = 0x400ca0;
(* vmov	s23, r0                                    #! PC = 0x400aec *)
mov s23_b r0_b;
mov s23_t r0_t;
mov s23 r0;
mov zeta_s23 zeta_r0;
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff204; Value = 0xb57b8eec; PC = 0x400af0 *)
mov r2_b L0xbefff204;
mov r2_t L0xbefff206;
(* ldr.w	r3, [r0, #4]                              #! EA = L0xbefff208; Value = 0xba51ab72; PC = 0x400af4 *)
mov r3_b L0xbefff208;
mov r3_t L0xbefff20a;
(* ldr.w	r4, [r0, #8]                              #! EA = L0xbefff20c; Value = 0x028d04ed; PC = 0x400af8 *)
mov r4_b L0xbefff20c;
mov r4_t L0xbefff20e;
(* ldr.w	r5, [r0, #12]                             #! EA = L0xbefff210; Value = 0x0e1afea6; PC = 0x400afc *)
mov r5_b L0xbefff210;
mov r5_t L0xbefff212;
(* ldr.w	r6, [r0, #16]                             #! EA = L0xbefff214; Value = 0xbb0efe3f; PC = 0x400b00 *)
mov r6_b L0xbefff214;
mov r6_t L0xbefff216;
(* ldr.w	r7, [r0, #20]                             #! EA = L0xbefff218; Value = 0xc2f5e32c; PC = 0x400b04 *)
mov r7_b L0xbefff218;
mov r7_t L0xbefff21a;
(* ldr.w	r8, [r0, #24]                             #! EA = L0xbefff21c; Value = 0x067205b3; PC = 0x400b08 *)
mov r8_b L0xbefff21c;
mov r8_t L0xbefff21e;
(* ldr.w	r9, [r0, #28]                             #! EA = L0xbefff220; Value = 0xc8c3fde5; PC = 0x400b0c *)
mov r9_b L0xbefff220;
mov r9_t L0xbefff222;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400b10 *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* ldr.w	r10, [r1], #4                             #! EA = L0x4016f0; Value = 0x66f8144e; PC = 0x400b14 *)
mov r10 L0x4016f0;
mov zeta_r10 zeta_L0x4016f0;
(* smulwb	lr, r10, r6                              #! PC = 0x400b18 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400b1c *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b20 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400b24 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x400b28 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x400b2c *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400b30 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x400b34 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400b38 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b3c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400b40 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400b44 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x400b48 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400b4c *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x400b50 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400b54 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b58 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400b5c *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400b60 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x400b64 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400b68 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x400b6c *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x400b70 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b74 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400b78 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400b7c *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x400b80 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400b84 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4016f4; Value = 0x5c0c9c92; PC = 0x400b88 *)
mov r10 L0x4016f4;
mov r11 L0x4016f8;
mov zeta_r10 zeta_L0x4016f4;
mov zeta_r11 zeta_L0x4016f8;
(* smulwb	lr, r10, r4                              #! PC = 0x400b8c *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x400b90 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b94 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400b98 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400b9c *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x400ba0 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400ba4 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x400ba8 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x400bac *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bb0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400bb4 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400bb8 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x400bbc *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400bc0 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400bc4 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x400bc8 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bcc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400bd0 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400bd4 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x400bd8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400bdc *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400be0 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400be4 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400be8 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400bec *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400bf0 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x400bf4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400bf8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4016fc; Value = 0x6c5a2074; PC = 0x400bfc *)
mov r10 L0x4016fc;
mov r11 L0x401700;
mov zeta_r10 zeta_L0x4016fc;
mov zeta_r11 zeta_L0x401700;
(* smulwb	lr, r10, r3                              #! PC = 0x400c00 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x400c04 *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c08 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400c0c *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x400c10 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x400c14 *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400c18 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x400c1c *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x400c20 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c24 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400c28 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400c2c *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x400c30 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400c34 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401704; Value = 0xfc4f0d9d; PC = 0x400c38 *)
mov r10 L0x401704;
mov r11 L0x401708;
mov zeta_r10 zeta_L0x401704;
mov zeta_r11 zeta_L0x401708;
(* smulwb	lr, r10, r7                              #! PC = 0x400c3c *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400c40 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c44 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400c48 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400c4c *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400c50 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400c54 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400c58 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400c5c *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c60 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400c64 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400c68 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x400c6c *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x400c70 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;
(* vmov	r0, s23                                    #! PC = 0x400c74 *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* str.w	r6, [r0, #16]                             #! EA = L0xbefff214; PC = 0x400c78 *)
mov L0xbefff214 r6_b;
mov L0xbefff216 r6_t;
(* str.w	r7, [r0, #20]                             #! EA = L0xbefff218; PC = 0x400c7c *)
mov L0xbefff218 r7_b;
mov L0xbefff21a r7_t;
(* str.w	r8, [r0, #24]                             #! EA = L0xbefff21c; PC = 0x400c80 *)
mov L0xbefff21c r8_b;
mov L0xbefff21e r8_t;
(* str.w	r9, [r0, #28]                             #! EA = L0xbefff220; PC = 0x400c84 *)
mov L0xbefff220 r9_b;
mov L0xbefff222 r9_t;
(* str.w	r3, [r0, #4]                              #! EA = L0xbefff208; PC = 0x400c88 *)
mov L0xbefff208 r3_b;
mov L0xbefff20a r3_t;
(* str.w	r4, [r0, #8]                              #! EA = L0xbefff20c; PC = 0x400c8c *)
mov L0xbefff20c r4_b;
mov L0xbefff20e r4_t;
(* str.w	r5, [r0, #12]                             #! EA = L0xbefff210; PC = 0x400c90 *)
mov L0xbefff210 r5_b;
mov L0xbefff212 r5_t;
(* str.w	r2, [r0], #32                             #! EA = L0xbefff204; PC = 0x400c94 *)
mov L0xbefff204 r2_b;
mov L0xbefff206 r2_t;
(* vmov	lr, s13                                    #! PC = 0x400c98 *)
mov lr_b s13_b;
mov lr_t s13_t;
mov lr s13;
mov zeta_lr zeta_s13;
(* #bne.w	0x400aec <ntt_fast+1244>                 #! PC = 0x400ca0 *)
#bne.w	0x400aec <ntt_fast+1244>                 #! 0x400ca0 = 0x400ca0;
(* vmov	s23, r0                                    #! PC = 0x400aec *)
mov s23_b r0_b;
mov s23_t r0_t;
mov s23 r0;
mov zeta_s23 zeta_r0;
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff224; Value = 0xb265925a; PC = 0x400af0 *)
mov r2_b L0xbefff224;
mov r2_t L0xbefff226;
(* ldr.w	r3, [r0, #4]                              #! EA = L0xbefff228; Value = 0xb871b55c; PC = 0x400af4 *)
mov r3_b L0xbefff228;
mov r3_t L0xbefff22a;
(* ldr.w	r4, [r0, #8]                              #! EA = L0xbefff22c; Value = 0x06270c9b; PC = 0x400af8 *)
mov r4_b L0xbefff22c;
mov r4_t L0xbefff22e;
(* ldr.w	r5, [r0, #12]                             #! EA = L0xbefff230; Value = 0x065af76e; PC = 0x400afc *)
mov r5_b L0xbefff230;
mov r5_t L0xbefff232;
(* ldr.w	r6, [r0, #16]                             #! EA = L0xbefff234; Value = 0xc0c00abf; PC = 0x400b00 *)
mov r6_b L0xbefff234;
mov r6_t L0xbefff236;
(* ldr.w	r7, [r0, #20]                             #! EA = L0xbefff238; Value = 0xba7dec6e; PC = 0x400b04 *)
mov r7_b L0xbefff238;
mov r7_t L0xbefff23a;
(* ldr.w	r8, [r0, #24]                             #! EA = L0xbefff23c; Value = 0x08a6f90b; PC = 0x400b08 *)
mov r8_b L0xbefff23c;
mov r8_t L0xbefff23e;
(* ldr.w	r9, [r0, #28]                             #! EA = L0xbefff240; Value = 0xbcf700c9; PC = 0x400b0c *)
mov r9_b L0xbefff240;
mov r9_t L0xbefff242;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400b10 *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* ldr.w	r10, [r1], #4                             #! EA = L0x40170c; Value = 0x71811d74; PC = 0x400b14 *)
mov r10 L0x40170c;
mov zeta_r10 zeta_L0x40170c;
(* smulwb	lr, r10, r6                              #! PC = 0x400b18 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400b1c *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b20 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400b24 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x400b28 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x400b2c *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400b30 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x400b34 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400b38 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b3c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400b40 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400b44 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x400b48 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400b4c *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x400b50 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400b54 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b58 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400b5c *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400b60 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x400b64 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400b68 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x400b6c *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x400b70 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b74 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400b78 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400b7c *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x400b80 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400b84 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401710; Value = 0xaf19ea51; PC = 0x400b88 *)
mov r10 L0x401710;
mov r11 L0x401714;
mov zeta_r10 zeta_L0x401710;
mov zeta_r11 zeta_L0x401714;
(* smulwb	lr, r10, r4                              #! PC = 0x400b8c *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x400b90 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b94 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400b98 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400b9c *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x400ba0 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400ba4 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x400ba8 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x400bac *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bb0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400bb4 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400bb8 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x400bbc *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400bc0 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400bc4 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x400bc8 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bcc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400bd0 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400bd4 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x400bd8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400bdc *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400be0 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400be4 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400be8 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400bec *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400bf0 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x400bf4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400bf8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401718; Value = 0x3a22e9a0; PC = 0x400bfc *)
mov r10 L0x401718;
mov r11 L0x40171c;
mov zeta_r10 zeta_L0x401718;
mov zeta_r11 zeta_L0x40171c;
(* smulwb	lr, r10, r3                              #! PC = 0x400c00 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x400c04 *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c08 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400c0c *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x400c10 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x400c14 *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400c18 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x400c1c *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x400c20 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c24 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400c28 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400c2c *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x400c30 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400c34 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401720; Value = 0xe7da790b; PC = 0x400c38 *)
mov r10 L0x401720;
mov r11 L0x401724;
mov zeta_r10 zeta_L0x401720;
mov zeta_r11 zeta_L0x401724;
(* smulwb	lr, r10, r7                              #! PC = 0x400c3c *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400c40 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c44 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400c48 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400c4c *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400c50 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400c54 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400c58 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400c5c *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c60 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400c64 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400c68 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x400c6c *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x400c70 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;
(* vmov	r0, s23                                    #! PC = 0x400c74 *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* str.w	r6, [r0, #16]                             #! EA = L0xbefff234; PC = 0x400c78 *)
mov L0xbefff234 r6_b;
mov L0xbefff236 r6_t;
(* str.w	r7, [r0, #20]                             #! EA = L0xbefff238; PC = 0x400c7c *)
mov L0xbefff238 r7_b;
mov L0xbefff23a r7_t;
(* str.w	r8, [r0, #24]                             #! EA = L0xbefff23c; PC = 0x400c80 *)
mov L0xbefff23c r8_b;
mov L0xbefff23e r8_t;
(* str.w	r9, [r0, #28]                             #! EA = L0xbefff240; PC = 0x400c84 *)
mov L0xbefff240 r9_b;
mov L0xbefff242 r9_t;
(* str.w	r3, [r0, #4]                              #! EA = L0xbefff228; PC = 0x400c88 *)
mov L0xbefff228 r3_b;
mov L0xbefff22a r3_t;
(* str.w	r4, [r0, #8]                              #! EA = L0xbefff22c; PC = 0x400c8c *)
mov L0xbefff22c r4_b;
mov L0xbefff22e r4_t;
(* str.w	r5, [r0, #12]                             #! EA = L0xbefff230; PC = 0x400c90 *)
mov L0xbefff230 r5_b;
mov L0xbefff232 r5_t;
(* str.w	r2, [r0], #32                             #! EA = L0xbefff224; PC = 0x400c94 *)
mov L0xbefff224 r2_b;
mov L0xbefff226 r2_t;
(* vmov	lr, s13                                    #! PC = 0x400c98 *)
mov lr_b s13_b;
mov lr_t s13_t;
mov lr s13;
mov zeta_lr zeta_s13;
(* #bne.w	0x400aec <ntt_fast+1244>                 #! PC = 0x400ca0 *)
#bne.w	0x400aec <ntt_fast+1244>                 #! 0x400ca0 = 0x400ca0;
(* vmov	s23, r0                                    #! PC = 0x400aec *)
mov s23_b r0_b;
mov s23_t r0_t;
mov s23 r0;
mov zeta_s23 zeta_r0;
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff244; Value = 0xb30c98d4; PC = 0x400af0 *)
mov r2_b L0xbefff244;
mov r2_t L0xbefff246;
(* ldr.w	r3, [r0, #4]                              #! EA = L0xbefff248; Value = 0xbb25a356; PC = 0x400af4 *)
mov r3_b L0xbefff248;
mov r3_t L0xbefff24a;
(* ldr.w	r4, [r0, #8]                              #! EA = L0xbefff24c; Value = 0x0064fcd1; PC = 0x400af8 *)
mov r4_b L0xbefff24c;
mov r4_t L0xbefff24e;
(* ldr.w	r5, [r0, #12]                             #! EA = L0xbefff250; Value = 0xfe890064; PC = 0x400afc *)
mov r5_b L0xbefff250;
mov r5_t L0xbefff252;
(* ldr.w	r6, [r0, #16]                             #! EA = L0xbefff254; Value = 0xbb391408; PC = 0x400b00 *)
mov r6_b L0xbefff254;
mov r6_t L0xbefff256;
(* ldr.w	r7, [r0, #20]                             #! EA = L0xbefff258; Value = 0xc21fea46; PC = 0x400b04 *)
mov r7_b L0xbefff258;
mov r7_t L0xbefff25a;
(* ldr.w	r8, [r0, #24]                             #! EA = L0xbefff25c; Value = 0x02d8f7a7; PC = 0x400b08 *)
mov r8_b L0xbefff25c;
mov r8_t L0xbefff25e;
(* ldr.w	r9, [r0, #28]                             #! EA = L0xbefff260; Value = 0xb2a3f859; PC = 0x400b0c *)
mov r9_b L0xbefff260;
mov r9_t L0xbefff262;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400b10 *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* ldr.w	r10, [r1], #4                             #! EA = L0x401728; Value = 0xea3cc040; PC = 0x400b14 *)
mov r10 L0x401728;
mov zeta_r10 zeta_L0x401728;
(* smulwb	lr, r10, r6                              #! PC = 0x400b18 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400b1c *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b20 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400b24 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x400b28 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x400b2c *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400b30 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x400b34 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400b38 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b3c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400b40 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400b44 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x400b48 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400b4c *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x400b50 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400b54 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b58 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400b5c *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400b60 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x400b64 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400b68 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x400b6c *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x400b70 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b74 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400b78 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400b7c *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x400b80 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400b84 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x40172c; Value = 0x31fc27af; PC = 0x400b88 *)
mov r10 L0x40172c;
mov r11 L0x401730;
mov zeta_r10 zeta_L0x40172c;
mov zeta_r11 zeta_L0x401730;
(* smulwb	lr, r10, r4                              #! PC = 0x400b8c *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x400b90 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b94 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400b98 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400b9c *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x400ba0 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400ba4 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x400ba8 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x400bac *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bb0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400bb4 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400bb8 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x400bbc *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400bc0 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400bc4 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x400bc8 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bcc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400bd0 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400bd4 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x400bd8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400bdc *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400be0 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400be4 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400be8 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400bec *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400bf0 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x400bf4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400bf8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401734; Value = 0x82f5ed16; PC = 0x400bfc *)
mov r10 L0x401734;
mov r11 L0x401738;
mov zeta_r10 zeta_L0x401734;
mov zeta_r11 zeta_L0x401738;
(* smulwb	lr, r10, r3                              #! PC = 0x400c00 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x400c04 *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c08 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400c0c *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x400c10 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x400c14 *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400c18 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x400c1c *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x400c20 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c24 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400c28 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400c2c *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x400c30 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400c34 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x40173c; Value = 0xd6795921; PC = 0x400c38 *)
mov r10 L0x40173c;
mov r11 L0x401740;
mov zeta_r10 zeta_L0x40173c;
mov zeta_r11 zeta_L0x401740;
(* smulwb	lr, r10, r7                              #! PC = 0x400c3c *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400c40 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c44 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400c48 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400c4c *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400c50 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400c54 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400c58 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400c5c *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c60 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400c64 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400c68 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x400c6c *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x400c70 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;
(* vmov	r0, s23                                    #! PC = 0x400c74 *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* str.w	r6, [r0, #16]                             #! EA = L0xbefff254; PC = 0x400c78 *)
mov L0xbefff254 r6_b;
mov L0xbefff256 r6_t;
(* str.w	r7, [r0, #20]                             #! EA = L0xbefff258; PC = 0x400c7c *)
mov L0xbefff258 r7_b;
mov L0xbefff25a r7_t;
(* str.w	r8, [r0, #24]                             #! EA = L0xbefff25c; PC = 0x400c80 *)
mov L0xbefff25c r8_b;
mov L0xbefff25e r8_t;
(* str.w	r9, [r0, #28]                             #! EA = L0xbefff260; PC = 0x400c84 *)
mov L0xbefff260 r9_b;
mov L0xbefff262 r9_t;
(* str.w	r3, [r0, #4]                              #! EA = L0xbefff248; PC = 0x400c88 *)
mov L0xbefff248 r3_b;
mov L0xbefff24a r3_t;
(* str.w	r4, [r0, #8]                              #! EA = L0xbefff24c; PC = 0x400c8c *)
mov L0xbefff24c r4_b;
mov L0xbefff24e r4_t;
(* str.w	r5, [r0, #12]                             #! EA = L0xbefff250; PC = 0x400c90 *)
mov L0xbefff250 r5_b;
mov L0xbefff252 r5_t;
(* str.w	r2, [r0], #32                             #! EA = L0xbefff244; PC = 0x400c94 *)
mov L0xbefff244 r2_b;
mov L0xbefff246 r2_t;
(* vmov	lr, s13                                    #! PC = 0x400c98 *)
mov lr_b s13_b;
mov lr_t s13_t;
mov lr s13;
mov zeta_lr zeta_s13;
(* #bne.w	0x400aec <ntt_fast+1244>                 #! PC = 0x400ca0 *)
#bne.w	0x400aec <ntt_fast+1244>                 #! 0x400ca0 = 0x400ca0;
(* vmov	s23, r0                                    #! PC = 0x400aec *)
mov s23_b r0_b;
mov s23_t r0_t;
mov s23 r0;
mov zeta_s23 zeta_r0;
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff264; Value = 0xb5aa9800; PC = 0x400af0 *)
mov r2_b L0xbefff264;
mov r2_t L0xbefff266;
(* ldr.w	r3, [r0, #4]                              #! EA = L0xbefff268; Value = 0xb78fafc6; PC = 0x400af4 *)
mov r3_b L0xbefff268;
mov r3_t L0xbefff26a;
(* ldr.w	r4, [r0, #8]                              #! EA = L0xbefff26c; Value = 0xf6e8fc25; PC = 0x400af8 *)
mov r4_b L0xbefff26c;
mov r4_t L0xbefff26e;
(* ldr.w	r5, [r0, #12]                             #! EA = L0xbefff270; Value = 0xfc6908cc; PC = 0x400afc *)
mov r5_b L0xbefff270;
mov r5_t L0xbefff272;
(* ldr.w	r6, [r0, #16]                             #! EA = L0xbefff274; Value = 0xb5b51138; PC = 0x400b00 *)
mov r6_b L0xbefff274;
mov r6_t L0xbefff276;
(* ldr.w	r7, [r0, #20]                             #! EA = L0xbefff278; Value = 0xb815e4b6; PC = 0x400b04 *)
mov r7_b L0xbefff278;
mov r7_t L0xbefff27a;
(* ldr.w	r8, [r0, #24]                             #! EA = L0xbefff27c; Value = 0x0c32fbf5; PC = 0x400b08 *)
mov r8_b L0xbefff27c;
mov r8_t L0xbefff27e;
(* ldr.w	r9, [r0, #28]                             #! EA = L0xbefff280; Value = 0xb495f229; PC = 0x400b0c *)
mov r9_b L0xbefff280;
mov r9_t L0xbefff282;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400b10 *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* ldr.w	r10, [r1], #4                             #! EA = L0x401744; Value = 0x044e701f; PC = 0x400b14 *)
mov r10 L0x401744;
mov zeta_r10 zeta_L0x401744;
(* smulwb	lr, r10, r6                              #! PC = 0x400b18 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400b1c *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b20 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400b24 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x400b28 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x400b2c *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400b30 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x400b34 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400b38 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b3c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400b40 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400b44 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x400b48 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400b4c *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x400b50 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400b54 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b58 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400b5c *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400b60 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x400b64 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400b68 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x400b6c *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x400b70 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b74 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400b78 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400b7c *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x400b80 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400b84 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401748; Value = 0xc13fe765; PC = 0x400b88 *)
mov r10 L0x401748;
mov r11 L0x40174c;
mov zeta_r10 zeta_L0x401748;
mov zeta_r11 zeta_L0x40174c;
(* smulwb	lr, r10, r4                              #! PC = 0x400b8c *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x400b90 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b94 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400b98 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400b9c *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x400ba0 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400ba4 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x400ba8 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x400bac *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bb0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400bb4 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400bb8 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x400bbc *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400bc0 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400bc4 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x400bc8 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bcc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400bd0 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400bd4 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x400bd8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400bdc *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400be0 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400be4 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400be8 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400bec *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400bf0 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x400bf4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400bf8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401750; Value = 0x8e08c440; PC = 0x400bfc *)
mov r10 L0x401750;
mov r11 L0x401754;
mov zeta_r10 zeta_L0x401750;
mov zeta_r11 zeta_L0x401754;
(* smulwb	lr, r10, r3                              #! PC = 0x400c00 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x400c04 *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c08 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400c0c *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x400c10 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x400c14 *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400c18 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x400c1c *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x400c20 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c24 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400c28 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400c2c *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x400c30 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400c34 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401758; Value = 0x7059d1b5; PC = 0x400c38 *)
mov r10 L0x401758;
mov r11 L0x40175c;
mov zeta_r10 zeta_L0x401758;
mov zeta_r11 zeta_L0x40175c;
(* smulwb	lr, r10, r7                              #! PC = 0x400c3c *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400c40 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c44 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400c48 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400c4c *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400c50 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400c54 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400c58 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400c5c *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c60 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400c64 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400c68 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x400c6c *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x400c70 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;
(* vmov	r0, s23                                    #! PC = 0x400c74 *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* str.w	r6, [r0, #16]                             #! EA = L0xbefff274; PC = 0x400c78 *)
mov L0xbefff274 r6_b;
mov L0xbefff276 r6_t;
(* str.w	r7, [r0, #20]                             #! EA = L0xbefff278; PC = 0x400c7c *)
mov L0xbefff278 r7_b;
mov L0xbefff27a r7_t;
(* str.w	r8, [r0, #24]                             #! EA = L0xbefff27c; PC = 0x400c80 *)
mov L0xbefff27c r8_b;
mov L0xbefff27e r8_t;
(* str.w	r9, [r0, #28]                             #! EA = L0xbefff280; PC = 0x400c84 *)
mov L0xbefff280 r9_b;
mov L0xbefff282 r9_t;
(* str.w	r3, [r0, #4]                              #! EA = L0xbefff268; PC = 0x400c88 *)
mov L0xbefff268 r3_b;
mov L0xbefff26a r3_t;
(* str.w	r4, [r0, #8]                              #! EA = L0xbefff26c; PC = 0x400c8c *)
mov L0xbefff26c r4_b;
mov L0xbefff26e r4_t;
(* str.w	r5, [r0, #12]                             #! EA = L0xbefff270; PC = 0x400c90 *)
mov L0xbefff270 r5_b;
mov L0xbefff272 r5_t;
(* str.w	r2, [r0], #32                             #! EA = L0xbefff264; PC = 0x400c94 *)
mov L0xbefff264 r2_b;
mov L0xbefff266 r2_t;
(* vmov	lr, s13                                    #! PC = 0x400c98 *)
mov lr_b s13_b;
mov lr_t s13_t;
mov lr s13;
mov zeta_lr zeta_s13;
(* #bne.w	0x400aec <ntt_fast+1244>                 #! PC = 0x400ca0 *)
#bne.w	0x400aec <ntt_fast+1244>                 #! 0x400ca0 = 0x400ca0;
(* vmov	s23, r0                                    #! PC = 0x400aec *)
mov s23_b r0_b;
mov s23_t r0_t;
mov s23 r0;
mov zeta_s23 zeta_r0;
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff284; Value = 0xb41a8c3a; PC = 0x400af0 *)
mov r2_b L0xbefff284;
mov r2_t L0xbefff286;
(* ldr.w	r3, [r0, #4]                              #! EA = L0xbefff288; Value = 0xb3cfbb8b; PC = 0x400af4 *)
mov r3_b L0xbefff288;
mov r3_t L0xbefff28a;
(* ldr.w	r4, [r0, #8]                              #! EA = L0xbefff28c; Value = 0xfe02f281; PC = 0x400af8 *)
mov r4_b L0xbefff28c;
mov r4_t L0xbefff28e;
(* ldr.w	r5, [r0, #12]                             #! EA = L0xbefff290; Value = 0xf536fcc2; PC = 0x400afc *)
mov r5_b L0xbefff290;
mov r5_t L0xbefff292;
(* ldr.w	r6, [r0, #16]                             #! EA = L0xbefff294; Value = 0xc5ec106a; PC = 0x400b00 *)
mov r6_b L0xbefff294;
mov r6_t L0xbefff296;
(* ldr.w	r7, [r0, #20]                             #! EA = L0xbefff298; Value = 0xba61f035; PC = 0x400b04 *)
mov r7_b L0xbefff298;
mov r7_t L0xbefff29a;
(* ldr.w	r8, [r0, #24]                             #! EA = L0xbefff29c; Value = 0x0294fae4; PC = 0x400b08 *)
mov r8_b L0xbefff29c;
mov r8_t L0xbefff29e;
(* ldr.w	r9, [r0, #28]                             #! EA = L0xbefff2a0; Value = 0xbc91eb3b; PC = 0x400b0c *)
mov r9_b L0xbefff2a0;
mov r9_t L0xbefff2a2;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400b10 *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* ldr.w	r10, [r1], #4                             #! EA = L0x401760; Value = 0xac4184cf; PC = 0x400b14 *)
mov r10 L0x401760;
mov zeta_r10 zeta_L0x401760;
(* smulwb	lr, r10, r6                              #! PC = 0x400b18 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400b1c *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b20 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400b24 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x400b28 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x400b2c *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400b30 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x400b34 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400b38 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b3c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400b40 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400b44 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x400b48 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400b4c *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x400b50 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400b54 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b58 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400b5c *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400b60 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x400b64 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400b68 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x400b6c *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x400b70 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b74 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400b78 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400b7c *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x400b80 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400b84 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401764; Value = 0xdc518394; PC = 0x400b88 *)
mov r10 L0x401764;
mov r11 L0x401768;
mov zeta_r10 zeta_L0x401764;
mov zeta_r11 zeta_L0x401768;
(* smulwb	lr, r10, r4                              #! PC = 0x400b8c *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x400b90 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b94 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400b98 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400b9c *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x400ba0 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400ba4 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x400ba8 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x400bac *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bb0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400bb4 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400bb8 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x400bbc *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400bc0 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400bc4 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x400bc8 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bcc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400bd0 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400bd4 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x400bd8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400bdc *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400be0 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400be4 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400be8 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400bec *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400bf0 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x400bf4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400bf8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x40176c; Value = 0x483585bb; PC = 0x400bfc *)
mov r10 L0x40176c;
mov r11 L0x401770;
mov zeta_r10 zeta_L0x40176c;
mov zeta_r11 zeta_L0x401770;
(* smulwb	lr, r10, r3                              #! PC = 0x400c00 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x400c04 *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c08 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400c0c *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x400c10 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x400c14 *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400c18 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x400c1c *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x400c20 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c24 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400c28 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400c2c *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x400c30 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400c34 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401774; Value = 0xbb67bcf2; PC = 0x400c38 *)
mov r10 L0x401774;
mov r11 L0x401778;
mov zeta_r10 zeta_L0x401774;
mov zeta_r11 zeta_L0x401778;
(* smulwb	lr, r10, r7                              #! PC = 0x400c3c *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400c40 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c44 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400c48 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400c4c *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400c50 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400c54 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400c58 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400c5c *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c60 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400c64 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400c68 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x400c6c *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x400c70 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;
(* vmov	r0, s23                                    #! PC = 0x400c74 *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* str.w	r6, [r0, #16]                             #! EA = L0xbefff294; PC = 0x400c78 *)
mov L0xbefff294 r6_b;
mov L0xbefff296 r6_t;
(* str.w	r7, [r0, #20]                             #! EA = L0xbefff298; PC = 0x400c7c *)
mov L0xbefff298 r7_b;
mov L0xbefff29a r7_t;
(* str.w	r8, [r0, #24]                             #! EA = L0xbefff29c; PC = 0x400c80 *)
mov L0xbefff29c r8_b;
mov L0xbefff29e r8_t;
(* str.w	r9, [r0, #28]                             #! EA = L0xbefff2a0; PC = 0x400c84 *)
mov L0xbefff2a0 r9_b;
mov L0xbefff2a2 r9_t;
(* str.w	r3, [r0, #4]                              #! EA = L0xbefff288; PC = 0x400c88 *)
mov L0xbefff288 r3_b;
mov L0xbefff28a r3_t;
(* str.w	r4, [r0, #8]                              #! EA = L0xbefff28c; PC = 0x400c8c *)
mov L0xbefff28c r4_b;
mov L0xbefff28e r4_t;
(* str.w	r5, [r0, #12]                             #! EA = L0xbefff290; PC = 0x400c90 *)
mov L0xbefff290 r5_b;
mov L0xbefff292 r5_t;
(* str.w	r2, [r0], #32                             #! EA = L0xbefff284; PC = 0x400c94 *)
mov L0xbefff284 r2_b;
mov L0xbefff286 r2_t;
(* vmov	lr, s13                                    #! PC = 0x400c98 *)
mov lr_b s13_b;
mov lr_t s13_t;
mov lr s13;
mov zeta_lr zeta_s13;
(* #bne.w	0x400aec <ntt_fast+1244>                 #! PC = 0x400ca0 *)
#bne.w	0x400aec <ntt_fast+1244>                 #! 0x400ca0 = 0x400ca0;
(* vmov	s23, r0                                    #! PC = 0x400aec *)
mov s23_b r0_b;
mov s23_t r0_t;
mov s23 r0;
mov zeta_s23 zeta_r0;
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff2a4; Value = 0xbee09312; PC = 0x400af0 *)
mov r2_b L0xbefff2a4;
mov r2_t L0xbefff2a6;
(* ldr.w	r3, [r0, #4]                              #! EA = L0xbefff2a8; Value = 0xb579b159; PC = 0x400af4 *)
mov r3_b L0xbefff2a8;
mov r3_t L0xbefff2aa;
(* ldr.w	r4, [r0, #8]                              #! EA = L0xbefff2ac; Value = 0xf94afc9d; PC = 0x400af8 *)
mov r4_b L0xbefff2ac;
mov r4_t L0xbefff2ae;
(* ldr.w	r5, [r0, #12]                             #! EA = L0xbefff2b0; Value = 0xfe70f686; PC = 0x400afc *)
mov r5_b L0xbefff2b0;
mov r5_t L0xbefff2b2;
(* ldr.w	r6, [r0, #16]                             #! EA = L0xbefff2b4; Value = 0xbfc60896; PC = 0x400b00 *)
mov r6_b L0xbefff2b4;
mov r6_t L0xbefff2b6;
(* ldr.w	r7, [r0, #20]                             #! EA = L0xbefff2b8; Value = 0xbc0ff143; PC = 0x400b04 *)
mov r7_b L0xbefff2b8;
mov r7_t L0xbefff2ba;
(* ldr.w	r8, [r0, #24]                             #! EA = L0xbefff2bc; Value = 0x030a07a0; PC = 0x400b08 *)
mov r8_b L0xbefff2bc;
mov r8_t L0xbefff2be;
(* ldr.w	r9, [r0, #28]                             #! EA = L0xbefff2c0; Value = 0xbb73ed07; PC = 0x400b0c *)
mov r9_b L0xbefff2c0;
mov r9_t L0xbefff2c2;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400b10 *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* ldr.w	r10, [r1], #4                             #! EA = L0x40177c; Value = 0x6681f601; PC = 0x400b14 *)
mov r10 L0x40177c;
mov zeta_r10 zeta_L0x40177c;
(* smulwb	lr, r10, r6                              #! PC = 0x400b18 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400b1c *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b20 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400b24 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x400b28 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x400b2c *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400b30 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x400b34 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400b38 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b3c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400b40 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400b44 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x400b48 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400b4c *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x400b50 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400b54 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b58 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400b5c *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400b60 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x400b64 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400b68 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x400b6c *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x400b70 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b74 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400b78 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400b7c *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x400b80 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400b84 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401780; Value = 0x658209b1; PC = 0x400b88 *)
mov r10 L0x401780;
mov r11 L0x401784;
mov zeta_r10 zeta_L0x401780;
mov zeta_r11 zeta_L0x401784;
(* smulwb	lr, r10, r4                              #! PC = 0x400b8c *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x400b90 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b94 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400b98 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400b9c *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x400ba0 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400ba4 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x400ba8 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x400bac *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bb0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400bb4 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400bb8 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x400bbc *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400bc0 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400bc4 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x400bc8 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bcc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400bd0 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400bd4 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x400bd8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400bdc *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400be0 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400be4 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400be8 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400bec *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400bf0 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x400bf4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400bf8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401788; Value = 0x385e2025; PC = 0x400bfc *)
mov r10 L0x401788;
mov r11 L0x40178c;
mov zeta_r10 zeta_L0x401788;
mov zeta_r11 zeta_L0x40178c;
(* smulwb	lr, r10, r3                              #! PC = 0x400c00 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x400c04 *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c08 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400c0c *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x400c10 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x400c14 *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400c18 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x400c1c *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x400c20 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c24 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400c28 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400c2c *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x400c30 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400c34 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401790; Value = 0x149bf401; PC = 0x400c38 *)
mov r10 L0x401790;
mov r11 L0x401794;
mov zeta_r10 zeta_L0x401790;
mov zeta_r11 zeta_L0x401794;
(* smulwb	lr, r10, r7                              #! PC = 0x400c3c *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400c40 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c44 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400c48 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400c4c *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400c50 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400c54 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400c58 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400c5c *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c60 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400c64 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400c68 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x400c6c *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x400c70 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;
(* vmov	r0, s23                                    #! PC = 0x400c74 *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* str.w	r6, [r0, #16]                             #! EA = L0xbefff2b4; PC = 0x400c78 *)
mov L0xbefff2b4 r6_b;
mov L0xbefff2b6 r6_t;
(* str.w	r7, [r0, #20]                             #! EA = L0xbefff2b8; PC = 0x400c7c *)
mov L0xbefff2b8 r7_b;
mov L0xbefff2ba r7_t;
(* str.w	r8, [r0, #24]                             #! EA = L0xbefff2bc; PC = 0x400c80 *)
mov L0xbefff2bc r8_b;
mov L0xbefff2be r8_t;
(* str.w	r9, [r0, #28]                             #! EA = L0xbefff2c0; PC = 0x400c84 *)
mov L0xbefff2c0 r9_b;
mov L0xbefff2c2 r9_t;
(* str.w	r3, [r0, #4]                              #! EA = L0xbefff2a8; PC = 0x400c88 *)
mov L0xbefff2a8 r3_b;
mov L0xbefff2aa r3_t;
(* str.w	r4, [r0, #8]                              #! EA = L0xbefff2ac; PC = 0x400c8c *)
mov L0xbefff2ac r4_b;
mov L0xbefff2ae r4_t;
(* str.w	r5, [r0, #12]                             #! EA = L0xbefff2b0; PC = 0x400c90 *)
mov L0xbefff2b0 r5_b;
mov L0xbefff2b2 r5_t;
(* str.w	r2, [r0], #32                             #! EA = L0xbefff2a4; PC = 0x400c94 *)
mov L0xbefff2a4 r2_b;
mov L0xbefff2a6 r2_t;
(* vmov	lr, s13                                    #! PC = 0x400c98 *)
mov lr_b s13_b;
mov lr_t s13_t;
mov lr s13;
mov zeta_lr zeta_s13;
(* #bne.w	0x400aec <ntt_fast+1244>                 #! PC = 0x400ca0 *)
#bne.w	0x400aec <ntt_fast+1244>                 #! 0x400ca0 = 0x400ca0;
(* vmov	s23, r0                                    #! PC = 0x400aec *)
mov s23_b r0_b;
mov s23_t r0_t;
mov s23 r0;
mov zeta_s23 zeta_r0;
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff2c4; Value = 0xb72ba011; PC = 0x400af0 *)
mov r2_b L0xbefff2c4;
mov r2_t L0xbefff2c6;
(* ldr.w	r3, [r0, #4]                              #! EA = L0xbefff2c8; Value = 0xb921b3bb; PC = 0x400af4 *)
mov r3_b L0xbefff2c8;
mov r3_t L0xbefff2ca;
(* ldr.w	r4, [r0, #8]                              #! EA = L0xbefff2cc; Value = 0xfd92f7ff; PC = 0x400af8 *)
mov r4_b L0xbefff2cc;
mov r4_t L0xbefff2ce;
(* ldr.w	r5, [r0, #12]                             #! EA = L0xbefff2d0; Value = 0xf861fd66; PC = 0x400afc *)
mov r5_b L0xbefff2d0;
mov r5_t L0xbefff2d2;
(* ldr.w	r6, [r0, #16]                             #! EA = L0xbefff2d4; Value = 0xadfbfb74; PC = 0x400b00 *)
mov r6_b L0xbefff2d4;
mov r6_t L0xbefff2d6;
(* ldr.w	r7, [r0, #20]                             #! EA = L0xbefff2d8; Value = 0xa80ae720; PC = 0x400b04 *)
mov r7_b L0xbefff2d8;
mov r7_t L0xbefff2da;
(* ldr.w	r8, [r0, #24]                             #! EA = L0xbefff2dc; Value = 0x01ae074d; PC = 0x400b08 *)
mov r8_b L0xbefff2dc;
mov r8_t L0xbefff2de;
(* ldr.w	r9, [r0, #28]                             #! EA = L0xbefff2e0; Value = 0xb0ff0270; PC = 0x400b0c *)
mov r9_b L0xbefff2e0;
mov r9_t L0xbefff2e2;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400b10 *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* ldr.w	r10, [r1], #4                             #! EA = L0x401798; Value = 0x6da8cba2; PC = 0x400b14 *)
mov r10 L0x401798;
mov zeta_r10 zeta_L0x401798;
(* smulwb	lr, r10, r6                              #! PC = 0x400b18 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400b1c *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b20 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400b24 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x400b28 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x400b2c *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400b30 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x400b34 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400b38 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b3c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400b40 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400b44 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x400b48 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400b4c *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x400b50 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400b54 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b58 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400b5c *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400b60 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x400b64 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400b68 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x400b6c *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x400b70 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b74 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400b78 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400b7c *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x400b80 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400b84 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x40179c; Value = 0xb254be68; PC = 0x400b88 *)
mov r10 L0x40179c;
mov r11 L0x4017a0;
mov zeta_r10 zeta_L0x40179c;
mov zeta_r11 zeta_L0x4017a0;
(* smulwb	lr, r10, r4                              #! PC = 0x400b8c *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x400b90 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b94 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400b98 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400b9c *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x400ba0 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400ba4 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x400ba8 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x400bac *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bb0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400bb4 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400bb8 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x400bbc *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400bc0 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400bc4 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x400bc8 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bcc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400bd0 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400bd4 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x400bd8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400bdc *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400be0 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400be4 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400be8 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400bec *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400bf0 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x400bf4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400bf8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4017a4; Value = 0x79cf3ed4; PC = 0x400bfc *)
mov r10 L0x4017a4;
mov r11 L0x4017a8;
mov zeta_r10 zeta_L0x4017a4;
mov zeta_r11 zeta_L0x4017a8;
(* smulwb	lr, r10, r3                              #! PC = 0x400c00 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x400c04 *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c08 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400c0c *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x400c10 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x400c14 *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400c18 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x400c1c *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x400c20 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c24 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400c28 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400c2c *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x400c30 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400c34 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4017ac; Value = 0x9ca52e5f; PC = 0x400c38 *)
mov r10 L0x4017ac;
mov r11 L0x4017b0;
mov zeta_r10 zeta_L0x4017ac;
mov zeta_r11 zeta_L0x4017b0;
(* smulwb	lr, r10, r7                              #! PC = 0x400c3c *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400c40 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c44 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400c48 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400c4c *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400c50 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400c54 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400c58 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400c5c *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c60 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400c64 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400c68 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x400c6c *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x400c70 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;
(* vmov	r0, s23                                    #! PC = 0x400c74 *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* str.w	r6, [r0, #16]                             #! EA = L0xbefff2d4; PC = 0x400c78 *)
mov L0xbefff2d4 r6_b;
mov L0xbefff2d6 r6_t;
(* str.w	r7, [r0, #20]                             #! EA = L0xbefff2d8; PC = 0x400c7c *)
mov L0xbefff2d8 r7_b;
mov L0xbefff2da r7_t;
(* str.w	r8, [r0, #24]                             #! EA = L0xbefff2dc; PC = 0x400c80 *)
mov L0xbefff2dc r8_b;
mov L0xbefff2de r8_t;
(* str.w	r9, [r0, #28]                             #! EA = L0xbefff2e0; PC = 0x400c84 *)
mov L0xbefff2e0 r9_b;
mov L0xbefff2e2 r9_t;
(* str.w	r3, [r0, #4]                              #! EA = L0xbefff2c8; PC = 0x400c88 *)
mov L0xbefff2c8 r3_b;
mov L0xbefff2ca r3_t;
(* str.w	r4, [r0, #8]                              #! EA = L0xbefff2cc; PC = 0x400c8c *)
mov L0xbefff2cc r4_b;
mov L0xbefff2ce r4_t;
(* str.w	r5, [r0, #12]                             #! EA = L0xbefff2d0; PC = 0x400c90 *)
mov L0xbefff2d0 r5_b;
mov L0xbefff2d2 r5_t;
(* str.w	r2, [r0], #32                             #! EA = L0xbefff2c4; PC = 0x400c94 *)
mov L0xbefff2c4 r2_b;
mov L0xbefff2c6 r2_t;
(* vmov	lr, s13                                    #! PC = 0x400c98 *)
mov lr_b s13_b;
mov lr_t s13_t;
mov lr s13;
mov zeta_lr zeta_s13;
(* #bne.w	0x400aec <ntt_fast+1244>                 #! PC = 0x400ca0 *)
#bne.w	0x400aec <ntt_fast+1244>                 #! 0x400ca0 = 0x400ca0;
(* vmov	s23, r0                                    #! PC = 0x400aec *)
mov s23_b r0_b;
mov s23_t r0_t;
mov s23 r0;
mov zeta_s23 zeta_r0;
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff2e4; Value = 0xb96d9485; PC = 0x400af0 *)
mov r2_b L0xbefff2e4;
mov r2_t L0xbefff2e6;
(* ldr.w	r3, [r0, #4]                              #! EA = L0xbefff2e8; Value = 0xbb3fb0cd; PC = 0x400af4 *)
mov r3_b L0xbefff2e8;
mov r3_t L0xbefff2ea;
(* ldr.w	r4, [r0, #8]                              #! EA = L0xbefff2ec; Value = 0xf79cfab7; PC = 0x400af8 *)
mov r4_b L0xbefff2ec;
mov r4_t L0xbefff2ee;
(* ldr.w	r5, [r0, #12]                             #! EA = L0xbefff2f0; Value = 0x047903c4; PC = 0x400afc *)
mov r5_b L0xbefff2f0;
mov r5_t L0xbefff2f2;
(* ldr.w	r6, [r0, #16]                             #! EA = L0xbefff2f4; Value = 0xa96bf424; PC = 0x400b00 *)
mov r6_b L0xbefff2f4;
mov r6_t L0xbefff2f6;
(* ldr.w	r7, [r0, #20]                             #! EA = L0xbefff2f8; Value = 0xb26ee5a4; PC = 0x400b04 *)
mov r7_b L0xbefff2f8;
mov r7_t L0xbefff2fa;
(* ldr.w	r8, [r0, #24]                             #! EA = L0xbefff2fc; Value = 0xfa600483; PC = 0x400b08 *)
mov r8_b L0xbefff2fc;
mov r8_t L0xbefff2fe;
(* ldr.w	r9, [r0, #28]                             #! EA = L0xbefff300; Value = 0xb281f5f2; PC = 0x400b0c *)
mov r9_b L0xbefff300;
mov r9_t L0xbefff302;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400b10 *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* ldr.w	r10, [r1], #4                             #! EA = L0x4017b4; Value = 0xa1074e36; PC = 0x400b14 *)
mov r10 L0x4017b4;
mov zeta_r10 zeta_L0x4017b4;
(* smulwb	lr, r10, r6                              #! PC = 0x400b18 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400b1c *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b20 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400b24 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x400b28 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x400b2c *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400b30 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x400b34 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400b38 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b3c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400b40 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400b44 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x400b48 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400b4c *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x400b50 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400b54 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b58 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400b5c *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400b60 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x400b64 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400b68 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x400b6c *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x400b70 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b74 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400b78 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400b7c *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x400b80 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400b84 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4017b8; Value = 0x3e0eeb29; PC = 0x400b88 *)
mov r10 L0x4017b8;
mov r11 L0x4017bc;
mov zeta_r10 zeta_L0x4017b8;
mov zeta_r11 zeta_L0x4017bc;
(* smulwb	lr, r10, r4                              #! PC = 0x400b8c *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x400b90 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b94 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400b98 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400b9c *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x400ba0 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400ba4 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x400ba8 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x400bac *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bb0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400bb4 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400bb8 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x400bbc *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400bc0 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400bc4 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x400bc8 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bcc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400bd0 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400bd4 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x400bd8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400bdc *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400be0 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400be4 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400be8 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400bec *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400bf0 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x400bf4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400bf8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4017c0; Value = 0x1cd665aa; PC = 0x400bfc *)
mov r10 L0x4017c0;
mov r11 L0x4017c4;
mov zeta_r10 zeta_L0x4017c0;
mov zeta_r11 zeta_L0x4017c4;
(* smulwb	lr, r10, r3                              #! PC = 0x400c00 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x400c04 *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c08 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400c0c *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x400c10 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x400c14 *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400c18 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x400c1c *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x400c20 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c24 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400c28 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400c2c *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x400c30 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400c34 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4017c8; Value = 0xa0b88f58; PC = 0x400c38 *)
mov r10 L0x4017c8;
mov r11 L0x4017cc;
mov zeta_r10 zeta_L0x4017c8;
mov zeta_r11 zeta_L0x4017cc;
(* smulwb	lr, r10, r7                              #! PC = 0x400c3c *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400c40 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c44 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400c48 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400c4c *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400c50 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400c54 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400c58 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400c5c *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c60 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400c64 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400c68 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x400c6c *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x400c70 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;
(* vmov	r0, s23                                    #! PC = 0x400c74 *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* str.w	r6, [r0, #16]                             #! EA = L0xbefff2f4; PC = 0x400c78 *)
mov L0xbefff2f4 r6_b;
mov L0xbefff2f6 r6_t;
(* str.w	r7, [r0, #20]                             #! EA = L0xbefff2f8; PC = 0x400c7c *)
mov L0xbefff2f8 r7_b;
mov L0xbefff2fa r7_t;
(* str.w	r8, [r0, #24]                             #! EA = L0xbefff2fc; PC = 0x400c80 *)
mov L0xbefff2fc r8_b;
mov L0xbefff2fe r8_t;
(* str.w	r9, [r0, #28]                             #! EA = L0xbefff300; PC = 0x400c84 *)
mov L0xbefff300 r9_b;
mov L0xbefff302 r9_t;
(* str.w	r3, [r0, #4]                              #! EA = L0xbefff2e8; PC = 0x400c88 *)
mov L0xbefff2e8 r3_b;
mov L0xbefff2ea r3_t;
(* str.w	r4, [r0, #8]                              #! EA = L0xbefff2ec; PC = 0x400c8c *)
mov L0xbefff2ec r4_b;
mov L0xbefff2ee r4_t;
(* str.w	r5, [r0, #12]                             #! EA = L0xbefff2f0; PC = 0x400c90 *)
mov L0xbefff2f0 r5_b;
mov L0xbefff2f2 r5_t;
(* str.w	r2, [r0], #32                             #! EA = L0xbefff2e4; PC = 0x400c94 *)
mov L0xbefff2e4 r2_b;
mov L0xbefff2e6 r2_t;
(* vmov	lr, s13                                    #! PC = 0x400c98 *)
mov lr_b s13_b;
mov lr_t s13_t;
mov lr s13;
mov zeta_lr zeta_s13;
(* #bne.w	0x400aec <ntt_fast+1244>                 #! PC = 0x400ca0 *)
#bne.w	0x400aec <ntt_fast+1244>                 #! 0x400ca0 = 0x400ca0;
(* vmov	s23, r0                                    #! PC = 0x400aec *)
mov s23_b r0_b;
mov s23_t r0_t;
mov s23 r0;
mov zeta_s23 zeta_r0;
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff304; Value = 0xb9a58ea4; PC = 0x400af0 *)
mov r2_b L0xbefff304;
mov r2_t L0xbefff306;
(* ldr.w	r3, [r0, #4]                              #! EA = L0xbefff308; Value = 0xae27a8d7; PC = 0x400af4 *)
mov r3_b L0xbefff308;
mov r3_t L0xbefff30a;
(* ldr.w	r4, [r0, #8]                              #! EA = L0xbefff30c; Value = 0xf8c103cb; PC = 0x400af8 *)
mov r4_b L0xbefff30c;
mov r4_t L0xbefff30e;
(* ldr.w	r5, [r0, #12]                             #! EA = L0xbefff310; Value = 0xf4810789; PC = 0x400afc *)
mov r5_b L0xbefff310;
mov r5_t L0xbefff312;
(* ldr.w	r6, [r0, #16]                             #! EA = L0xbefff314; Value = 0xa642017b; PC = 0x400b00 *)
mov r6_b L0xbefff314;
mov r6_t L0xbefff316;
(* ldr.w	r7, [r0, #20]                             #! EA = L0xbefff318; Value = 0xbb72eca7; PC = 0x400b04 *)
mov r7_b L0xbefff318;
mov r7_t L0xbefff31a;
(* ldr.w	r8, [r0, #24]                             #! EA = L0xbefff31c; Value = 0xf6940234; PC = 0x400b08 *)
mov r8_b L0xbefff31c;
mov r8_t L0xbefff31e;
(* ldr.w	r9, [r0, #28]                             #! EA = L0xbefff320; Value = 0xab29f765; PC = 0x400b0c *)
mov r9_b L0xbefff320;
mov r9_t L0xbefff322;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400b10 *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* ldr.w	r10, [r1], #4                             #! EA = L0x4017d0; Value = 0x2924384b; PC = 0x400b14 *)
mov r10 L0x4017d0;
mov zeta_r10 zeta_L0x4017d0;
(* smulwb	lr, r10, r6                              #! PC = 0x400b18 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400b1c *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b20 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400b24 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x400b28 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x400b2c *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400b30 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x400b34 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400b38 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b3c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400b40 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400b44 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x400b48 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400b4c *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x400b50 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400b54 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b58 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400b5c *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400b60 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x400b64 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400b68 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x400b6c *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x400b70 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b74 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400b78 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400b7c *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x400b80 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400b84 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4017d4; Value = 0x6e95083b; PC = 0x400b88 *)
mov r10 L0x4017d4;
mov r11 L0x4017d8;
mov zeta_r10 zeta_L0x4017d4;
mov zeta_r11 zeta_L0x4017d8;
(* smulwb	lr, r10, r4                              #! PC = 0x400b8c *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x400b90 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b94 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400b98 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400b9c *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x400ba0 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400ba4 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x400ba8 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x400bac *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bb0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400bb4 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400bb8 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x400bbc *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400bc0 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400bc4 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x400bc8 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bcc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400bd0 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400bd4 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x400bd8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400bdc *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400be0 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400be4 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400be8 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400bec *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400bf0 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x400bf4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400bf8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4017dc; Value = 0x51bea292; PC = 0x400bfc *)
mov r10 L0x4017dc;
mov r11 L0x4017e0;
mov zeta_r10 zeta_L0x4017dc;
mov zeta_r11 zeta_L0x4017e0;
(* smulwb	lr, r10, r3                              #! PC = 0x400c00 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x400c04 *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c08 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400c0c *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x400c10 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x400c14 *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400c18 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x400c1c *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x400c20 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c24 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400c28 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400c2c *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x400c30 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400c34 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4017e4; Value = 0xd53e5dab; PC = 0x400c38 *)
mov r10 L0x4017e4;
mov r11 L0x4017e8;
mov zeta_r10 zeta_L0x4017e4;
mov zeta_r11 zeta_L0x4017e8;
(* smulwb	lr, r10, r7                              #! PC = 0x400c3c *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400c40 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c44 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400c48 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400c4c *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400c50 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400c54 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400c58 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400c5c *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c60 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400c64 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400c68 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x400c6c *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x400c70 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;
(* vmov	r0, s23                                    #! PC = 0x400c74 *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* str.w	r6, [r0, #16]                             #! EA = L0xbefff314; PC = 0x400c78 *)
mov L0xbefff314 r6_b;
mov L0xbefff316 r6_t;
(* str.w	r7, [r0, #20]                             #! EA = L0xbefff318; PC = 0x400c7c *)
mov L0xbefff318 r7_b;
mov L0xbefff31a r7_t;
(* str.w	r8, [r0, #24]                             #! EA = L0xbefff31c; PC = 0x400c80 *)
mov L0xbefff31c r8_b;
mov L0xbefff31e r8_t;
(* str.w	r9, [r0, #28]                             #! EA = L0xbefff320; PC = 0x400c84 *)
mov L0xbefff320 r9_b;
mov L0xbefff322 r9_t;
(* str.w	r3, [r0, #4]                              #! EA = L0xbefff308; PC = 0x400c88 *)
mov L0xbefff308 r3_b;
mov L0xbefff30a r3_t;
(* str.w	r4, [r0, #8]                              #! EA = L0xbefff30c; PC = 0x400c8c *)
mov L0xbefff30c r4_b;
mov L0xbefff30e r4_t;
(* str.w	r5, [r0, #12]                             #! EA = L0xbefff310; PC = 0x400c90 *)
mov L0xbefff310 r5_b;
mov L0xbefff312 r5_t;
(* str.w	r2, [r0], #32                             #! EA = L0xbefff304; PC = 0x400c94 *)
mov L0xbefff304 r2_b;
mov L0xbefff306 r2_t;
(* vmov	lr, s13                                    #! PC = 0x400c98 *)
mov lr_b s13_b;
mov lr_t s13_t;
mov lr s13;
mov zeta_lr zeta_s13;
(* #bne.w	0x400aec <ntt_fast+1244>                 #! PC = 0x400ca0 *)
#bne.w	0x400aec <ntt_fast+1244>                 #! 0x400ca0 = 0x400ca0;
(* vmov	s23, r0                                    #! PC = 0x400aec *)
mov s23_b r0_b;
mov s23_t r0_t;
mov s23 r0;
mov zeta_s23 zeta_r0;
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff324; Value = 0xb1738ce6; PC = 0x400af0 *)
mov r2_b L0xbefff324;
mov r2_t L0xbefff326;
(* ldr.w	r3, [r0, #4]                              #! EA = L0xbefff328; Value = 0xb975b2a1; PC = 0x400af4 *)
mov r3_b L0xbefff328;
mov r3_t L0xbefff32a;
(* ldr.w	r4, [r0, #8]                              #! EA = L0xbefff32c; Value = 0xfc6d054b; PC = 0x400af8 *)
mov r4_b L0xbefff32c;
mov r4_t L0xbefff32e;
(* ldr.w	r5, [r0, #12]                             #! EA = L0xbefff330; Value = 0xf9010b01; PC = 0x400afc *)
mov r5_b L0xbefff330;
mov r5_t L0xbefff332;
(* ldr.w	r6, [r0, #16]                             #! EA = L0xbefff334; Value = 0xb2c4fe81; PC = 0x400b00 *)
mov r6_b L0xbefff334;
mov r6_t L0xbefff336;
(* ldr.w	r7, [r0, #20]                             #! EA = L0xbefff338; Value = 0xb16ae619; PC = 0x400b04 *)
mov r7_b L0xbefff338;
mov r7_t L0xbefff33a;
(* ldr.w	r8, [r0, #24]                             #! EA = L0xbefff33c; Value = 0xf8b6fbf4; PC = 0x400b08 *)
mov r8_b L0xbefff33c;
mov r8_t L0xbefff33e;
(* ldr.w	r9, [r0, #28]                             #! EA = L0xbefff340; Value = 0xa307eba9; PC = 0x400b0c *)
mov r9_b L0xbefff340;
mov r9_t L0xbefff342;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400b10 *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* ldr.w	r10, [r1], #4                             #! EA = L0x4017ec; Value = 0xdda02ec2; PC = 0x400b14 *)
mov r10 L0x4017ec;
mov zeta_r10 zeta_L0x4017ec;
(* smulwb	lr, r10, r6                              #! PC = 0x400b18 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400b1c *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b20 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400b24 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x400b28 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x400b2c *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400b30 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x400b34 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400b38 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b3c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400b40 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400b44 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x400b48 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400b4c *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x400b50 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400b54 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b58 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400b5c *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400b60 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x400b64 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400b68 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x400b6c *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x400b70 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b74 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400b78 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400b7c *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x400b80 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400b84 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4017f0; Value = 0x75f6ed02; PC = 0x400b88 *)
mov r10 L0x4017f0;
mov r11 L0x4017f4;
mov zeta_r10 zeta_L0x4017f0;
mov zeta_r11 zeta_L0x4017f4;
(* smulwb	lr, r10, r4                              #! PC = 0x400b8c *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x400b90 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b94 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400b98 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400b9c *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x400ba0 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400ba4 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x400ba8 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x400bac *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bb0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400bb4 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400bb8 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x400bbc *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400bc0 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400bc4 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x400bc8 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bcc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400bd0 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400bd4 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x400bd8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400bdc *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400be0 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400be4 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400be8 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400bec *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400bf0 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x400bf4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400bf8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x4017f8; Value = 0xa169bccb; PC = 0x400bfc *)
mov r10 L0x4017f8;
mov r11 L0x4017fc;
mov zeta_r10 zeta_L0x4017f8;
mov zeta_r11 zeta_L0x4017fc;
(* smulwb	lr, r10, r3                              #! PC = 0x400c00 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x400c04 *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c08 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400c0c *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x400c10 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x400c14 *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400c18 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x400c1c *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x400c20 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c24 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400c28 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400c2c *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x400c30 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400c34 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401800; Value = 0xbda2a4b9; PC = 0x400c38 *)
mov r10 L0x401800;
mov r11 L0x401804;
mov zeta_r10 zeta_L0x401800;
mov zeta_r11 zeta_L0x401804;
(* smulwb	lr, r10, r7                              #! PC = 0x400c3c *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400c40 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c44 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400c48 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400c4c *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400c50 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400c54 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400c58 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400c5c *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c60 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400c64 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400c68 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x400c6c *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x400c70 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;
(* vmov	r0, s23                                    #! PC = 0x400c74 *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* str.w	r6, [r0, #16]                             #! EA = L0xbefff334; PC = 0x400c78 *)
mov L0xbefff334 r6_b;
mov L0xbefff336 r6_t;
(* str.w	r7, [r0, #20]                             #! EA = L0xbefff338; PC = 0x400c7c *)
mov L0xbefff338 r7_b;
mov L0xbefff33a r7_t;
(* str.w	r8, [r0, #24]                             #! EA = L0xbefff33c; PC = 0x400c80 *)
mov L0xbefff33c r8_b;
mov L0xbefff33e r8_t;
(* str.w	r9, [r0, #28]                             #! EA = L0xbefff340; PC = 0x400c84 *)
mov L0xbefff340 r9_b;
mov L0xbefff342 r9_t;
(* str.w	r3, [r0, #4]                              #! EA = L0xbefff328; PC = 0x400c88 *)
mov L0xbefff328 r3_b;
mov L0xbefff32a r3_t;
(* str.w	r4, [r0, #8]                              #! EA = L0xbefff32c; PC = 0x400c8c *)
mov L0xbefff32c r4_b;
mov L0xbefff32e r4_t;
(* str.w	r5, [r0, #12]                             #! EA = L0xbefff330; PC = 0x400c90 *)
mov L0xbefff330 r5_b;
mov L0xbefff332 r5_t;
(* str.w	r2, [r0], #32                             #! EA = L0xbefff324; PC = 0x400c94 *)
mov L0xbefff324 r2_b;
mov L0xbefff326 r2_t;
(* vmov	lr, s13                                    #! PC = 0x400c98 *)
mov lr_b s13_b;
mov lr_t s13_t;
mov lr s13;
mov zeta_lr zeta_s13;
(* #bne.w	0x400aec <ntt_fast+1244>                 #! PC = 0x400ca0 *)
#bne.w	0x400aec <ntt_fast+1244>                 #! 0x400ca0 = 0x400ca0;
(* vmov	s23, r0                                    #! PC = 0x400aec *)
mov s23_b r0_b;
mov s23_t r0_t;
mov s23 r0;
mov zeta_s23 zeta_r0;
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff344; Value = 0xae6d8d80; PC = 0x400af0 *)
mov r2_b L0xbefff344;
mov r2_t L0xbefff346;
(* ldr.w	r3, [r0, #4]                              #! EA = L0xbefff348; Value = 0xbaa3b0cd; PC = 0x400af4 *)
mov r3_b L0xbefff348;
mov r3_t L0xbefff34a;
(* ldr.w	r4, [r0, #8]                              #! EA = L0xbefff34c; Value = 0x09580338; PC = 0x400af8 *)
mov r4_b L0xbefff34c;
mov r4_t L0xbefff34e;
(* ldr.w	r5, [r0, #12]                             #! EA = L0xbefff350; Value = 0x076dfd64; PC = 0x400afc *)
mov r5_b L0xbefff350;
mov r5_t L0xbefff352;
(* ldr.w	r6, [r0, #16]                             #! EA = L0xbefff354; Value = 0xb52cfc4f; PC = 0x400b00 *)
mov r6_b L0xbefff354;
mov r6_t L0xbefff356;
(* ldr.w	r7, [r0, #20]                             #! EA = L0xbefff358; Value = 0xb015dcb0; PC = 0x400b04 *)
mov r7_b L0xbefff358;
mov r7_t L0xbefff35a;
(* ldr.w	r8, [r0, #24]                             #! EA = L0xbefff35c; Value = 0xffa70786; PC = 0x400b08 *)
mov r8_b L0xbefff35c;
mov r8_t L0xbefff35e;
(* ldr.w	r9, [r0, #28]                             #! EA = L0xbefff360; Value = 0xb78e023c; PC = 0x400b0c *)
mov r9_b L0xbefff360;
mov r9_t L0xbefff362;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400b10 *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* ldr.w	r10, [r1], #4                             #! EA = L0x401808; Value = 0xb805896c; PC = 0x400b14 *)
mov r10 L0x401808;
mov zeta_r10 zeta_L0x401808;
(* smulwb	lr, r10, r6                              #! PC = 0x400b18 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400b1c *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b20 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400b24 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x400b28 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x400b2c *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400b30 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x400b34 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400b38 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b3c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400b40 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400b44 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x400b48 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400b4c *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x400b50 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400b54 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b58 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400b5c *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400b60 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x400b64 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400b68 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x400b6c *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x400b70 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b74 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400b78 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400b7c *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x400b80 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400b84 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x40180c; Value = 0xcb8de165; PC = 0x400b88 *)
mov r10 L0x40180c;
mov r11 L0x401810;
mov zeta_r10 zeta_L0x40180c;
mov zeta_r11 zeta_L0x401810;
(* smulwb	lr, r10, r4                              #! PC = 0x400b8c *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x400b90 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b94 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400b98 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400b9c *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x400ba0 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400ba4 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x400ba8 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x400bac *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bb0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400bb4 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400bb8 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x400bbc *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400bc0 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400bc4 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x400bc8 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bcc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400bd0 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400bd4 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x400bd8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400bdc *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400be0 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400be4 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400be8 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400bec *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400bf0 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x400bf4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400bf8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401814; Value = 0xd7a0a4e0; PC = 0x400bfc *)
mov r10 L0x401814;
mov r11 L0x401818;
mov zeta_r10 zeta_L0x401814;
mov zeta_r11 zeta_L0x401818;
(* smulwb	lr, r10, r3                              #! PC = 0x400c00 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x400c04 *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c08 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400c0c *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x400c10 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x400c14 *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400c18 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x400c1c *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x400c20 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c24 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400c28 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400c2c *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x400c30 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400c34 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x40181c; Value = 0x1efd9db9; PC = 0x400c38 *)
mov r10 L0x40181c;
mov r11 L0x401820;
mov zeta_r10 zeta_L0x40181c;
mov zeta_r11 zeta_L0x401820;
(* smulwb	lr, r10, r7                              #! PC = 0x400c3c *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400c40 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c44 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400c48 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400c4c *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400c50 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400c54 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400c58 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400c5c *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c60 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400c64 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400c68 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x400c6c *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x400c70 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;
(* vmov	r0, s23                                    #! PC = 0x400c74 *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* str.w	r6, [r0, #16]                             #! EA = L0xbefff354; PC = 0x400c78 *)
mov L0xbefff354 r6_b;
mov L0xbefff356 r6_t;
(* str.w	r7, [r0, #20]                             #! EA = L0xbefff358; PC = 0x400c7c *)
mov L0xbefff358 r7_b;
mov L0xbefff35a r7_t;
(* str.w	r8, [r0, #24]                             #! EA = L0xbefff35c; PC = 0x400c80 *)
mov L0xbefff35c r8_b;
mov L0xbefff35e r8_t;
(* str.w	r9, [r0, #28]                             #! EA = L0xbefff360; PC = 0x400c84 *)
mov L0xbefff360 r9_b;
mov L0xbefff362 r9_t;
(* str.w	r3, [r0, #4]                              #! EA = L0xbefff348; PC = 0x400c88 *)
mov L0xbefff348 r3_b;
mov L0xbefff34a r3_t;
(* str.w	r4, [r0, #8]                              #! EA = L0xbefff34c; PC = 0x400c8c *)
mov L0xbefff34c r4_b;
mov L0xbefff34e r4_t;
(* str.w	r5, [r0, #12]                             #! EA = L0xbefff350; PC = 0x400c90 *)
mov L0xbefff350 r5_b;
mov L0xbefff352 r5_t;
(* str.w	r2, [r0], #32                             #! EA = L0xbefff344; PC = 0x400c94 *)
mov L0xbefff344 r2_b;
mov L0xbefff346 r2_t;
(* vmov	lr, s13                                    #! PC = 0x400c98 *)
mov lr_b s13_b;
mov lr_t s13_t;
mov lr s13;
mov zeta_lr zeta_s13;
(* #bne.w	0x400aec <ntt_fast+1244>                 #! PC = 0x400ca0 *)
#bne.w	0x400aec <ntt_fast+1244>                 #! 0x400ca0 = 0x400ca0;
(* vmov	s23, r0                                    #! PC = 0x400aec *)
mov s23_b r0_b;
mov s23_t r0_t;
mov s23 r0;
mov zeta_s23 zeta_r0;
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff364; Value = 0xbabd8f14; PC = 0x400af0 *)
mov r2_b L0xbefff364;
mov r2_t L0xbefff366;
(* ldr.w	r3, [r0, #4]                              #! EA = L0xbefff368; Value = 0xbbedb561; PC = 0x400af4 *)
mov r3_b L0xbefff368;
mov r3_t L0xbefff36a;
(* ldr.w	r4, [r0, #8]                              #! EA = L0xbefff36c; Value = 0x017a05b6; PC = 0x400af8 *)
mov r4_b L0xbefff36c;
mov r4_t L0xbefff36e;
(* ldr.w	r5, [r0, #12]                             #! EA = L0xbefff370; Value = 0x02b5fc26; PC = 0x400afc *)
mov r5_b L0xbefff370;
mov r5_t L0xbefff372;
(* ldr.w	r6, [r0, #16]                             #! EA = L0xbefff374; Value = 0xba52072f; PC = 0x400b00 *)
mov r6_b L0xbefff374;
mov r6_t L0xbefff376;
(* ldr.w	r7, [r0, #20]                             #! EA = L0xbefff378; Value = 0xb2a3e7e6; PC = 0x400b04 *)
mov r7_b L0xbefff378;
mov r7_t L0xbefff37a;
(* ldr.w	r8, [r0, #24]                             #! EA = L0xbefff37c; Value = 0xfe97070a; PC = 0x400b08 *)
mov r8_b L0xbefff37c;
mov r8_t L0xbefff37e;
(* ldr.w	r9, [r0, #28]                             #! EA = L0xbefff380; Value = 0xb178021a; PC = 0x400b0c *)
mov r9_b L0xbefff380;
mov r9_t L0xbefff382;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400b10 *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* ldr.w	r10, [r1], #4                             #! EA = L0x401824; Value = 0xdd651f9c; PC = 0x400b14 *)
mov r10 L0x401824;
mov zeta_r10 zeta_L0x401824;
(* smulwb	lr, r10, r6                              #! PC = 0x400b18 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400b1c *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b20 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400b24 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x400b28 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x400b2c *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400b30 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x400b34 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400b38 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b3c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400b40 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400b44 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x400b48 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400b4c *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x400b50 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400b54 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b58 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400b5c *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400b60 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x400b64 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400b68 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x400b6c *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x400b70 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b74 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400b78 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400b7c *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x400b80 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400b84 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401828; Value = 0x71e38c09; PC = 0x400b88 *)
mov r10 L0x401828;
mov r11 L0x40182c;
mov zeta_r10 zeta_L0x401828;
mov zeta_r11 zeta_L0x40182c;
(* smulwb	lr, r10, r4                              #! PC = 0x400b8c *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x400b90 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b94 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400b98 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400b9c *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x400ba0 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400ba4 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x400ba8 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x400bac *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bb0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400bb4 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400bb8 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x400bbc *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400bc0 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400bc4 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x400bc8 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bcc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400bd0 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400bd4 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x400bd8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400bdc *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400be0 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400be4 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400be8 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400bec *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400bf0 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x400bf4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400bf8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401830; Value = 0x57e58be2; PC = 0x400bfc *)
mov r10 L0x401830;
mov r11 L0x401834;
mov zeta_r10 zeta_L0x401830;
mov zeta_r11 zeta_L0x401834;
(* smulwb	lr, r10, r3                              #! PC = 0x400c00 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x400c04 *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c08 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400c0c *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x400c10 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x400c14 *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400c18 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x400c1c *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x400c20 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c24 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400c28 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400c2c *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x400c30 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400c34 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401838; Value = 0xd565bd19; PC = 0x400c38 *)
mov r10 L0x401838;
mov r11 L0x40183c;
mov zeta_r10 zeta_L0x401838;
mov zeta_r11 zeta_L0x40183c;
(* smulwb	lr, r10, r7                              #! PC = 0x400c3c *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400c40 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c44 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400c48 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400c4c *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400c50 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400c54 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400c58 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400c5c *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c60 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400c64 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400c68 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x400c6c *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x400c70 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;
(* vmov	r0, s23                                    #! PC = 0x400c74 *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* str.w	r6, [r0, #16]                             #! EA = L0xbefff374; PC = 0x400c78 *)
mov L0xbefff374 r6_b;
mov L0xbefff376 r6_t;
(* str.w	r7, [r0, #20]                             #! EA = L0xbefff378; PC = 0x400c7c *)
mov L0xbefff378 r7_b;
mov L0xbefff37a r7_t;
(* str.w	r8, [r0, #24]                             #! EA = L0xbefff37c; PC = 0x400c80 *)
mov L0xbefff37c r8_b;
mov L0xbefff37e r8_t;
(* str.w	r9, [r0, #28]                             #! EA = L0xbefff380; PC = 0x400c84 *)
mov L0xbefff380 r9_b;
mov L0xbefff382 r9_t;
(* str.w	r3, [r0, #4]                              #! EA = L0xbefff368; PC = 0x400c88 *)
mov L0xbefff368 r3_b;
mov L0xbefff36a r3_t;
(* str.w	r4, [r0, #8]                              #! EA = L0xbefff36c; PC = 0x400c8c *)
mov L0xbefff36c r4_b;
mov L0xbefff36e r4_t;
(* str.w	r5, [r0, #12]                             #! EA = L0xbefff370; PC = 0x400c90 *)
mov L0xbefff370 r5_b;
mov L0xbefff372 r5_t;
(* str.w	r2, [r0], #32                             #! EA = L0xbefff364; PC = 0x400c94 *)
mov L0xbefff364 r2_b;
mov L0xbefff366 r2_t;
(* vmov	lr, s13                                    #! PC = 0x400c98 *)
mov lr_b s13_b;
mov lr_t s13_t;
mov lr s13;
mov zeta_lr zeta_s13;
(* #bne.w	0x400aec <ntt_fast+1244>                 #! PC = 0x400ca0 *)
#bne.w	0x400aec <ntt_fast+1244>                 #! 0x400ca0 = 0x400ca0;
(* vmov	s23, r0                                    #! PC = 0x400aec *)
mov s23_b r0_b;
mov s23_t r0_t;
mov s23 r0;
mov zeta_s23 zeta_r0;
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff384; Value = 0xb4529a2a; PC = 0x400af0 *)
mov r2_b L0xbefff384;
mov r2_t L0xbefff386;
(* ldr.w	r3, [r0, #4]                              #! EA = L0xbefff388; Value = 0xac6fa9d6; PC = 0x400af4 *)
mov r3_b L0xbefff388;
mov r3_t L0xbefff38a;
(* ldr.w	r4, [r0, #8]                              #! EA = L0xbefff38c; Value = 0x004bf90d; PC = 0x400af8 *)
mov r4_b L0xbefff38c;
mov r4_t L0xbefff38e;
(* ldr.w	r5, [r0, #12]                             #! EA = L0xbefff390; Value = 0x0c2bfacb; PC = 0x400afc *)
mov r5_b L0xbefff390;
mov r5_t L0xbefff392;
(* ldr.w	r6, [r0, #16]                             #! EA = L0xbefff394; Value = 0xbd640654; PC = 0x400b00 *)
mov r6_b L0xbefff394;
mov r6_t L0xbefff396;
(* ldr.w	r7, [r0, #20]                             #! EA = L0xbefff398; Value = 0xb158ecb5; PC = 0x400b04 *)
mov r7_b L0xbefff398;
mov r7_t L0xbefff39a;
(* ldr.w	r8, [r0, #24]                             #! EA = L0xbefff39c; Value = 0xf0bef766; PC = 0x400b08 *)
mov r8_b L0xbefff39c;
mov r8_t L0xbefff39e;
(* ldr.w	r9, [r0, #28]                             #! EA = L0xbefff3a0; Value = 0xb395fe50; PC = 0x400b0c *)
mov r9_b L0xbefff3a0;
mov r9_t L0xbefff3a2;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400b10 *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* ldr.w	r10, [r1], #4                             #! EA = L0x401840; Value = 0x97ccf03d; PC = 0x400b14 *)
mov r10 L0x401840;
mov zeta_r10 zeta_L0x401840;
(* smulwb	lr, r10, r6                              #! PC = 0x400b18 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400b1c *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b20 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400b24 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x400b28 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x400b2c *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400b30 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x400b34 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400b38 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b3c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400b40 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400b44 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x400b48 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400b4c *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x400b50 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400b54 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b58 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400b5c *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400b60 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x400b64 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400b68 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x400b6c *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x400b70 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b74 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400b78 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400b7c *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x400b80 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400b84 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401844; Value = 0xbe402274; PC = 0x400b88 *)
mov r10 L0x401844;
mov r11 L0x401848;
mov zeta_r10 zeta_L0x401844;
mov zeta_r11 zeta_L0x401848;
(* smulwb	lr, r10, r4                              #! PC = 0x400b8c *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x400b90 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b94 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400b98 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400b9c *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x400ba0 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400ba4 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x400ba8 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x400bac *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bb0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400bb4 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400bb8 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x400bbc *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400bc0 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400bc4 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x400bc8 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bcc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400bd0 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400bd4 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x400bd8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400bdc *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400be0 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400be4 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400be8 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400bec *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400bf0 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x400bf4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400bf8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x40184c; Value = 0x846bf7b2; PC = 0x400bfc *)
mov r10 L0x40184c;
mov r11 L0x401850;
mov zeta_r10 zeta_L0x40184c;
mov zeta_r11 zeta_L0x401850;
(* smulwb	lr, r10, r3                              #! PC = 0x400c00 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x400c04 *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c08 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400c0c *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x400c10 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x400c14 *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400c18 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x400c1c *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x400c20 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c24 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400c28 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400c2c *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x400c30 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400c34 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401854; Value = 0x901c4c98; PC = 0x400c38 *)
mov r10 L0x401854;
mov r11 L0x401858;
mov zeta_r10 zeta_L0x401854;
mov zeta_r11 zeta_L0x401858;
(* smulwb	lr, r10, r7                              #! PC = 0x400c3c *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400c40 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c44 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400c48 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400c4c *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400c50 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400c54 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400c58 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400c5c *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c60 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400c64 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400c68 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x400c6c *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x400c70 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;
(* vmov	r0, s23                                    #! PC = 0x400c74 *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* str.w	r6, [r0, #16]                             #! EA = L0xbefff394; PC = 0x400c78 *)
mov L0xbefff394 r6_b;
mov L0xbefff396 r6_t;
(* str.w	r7, [r0, #20]                             #! EA = L0xbefff398; PC = 0x400c7c *)
mov L0xbefff398 r7_b;
mov L0xbefff39a r7_t;
(* str.w	r8, [r0, #24]                             #! EA = L0xbefff39c; PC = 0x400c80 *)
mov L0xbefff39c r8_b;
mov L0xbefff39e r8_t;
(* str.w	r9, [r0, #28]                             #! EA = L0xbefff3a0; PC = 0x400c84 *)
mov L0xbefff3a0 r9_b;
mov L0xbefff3a2 r9_t;
(* str.w	r3, [r0, #4]                              #! EA = L0xbefff388; PC = 0x400c88 *)
mov L0xbefff388 r3_b;
mov L0xbefff38a r3_t;
(* str.w	r4, [r0, #8]                              #! EA = L0xbefff38c; PC = 0x400c8c *)
mov L0xbefff38c r4_b;
mov L0xbefff38e r4_t;
(* str.w	r5, [r0, #12]                             #! EA = L0xbefff390; PC = 0x400c90 *)
mov L0xbefff390 r5_b;
mov L0xbefff392 r5_t;
(* str.w	r2, [r0], #32                             #! EA = L0xbefff384; PC = 0x400c94 *)
mov L0xbefff384 r2_b;
mov L0xbefff386 r2_t;
(* vmov	lr, s13                                    #! PC = 0x400c98 *)
mov lr_b s13_b;
mov lr_t s13_t;
mov lr s13;
mov zeta_lr zeta_s13;
(* #bne.w	0x400aec <ntt_fast+1244>                 #! PC = 0x400ca0 *)
#bne.w	0x400aec <ntt_fast+1244>                 #! 0x400ca0 = 0x400ca0;
(* vmov	s23, r0                                    #! PC = 0x400aec *)
mov s23_b r0_b;
mov s23_t r0_t;
mov s23 r0;
mov zeta_s23 zeta_r0;
(* ldr.w	r2, [r0]                                  #! EA = L0xbefff3a4; Value = 0xbe349962; PC = 0x400af0 *)
mov r2_b L0xbefff3a4;
mov r2_t L0xbefff3a6;
(* ldr.w	r3, [r0, #4]                              #! EA = L0xbefff3a8; Value = 0xb8fdaffc; PC = 0x400af4 *)
mov r3_b L0xbefff3a8;
mov r3_t L0xbefff3aa;
(* ldr.w	r4, [r0, #8]                              #! EA = L0xbefff3ac; Value = 0x0a8702a1; PC = 0x400af8 *)
mov r4_b L0xbefff3ac;
mov r4_t L0xbefff3ae;
(* ldr.w	r5, [r0, #12]                             #! EA = L0xbefff3b0; Value = 0xff57f7f7; PC = 0x400afc *)
mov r5_b L0xbefff3b0;
mov r5_t L0xbefff3b2;
(* ldr.w	r6, [r0, #16]                             #! EA = L0xbefff3b4; Value = 0xb152064a; PC = 0x400b00 *)
mov r6_b L0xbefff3b4;
mov r6_t L0xbefff3b6;
(* ldr.w	r7, [r0, #20]                             #! EA = L0xbefff3b8; Value = 0xb344e629; PC = 0x400b04 *)
mov r7_b L0xbefff3b8;
mov r7_t L0xbefff3ba;
(* ldr.w	r8, [r0, #24]                             #! EA = L0xbefff3bc; Value = 0xfc5c0402; PC = 0x400b08 *)
mov r8_b L0xbefff3bc;
mov r8_t L0xbefff3be;
(* ldr.w	r9, [r0, #28]                             #! EA = L0xbefff3c0; Value = 0xc05dfdd2; PC = 0x400b0c *)
mov r9_b L0xbefff3c0;
mov r9_t L0xbefff3c2;
(* movw	r0, #26632	; 0x6808                        #! PC = 0x400b10 *)
mov r0_b 26632@uint16;
mov r0_t 0@sint16;
(* ldr.w	r10, [r1], #4                             #! EA = L0x40185c; Value = 0x3f228731; PC = 0x400b14 *)
mov r10 L0x40185c;
mov zeta_r10 zeta_L0x40185c;
(* smulwb	lr, r10, r6                              #! PC = 0x400b18 *)
cast r6_lsb@sint32 r6_b;
mull tmp_t tmp_b r10 r6_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r6_b;
assert A_lb = r6_b && true;
assume A_lb = r6_b && true;
cast A_lt@sint32 r6_t;
assert A_lt = r6_t && true;
assume A_lt = r6_t && true;
mov zeta zeta_r10;
(* smulwt	r6, r10, r6                              #! PC = 0x400b1c *)
cast r6_lst@sint32 r6_t;
mull tmp_t tmp_b r10 r6_lst;
spl dontcare r6_t tmp_t 16;
spl r6_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b20 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r6, r6, r12, r0                          #! PC = 0x400b24 *)
cast r6_sb@sint16 r6_b;
mull tmp_t tmp_b r6_sb r12_t;
uadds carry r6_b tmp_b r0_b;
adc r6_t tmp_t r0_t carry;
(* pkhtb	lr, r6, lr, asr #16                       #! PC = 0x400b28 *)
mov tmp_b lr_t;
mov tmp_t r6_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r6, r2, lr                               #! PC = 0x400b2c *)
sub r6_b r2_b lr_b;
sub r6_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400b30 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r7                              #! PC = 0x400b34 *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400b38 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b3c *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400b40 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400b44 *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r3, lr                               #! PC = 0x400b48 *)
sub r7_b r3_b lr_b;
sub r7_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400b4c *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r10, r8                              #! PC = 0x400b50 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r10 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r10;
(* smulwt	r8, r10, r8                              #! PC = 0x400b54 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r10 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b58 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400b5c *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400b60 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r4, lr                               #! PC = 0x400b64 *)
sub r8_b r4_b lr_b;
sub r8_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400b68 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* smulwb	lr, r10, r9                              #! PC = 0x400b6c *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r10 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r10;
(* smulwt	r9, r10, r9                              #! PC = 0x400b70 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r10 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b74 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400b78 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400b7c *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r5, lr                               #! PC = 0x400b80 *)
sub r9_b r5_b lr_b;
sub r9_t r5_t lr_t;
(* uadd16	r5, r5, lr                               #! PC = 0x400b84 *)
add r5_b r5_b lr_b;
add r5_t r5_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401860; Value = 0x5e5b3410; PC = 0x400b88 *)
mov r10 L0x401860;
mov r11 L0x401864;
mov zeta_r10 zeta_L0x401860;
mov zeta_r11 zeta_L0x401864;
(* smulwb	lr, r10, r4                              #! PC = 0x400b8c *)
cast r4_lsb@sint32 r4_b;
mull tmp_t tmp_b r10 r4_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r4_b;
assert A_lb = r4_b && true;
assume A_lb = r4_b && true;
cast A_lt@sint32 r4_t;
assert A_lt = r4_t && true;
assume A_lt = r4_t && true;
mov zeta zeta_r10;
(* smulwt	r4, r10, r4                              #! PC = 0x400b90 *)
cast r4_lst@sint32 r4_t;
mull tmp_t tmp_b r10 r4_lst;
spl dontcare r4_t tmp_t 16;
spl r4_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400b94 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r4, r4, r12, r0                          #! PC = 0x400b98 *)
cast r4_sb@sint16 r4_b;
mull tmp_t tmp_b r4_sb r12_t;
uadds carry r4_b tmp_b r0_b;
adc r4_t tmp_t r0_t carry;
(* pkhtb	lr, r4, lr, asr #16                       #! PC = 0x400b9c *)
mov tmp_b lr_t;
mov tmp_t r4_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r4, r2, lr                               #! PC = 0x400ba0 *)
sub r4_b r2_b lr_b;
sub r4_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400ba4 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r10, r5                              #! PC = 0x400ba8 *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r10 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r10;
(* smulwt	r5, r10, r5                              #! PC = 0x400bac *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r10 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bb0 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400bb4 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400bb8 *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r3, lr                               #! PC = 0x400bbc *)
sub r5_b r3_b lr_b;
sub r5_t r3_t lr_t;
(* uadd16	r3, r3, lr                               #! PC = 0x400bc0 *)
add r3_b r3_b lr_b;
add r3_t r3_t lr_t;
(* smulwb	lr, r11, r8                              #! PC = 0x400bc4 *)
cast r8_lsb@sint32 r8_b;
mull tmp_t tmp_b r11 r8_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r8_b;
assert A_lb = r8_b && true;
assume A_lb = r8_b && true;
cast A_lt@sint32 r8_t;
assert A_lt = r8_t && true;
assume A_lt = r8_t && true;
mov zeta zeta_r11;
(* smulwt	r8, r11, r8                              #! PC = 0x400bc8 *)
cast r8_lst@sint32 r8_t;
mull tmp_t tmp_b r11 r8_lst;
spl dontcare r8_t tmp_t 16;
spl r8_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400bcc *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r8, r8, r12, r0                          #! PC = 0x400bd0 *)
cast r8_sb@sint16 r8_b;
mull tmp_t tmp_b r8_sb r12_t;
uadds carry r8_b tmp_b r0_b;
adc r8_t tmp_t r0_t carry;
(* pkhtb	lr, r8, lr, asr #16                       #! PC = 0x400bd4 *)
mov tmp_b lr_t;
mov tmp_t r8_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r8, r6, lr                               #! PC = 0x400bd8 *)
sub r8_b r6_b lr_b;
sub r8_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400bdc *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400be0 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400be4 *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400be8 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400bec *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400bf0 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r7, lr                               #! PC = 0x400bf4 *)
sub r9_b r7_b lr_b;
sub r9_t r7_t lr_t;
(* uadd16	r7, r7, lr                               #! PC = 0x400bf8 *)
add r7_b r7_b lr_b;
add r7_t r7_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401868; Value = 0xa24249ac; PC = 0x400bfc *)
mov r10 L0x401868;
mov r11 L0x40186c;
mov zeta_r10 zeta_L0x401868;
mov zeta_r11 zeta_L0x40186c;
(* smulwb	lr, r10, r3                              #! PC = 0x400c00 *)
cast r3_lsb@sint32 r3_b;
mull tmp_t tmp_b r10 r3_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r3_b;
assert A_lb = r3_b && true;
assume A_lb = r3_b && true;
cast A_lt@sint32 r3_t;
assert A_lt = r3_t && true;
assume A_lt = r3_t && true;
mov zeta zeta_r10;
(* smulwt	r3, r10, r3                              #! PC = 0x400c04 *)
cast r3_lst@sint32 r3_t;
mull tmp_t tmp_b r10 r3_lst;
spl dontcare r3_t tmp_t 16;
spl r3_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c08 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r3, r3, r12, r0                          #! PC = 0x400c0c *)
cast r3_sb@sint16 r3_b;
mull tmp_t tmp_b r3_sb r12_t;
uadds carry r3_b tmp_b r0_b;
adc r3_t tmp_t r0_t carry;
(* pkhtb	lr, r3, lr, asr #16                       #! PC = 0x400c10 *)
mov tmp_b lr_t;
mov tmp_t r3_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r3, r2, lr                               #! PC = 0x400c14 *)
sub r3_b r2_b lr_b;
sub r3_t r2_t lr_t;
(* uadd16	r2, r2, lr                               #! PC = 0x400c18 *)
add r2_b r2_b lr_b;
add r2_t r2_t lr_t;
(* smulwb	lr, r11, r5                              #! PC = 0x400c1c *)
cast r5_lsb@sint32 r5_b;
mull tmp_t tmp_b r11 r5_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r5_b;
assert A_lb = r5_b && true;
assume A_lb = r5_b && true;
cast A_lt@sint32 r5_t;
assert A_lt = r5_t && true;
assume A_lt = r5_t && true;
mov zeta zeta_r11;
(* smulwt	r5, r11, r5                              #! PC = 0x400c20 *)
cast r5_lst@sint32 r5_t;
mull tmp_t tmp_b r11 r5_lst;
spl dontcare r5_t tmp_t 16;
spl r5_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c24 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r5, r5, r12, r0                          #! PC = 0x400c28 *)
cast r5_sb@sint16 r5_b;
mull tmp_t tmp_b r5_sb r12_t;
uadds carry r5_b tmp_b r0_b;
adc r5_t tmp_t r0_t carry;
(* pkhtb	lr, r5, lr, asr #16                       #! PC = 0x400c2c *)
mov tmp_b lr_t;
mov tmp_t r5_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r5, r4, lr                               #! PC = 0x400c30 *)
sub r5_b r4_b lr_b;
sub r5_t r4_t lr_t;
(* uadd16	r4, r4, lr                               #! PC = 0x400c34 *)
add r4_b r4_b lr_b;
add r4_t r4_t lr_t;
(* ldrd	r10, r11, [r1], #8                         #! EA = L0x401870; Value = 0x440e750b; PC = 0x400c38 *)
mov r10 L0x401870;
mov r11 L0x401874;
mov zeta_r10 zeta_L0x401870;
mov zeta_r11 zeta_L0x401874;
(* smulwb	lr, r10, r7                              #! PC = 0x400c3c *)
cast r7_lsb@sint32 r7_b;
mull tmp_t tmp_b r10 r7_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r7_b;
assert A_lb = r7_b && true;
assume A_lb = r7_b && true;
cast A_lt@sint32 r7_t;
assert A_lt = r7_t && true;
assume A_lt = r7_t && true;
mov zeta zeta_r10;
(* smulwt	r7, r10, r7                              #! PC = 0x400c40 *)
cast r7_lst@sint32 r7_t;
mull tmp_t tmp_b r10 r7_lst;
spl dontcare r7_t tmp_t 16;
spl r7_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c44 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r7, r7, r12, r0                          #! PC = 0x400c48 *)
cast r7_sb@sint16 r7_b;
mull tmp_t tmp_b r7_sb r12_t;
uadds carry r7_b tmp_b r0_b;
adc r7_t tmp_t r0_t carry;
(* pkhtb	lr, r7, lr, asr #16                       #! PC = 0x400c4c *)
mov tmp_b lr_t;
mov tmp_t r7_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r7, r6, lr                               #! PC = 0x400c50 *)
sub r7_b r6_b lr_b;
sub r7_t r6_t lr_t;
(* uadd16	r6, r6, lr                               #! PC = 0x400c54 *)
add r6_b r6_b lr_b;
add r6_t r6_t lr_t;
(* smulwb	lr, r11, r9                              #! PC = 0x400c58 *)
cast r9_lsb@sint32 r9_b;
mull tmp_t tmp_b r11 r9_lsb;
spl dontcare lr_t tmp_t 16;
spl lr_b dontcare tmp_b 16;
cast A_lb@sint32 r9_b;
assert A_lb = r9_b && true;
assume A_lb = r9_b && true;
cast A_lt@sint32 r9_t;
assert A_lt = r9_t && true;
assume A_lt = r9_t && true;
mov zeta zeta_r11;
(* smulwt	r9, r11, r9                              #! PC = 0x400c5c *)
cast r9_lst@sint32 r9_t;
mull tmp_t tmp_b r11 r9_lst;
spl dontcare r9_t tmp_t 16;
spl r9_b dontcare tmp_b 16;
(* smlabt	lr, lr, r12, r0                          #! PC = 0x400c60 *)
cast lr_sb@sint16 lr_b;
mull tmp_t tmp_b lr_sb r12_t;
uadds carry lr_b tmp_b r0_b;
adc lr_t tmp_t r0_t carry;
(* smlabt	r9, r9, r12, r0                          #! PC = 0x400c64 *)
cast r9_sb@sint16 r9_b;
mull tmp_t tmp_b r9_sb r12_t;
uadds carry r9_b tmp_b r0_b;
adc r9_t tmp_t r0_t carry;
(* pkhtb	lr, r9, lr, asr #16                       #! PC = 0x400c68 *)
mov tmp_b lr_t;
mov tmp_t r9_t;
mov lr_b tmp_b;
mov lr_t tmp_t;
cast C_lb@sint32 lr_b;
assert C_lb = lr_b && true;
assume C_lb = lr_b && true;
cast C_lt@sint32 lr_t;
assert C_lt = lr_t && true;
assume C_lt = lr_t && true;
assert true && eqsmod C_lb (A_lb * zeta) 3329@32;
assume eqmod C_lb (A_lb * zeta) 3329 && true;
assert true && eqsmod C_lt (A_lt * zeta) 3329@32;
assume eqmod C_lt (A_lt * zeta) 3329 && true;
(* usub16	r9, r8, lr                               #! PC = 0x400c6c *)
sub r9_b r8_b lr_b;
sub r9_t r8_t lr_t;
(* uadd16	r8, r8, lr                               #! PC = 0x400c70 *)
add r8_b r8_b lr_b;
add r8_t r8_t lr_t;
(* vmov	r0, s23                                    #! PC = 0x400c74 *)
mov r0_b s23_b;
mov r0_t s23_t;
mov r0 s23;
mov zeta_r0 zeta_s23;
(* str.w	r6, [r0, #16]                             #! EA = L0xbefff3b4; PC = 0x400c78 *)
mov L0xbefff3b4 r6_b;
mov L0xbefff3b6 r6_t;
(* str.w	r7, [r0, #20]                             #! EA = L0xbefff3b8; PC = 0x400c7c *)
mov L0xbefff3b8 r7_b;
mov L0xbefff3ba r7_t;
(* str.w	r8, [r0, #24]                             #! EA = L0xbefff3bc; PC = 0x400c80 *)
mov L0xbefff3bc r8_b;
mov L0xbefff3be r8_t;
(* str.w	r9, [r0, #28]                             #! EA = L0xbefff3c0; PC = 0x400c84 *)
mov L0xbefff3c0 r9_b;
mov L0xbefff3c2 r9_t;
(* str.w	r3, [r0, #4]                              #! EA = L0xbefff3a8; PC = 0x400c88 *)
mov L0xbefff3a8 r3_b;
mov L0xbefff3aa r3_t;
(* str.w	r4, [r0, #8]                              #! EA = L0xbefff3ac; PC = 0x400c8c *)
mov L0xbefff3ac r4_b;
mov L0xbefff3ae r4_t;
(* str.w	r5, [r0, #12]                             #! EA = L0xbefff3b0; PC = 0x400c90 *)
mov L0xbefff3b0 r5_b;
mov L0xbefff3b2 r5_t;
(* str.w	r2, [r0], #32                             #! EA = L0xbefff3a4; PC = 0x400c94 *)
mov L0xbefff3a4 r2_b;
mov L0xbefff3a6 r2_t;
(* vmov	lr, s13                                    #! PC = 0x400c98 *)
mov lr_b s13_b;
mov lr_t s13_t;
mov lr s13;
mov zeta_lr zeta_s13;
(* #bne.w	0x400aec <ntt_fast+1244>                 #! PC = 0x400ca0 *)
#bne.w	0x400aec <ntt_fast+1244>                 #! 0x400ca0 = 0x400ca0;


(*== post condition ==*)
{
  and [
    (-9) * 1664 <= L0xbefff1c4, L0xbefff1c4 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1c6, L0xbefff1c6 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1c8, L0xbefff1c8 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1ca, L0xbefff1ca <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1cc, L0xbefff1cc <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1ce, L0xbefff1ce <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1d0, L0xbefff1d0 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1d2, L0xbefff1d2 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1d4, L0xbefff1d4 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1d6, L0xbefff1d6 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1d8, L0xbefff1d8 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1da, L0xbefff1da <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1dc, L0xbefff1dc <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1de, L0xbefff1de <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1e0, L0xbefff1e0 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1e2, L0xbefff1e2 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1e4, L0xbefff1e4 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1e6, L0xbefff1e6 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1e8, L0xbefff1e8 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1ea, L0xbefff1ea <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1ec, L0xbefff1ec <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1ee, L0xbefff1ee <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1f0, L0xbefff1f0 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1f2, L0xbefff1f2 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1f4, L0xbefff1f4 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1f6, L0xbefff1f6 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1f8, L0xbefff1f8 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1fa, L0xbefff1fa <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1fc, L0xbefff1fc <= 9 * 1664,
    (-9) * 1664 <= L0xbefff1fe, L0xbefff1fe <= 9 * 1664,
    (-9) * 1664 <= L0xbefff200, L0xbefff200 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff202, L0xbefff202 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff204, L0xbefff204 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff206, L0xbefff206 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff208, L0xbefff208 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff20a, L0xbefff20a <= 9 * 1664,
    (-9) * 1664 <= L0xbefff20c, L0xbefff20c <= 9 * 1664,
    (-9) * 1664 <= L0xbefff20e, L0xbefff20e <= 9 * 1664,
    (-9) * 1664 <= L0xbefff210, L0xbefff210 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff212, L0xbefff212 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff214, L0xbefff214 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff216, L0xbefff216 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff218, L0xbefff218 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff21a, L0xbefff21a <= 9 * 1664,
    (-9) * 1664 <= L0xbefff21c, L0xbefff21c <= 9 * 1664,
    (-9) * 1664 <= L0xbefff21e, L0xbefff21e <= 9 * 1664,
    (-9) * 1664 <= L0xbefff220, L0xbefff220 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff222, L0xbefff222 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff224, L0xbefff224 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff226, L0xbefff226 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff228, L0xbefff228 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff22a, L0xbefff22a <= 9 * 1664,
    (-9) * 1664 <= L0xbefff22c, L0xbefff22c <= 9 * 1664,
    (-9) * 1664 <= L0xbefff22e, L0xbefff22e <= 9 * 1664,
    (-9) * 1664 <= L0xbefff230, L0xbefff230 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff232, L0xbefff232 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff234, L0xbefff234 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff236, L0xbefff236 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff238, L0xbefff238 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff23a, L0xbefff23a <= 9 * 1664,
    (-9) * 1664 <= L0xbefff23c, L0xbefff23c <= 9 * 1664,
    (-9) * 1664 <= L0xbefff23e, L0xbefff23e <= 9 * 1664,
    (-9) * 1664 <= L0xbefff240, L0xbefff240 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff242, L0xbefff242 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff244, L0xbefff244 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff246, L0xbefff246 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff248, L0xbefff248 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff24a, L0xbefff24a <= 9 * 1664,
    (-9) * 1664 <= L0xbefff24c, L0xbefff24c <= 9 * 1664,
    (-9) * 1664 <= L0xbefff24e, L0xbefff24e <= 9 * 1664,
    (-9) * 1664 <= L0xbefff250, L0xbefff250 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff252, L0xbefff252 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff254, L0xbefff254 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff256, L0xbefff256 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff258, L0xbefff258 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff25a, L0xbefff25a <= 9 * 1664,
    (-9) * 1664 <= L0xbefff25c, L0xbefff25c <= 9 * 1664,
    (-9) * 1664 <= L0xbefff25e, L0xbefff25e <= 9 * 1664,
    (-9) * 1664 <= L0xbefff260, L0xbefff260 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff262, L0xbefff262 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff264, L0xbefff264 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff266, L0xbefff266 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff268, L0xbefff268 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff26a, L0xbefff26a <= 9 * 1664,
    (-9) * 1664 <= L0xbefff26c, L0xbefff26c <= 9 * 1664,
    (-9) * 1664 <= L0xbefff26e, L0xbefff26e <= 9 * 1664,
    (-9) * 1664 <= L0xbefff270, L0xbefff270 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff272, L0xbefff272 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff274, L0xbefff274 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff276, L0xbefff276 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff278, L0xbefff278 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff27a, L0xbefff27a <= 9 * 1664,
    (-9) * 1664 <= L0xbefff27c, L0xbefff27c <= 9 * 1664,
    (-9) * 1664 <= L0xbefff27e, L0xbefff27e <= 9 * 1664,
    (-9) * 1664 <= L0xbefff280, L0xbefff280 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff282, L0xbefff282 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff284, L0xbefff284 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff286, L0xbefff286 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff288, L0xbefff288 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff28a, L0xbefff28a <= 9 * 1664,
    (-9) * 1664 <= L0xbefff28c, L0xbefff28c <= 9 * 1664,
    (-9) * 1664 <= L0xbefff28e, L0xbefff28e <= 9 * 1664,
    (-9) * 1664 <= L0xbefff290, L0xbefff290 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff292, L0xbefff292 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff294, L0xbefff294 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff296, L0xbefff296 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff298, L0xbefff298 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff29a, L0xbefff29a <= 9 * 1664,
    (-9) * 1664 <= L0xbefff29c, L0xbefff29c <= 9 * 1664,
    (-9) * 1664 <= L0xbefff29e, L0xbefff29e <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2a0, L0xbefff2a0 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2a2, L0xbefff2a2 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2a4, L0xbefff2a4 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2a6, L0xbefff2a6 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2a8, L0xbefff2a8 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2aa, L0xbefff2aa <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2ac, L0xbefff2ac <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2ae, L0xbefff2ae <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2b0, L0xbefff2b0 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2b2, L0xbefff2b2 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2b4, L0xbefff2b4 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2b6, L0xbefff2b6 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2b8, L0xbefff2b8 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2ba, L0xbefff2ba <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2bc, L0xbefff2bc <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2be, L0xbefff2be <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2c0, L0xbefff2c0 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2c2, L0xbefff2c2 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2c4, L0xbefff2c4 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2c6, L0xbefff2c6 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2c8, L0xbefff2c8 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2ca, L0xbefff2ca <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2cc, L0xbefff2cc <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2ce, L0xbefff2ce <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2d0, L0xbefff2d0 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2d2, L0xbefff2d2 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2d4, L0xbefff2d4 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2d6, L0xbefff2d6 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2d8, L0xbefff2d8 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2da, L0xbefff2da <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2dc, L0xbefff2dc <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2de, L0xbefff2de <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2e0, L0xbefff2e0 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2e2, L0xbefff2e2 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2e4, L0xbefff2e4 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2e6, L0xbefff2e6 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2e8, L0xbefff2e8 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2ea, L0xbefff2ea <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2ec, L0xbefff2ec <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2ee, L0xbefff2ee <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2f0, L0xbefff2f0 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2f2, L0xbefff2f2 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2f4, L0xbefff2f4 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2f6, L0xbefff2f6 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2f8, L0xbefff2f8 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2fa, L0xbefff2fa <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2fc, L0xbefff2fc <= 9 * 1664,
    (-9) * 1664 <= L0xbefff2fe, L0xbefff2fe <= 9 * 1664,
    (-9) * 1664 <= L0xbefff300, L0xbefff300 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff302, L0xbefff302 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff304, L0xbefff304 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff306, L0xbefff306 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff308, L0xbefff308 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff30a, L0xbefff30a <= 9 * 1664,
    (-9) * 1664 <= L0xbefff30c, L0xbefff30c <= 9 * 1664,
    (-9) * 1664 <= L0xbefff30e, L0xbefff30e <= 9 * 1664,
    (-9) * 1664 <= L0xbefff310, L0xbefff310 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff312, L0xbefff312 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff314, L0xbefff314 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff316, L0xbefff316 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff318, L0xbefff318 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff31a, L0xbefff31a <= 9 * 1664,
    (-9) * 1664 <= L0xbefff31c, L0xbefff31c <= 9 * 1664,
    (-9) * 1664 <= L0xbefff31e, L0xbefff31e <= 9 * 1664,
    (-9) * 1664 <= L0xbefff320, L0xbefff320 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff322, L0xbefff322 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff324, L0xbefff324 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff326, L0xbefff326 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff328, L0xbefff328 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff32a, L0xbefff32a <= 9 * 1664,
    (-9) * 1664 <= L0xbefff32c, L0xbefff32c <= 9 * 1664,
    (-9) * 1664 <= L0xbefff32e, L0xbefff32e <= 9 * 1664,
    (-9) * 1664 <= L0xbefff330, L0xbefff330 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff332, L0xbefff332 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff334, L0xbefff334 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff336, L0xbefff336 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff338, L0xbefff338 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff33a, L0xbefff33a <= 9 * 1664,
    (-9) * 1664 <= L0xbefff33c, L0xbefff33c <= 9 * 1664,
    (-9) * 1664 <= L0xbefff33e, L0xbefff33e <= 9 * 1664,
    (-9) * 1664 <= L0xbefff340, L0xbefff340 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff342, L0xbefff342 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff344, L0xbefff344 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff346, L0xbefff346 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff348, L0xbefff348 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff34a, L0xbefff34a <= 9 * 1664,
    (-9) * 1664 <= L0xbefff34c, L0xbefff34c <= 9 * 1664,
    (-9) * 1664 <= L0xbefff34e, L0xbefff34e <= 9 * 1664,
    (-9) * 1664 <= L0xbefff350, L0xbefff350 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff352, L0xbefff352 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff354, L0xbefff354 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff356, L0xbefff356 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff358, L0xbefff358 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff35a, L0xbefff35a <= 9 * 1664,
    (-9) * 1664 <= L0xbefff35c, L0xbefff35c <= 9 * 1664,
    (-9) * 1664 <= L0xbefff35e, L0xbefff35e <= 9 * 1664,
    (-9) * 1664 <= L0xbefff360, L0xbefff360 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff362, L0xbefff362 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff364, L0xbefff364 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff366, L0xbefff366 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff368, L0xbefff368 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff36a, L0xbefff36a <= 9 * 1664,
    (-9) * 1664 <= L0xbefff36c, L0xbefff36c <= 9 * 1664,
    (-9) * 1664 <= L0xbefff36e, L0xbefff36e <= 9 * 1664,
    (-9) * 1664 <= L0xbefff370, L0xbefff370 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff372, L0xbefff372 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff374, L0xbefff374 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff376, L0xbefff376 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff378, L0xbefff378 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff37a, L0xbefff37a <= 9 * 1664,
    (-9) * 1664 <= L0xbefff37c, L0xbefff37c <= 9 * 1664,
    (-9) * 1664 <= L0xbefff37e, L0xbefff37e <= 9 * 1664,
    (-9) * 1664 <= L0xbefff380, L0xbefff380 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff382, L0xbefff382 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff384, L0xbefff384 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff386, L0xbefff386 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff388, L0xbefff388 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff38a, L0xbefff38a <= 9 * 1664,
    (-9) * 1664 <= L0xbefff38c, L0xbefff38c <= 9 * 1664,
    (-9) * 1664 <= L0xbefff38e, L0xbefff38e <= 9 * 1664,
    (-9) * 1664 <= L0xbefff390, L0xbefff390 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff392, L0xbefff392 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff394, L0xbefff394 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff396, L0xbefff396 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff398, L0xbefff398 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff39a, L0xbefff39a <= 9 * 1664,
    (-9) * 1664 <= L0xbefff39c, L0xbefff39c <= 9 * 1664,
    (-9) * 1664 <= L0xbefff39e, L0xbefff39e <= 9 * 1664,
    (-9) * 1664 <= L0xbefff3a0, L0xbefff3a0 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff3a2, L0xbefff3a2 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff3a4, L0xbefff3a4 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff3a6, L0xbefff3a6 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff3a8, L0xbefff3a8 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff3aa, L0xbefff3aa <= 9 * 1664,
    (-9) * 1664 <= L0xbefff3ac, L0xbefff3ac <= 9 * 1664,
    (-9) * 1664 <= L0xbefff3ae, L0xbefff3ae <= 9 * 1664,
    (-9) * 1664 <= L0xbefff3b0, L0xbefff3b0 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff3b2, L0xbefff3b2 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff3b4, L0xbefff3b4 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff3b6, L0xbefff3b6 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff3b8, L0xbefff3b8 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff3ba, L0xbefff3ba <= 9 * 1664,
    (-9) * 1664 <= L0xbefff3bc, L0xbefff3bc <= 9 * 1664,
    (-9) * 1664 <= L0xbefff3be, L0xbefff3be <= 9 * 1664,
    (-9) * 1664 <= L0xbefff3c0, L0xbefff3c0 <= 9 * 1664,
    (-9) * 1664 <= L0xbefff3c2, L0xbefff3c2 <= 9 * 1664
  ] prove with [ algebra solver isl ]
  &&
  and [
    (-9)@16 * 1664@16 <=s L0xbefff1c4, L0xbefff1c4 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1c6, L0xbefff1c6 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1c8, L0xbefff1c8 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1ca, L0xbefff1ca <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1cc, L0xbefff1cc <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1ce, L0xbefff1ce <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1d0, L0xbefff1d0 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1d2, L0xbefff1d2 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1d4, L0xbefff1d4 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1d6, L0xbefff1d6 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1d8, L0xbefff1d8 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1da, L0xbefff1da <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1dc, L0xbefff1dc <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1de, L0xbefff1de <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1e0, L0xbefff1e0 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1e2, L0xbefff1e2 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1e4, L0xbefff1e4 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1e6, L0xbefff1e6 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1e8, L0xbefff1e8 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1ea, L0xbefff1ea <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1ec, L0xbefff1ec <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1ee, L0xbefff1ee <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1f0, L0xbefff1f0 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1f2, L0xbefff1f2 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1f4, L0xbefff1f4 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1f6, L0xbefff1f6 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1f8, L0xbefff1f8 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1fa, L0xbefff1fa <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1fc, L0xbefff1fc <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff1fe, L0xbefff1fe <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff200, L0xbefff200 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff202, L0xbefff202 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff204, L0xbefff204 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff206, L0xbefff206 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff208, L0xbefff208 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff20a, L0xbefff20a <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff20c, L0xbefff20c <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff20e, L0xbefff20e <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff210, L0xbefff210 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff212, L0xbefff212 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff214, L0xbefff214 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff216, L0xbefff216 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff218, L0xbefff218 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff21a, L0xbefff21a <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff21c, L0xbefff21c <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff21e, L0xbefff21e <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff220, L0xbefff220 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff222, L0xbefff222 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff224, L0xbefff224 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff226, L0xbefff226 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff228, L0xbefff228 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff22a, L0xbefff22a <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff22c, L0xbefff22c <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff22e, L0xbefff22e <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff230, L0xbefff230 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff232, L0xbefff232 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff234, L0xbefff234 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff236, L0xbefff236 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff238, L0xbefff238 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff23a, L0xbefff23a <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff23c, L0xbefff23c <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff23e, L0xbefff23e <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff240, L0xbefff240 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff242, L0xbefff242 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff244, L0xbefff244 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff246, L0xbefff246 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff248, L0xbefff248 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff24a, L0xbefff24a <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff24c, L0xbefff24c <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff24e, L0xbefff24e <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff250, L0xbefff250 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff252, L0xbefff252 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff254, L0xbefff254 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff256, L0xbefff256 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff258, L0xbefff258 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff25a, L0xbefff25a <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff25c, L0xbefff25c <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff25e, L0xbefff25e <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff260, L0xbefff260 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff262, L0xbefff262 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff264, L0xbefff264 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff266, L0xbefff266 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff268, L0xbefff268 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff26a, L0xbefff26a <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff26c, L0xbefff26c <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff26e, L0xbefff26e <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff270, L0xbefff270 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff272, L0xbefff272 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff274, L0xbefff274 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff276, L0xbefff276 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff278, L0xbefff278 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff27a, L0xbefff27a <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff27c, L0xbefff27c <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff27e, L0xbefff27e <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff280, L0xbefff280 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff282, L0xbefff282 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff284, L0xbefff284 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff286, L0xbefff286 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff288, L0xbefff288 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff28a, L0xbefff28a <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff28c, L0xbefff28c <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff28e, L0xbefff28e <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff290, L0xbefff290 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff292, L0xbefff292 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff294, L0xbefff294 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff296, L0xbefff296 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff298, L0xbefff298 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff29a, L0xbefff29a <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff29c, L0xbefff29c <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff29e, L0xbefff29e <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2a0, L0xbefff2a0 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2a2, L0xbefff2a2 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2a4, L0xbefff2a4 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2a6, L0xbefff2a6 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2a8, L0xbefff2a8 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2aa, L0xbefff2aa <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2ac, L0xbefff2ac <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2ae, L0xbefff2ae <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2b0, L0xbefff2b0 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2b2, L0xbefff2b2 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2b4, L0xbefff2b4 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2b6, L0xbefff2b6 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2b8, L0xbefff2b8 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2ba, L0xbefff2ba <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2bc, L0xbefff2bc <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2be, L0xbefff2be <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2c0, L0xbefff2c0 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2c2, L0xbefff2c2 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2c4, L0xbefff2c4 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2c6, L0xbefff2c6 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2c8, L0xbefff2c8 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2ca, L0xbefff2ca <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2cc, L0xbefff2cc <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2ce, L0xbefff2ce <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2d0, L0xbefff2d0 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2d2, L0xbefff2d2 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2d4, L0xbefff2d4 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2d6, L0xbefff2d6 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2d8, L0xbefff2d8 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2da, L0xbefff2da <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2dc, L0xbefff2dc <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2de, L0xbefff2de <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2e0, L0xbefff2e0 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2e2, L0xbefff2e2 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2e4, L0xbefff2e4 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2e6, L0xbefff2e6 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2e8, L0xbefff2e8 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2ea, L0xbefff2ea <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2ec, L0xbefff2ec <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2ee, L0xbefff2ee <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2f0, L0xbefff2f0 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2f2, L0xbefff2f2 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2f4, L0xbefff2f4 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2f6, L0xbefff2f6 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2f8, L0xbefff2f8 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2fa, L0xbefff2fa <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2fc, L0xbefff2fc <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff2fe, L0xbefff2fe <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff300, L0xbefff300 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff302, L0xbefff302 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff304, L0xbefff304 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff306, L0xbefff306 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff308, L0xbefff308 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff30a, L0xbefff30a <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff30c, L0xbefff30c <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff30e, L0xbefff30e <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff310, L0xbefff310 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff312, L0xbefff312 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff314, L0xbefff314 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff316, L0xbefff316 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff318, L0xbefff318 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff31a, L0xbefff31a <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff31c, L0xbefff31c <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff31e, L0xbefff31e <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff320, L0xbefff320 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff322, L0xbefff322 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff324, L0xbefff324 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff326, L0xbefff326 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff328, L0xbefff328 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff32a, L0xbefff32a <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff32c, L0xbefff32c <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff32e, L0xbefff32e <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff330, L0xbefff330 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff332, L0xbefff332 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff334, L0xbefff334 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff336, L0xbefff336 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff338, L0xbefff338 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff33a, L0xbefff33a <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff33c, L0xbefff33c <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff33e, L0xbefff33e <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff340, L0xbefff340 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff342, L0xbefff342 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff344, L0xbefff344 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff346, L0xbefff346 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff348, L0xbefff348 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff34a, L0xbefff34a <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff34c, L0xbefff34c <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff34e, L0xbefff34e <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff350, L0xbefff350 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff352, L0xbefff352 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff354, L0xbefff354 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff356, L0xbefff356 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff358, L0xbefff358 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff35a, L0xbefff35a <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff35c, L0xbefff35c <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff35e, L0xbefff35e <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff360, L0xbefff360 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff362, L0xbefff362 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff364, L0xbefff364 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff366, L0xbefff366 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff368, L0xbefff368 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff36a, L0xbefff36a <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff36c, L0xbefff36c <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff36e, L0xbefff36e <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff370, L0xbefff370 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff372, L0xbefff372 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff374, L0xbefff374 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff376, L0xbefff376 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff378, L0xbefff378 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff37a, L0xbefff37a <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff37c, L0xbefff37c <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff37e, L0xbefff37e <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff380, L0xbefff380 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff382, L0xbefff382 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff384, L0xbefff384 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff386, L0xbefff386 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff388, L0xbefff388 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff38a, L0xbefff38a <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff38c, L0xbefff38c <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff38e, L0xbefff38e <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff390, L0xbefff390 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff392, L0xbefff392 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff394, L0xbefff394 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff396, L0xbefff396 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff398, L0xbefff398 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff39a, L0xbefff39a <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff39c, L0xbefff39c <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff39e, L0xbefff39e <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff3a0, L0xbefff3a0 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff3a2, L0xbefff3a2 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff3a4, L0xbefff3a4 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff3a6, L0xbefff3a6 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff3a8, L0xbefff3a8 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff3aa, L0xbefff3aa <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff3ac, L0xbefff3ac <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff3ae, L0xbefff3ae <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff3b0, L0xbefff3b0 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff3b2, L0xbefff3b2 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff3b4, L0xbefff3b4 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff3b6, L0xbefff3b6 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff3b8, L0xbefff3b8 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff3ba, L0xbefff3ba <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff3bc, L0xbefff3bc <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff3be, L0xbefff3be <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff3c0, L0xbefff3c0 <=s 9@16 * 1664@16,
    (-9)@16 * 1664@16 <=s L0xbefff3c2, L0xbefff3c2 <=s 9@16 * 1664@16
  ]
}


(* { *)
(* algebraic condition *)
(*
  and [eqmod (inp_poly * inp_poly) (L0xbefff1c4 * (x**0) + L0xbefff1c6 * (x**1)) [3329, x**2 - 17], eqmod (inp_poly * inp_poly) (L0xbefff1c8 * (x**0) + L0xbefff1ca * (x**1)) [3329, x**2 - 3312], eqmod (inp_poly * inp_poly) (L0xbefff1cc * (x**0) + L0xbefff1ce * (x**1)) [3329, x**2 - 2761], eqmod (inp_poly * inp_poly) (L0xbefff1d0 * (x**0) + L0xbefff1d2 * (x**1)) [3329, x**2 - 568], eqmod (inp_poly * inp_poly) (L0xbefff1d4 * (x**0) + L0xbefff1d6 * (x**1)) [3329, x**2 - 583], eqmod (inp_poly * inp_poly) (L0xbefff1d8 * (x**0) + L0xbefff1da * (x**1)) [3329, x**2 - 2746], eqmod (inp_poly * inp_poly) (L0xbefff1dc * (x**0) + L0xbefff1de * (x**1)) [3329, x**2 - 2649], eqmod (inp_poly * inp_poly) (L0xbefff1e0 * (x**0) + L0xbefff1e2 * (x**1)) [3329, x**2 - 680], eqmod (inp_poly * inp_poly) (L0xbefff1e4 * (x**0) + L0xbefff1e6 * (x**1)) [3329, x**2 - 1637], eqmod (inp_poly * inp_poly) (L0xbefff1e8 * (x**0) + L0xbefff1ea * (x**1)) [3329, x**2 - 1692], eqmod (inp_poly * inp_poly) (L0xbefff1ec * (x**0) + L0xbefff1ee * (x**1)) [3329, x**2 - 723], eqmod (inp_poly * inp_poly) (L0xbefff1f0 * (x**0) + L0xbefff1f2 * (x**1)) [3329, x**2 - 2606], eqmod (inp_poly * inp_poly) (L0xbefff1f4 * (x**0) + L0xbefff1f6 * (x**1)) [3329, x**2 - 2288], eqmod (inp_poly * inp_poly) (L0xbefff1f8 * (x**0) + L0xbefff1fa * (x**1)) [3329, x**2 - 1041], eqmod (inp_poly * inp_poly) (L0xbefff1fc * (x**0) + L0xbefff1fe * (x**1)) [3329, x**2 - 1100], eqmod (inp_poly * inp_poly) (L0xbefff200 * (x**0) + L0xbefff202 * (x**1)) [3329, x**2 - 2229], eqmod (inp_poly * inp_poly) (L0xbefff204 * (x**0) + L0xbefff206 * (x**1)) [3329, x**2 - 1409], eqmod (inp_poly * inp_poly) (L0xbefff208 * (x**0) + L0xbefff20a * (x**1)) [3329, x**2 - 1920], eqmod (inp_poly * inp_poly) (L0xbefff20c * (x**0) + L0xbefff20e * (x**1)) [3329, x**2 - 2662], eqmod (inp_poly * inp_poly) (L0xbefff210 * (x**0) + L0xbefff212 * (x**1)) [3329, x**2 - 667], eqmod (inp_poly * inp_poly) (L0xbefff214 * (x**0) + L0xbefff216 * (x**1)) [3329, x**2 - 3281], eqmod (inp_poly * inp_poly) (L0xbefff218 * (x**0) + L0xbefff21a * (x**1)) [3329, x**2 - 48], eqmod (inp_poly * inp_poly) (L0xbefff21c * (x**0) + L0xbefff21e * (x**1)) [3329, x**2 - 233], eqmod (inp_poly * inp_poly) (L0xbefff220 * (x**0) + L0xbefff222 * (x**1)) [3329, x**2 - 3096], eqmod (inp_poly * inp_poly) (L0xbefff224 * (x**0) + L0xbefff226 * (x**1)) [3329, x**2 - 756], eqmod (inp_poly * inp_poly) (L0xbefff228 * (x**0) + L0xbefff22a * (x**1)) [3329, x**2 - 2573], eqmod (inp_poly * inp_poly) (L0xbefff22c * (x**0) + L0xbefff22e * (x**1)) [3329, x**2 - 2156], eqmod (inp_poly * inp_poly) (L0xbefff230 * (x**0) + L0xbefff232 * (x**1)) [3329, x**2 - 1173], eqmod (inp_poly * inp_poly) (L0xbefff234 * (x**0) + L0xbefff236 * (x**1)) [3329, x**2 - 3015], eqmod (inp_poly * inp_poly) (L0xbefff238 * (x**0) + L0xbefff23a * (x**1)) [3329, x**2 - 314], eqmod (inp_poly * inp_poly) (L0xbefff23c * (x**0) + L0xbefff23e * (x**1)) [3329, x**2 - 3050], eqmod (inp_poly * inp_poly) (L0xbefff240 * (x**0) + L0xbefff242 * (x**1)) [3329, x**2 - 279], eqmod (inp_poly * inp_poly) (L0xbefff244 * (x**0) + L0xbefff246 * (x**1)) [3329, x**2 - 1703], eqmod (inp_poly * inp_poly) (L0xbefff248 * (x**0) + L0xbefff24a * (x**1)) [3329, x**2 - 1626], eqmod (inp_poly * inp_poly) (L0xbefff24c * (x**0) + L0xbefff24e * (x**1)) [3329, x**2 - 1651], eqmod (inp_poly * inp_poly) (L0xbefff250 * (x**0) + L0xbefff252 * (x**1)) [3329, x**2 - 1678], eqmod (inp_poly * inp_poly) (L0xbefff254 * (x**0) + L0xbefff256 * (x**1)) [3329, x**2 - 2789], eqmod (inp_poly * inp_poly) (L0xbefff258 * (x**0) + L0xbefff25a * (x**1)) [3329, x**2 - 540], eqmod (inp_poly * inp_poly) (L0xbefff25c * (x**0) + L0xbefff25e * (x**1)) [3329, x**2 - 1789], eqmod (inp_poly * inp_poly) (L0xbefff260 * (x**0) + L0xbefff262 * (x**1)) [3329, x**2 - 1540], eqmod (inp_poly * inp_poly) (L0xbefff264 * (x**0) + L0xbefff266 * (x**1)) [3329, x**2 - 1847], eqmod (inp_poly * inp_poly) (L0xbefff268 * (x**0) + L0xbefff26a * (x**1)) [3329, x**2 - 1482], eqmod (inp_poly * inp_poly) (L0xbefff26c * (x**0) + L0xbefff26e * (x**1)) [3329, x**2 - 952], eqmod (inp_poly * inp_poly) (L0xbefff270 * (x**0) + L0xbefff272 * (x**1)) [3329, x**2 - 2377], eqmod (inp_poly * inp_poly) (L0xbefff274 * (x**0) + L0xbefff276 * (x**1)) [3329, x**2 - 1461], eqmod (inp_poly * inp_poly) (L0xbefff278 * (x**0) + L0xbefff27a * (x**1)) [3329, x**2 - 1868], eqmod (inp_poly * inp_poly) (L0xbefff27c * (x**0) + L0xbefff27e * (x**1)) [3329, x**2 - 2687], eqmod (inp_poly * inp_poly) (L0xbefff280 * (x**0) + L0xbefff282 * (x**1)) [3329, x**2 - 642], eqmod (inp_poly * inp_poly) (L0xbefff284 * (x**0) + L0xbefff286 * (x**1)) [3329, x**2 - 939], eqmod (inp_poly * inp_poly) (L0xbefff288 * (x**0) + L0xbefff28a * (x**1)) [3329, x**2 - 2390], eqmod (inp_poly * inp_poly) (L0xbefff28c * (x**0) + L0xbefff28e * (x**1)) [3329, x**2 - 2308], eqmod (inp_poly * inp_poly) (L0xbefff290 * (x**0) + L0xbefff292 * (x**1)) [3329, x**2 - 1021], eqmod (inp_poly * inp_poly) (L0xbefff294 * (x**0) + L0xbefff296 * (x**1)) [3329, x**2 - 2437], eqmod (inp_poly * inp_poly) (L0xbefff298 * (x**0) + L0xbefff29a * (x**1)) [3329, x**2 - 892], eqmod (inp_poly * inp_poly) (L0xbefff29c * (x**0) + L0xbefff29e * (x**1)) [3329, x**2 - 2388], eqmod (inp_poly * inp_poly) (L0xbefff2a0 * (x**0) + L0xbefff2a2 * (x**1)) [3329, x**2 - 941], eqmod (inp_poly * inp_poly) (L0xbefff2a4 * (x**0) + L0xbefff2a6 * (x**1)) [3329, x**2 - 733], eqmod (inp_poly * inp_poly) (L0xbefff2a8 * (x**0) + L0xbefff2aa * (x**1)) [3329, x**2 - 2596], eqmod (inp_poly * inp_poly) (L0xbefff2ac * (x**0) + L0xbefff2ae * (x**1)) [3329, x**2 - 2337], eqmod (inp_poly * inp_poly) (L0xbefff2b0 * (x**0) + L0xbefff2b2 * (x**1)) [3329, x**2 - 992], eqmod (inp_poly * inp_poly) (L0xbefff2b4 * (x**0) + L0xbefff2b6 * (x**1)) [3329, x**2 - 268], eqmod (inp_poly * inp_poly) (L0xbefff2b8 * (x**0) + L0xbefff2ba * (x**1)) [3329, x**2 - 3061], eqmod (inp_poly * inp_poly) (L0xbefff2bc * (x**0) + L0xbefff2be * (x**1)) [3329, x**2 - 641], eqmod (inp_poly * inp_poly) (L0xbefff2c0 * (x**0) + L0xbefff2c2 * (x**1)) [3329, x**2 - 2688], eqmod (inp_poly * inp_poly) (L0xbefff2c4 * (x**0) + L0xbefff2c6 * (x**1)) [3329, x**2 - 1584], eqmod (inp_poly * inp_poly) (L0xbefff2c8 * (x**0) + L0xbefff2ca * (x**1)) [3329, x**2 - 1745], eqmod (inp_poly * inp_poly) (L0xbefff2cc * (x**0) + L0xbefff2ce * (x**1)) [3329, x**2 - 2298], eqmod (inp_poly * inp_poly) (L0xbefff2d0 * (x**0) + L0xbefff2d2 * (x**1)) [3329, x**2 - 1031], eqmod (inp_poly * inp_poly) (L0xbefff2d4 * (x**0) + L0xbefff2d6 * (x**1)) [3329, x**2 - 2037], eqmod (inp_poly * inp_poly) (L0xbefff2d8 * (x**0) + L0xbefff2da * (x**1)) [3329, x**2 - 1292], eqmod (inp_poly * inp_poly) (L0xbefff2dc * (x**0) + L0xbefff2de * (x**1)) [3329, x**2 - 3220], eqmod (inp_poly * inp_poly) (L0xbefff2e0 * (x**0) + L0xbefff2e2 * (x**1)) [3329, x**2 - 109], eqmod (inp_poly * inp_poly) (L0xbefff2e4 * (x**0) + L0xbefff2e6 * (x**1)) [3329, x**2 - 375], eqmod (inp_poly * inp_poly) (L0xbefff2e8 * (x**0) + L0xbefff2ea * (x**1)) [3329, x**2 - 2954], eqmod (inp_poly * inp_poly) (L0xbefff2ec * (x**0) + L0xbefff2ee * (x**1)) [3329, x**2 - 2549], eqmod (inp_poly * inp_poly) (L0xbefff2f0 * (x**0) + L0xbefff2f2 * (x**1)) [3329, x**2 - 780], eqmod (inp_poly * inp_poly) (L0xbefff2f4 * (x**0) + L0xbefff2f6 * (x**1)) [3329, x**2 - 2090], eqmod (inp_poly * inp_poly) (L0xbefff2f8 * (x**0) + L0xbefff2fa * (x**1)) [3329, x**2 - 1239], eqmod (inp_poly * inp_poly) (L0xbefff2fc * (x**0) + L0xbefff2fe * (x**1)) [3329, x**2 - 1645], eqmod (inp_poly * inp_poly) (L0xbefff300 * (x**0) + L0xbefff302 * (x**1)) [3329, x**2 - 1684], eqmod (inp_poly * inp_poly) (L0xbefff304 * (x**0) + L0xbefff306 * (x**1)) [3329, x**2 - 1063], eqmod (inp_poly * inp_poly) (L0xbefff308 * (x**0) + L0xbefff30a * (x**1)) [3329, x**2 - 2266], eqmod (inp_poly * inp_poly) (L0xbefff30c * (x**0) + L0xbefff30e * (x**1)) [3329, x**2 - 319], eqmod (inp_poly * inp_poly) (L0xbefff310 * (x**0) + L0xbefff312 * (x**1)) [3329, x**2 - 3010], eqmod (inp_poly * inp_poly) (L0xbefff314 * (x**0) + L0xbefff316 * (x**1)) [3329, x**2 - 2773], eqmod (inp_poly * inp_poly) (L0xbefff318 * (x**0) + L0xbefff31a * (x**1)) [3329, x**2 - 556], eqmod (inp_poly * inp_poly) (L0xbefff31c * (x**0) + L0xbefff31e * (x**1)) [3329, x**2 - 757], eqmod (inp_poly * inp_poly) (L0xbefff320 * (x**0) + L0xbefff322 * (x**1)) [3329, x**2 - 2572], eqmod (inp_poly * inp_poly) (L0xbefff324 * (x**0) + L0xbefff326 * (x**1)) [3329, x**2 - 2099], eqmod (inp_poly * inp_poly) (L0xbefff328 * (x**0) + L0xbefff32a * (x**1)) [3329, x**2 - 1230], eqmod (inp_poly * inp_poly) (L0xbefff32c * (x**0) + L0xbefff32e * (x**1)) [3329, x**2 - 561], eqmod (inp_poly * inp_poly) (L0xbefff330 * (x**0) + L0xbefff332 * (x**1)) [3329, x**2 - 2768], eqmod (inp_poly * inp_poly) (L0xbefff334 * (x**0) + L0xbefff336 * (x**1)) [3329, x**2 - 2466], eqmod (inp_poly * inp_poly) (L0xbefff338 * (x**0) + L0xbefff33a * (x**1)) [3329, x**2 - 863], eqmod (inp_poly * inp_poly) (L0xbefff33c * (x**0) + L0xbefff33e * (x**1)) [3329, x**2 - 2594], eqmod (inp_poly * inp_poly) (L0xbefff340 * (x**0) + L0xbefff342 * (x**1)) [3329, x**2 - 735], eqmod (inp_poly * inp_poly) (L0xbefff344 * (x**0) + L0xbefff346 * (x**1)) [3329, x**2 - 2804], eqmod (inp_poly * inp_poly) (L0xbefff348 * (x**0) + L0xbefff34a * (x**1)) [3329, x**2 - 525], eqmod (inp_poly * inp_poly) (L0xbefff34c * (x**0) + L0xbefff34e * (x**1)) [3329, x**2 - 1092], eqmod (inp_poly * inp_poly) (L0xbefff350 * (x**0) + L0xbefff352 * (x**1)) [3329, x**2 - 2237], eqmod (inp_poly * inp_poly) (L0xbefff354 * (x**0) + L0xbefff356 * (x**1)) [3329, x**2 - 403], eqmod (inp_poly * inp_poly) (L0xbefff358 * (x**0) + L0xbefff35a * (x**1)) [3329, x**2 - 2926], eqmod (inp_poly * inp_poly) (L0xbefff35c * (x**0) + L0xbefff35e * (x**1)) [3329, x**2 - 1026], eqmod (inp_poly * inp_poly) (L0xbefff360 * (x**0) + L0xbefff362 * (x**1)) [3329, x**2 - 2303], eqmod (inp_poly * inp_poly) (L0xbefff364 * (x**0) + L0xbefff366 * (x**1)) [3329, x**2 - 1143], eqmod (inp_poly * inp_poly) (L0xbefff368 * (x**0) + L0xbefff36a * (x**1)) [3329, x**2 - 2186], eqmod (inp_poly * inp_poly) (L0xbefff36c * (x**0) + L0xbefff36e * (x**1)) [3329, x**2 - 2150], eqmod (inp_poly * inp_poly) (L0xbefff370 * (x**0) + L0xbefff372 * (x**1)) [3329, x**2 - 1179], eqmod (inp_poly * inp_poly) (L0xbefff374 * (x**0) + L0xbefff376 * (x**1)) [3329, x**2 - 2775], eqmod (inp_poly * inp_poly) (L0xbefff378 * (x**0) + L0xbefff37a * (x**1)) [3329, x**2 - 554], eqmod (inp_poly * inp_poly) (L0xbefff37c * (x**0) + L0xbefff37e * (x**1)) [3329, x**2 - 886], eqmod (inp_poly * inp_poly) (L0xbefff380 * (x**0) + L0xbefff382 * (x**1)) [3329, x**2 - 2443], eqmod (inp_poly * inp_poly) (L0xbefff384 * (x**0) + L0xbefff386 * (x**1)) [3329, x**2 - 1722], eqmod (inp_poly * inp_poly) (L0xbefff388 * (x**0) + L0xbefff38a * (x**1)) [3329, x**2 - 1607], eqmod (inp_poly * inp_poly) (L0xbefff38c * (x**0) + L0xbefff38e * (x**1)) [3329, x**2 - 1212], eqmod (inp_poly * inp_poly) (L0xbefff390 * (x**0) + L0xbefff392 * (x**1)) [3329, x**2 - 2117], eqmod (inp_poly * inp_poly) (L0xbefff394 * (x**0) + L0xbefff396 * (x**1)) [3329, x**2 - 1874], eqmod (inp_poly * inp_poly) (L0xbefff398 * (x**0) + L0xbefff39a * (x**1)) [3329, x**2 - 1455], eqmod (inp_poly * inp_poly) (L0xbefff39c * (x**0) + L0xbefff39e * (x**1)) [3329, x**2 - 1029], eqmod (inp_poly * inp_poly) (L0xbefff3a0 * (x**0) + L0xbefff3a2 * (x**1)) [3329, x**2 - 2300], eqmod (inp_poly * inp_poly) (L0xbefff3a4 * (x**0) + L0xbefff3a6 * (x**1)) [3329, x**2 - 2110], eqmod (inp_poly * inp_poly) (L0xbefff3a8 * (x**0) + L0xbefff3aa * (x**1)) [3329, x**2 - 1219], eqmod (inp_poly * inp_poly) (L0xbefff3ac * (x**0) + L0xbefff3ae * (x**1)) [3329, x**2 - 2935], eqmod (inp_poly * inp_poly) (L0xbefff3b0 * (x**0) + L0xbefff3b2 * (x**1)) [3329, x**2 - 394], eqmod (inp_poly * inp_poly) (L0xbefff3b4 * (x**0) + L0xbefff3b6 * (x**1)) [3329, x**2 - 885], eqmod (inp_poly * inp_poly) (L0xbefff3b8 * (x**0) + L0xbefff3ba * (x**1)) [3329, x**2 - 2444], eqmod (inp_poly * inp_poly) (L0xbefff3bc * (x**0) + L0xbefff3be * (x**1)) [3329, x**2 - 2154], eqmod (inp_poly * inp_poly) (L0xbefff3c0 * (x**0) + L0xbefff3c2 * (x**1)) [3329, x**2 - 1175]]
  &&
  and [(-9)@16 * 1664@16 <=s L0xbefff1c4, L0xbefff1c4 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1c6, L0xbefff1c6 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1c8, L0xbefff1c8 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1ca, L0xbefff1ca <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1cc, L0xbefff1cc <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1ce, L0xbefff1ce <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1d0, L0xbefff1d0 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1d2, L0xbefff1d2 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1d4, L0xbefff1d4 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1d6, L0xbefff1d6 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1d8, L0xbefff1d8 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1da, L0xbefff1da <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1dc, L0xbefff1dc <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1de, L0xbefff1de <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1e0, L0xbefff1e0 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1e2, L0xbefff1e2 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1e4, L0xbefff1e4 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1e6, L0xbefff1e6 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1e8, L0xbefff1e8 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1ea, L0xbefff1ea <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1ec, L0xbefff1ec <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1ee, L0xbefff1ee <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1f0, L0xbefff1f0 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1f2, L0xbefff1f2 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1f4, L0xbefff1f4 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1f6, L0xbefff1f6 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1f8, L0xbefff1f8 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1fa, L0xbefff1fa <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1fc, L0xbefff1fc <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff1fe, L0xbefff1fe <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff200, L0xbefff200 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff202, L0xbefff202 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff204, L0xbefff204 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff206, L0xbefff206 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff208, L0xbefff208 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff20a, L0xbefff20a <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff20c, L0xbefff20c <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff20e, L0xbefff20e <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff210, L0xbefff210 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff212, L0xbefff212 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff214, L0xbefff214 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff216, L0xbefff216 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff218, L0xbefff218 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff21a, L0xbefff21a <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff21c, L0xbefff21c <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff21e, L0xbefff21e <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff220, L0xbefff220 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff222, L0xbefff222 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff224, L0xbefff224 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff226, L0xbefff226 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff228, L0xbefff228 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff22a, L0xbefff22a <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff22c, L0xbefff22c <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff22e, L0xbefff22e <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff230, L0xbefff230 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff232, L0xbefff232 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff234, L0xbefff234 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff236, L0xbefff236 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff238, L0xbefff238 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff23a, L0xbefff23a <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff23c, L0xbefff23c <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff23e, L0xbefff23e <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff240, L0xbefff240 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff242, L0xbefff242 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff244, L0xbefff244 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff246, L0xbefff246 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff248, L0xbefff248 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff24a, L0xbefff24a <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff24c, L0xbefff24c <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff24e, L0xbefff24e <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff250, L0xbefff250 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff252, L0xbefff252 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff254, L0xbefff254 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff256, L0xbefff256 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff258, L0xbefff258 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff25a, L0xbefff25a <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff25c, L0xbefff25c <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff25e, L0xbefff25e <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff260, L0xbefff260 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff262, L0xbefff262 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff264, L0xbefff264 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff266, L0xbefff266 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff268, L0xbefff268 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff26a, L0xbefff26a <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff26c, L0xbefff26c <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff26e, L0xbefff26e <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff270, L0xbefff270 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff272, L0xbefff272 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff274, L0xbefff274 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff276, L0xbefff276 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff278, L0xbefff278 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff27a, L0xbefff27a <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff27c, L0xbefff27c <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff27e, L0xbefff27e <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff280, L0xbefff280 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff282, L0xbefff282 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff284, L0xbefff284 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff286, L0xbefff286 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff288, L0xbefff288 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff28a, L0xbefff28a <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff28c, L0xbefff28c <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff28e, L0xbefff28e <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff290, L0xbefff290 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff292, L0xbefff292 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff294, L0xbefff294 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff296, L0xbefff296 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff298, L0xbefff298 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff29a, L0xbefff29a <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff29c, L0xbefff29c <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff29e, L0xbefff29e <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2a0, L0xbefff2a0 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2a2, L0xbefff2a2 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2a4, L0xbefff2a4 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2a6, L0xbefff2a6 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2a8, L0xbefff2a8 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2aa, L0xbefff2aa <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2ac, L0xbefff2ac <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2ae, L0xbefff2ae <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2b0, L0xbefff2b0 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2b2, L0xbefff2b2 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2b4, L0xbefff2b4 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2b6, L0xbefff2b6 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2b8, L0xbefff2b8 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2ba, L0xbefff2ba <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2bc, L0xbefff2bc <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2be, L0xbefff2be <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2c0, L0xbefff2c0 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2c2, L0xbefff2c2 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2c4, L0xbefff2c4 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2c6, L0xbefff2c6 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2c8, L0xbefff2c8 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2ca, L0xbefff2ca <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2cc, L0xbefff2cc <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2ce, L0xbefff2ce <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2d0, L0xbefff2d0 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2d2, L0xbefff2d2 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2d4, L0xbefff2d4 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2d6, L0xbefff2d6 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2d8, L0xbefff2d8 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2da, L0xbefff2da <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2dc, L0xbefff2dc <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2de, L0xbefff2de <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2e0, L0xbefff2e0 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2e2, L0xbefff2e2 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2e4, L0xbefff2e4 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2e6, L0xbefff2e6 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2e8, L0xbefff2e8 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2ea, L0xbefff2ea <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2ec, L0xbefff2ec <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2ee, L0xbefff2ee <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2f0, L0xbefff2f0 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2f2, L0xbefff2f2 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2f4, L0xbefff2f4 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2f6, L0xbefff2f6 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2f8, L0xbefff2f8 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2fa, L0xbefff2fa <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2fc, L0xbefff2fc <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff2fe, L0xbefff2fe <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff300, L0xbefff300 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff302, L0xbefff302 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff304, L0xbefff304 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff306, L0xbefff306 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff308, L0xbefff308 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff30a, L0xbefff30a <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff30c, L0xbefff30c <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff30e, L0xbefff30e <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff310, L0xbefff310 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff312, L0xbefff312 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff314, L0xbefff314 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff316, L0xbefff316 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff318, L0xbefff318 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff31a, L0xbefff31a <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff31c, L0xbefff31c <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff31e, L0xbefff31e <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff320, L0xbefff320 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff322, L0xbefff322 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff324, L0xbefff324 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff326, L0xbefff326 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff328, L0xbefff328 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff32a, L0xbefff32a <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff32c, L0xbefff32c <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff32e, L0xbefff32e <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff330, L0xbefff330 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff332, L0xbefff332 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff334, L0xbefff334 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff336, L0xbefff336 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff338, L0xbefff338 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff33a, L0xbefff33a <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff33c, L0xbefff33c <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff33e, L0xbefff33e <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff340, L0xbefff340 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff342, L0xbefff342 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff344, L0xbefff344 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff346, L0xbefff346 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff348, L0xbefff348 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff34a, L0xbefff34a <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff34c, L0xbefff34c <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff34e, L0xbefff34e <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff350, L0xbefff350 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff352, L0xbefff352 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff354, L0xbefff354 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff356, L0xbefff356 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff358, L0xbefff358 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff35a, L0xbefff35a <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff35c, L0xbefff35c <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff35e, L0xbefff35e <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff360, L0xbefff360 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff362, L0xbefff362 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff364, L0xbefff364 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff366, L0xbefff366 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff368, L0xbefff368 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff36a, L0xbefff36a <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff36c, L0xbefff36c <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff36e, L0xbefff36e <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff370, L0xbefff370 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff372, L0xbefff372 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff374, L0xbefff374 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff376, L0xbefff376 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff378, L0xbefff378 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff37a, L0xbefff37a <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff37c, L0xbefff37c <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff37e, L0xbefff37e <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff380, L0xbefff380 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff382, L0xbefff382 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff384, L0xbefff384 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff386, L0xbefff386 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff388, L0xbefff388 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff38a, L0xbefff38a <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff38c, L0xbefff38c <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff38e, L0xbefff38e <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff390, L0xbefff390 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff392, L0xbefff392 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff394, L0xbefff394 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff396, L0xbefff396 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff398, L0xbefff398 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff39a, L0xbefff39a <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff39c, L0xbefff39c <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff39e, L0xbefff39e <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff3a0, L0xbefff3a0 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff3a2, L0xbefff3a2 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff3a4, L0xbefff3a4 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff3a6, L0xbefff3a6 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff3a8, L0xbefff3a8 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff3aa, L0xbefff3aa <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff3ac, L0xbefff3ac <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff3ae, L0xbefff3ae <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff3b0, L0xbefff3b0 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff3b2, L0xbefff3b2 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff3b4, L0xbefff3b4 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff3b6, L0xbefff3b6 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff3b8, L0xbefff3b8 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff3ba, L0xbefff3ba <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff3bc, L0xbefff3bc <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff3be, L0xbefff3be <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff3c0, L0xbefff3c0 <=s 9@16 * 1664@16, (-9)@16 * 1664@16 <=s L0xbefff3c2, L0xbefff3c2 <=s 9@16 * 1664@16]
}
*)
