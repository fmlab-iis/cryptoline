(* on frege: -v -slicing -isafety -jobs 24 -no_carry_constraint
Parsing CryptoLine file:                        [OK]            0.1286 seconds
Checking well-formedness:                       [OK]            0.0385 seconds

Procedure main
--------------
Transforming to SSA form:                       [OK]            0.0278 seconds
Normalizing specification:                      [OK]            0.0306 seconds
Rewriting assignments:                          [OK]            0.0551 seconds
Verifying program safety:                       [OK]            67671.0334 seconds
Verifying range assertions:                     [OK]            0.0974 seconds
Verifying range specification:                  [OK]            17475.3663 seconds
Rewriting value-preserved casting:              [OK]            0.0089 seconds
Verifying algebraic assertions:                 [OK]            32.7689 seconds
Verifying algebraic specification:              [OK]            6862.9469 seconds
Procedure verification:                         [OK]            92042.3516 seconds

Summary
-------
Verification result:                            [OK]            92042.5193 seconds
*)

proc main (
  sint32 f000, sint32 f001, sint32 f002, sint32 f003, sint32 f004, sint32 f005, sint32 f006, sint32 f007,
  sint32 f008, sint32 f009, sint32 f010, sint32 f011, sint32 f012, sint32 f013, sint32 f014, sint32 f015,
  sint32 f016, sint32 f017, sint32 f018, sint32 f019, sint32 f020, sint32 f021, sint32 f022, sint32 f023,
  sint32 f024, sint32 f025, sint32 f026, sint32 f027, sint32 f028, sint32 f029, sint32 f030, sint32 f031,
  sint32 f032, sint32 f033, sint32 f034, sint32 f035, sint32 f036, sint32 f037, sint32 f038, sint32 f039,
  sint32 f040, sint32 f041, sint32 f042, sint32 f043, sint32 f044, sint32 f045, sint32 f046, sint32 f047,
  sint32 f048, sint32 f049, sint32 f050, sint32 f051, sint32 f052, sint32 f053, sint32 f054, sint32 f055,
  sint32 f056, sint32 f057, sint32 f058, sint32 f059, sint32 f060, sint32 f061, sint32 f062, sint32 f063,
  sint32 f064, sint32 f065, sint32 f066, sint32 f067, sint32 f068, sint32 f069, sint32 f070, sint32 f071,
  sint32 f072, sint32 f073, sint32 f074, sint32 f075, sint32 f076, sint32 f077, sint32 f078, sint32 f079,
  sint32 f080, sint32 f081, sint32 f082, sint32 f083, sint32 f084, sint32 f085, sint32 f086, sint32 f087,
  sint32 f088, sint32 f089, sint32 f090, sint32 f091, sint32 f092, sint32 f093, sint32 f094, sint32 f095,
  sint32 f096, sint32 f097, sint32 f098, sint32 f099, sint32 f100, sint32 f101, sint32 f102, sint32 f103,
  sint32 f104, sint32 f105, sint32 f106, sint32 f107, sint32 f108, sint32 f109, sint32 f110, sint32 f111,
  sint32 f112, sint32 f113, sint32 f114, sint32 f115, sint32 f116, sint32 f117, sint32 f118, sint32 f119,
  sint32 f120, sint32 f121, sint32 f122, sint32 f123, sint32 f124, sint32 f125, sint32 f126, sint32 f127,
  sint32 f128, sint32 f129, sint32 f130, sint32 f131, sint32 f132, sint32 f133, sint32 f134, sint32 f135,
  sint32 f136, sint32 f137, sint32 f138, sint32 f139, sint32 f140, sint32 f141, sint32 f142, sint32 f143,
  sint32 f144, sint32 f145, sint32 f146, sint32 f147, sint32 f148, sint32 f149, sint32 f150, sint32 f151,
  sint32 f152, sint32 f153, sint32 f154, sint32 f155, sint32 f156, sint32 f157, sint32 f158, sint32 f159,
  sint32 f160, sint32 f161, sint32 f162, sint32 f163, sint32 f164, sint32 f165, sint32 f166, sint32 f167,
  sint32 f168, sint32 f169, sint32 f170, sint32 f171, sint32 f172, sint32 f173, sint32 f174, sint32 f175,
  sint32 f176, sint32 f177, sint32 f178, sint32 f179, sint32 f180, sint32 f181, sint32 f182, sint32 f183,
  sint32 f184, sint32 f185, sint32 f186, sint32 f187, sint32 f188, sint32 f189, sint32 f190, sint32 f191,
  sint32 f192, sint32 f193, sint32 f194, sint32 f195, sint32 f196, sint32 f197, sint32 f198, sint32 f199,
  sint32 f200, sint32 f201, sint32 f202, sint32 f203, sint32 f204, sint32 f205, sint32 f206, sint32 f207,
  sint32 f208, sint32 f209, sint32 f210, sint32 f211, sint32 f212, sint32 f213, sint32 f214, sint32 f215,
  sint32 f216, sint32 f217, sint32 f218, sint32 f219, sint32 f220, sint32 f221, sint32 f222, sint32 f223,
  sint32 f224, sint32 f225, sint32 f226, sint32 f227, sint32 f228, sint32 f229, sint32 f230, sint32 f231,
  sint32 f232, sint32 f233, sint32 f234, sint32 f235, sint32 f236, sint32 f237, sint32 f238, sint32 f239,
  sint32 f240, sint32 f241, sint32 f242, sint32 f243, sint32 f244, sint32 f245, sint32 f246, sint32 f247,
  sint32 f248, sint32 f249, sint32 f250, sint32 f251, sint32 f252, sint32 f253, sint32 f254, sint32 f255
) =
{
  true
  &&&
  and [
    (-8380417)@32 <s f000, f000 <s (8380417)@32, (-8380417)@32 <s f001, f001 <s (8380417)@32, (-8380417)@32 <s f002, f002 <s (8380417)@32, (-8380417)@32 <s f003, f003 <s (8380417)@32,
    (-8380417)@32 <s f004, f004 <s (8380417)@32, (-8380417)@32 <s f005, f005 <s (8380417)@32, (-8380417)@32 <s f006, f006 <s (8380417)@32, (-8380417)@32 <s f007, f007 <s (8380417)@32,
    (-8380417)@32 <s f008, f008 <s (8380417)@32, (-8380417)@32 <s f009, f009 <s (8380417)@32, (-8380417)@32 <s f010, f010 <s (8380417)@32, (-8380417)@32 <s f011, f011 <s (8380417)@32,
    (-8380417)@32 <s f012, f012 <s (8380417)@32, (-8380417)@32 <s f013, f013 <s (8380417)@32, (-8380417)@32 <s f014, f014 <s (8380417)@32, (-8380417)@32 <s f015, f015 <s (8380417)@32,
    (-8380417)@32 <s f016, f016 <s (8380417)@32, (-8380417)@32 <s f017, f017 <s (8380417)@32, (-8380417)@32 <s f018, f018 <s (8380417)@32, (-8380417)@32 <s f019, f019 <s (8380417)@32,
    (-8380417)@32 <s f020, f020 <s (8380417)@32, (-8380417)@32 <s f021, f021 <s (8380417)@32, (-8380417)@32 <s f022, f022 <s (8380417)@32, (-8380417)@32 <s f023, f023 <s (8380417)@32,
    (-8380417)@32 <s f024, f024 <s (8380417)@32, (-8380417)@32 <s f025, f025 <s (8380417)@32, (-8380417)@32 <s f026, f026 <s (8380417)@32, (-8380417)@32 <s f027, f027 <s (8380417)@32,
    (-8380417)@32 <s f028, f028 <s (8380417)@32, (-8380417)@32 <s f029, f029 <s (8380417)@32, (-8380417)@32 <s f030, f030 <s (8380417)@32, (-8380417)@32 <s f031, f031 <s (8380417)@32,
    (-8380417)@32 <s f032, f032 <s (8380417)@32, (-8380417)@32 <s f033, f033 <s (8380417)@32, (-8380417)@32 <s f034, f034 <s (8380417)@32, (-8380417)@32 <s f035, f035 <s (8380417)@32,
    (-8380417)@32 <s f036, f036 <s (8380417)@32, (-8380417)@32 <s f037, f037 <s (8380417)@32, (-8380417)@32 <s f038, f038 <s (8380417)@32, (-8380417)@32 <s f039, f039 <s (8380417)@32,
    (-8380417)@32 <s f040, f040 <s (8380417)@32, (-8380417)@32 <s f041, f041 <s (8380417)@32, (-8380417)@32 <s f042, f042 <s (8380417)@32, (-8380417)@32 <s f043, f043 <s (8380417)@32,
    (-8380417)@32 <s f044, f044 <s (8380417)@32, (-8380417)@32 <s f045, f045 <s (8380417)@32, (-8380417)@32 <s f046, f046 <s (8380417)@32, (-8380417)@32 <s f047, f047 <s (8380417)@32,
    (-8380417)@32 <s f048, f048 <s (8380417)@32, (-8380417)@32 <s f049, f049 <s (8380417)@32, (-8380417)@32 <s f050, f050 <s (8380417)@32, (-8380417)@32 <s f051, f051 <s (8380417)@32,
    (-8380417)@32 <s f052, f052 <s (8380417)@32, (-8380417)@32 <s f053, f053 <s (8380417)@32, (-8380417)@32 <s f054, f054 <s (8380417)@32, (-8380417)@32 <s f055, f055 <s (8380417)@32,
    (-8380417)@32 <s f056, f056 <s (8380417)@32, (-8380417)@32 <s f057, f057 <s (8380417)@32, (-8380417)@32 <s f058, f058 <s (8380417)@32, (-8380417)@32 <s f059, f059 <s (8380417)@32,
    (-8380417)@32 <s f060, f060 <s (8380417)@32, (-8380417)@32 <s f061, f061 <s (8380417)@32, (-8380417)@32 <s f062, f062 <s (8380417)@32, (-8380417)@32 <s f063, f063 <s (8380417)@32,
    (-8380417)@32 <s f064, f064 <s (8380417)@32, (-8380417)@32 <s f065, f065 <s (8380417)@32, (-8380417)@32 <s f066, f066 <s (8380417)@32, (-8380417)@32 <s f067, f067 <s (8380417)@32,
    (-8380417)@32 <s f068, f068 <s (8380417)@32, (-8380417)@32 <s f069, f069 <s (8380417)@32, (-8380417)@32 <s f070, f070 <s (8380417)@32, (-8380417)@32 <s f071, f071 <s (8380417)@32,
    (-8380417)@32 <s f072, f072 <s (8380417)@32, (-8380417)@32 <s f073, f073 <s (8380417)@32, (-8380417)@32 <s f074, f074 <s (8380417)@32, (-8380417)@32 <s f075, f075 <s (8380417)@32,
    (-8380417)@32 <s f076, f076 <s (8380417)@32, (-8380417)@32 <s f077, f077 <s (8380417)@32, (-8380417)@32 <s f078, f078 <s (8380417)@32, (-8380417)@32 <s f079, f079 <s (8380417)@32,
    (-8380417)@32 <s f080, f080 <s (8380417)@32, (-8380417)@32 <s f081, f081 <s (8380417)@32, (-8380417)@32 <s f082, f082 <s (8380417)@32, (-8380417)@32 <s f083, f083 <s (8380417)@32,
    (-8380417)@32 <s f084, f084 <s (8380417)@32, (-8380417)@32 <s f085, f085 <s (8380417)@32, (-8380417)@32 <s f086, f086 <s (8380417)@32, (-8380417)@32 <s f087, f087 <s (8380417)@32,
    (-8380417)@32 <s f088, f088 <s (8380417)@32, (-8380417)@32 <s f089, f089 <s (8380417)@32, (-8380417)@32 <s f090, f090 <s (8380417)@32, (-8380417)@32 <s f091, f091 <s (8380417)@32,
    (-8380417)@32 <s f092, f092 <s (8380417)@32, (-8380417)@32 <s f093, f093 <s (8380417)@32, (-8380417)@32 <s f094, f094 <s (8380417)@32, (-8380417)@32 <s f095, f095 <s (8380417)@32,
    (-8380417)@32 <s f096, f096 <s (8380417)@32, (-8380417)@32 <s f097, f097 <s (8380417)@32, (-8380417)@32 <s f098, f098 <s (8380417)@32, (-8380417)@32 <s f099, f099 <s (8380417)@32,
    (-8380417)@32 <s f100, f100 <s (8380417)@32, (-8380417)@32 <s f101, f101 <s (8380417)@32, (-8380417)@32 <s f102, f102 <s (8380417)@32, (-8380417)@32 <s f103, f103 <s (8380417)@32,
    (-8380417)@32 <s f104, f104 <s (8380417)@32, (-8380417)@32 <s f105, f105 <s (8380417)@32, (-8380417)@32 <s f106, f106 <s (8380417)@32, (-8380417)@32 <s f107, f107 <s (8380417)@32,
    (-8380417)@32 <s f108, f108 <s (8380417)@32, (-8380417)@32 <s f109, f109 <s (8380417)@32, (-8380417)@32 <s f110, f110 <s (8380417)@32, (-8380417)@32 <s f111, f111 <s (8380417)@32,
    (-8380417)@32 <s f112, f112 <s (8380417)@32, (-8380417)@32 <s f113, f113 <s (8380417)@32, (-8380417)@32 <s f114, f114 <s (8380417)@32, (-8380417)@32 <s f115, f115 <s (8380417)@32,
    (-8380417)@32 <s f116, f116 <s (8380417)@32, (-8380417)@32 <s f117, f117 <s (8380417)@32, (-8380417)@32 <s f118, f118 <s (8380417)@32, (-8380417)@32 <s f119, f119 <s (8380417)@32,
    (-8380417)@32 <s f120, f120 <s (8380417)@32, (-8380417)@32 <s f121, f121 <s (8380417)@32, (-8380417)@32 <s f122, f122 <s (8380417)@32, (-8380417)@32 <s f123, f123 <s (8380417)@32,
    (-8380417)@32 <s f124, f124 <s (8380417)@32, (-8380417)@32 <s f125, f125 <s (8380417)@32, (-8380417)@32 <s f126, f126 <s (8380417)@32, (-8380417)@32 <s f127, f127 <s (8380417)@32,
    (-8380417)@32 <s f128, f128 <s (8380417)@32, (-8380417)@32 <s f129, f129 <s (8380417)@32, (-8380417)@32 <s f130, f130 <s (8380417)@32, (-8380417)@32 <s f131, f131 <s (8380417)@32,
    (-8380417)@32 <s f132, f132 <s (8380417)@32, (-8380417)@32 <s f133, f133 <s (8380417)@32, (-8380417)@32 <s f134, f134 <s (8380417)@32, (-8380417)@32 <s f135, f135 <s (8380417)@32,
    (-8380417)@32 <s f136, f136 <s (8380417)@32, (-8380417)@32 <s f137, f137 <s (8380417)@32, (-8380417)@32 <s f138, f138 <s (8380417)@32, (-8380417)@32 <s f139, f139 <s (8380417)@32,
    (-8380417)@32 <s f140, f140 <s (8380417)@32, (-8380417)@32 <s f141, f141 <s (8380417)@32, (-8380417)@32 <s f142, f142 <s (8380417)@32, (-8380417)@32 <s f143, f143 <s (8380417)@32,
    (-8380417)@32 <s f144, f144 <s (8380417)@32, (-8380417)@32 <s f145, f145 <s (8380417)@32, (-8380417)@32 <s f146, f146 <s (8380417)@32, (-8380417)@32 <s f147, f147 <s (8380417)@32,
    (-8380417)@32 <s f148, f148 <s (8380417)@32, (-8380417)@32 <s f149, f149 <s (8380417)@32, (-8380417)@32 <s f150, f150 <s (8380417)@32, (-8380417)@32 <s f151, f151 <s (8380417)@32,
    (-8380417)@32 <s f152, f152 <s (8380417)@32, (-8380417)@32 <s f153, f153 <s (8380417)@32, (-8380417)@32 <s f154, f154 <s (8380417)@32, (-8380417)@32 <s f155, f155 <s (8380417)@32,
    (-8380417)@32 <s f156, f156 <s (8380417)@32, (-8380417)@32 <s f157, f157 <s (8380417)@32, (-8380417)@32 <s f158, f158 <s (8380417)@32, (-8380417)@32 <s f159, f159 <s (8380417)@32,
    (-8380417)@32 <s f160, f160 <s (8380417)@32, (-8380417)@32 <s f161, f161 <s (8380417)@32, (-8380417)@32 <s f162, f162 <s (8380417)@32, (-8380417)@32 <s f163, f163 <s (8380417)@32,
    (-8380417)@32 <s f164, f164 <s (8380417)@32, (-8380417)@32 <s f165, f165 <s (8380417)@32, (-8380417)@32 <s f166, f166 <s (8380417)@32, (-8380417)@32 <s f167, f167 <s (8380417)@32,
    (-8380417)@32 <s f168, f168 <s (8380417)@32, (-8380417)@32 <s f169, f169 <s (8380417)@32, (-8380417)@32 <s f170, f170 <s (8380417)@32, (-8380417)@32 <s f171, f171 <s (8380417)@32,
    (-8380417)@32 <s f172, f172 <s (8380417)@32, (-8380417)@32 <s f173, f173 <s (8380417)@32, (-8380417)@32 <s f174, f174 <s (8380417)@32, (-8380417)@32 <s f175, f175 <s (8380417)@32,
    (-8380417)@32 <s f176, f176 <s (8380417)@32, (-8380417)@32 <s f177, f177 <s (8380417)@32, (-8380417)@32 <s f178, f178 <s (8380417)@32, (-8380417)@32 <s f179, f179 <s (8380417)@32,
    (-8380417)@32 <s f180, f180 <s (8380417)@32, (-8380417)@32 <s f181, f181 <s (8380417)@32, (-8380417)@32 <s f182, f182 <s (8380417)@32, (-8380417)@32 <s f183, f183 <s (8380417)@32,
    (-8380417)@32 <s f184, f184 <s (8380417)@32, (-8380417)@32 <s f185, f185 <s (8380417)@32, (-8380417)@32 <s f186, f186 <s (8380417)@32, (-8380417)@32 <s f187, f187 <s (8380417)@32,
    (-8380417)@32 <s f188, f188 <s (8380417)@32, (-8380417)@32 <s f189, f189 <s (8380417)@32, (-8380417)@32 <s f190, f190 <s (8380417)@32, (-8380417)@32 <s f191, f191 <s (8380417)@32,
    (-8380417)@32 <s f192, f192 <s (8380417)@32, (-8380417)@32 <s f193, f193 <s (8380417)@32, (-8380417)@32 <s f194, f194 <s (8380417)@32, (-8380417)@32 <s f195, f195 <s (8380417)@32,
    (-8380417)@32 <s f196, f196 <s (8380417)@32, (-8380417)@32 <s f197, f197 <s (8380417)@32, (-8380417)@32 <s f198, f198 <s (8380417)@32, (-8380417)@32 <s f199, f199 <s (8380417)@32,
    (-8380417)@32 <s f200, f200 <s (8380417)@32, (-8380417)@32 <s f201, f201 <s (8380417)@32, (-8380417)@32 <s f202, f202 <s (8380417)@32, (-8380417)@32 <s f203, f203 <s (8380417)@32,
    (-8380417)@32 <s f204, f204 <s (8380417)@32, (-8380417)@32 <s f205, f205 <s (8380417)@32, (-8380417)@32 <s f206, f206 <s (8380417)@32, (-8380417)@32 <s f207, f207 <s (8380417)@32,
    (-8380417)@32 <s f208, f208 <s (8380417)@32, (-8380417)@32 <s f209, f209 <s (8380417)@32, (-8380417)@32 <s f210, f210 <s (8380417)@32, (-8380417)@32 <s f211, f211 <s (8380417)@32,
    (-8380417)@32 <s f212, f212 <s (8380417)@32, (-8380417)@32 <s f213, f213 <s (8380417)@32, (-8380417)@32 <s f214, f214 <s (8380417)@32, (-8380417)@32 <s f215, f215 <s (8380417)@32,
    (-8380417)@32 <s f216, f216 <s (8380417)@32, (-8380417)@32 <s f217, f217 <s (8380417)@32, (-8380417)@32 <s f218, f218 <s (8380417)@32, (-8380417)@32 <s f219, f219 <s (8380417)@32,
    (-8380417)@32 <s f220, f220 <s (8380417)@32, (-8380417)@32 <s f221, f221 <s (8380417)@32, (-8380417)@32 <s f222, f222 <s (8380417)@32, (-8380417)@32 <s f223, f223 <s (8380417)@32,
    (-8380417)@32 <s f224, f224 <s (8380417)@32, (-8380417)@32 <s f225, f225 <s (8380417)@32, (-8380417)@32 <s f226, f226 <s (8380417)@32, (-8380417)@32 <s f227, f227 <s (8380417)@32,
    (-8380417)@32 <s f228, f228 <s (8380417)@32, (-8380417)@32 <s f229, f229 <s (8380417)@32, (-8380417)@32 <s f230, f230 <s (8380417)@32, (-8380417)@32 <s f231, f231 <s (8380417)@32,
    (-8380417)@32 <s f232, f232 <s (8380417)@32, (-8380417)@32 <s f233, f233 <s (8380417)@32, (-8380417)@32 <s f234, f234 <s (8380417)@32, (-8380417)@32 <s f235, f235 <s (8380417)@32,
    (-8380417)@32 <s f236, f236 <s (8380417)@32, (-8380417)@32 <s f237, f237 <s (8380417)@32, (-8380417)@32 <s f238, f238 <s (8380417)@32, (-8380417)@32 <s f239, f239 <s (8380417)@32,
    (-8380417)@32 <s f240, f240 <s (8380417)@32, (-8380417)@32 <s f241, f241 <s (8380417)@32, (-8380417)@32 <s f242, f242 <s (8380417)@32, (-8380417)@32 <s f243, f243 <s (8380417)@32,
    (-8380417)@32 <s f244, f244 <s (8380417)@32, (-8380417)@32 <s f245, f245 <s (8380417)@32, (-8380417)@32 <s f246, f246 <s (8380417)@32, (-8380417)@32 <s f247, f247 <s (8380417)@32,
    (-8380417)@32 <s f248, f248 <s (8380417)@32, (-8380417)@32 <s f249, f249 <s (8380417)@32, (-8380417)@32 <s f250, f250 <s (8380417)@32, (-8380417)@32 <s f251, f251 <s (8380417)@32,
    (-8380417)@32 <s f252, f252 <s (8380417)@32, (-8380417)@32 <s f253, f253 <s (8380417)@32, (-8380417)@32 <s f254, f254 <s (8380417)@32, (-8380417)@32 <s f255, f255 <s (8380417)@32
  ]
}
(* Q *)
mov L0x555555579de0 8380417@sint32; mov L0x555555579de4 8380417@sint32; mov L0x555555579de8 8380417@sint32; mov L0x555555579dec 8380417@sint32;
mov L0x555555579df0 8380417@sint32; mov L0x555555579df4 8380417@sint32; mov L0x555555579df8 8380417@sint32; mov L0x555555579dfc 8380417@sint32;
(* ZETAS_QINV *)
mov L0x555555579e60 ( -151046689)@sint32; mov L0x555555579e64 ( 1830765815)@sint32; mov L0x555555579e68 (-1929875198)@sint32; mov L0x555555579e6c (-1927777021)@sint32;
mov L0x555555579e70 ( 1640767044)@sint32; mov L0x555555579e74 ( 1477910808)@sint32; mov L0x555555579e78 ( 1612161320)@sint32; mov L0x555555579e7c ( 1640734244)@sint32;
mov L0x555555579e80 (  308362795)@sint32; mov L0x555555579e84 (  308362795)@sint32; mov L0x555555579e88 (  308362795)@sint32; mov L0x555555579e8c (  308362795)@sint32;
mov L0x555555579e90 (-1815525077)@sint32; mov L0x555555579e94 (-1815525077)@sint32; mov L0x555555579e98 (-1815525077)@sint32; mov L0x555555579e9c (-1815525077)@sint32;
mov L0x555555579ea0 (-1374673747)@sint32; mov L0x555555579ea4 (-1374673747)@sint32; mov L0x555555579ea8 (-1374673747)@sint32; mov L0x555555579eac (-1374673747)@sint32;
mov L0x555555579eb0 (-1091570561)@sint32; mov L0x555555579eb4 (-1091570561)@sint32; mov L0x555555579eb8 (-1091570561)@sint32; mov L0x555555579ebc (-1091570561)@sint32;
mov L0x555555579ec0 (-1929495947)@sint32; mov L0x555555579ec4 (-1929495947)@sint32; mov L0x555555579ec8 (-1929495947)@sint32; mov L0x555555579ecc (-1929495947)@sint32;
mov L0x555555579ed0 (  515185417)@sint32; mov L0x555555579ed4 (  515185417)@sint32; mov L0x555555579ed8 (  515185417)@sint32; mov L0x555555579edc (  515185417)@sint32;
mov L0x555555579ee0 ( -285697463)@sint32; mov L0x555555579ee4 ( -285697463)@sint32; mov L0x555555579ee8 ( -285697463)@sint32; mov L0x555555579eec ( -285697463)@sint32;
mov L0x555555579ef0 (  625853735)@sint32; mov L0x555555579ef4 (  625853735)@sint32; mov L0x555555579ef8 (  625853735)@sint32; mov L0x555555579efc (  625853735)@sint32;
mov L0x555555579f00 ( 1727305304)@sint32; mov L0x555555579f04 ( 1727305304)@sint32; mov L0x555555579f08 ( 2082316400)@sint32; mov L0x555555579f0c ( 2082316400)@sint32;
mov L0x555555579f10 (-1364982364)@sint32; mov L0x555555579f14 (-1364982364)@sint32; mov L0x555555579f18 (  858240904)@sint32; mov L0x555555579f1c (  858240904)@sint32;
mov L0x555555579f20 ( 1806278032)@sint32; mov L0x555555579f24 ( 1806278032)@sint32; mov L0x555555579f28 (  222489248)@sint32; mov L0x555555579f2c (  222489248)@sint32;
mov L0x555555579f30 ( -346752664)@sint32; mov L0x555555579f34 ( -346752664)@sint32; mov L0x555555579f38 (  684667771)@sint32; mov L0x555555579f3c (  684667771)@sint32;
mov L0x555555579f40 ( 1654287830)@sint32; mov L0x555555579f44 ( 1654287830)@sint32; mov L0x555555579f48 ( -878576921)@sint32; mov L0x555555579f4c ( -878576921)@sint32;
mov L0x555555579f50 (-1257667337)@sint32; mov L0x555555579f54 (-1257667337)@sint32; mov L0x555555579f58 ( -748618600)@sint32; mov L0x555555579f5c ( -748618600)@sint32;
mov L0x555555579f60 (  329347125)@sint32; mov L0x555555579f64 (  329347125)@sint32; mov L0x555555579f68 ( 1837364258)@sint32; mov L0x555555579f6c ( 1837364258)@sint32;
mov L0x555555579f70 (-1443016191)@sint32; mov L0x555555579f74 (-1443016191)@sint32; mov L0x555555579f78 (-1170414139)@sint32; mov L0x555555579f7c (-1170414139)@sint32;
mov L0x555555579f80 (-1846138265)@sint32; mov L0x555555579f84 (-1631226336)@sint32; mov L0x555555579f88 (-1404529459)@sint32; mov L0x555555579f8c ( 1838055109)@sint32;
mov L0x555555579f90 ( 1594295555)@sint32; mov L0x555555579f94 (-1076973524)@sint32; mov L0x555555579f98 (-1898723372)@sint32; mov L0x555555579f9c ( -594436433)@sint32;
mov L0x555555579fa0 ( -202001019)@sint32; mov L0x555555579fa4 ( -475984260)@sint32; mov L0x555555579fa8 ( -561427818)@sint32; mov L0x555555579fac ( 1797021249)@sint32;
mov L0x555555579fb0 (-1061813248)@sint32; mov L0x555555579fb4 ( 2059733581)@sint32; mov L0x555555579fb8 (-1661512036)@sint32; mov L0x555555579fbc (-1104976547)@sint32;
mov L0x555555579fc0 (-1750224323)@sint32; mov L0x555555579fc4 ( -901666090)@sint32; mov L0x555555579fc8 (  418987550)@sint32; mov L0x555555579fcc ( 1831915353)@sint32;
mov L0x555555579fd0 (-1925356481)@sint32; mov L0x555555579fd4 (  992097815)@sint32; mov L0x555555579fd8 (  879957084)@sint32; mov L0x555555579fdc ( 2024403852)@sint32;
mov L0x555555579fe0 ( 1484874664)@sint32; mov L0x555555579fe4 (-1636082790)@sint32; mov L0x555555579fe8 ( -285388938)@sint32; mov L0x555555579fec (-1983539117)@sint32;
mov L0x555555579ff0 (-1495136972)@sint32; mov L0x555555579ff4 ( -950076368)@sint32; mov L0x555555579ff8 (-1714807468)@sint32; mov L0x555555579ffc ( -952438995)@sint32;
mov L0x55555557a000 (-1574918427)@sint32; mov L0x55555557a004 ( 1350681039)@sint32; mov L0x55555557a008 (-2143979939)@sint32; mov L0x55555557a00c ( 1599739335)@sint32;
mov L0x55555557a010 (-1285853323)@sint32; mov L0x55555557a014 ( -993005454)@sint32; mov L0x55555557a018 (-1440787840)@sint32; mov L0x55555557a01c (  568627424)@sint32;
mov L0x55555557a020 ( -783134478)@sint32; mov L0x55555557a024 ( -588790216)@sint32; mov L0x55555557a028 (  289871779)@sint32; mov L0x55555557a02c (-1262003603)@sint32;
mov L0x55555557a030 ( 2135294594)@sint32; mov L0x55555557a034 (-1018755525)@sint32; mov L0x55555557a038 ( -889861155)@sint32; mov L0x55555557a03c ( 1665705315)@sint32;
mov L0x55555557a040 ( 1321868265)@sint32; mov L0x55555557a044 ( 1225434135)@sint32; mov L0x55555557a048 (-1784632064)@sint32; mov L0x55555557a04c (  666258756)@sint32;
mov L0x55555557a050 (  675310538)@sint32; mov L0x55555557a054 (-1555941048)@sint32; mov L0x55555557a058 (-1999506068)@sint32; mov L0x55555557a05c (-1499481951)@sint32;
mov L0x55555557a060 ( -695180180)@sint32; mov L0x55555557a064 (-1375177022)@sint32; mov L0x55555557a068 ( 1777179795)@sint32; mov L0x55555557a06c (  334803717)@sint32;
mov L0x55555557a070 ( -178766299)@sint32; mov L0x55555557a074 ( -518252220)@sint32; mov L0x55555557a078 ( 1957047970)@sint32; mov L0x55555557a07c ( 1146323031)@sint32;
mov L0x55555557a080 ( -654783359)@sint32; mov L0x55555557a084 (-1974159335)@sint32; mov L0x55555557a088 ( 1651689966)@sint32; mov L0x55555557a08c (  140455867)@sint32;
mov L0x55555557a090 (-1039411342)@sint32; mov L0x55555557a094 ( 1955560694)@sint32; mov L0x55555557a098 ( 1529189038)@sint32; mov L0x55555557a09c (-2131021878)@sint32;
mov L0x55555557a0a0 ( -247357819)@sint32; mov L0x55555557a0a4 ( 1518161567)@sint32; mov L0x55555557a0a8 (  -86965173)@sint32; mov L0x55555557a0ac ( 1708872713)@sint32;
mov L0x55555557a0b0 ( 1787797779)@sint32; mov L0x55555557a0b4 ( 1638590967)@sint32; mov L0x55555557a0b8 ( -120646188)@sint32; mov L0x55555557a0bc (-1669960606)@sint32;
mov L0x55555557a0c0 ( -916321552)@sint32; mov L0x55555557a0c4 ( 1155548552)@sint32; mov L0x55555557a0c8 ( 2143745726)@sint32; mov L0x55555557a0cc ( 1210558298)@sint32;
mov L0x55555557a0d0 (-1261461890)@sint32; mov L0x55555557a0d4 ( -318346816)@sint32; mov L0x55555557a0d8 (  628664287)@sint32; mov L0x55555557a0dc (-1729304568)@sint32;
mov L0x55555557a0e0 ( 1422575624)@sint32; mov L0x55555557a0e4 ( 1424130038)@sint32; mov L0x55555557a0e8 (-1185330464)@sint32; mov L0x55555557a0ec (  235321234)@sint32;
mov L0x55555557a0f0 (  168022240)@sint32; mov L0x55555557a0f4 ( 1206536194)@sint32; mov L0x55555557a0f8 (  985155484)@sint32; mov L0x55555557a0fc ( -894060583)@sint32;
mov L0x55555557a100 (    -898413)@sint32; mov L0x55555557a104 (-1363460238)@sint32; mov L0x55555557a108 ( -605900043)@sint32; mov L0x55555557a10c ( 2027833504)@sint32;
mov L0x55555557a110 (   14253662)@sint32; mov L0x55555557a114 ( 1014493059)@sint32; mov L0x55555557a118 (  863641633)@sint32; mov L0x55555557a11c ( 1819892093)@sint32;
mov L0x55555557a120 ( 2124962073)@sint32; mov L0x55555557a124 (-1223601433)@sint32; mov L0x55555557a128 (-1920467227)@sint32; mov L0x55555557a12c (-1637785316)@sint32;
mov L0x55555557a130 (-1536588520)@sint32; mov L0x55555557a134 (  694382729)@sint32; mov L0x55555557a138 (  235104446)@sint32; mov L0x55555557a13c (-1045062172)@sint32;
mov L0x55555557a140 (  831969619)@sint32; mov L0x55555557a144 ( -300448763)@sint32; mov L0x55555557a148 (  756955444)@sint32; mov L0x55555557a14c ( -260312805)@sint32;
mov L0x55555557a150 ( 1554794072)@sint32; mov L0x55555557a154 ( 1339088280)@sint32; mov L0x55555557a158 (-2040058690)@sint32; mov L0x55555557a15c ( -853476187)@sint32;
mov L0x55555557a160 (-2047270596)@sint32; mov L0x55555557a164 (-1723816713)@sint32; mov L0x55555557a168 (-1591599803)@sint32; mov L0x55555557a16c ( -440824168)@sint32;
mov L0x55555557a170 ( 1119856484)@sint32; mov L0x55555557a174 ( 1544891539)@sint32; mov L0x55555557a178 (  155290192)@sint32; mov L0x55555557a17c ( -973777462)@sint32;
mov L0x55555557a180 (  991903578)@sint32; mov L0x55555557a184 (  912367099)@sint32; mov L0x55555557a188 (  -44694137)@sint32; mov L0x55555557a18c ( 1176904444)@sint32;
mov L0x55555557a190 ( -421552614)@sint32; mov L0x55555557a194 ( -818371958)@sint32; mov L0x55555557a198 ( 1747917558)@sint32; mov L0x55555557a19c ( -325927722)@sint32;
mov L0x55555557a1a0 (  908452108)@sint32; mov L0x55555557a1a4 ( 1851023419)@sint32; mov L0x55555557a1a8 (-1176751719)@sint32; mov L0x55555557a1ac (-1354528380)@sint32;
mov L0x55555557a1b0 (  -72690498)@sint32; mov L0x55555557a1b4 ( -314284737)@sint32; mov L0x55555557a1b8 (  985022747)@sint32; mov L0x55555557a1bc (  963438279)@sint32;
mov L0x55555557a1c0 (-1078959975)@sint32; mov L0x55555557a1c4 (  604552167)@sint32; mov L0x55555557a1c8 (-1021949428)@sint32; mov L0x55555557a1cc (  608791570)@sint32;
mov L0x55555557a1d0 (  173440395)@sint32; mov L0x55555557a1d4 (-2126092136)@sint32; mov L0x55555557a1d8 (-1316619236)@sint32; mov L0x55555557a1dc (-1039370342)@sint32;
mov L0x55555557a1e0 (    6087993)@sint32; mov L0x55555557a1e4 ( -110126092)@sint32; mov L0x55555557a1e8 (  565464272)@sint32; mov L0x55555557a1ec (-1758099917)@sint32;
mov L0x55555557a1f0 (-1600929361)@sint32; mov L0x55555557a1f4 (  879867909)@sint32; mov L0x55555557a1f8 (-1809756372)@sint32; mov L0x55555557a1fc (  400711272)@sint32;
mov L0x55555557a200 ( 1363007700)@sint32; mov L0x55555557a204 (   30313375)@sint32; mov L0x55555557a208 ( -326425360)@sint32; mov L0x55555557a20c ( 1683520342)@sint32;
mov L0x55555557a210 ( -517299994)@sint32; mov L0x55555557a214 ( 2027935492)@sint32; mov L0x55555557a218 (-1372618620)@sint32; mov L0x55555557a21c (  128353682)@sint32;
mov L0x55555557a220 (-1123881663)@sint32; mov L0x55555557a224 (  137583815)@sint32; mov L0x55555557a228 ( -635454918)@sint32; mov L0x55555557a22c ( -642772911)@sint32;
mov L0x55555557a230 (   45766801)@sint32; mov L0x55555557a234 (  671509323)@sint32; mov L0x55555557a238 (-2070602178)@sint32; mov L0x55555557a23c (  419615363)@sint32;
mov L0x55555557a240 ( 1216882040)@sint32; mov L0x55555557a244 ( -270590488)@sint32; mov L0x55555557a248 (-1276805128)@sint32; mov L0x55555557a24c (  371462360)@sint32;
mov L0x55555557a250 (-1357098057)@sint32; mov L0x55555557a254 ( -384158533)@sint32; mov L0x55555557a258 (  827959816)@sint32; mov L0x55555557a25c ( -596344473)@sint32;
mov L0x55555557a260 (  702390549)@sint32; mov L0x55555557a264 ( -279505433)@sint32; mov L0x55555557a268 ( -260424530)@sint32; mov L0x55555557a26c (  -71875110)@sint32;
mov L0x55555557a270 (-1208667171)@sint32; mov L0x55555557a274 (-1499603926)@sint32; mov L0x55555557a278 ( 2036925262)@sint32; mov L0x55555557a27c ( -540420426)@sint32;
mov L0x55555557a280 (  746144248)@sint32; mov L0x55555557a284 (-1420958686)@sint32; mov L0x55555557a288 ( 2032221021)@sint32; mov L0x55555557a28c ( 1904936414)@sint32;
mov L0x55555557a290 ( 1257750362)@sint32; mov L0x55555557a294 ( 1926727420)@sint32; mov L0x55555557a298 ( 1931587462)@sint32; mov L0x55555557a29c ( 1258381762)@sint32;
mov L0x55555557a2a0 (  885133339)@sint32; mov L0x55555557a2a4 ( 1629985060)@sint32; mov L0x55555557a2a8 ( 1967222129)@sint32; mov L0x55555557a2ac (    6363718)@sint32;
mov L0x55555557a2b0 (-1287922800)@sint32; mov L0x55555557a2b4 ( 1136965286)@sint32; mov L0x55555557a2b8 ( 1779436847)@sint32; mov L0x55555557a2bc ( 1116720494)@sint32;
mov L0x55555557a2c0 ( 1042326957)@sint32; mov L0x55555557a2c4 ( 1405999311)@sint32; mov L0x55555557a2c8 (  713994583)@sint32; mov L0x55555557a2cc (  940195359)@sint32;
mov L0x55555557a2d0 (-1542497137)@sint32; mov L0x55555557a2d4 ( 2061661095)@sint32; mov L0x55555557a2d8 ( -883155599)@sint32; mov L0x55555557a2dc ( 1726753853)@sint32;
mov L0x55555557a2e0 (-1547952704)@sint32; mov L0x55555557a2e4 (  394851342)@sint32; mov L0x55555557a2e8 (  283780712)@sint32; mov L0x55555557a2ec (  776003547)@sint32;
mov L0x55555557a2f0 ( 1123958025)@sint32; mov L0x55555557a2f4 (  201262505)@sint32; mov L0x55555557a2f8 ( 1934038751)@sint32; mov L0x55555557a2fc (  374860238)@sint32;
(* ZETAS *)
mov L0x55555557a300 (-3975713)@sint32; mov L0x55555557a304 (   25847)@sint32; mov L0x55555557a308 (-2608894)@sint32; mov L0x55555557a30c ( -518909)@sint32;
mov L0x55555557a310 (  237124)@sint32; mov L0x55555557a314 ( -777960)@sint32; mov L0x55555557a318 ( -876248)@sint32; mov L0x55555557a31c (  466468)@sint32;
mov L0x55555557a320 ( 1826347)@sint32; mov L0x55555557a324 ( 1826347)@sint32; mov L0x55555557a328 ( 1826347)@sint32; mov L0x55555557a32c ( 1826347)@sint32;
mov L0x55555557a330 ( 2353451)@sint32; mov L0x55555557a334 ( 2353451)@sint32; mov L0x55555557a338 ( 2353451)@sint32; mov L0x55555557a33c ( 2353451)@sint32;
mov L0x55555557a340 ( -359251)@sint32; mov L0x55555557a344 ( -359251)@sint32; mov L0x55555557a348 ( -359251)@sint32; mov L0x55555557a34c ( -359251)@sint32;
mov L0x55555557a350 (-2091905)@sint32; mov L0x55555557a354 (-2091905)@sint32; mov L0x55555557a358 (-2091905)@sint32; mov L0x55555557a35c (-2091905)@sint32;
mov L0x55555557a360 ( 3119733)@sint32; mov L0x55555557a364 ( 3119733)@sint32; mov L0x55555557a368 ( 3119733)@sint32; mov L0x55555557a36c ( 3119733)@sint32;
mov L0x55555557a370 (-2884855)@sint32; mov L0x55555557a374 (-2884855)@sint32; mov L0x55555557a378 (-2884855)@sint32; mov L0x55555557a37c (-2884855)@sint32;
mov L0x55555557a380 ( 3111497)@sint32; mov L0x55555557a384 ( 3111497)@sint32; mov L0x55555557a388 ( 3111497)@sint32; mov L0x55555557a38c ( 3111497)@sint32;
mov L0x55555557a390 ( 2680103)@sint32; mov L0x55555557a394 ( 2680103)@sint32; mov L0x55555557a398 ( 2680103)@sint32; mov L0x55555557a39c ( 2680103)@sint32;
mov L0x55555557a3a0 ( 2725464)@sint32; mov L0x55555557a3a4 ( 2725464)@sint32; mov L0x55555557a3a8 ( 1024112)@sint32; mov L0x55555557a3ac ( 1024112)@sint32;
mov L0x55555557a3b0 (-1079900)@sint32; mov L0x55555557a3b4 (-1079900)@sint32; mov L0x55555557a3b8 ( 3585928)@sint32; mov L0x55555557a3bc ( 3585928)@sint32;
mov L0x55555557a3c0 ( -549488)@sint32; mov L0x55555557a3c4 ( -549488)@sint32; mov L0x55555557a3c8 (-1119584)@sint32; mov L0x55555557a3cc (-1119584)@sint32;
mov L0x55555557a3d0 ( 2619752)@sint32; mov L0x55555557a3d4 ( 2619752)@sint32; mov L0x55555557a3d8 (-2108549)@sint32; mov L0x55555557a3dc (-2108549)@sint32;
mov L0x55555557a3e0 (-2118186)@sint32; mov L0x55555557a3e4 (-2118186)@sint32; mov L0x55555557a3e8 (-3859737)@sint32; mov L0x55555557a3ec (-3859737)@sint32;
mov L0x55555557a3f0 (-1399561)@sint32; mov L0x55555557a3f4 (-1399561)@sint32; mov L0x55555557a3f8 (-3277672)@sint32; mov L0x55555557a3fc (-3277672)@sint32;
mov L0x55555557a400 ( 1757237)@sint32; mov L0x55555557a404 ( 1757237)@sint32; mov L0x55555557a408 (  -19422)@sint32; mov L0x55555557a40c (  -19422)@sint32;
mov L0x55555557a410 ( 4010497)@sint32; mov L0x55555557a414 ( 4010497)@sint32; mov L0x55555557a418 (  280005)@sint32; mov L0x55555557a41c (  280005)@sint32;
mov L0x55555557a420 ( 2706023)@sint32; mov L0x55555557a424 (   95776)@sint32; mov L0x55555557a428 ( 3077325)@sint32; mov L0x55555557a42c ( 3530437)@sint32;
mov L0x55555557a430 (-1661693)@sint32; mov L0x55555557a434 (-3592148)@sint32; mov L0x55555557a438 (-2537516)@sint32; mov L0x55555557a43c ( 3915439)@sint32;
mov L0x55555557a440 (-3861115)@sint32; mov L0x55555557a444 (-3043716)@sint32; mov L0x55555557a448 ( 3574422)@sint32; mov L0x55555557a44c (-2867647)@sint32;
mov L0x55555557a450 ( 3539968)@sint32; mov L0x55555557a454 ( -300467)@sint32; mov L0x55555557a458 ( 2348700)@sint32; mov L0x55555557a45c ( -539299)@sint32;
mov L0x55555557a460 (-1699267)@sint32; mov L0x55555557a464 (-1643818)@sint32; mov L0x55555557a468 ( 3505694)@sint32; mov L0x55555557a46c (-3821735)@sint32;
mov L0x55555557a470 ( 3507263)@sint32; mov L0x55555557a474 (-2140649)@sint32; mov L0x55555557a478 (-1600420)@sint32; mov L0x55555557a47c ( 3699596)@sint32;
mov L0x55555557a480 (  811944)@sint32; mov L0x55555557a484 (  531354)@sint32; mov L0x55555557a488 (  954230)@sint32; mov L0x55555557a48c ( 3881043)@sint32;
mov L0x55555557a490 ( 3900724)@sint32; mov L0x55555557a494 (-2556880)@sint32; mov L0x55555557a498 ( 2071892)@sint32; mov L0x55555557a49c (-2797779)@sint32;
mov L0x55555557a4a0 (-3930395)@sint32; mov L0x55555557a4a4 (-3677745)@sint32; mov L0x55555557a4a8 (-1452451)@sint32; mov L0x55555557a4ac ( 2176455)@sint32;
mov L0x55555557a4b0 (-1257611)@sint32; mov L0x55555557a4b4 (-4083598)@sint32; mov L0x55555557a4b8 (-3190144)@sint32; mov L0x55555557a4bc (-3632928)@sint32;
mov L0x55555557a4c0 ( 3412210)@sint32; mov L0x55555557a4c4 ( 2147896)@sint32; mov L0x55555557a4c8 (-2967645)@sint32; mov L0x55555557a4cc ( -411027)@sint32;
mov L0x55555557a4d0 ( -671102)@sint32; mov L0x55555557a4d4 (  -22981)@sint32; mov L0x55555557a4d8 ( -381987)@sint32; mov L0x55555557a4dc ( 1852771)@sint32;
mov L0x55555557a4e0 (-3343383)@sint32; mov L0x55555557a4e4 (  508951)@sint32; mov L0x55555557a4e8 (   44288)@sint32; mov L0x55555557a4ec (  904516)@sint32;
mov L0x55555557a4f0 (-3724342)@sint32; mov L0x55555557a4f4 ( 1653064)@sint32; mov L0x55555557a4f8 ( 2389356)@sint32; mov L0x55555557a4fc (  759969)@sint32;
mov L0x55555557a500 (  189548)@sint32; mov L0x55555557a504 ( 3159746)@sint32; mov L0x55555557a508 (-2409325)@sint32; mov L0x55555557a50c ( 1315589)@sint32;
mov L0x55555557a510 ( 1285669)@sint32; mov L0x55555557a514 ( -812732)@sint32; mov L0x55555557a518 (-3019102)@sint32; mov L0x55555557a51c (-3628969)@sint32;
mov L0x55555557a520 (-1528703)@sint32; mov L0x55555557a524 (-3041255)@sint32; mov L0x55555557a528 ( 3475950)@sint32; mov L0x55555557a52c (-1585221)@sint32;
mov L0x55555557a530 ( 1939314)@sint32; mov L0x55555557a534 (-1000202)@sint32; mov L0x55555557a538 (-3157330)@sint32; mov L0x55555557a53c (  126922)@sint32;
mov L0x55555557a540 ( -983419)@sint32; mov L0x55555557a544 ( 2715295)@sint32; mov L0x55555557a548 (-3693493)@sint32; mov L0x55555557a54c (-2477047)@sint32;
mov L0x55555557a550 (-1228525)@sint32; mov L0x55555557a554 (-1308169)@sint32; mov L0x55555557a558 ( 1349076)@sint32; mov L0x55555557a55c (-1430430)@sint32;
mov L0x55555557a560 (  264944)@sint32; mov L0x55555557a564 ( 3097992)@sint32; mov L0x55555557a568 (-1100098)@sint32; mov L0x55555557a56c ( 3958618)@sint32;
mov L0x55555557a570 (   -8578)@sint32; mov L0x55555557a574 (-3249728)@sint32; mov L0x55555557a578 ( -210977)@sint32; mov L0x55555557a57c (-1316856)@sint32;
mov L0x55555557a580 (-3553272)@sint32; mov L0x55555557a584 (-1851402)@sint32; mov L0x55555557a588 ( -177440)@sint32; mov L0x55555557a58c ( 1341330)@sint32;
mov L0x55555557a590 (-1584928)@sint32; mov L0x55555557a594 (-1439742)@sint32; mov L0x55555557a598 (-3881060)@sint32; mov L0x55555557a59c ( 3839961)@sint32;
mov L0x55555557a5a0 ( 2091667)@sint32; mov L0x55555557a5a4 (-3342478)@sint32; mov L0x55555557a5a8 (  266997)@sint32; mov L0x55555557a5ac (-3520352)@sint32;
mov L0x55555557a5b0 (  900702)@sint32; mov L0x55555557a5b4 (  495491)@sint32; mov L0x55555557a5b8 ( -655327)@sint32; mov L0x55555557a5bc (-3556995)@sint32;
mov L0x55555557a5c0 (  342297)@sint32; mov L0x55555557a5c4 ( 3437287)@sint32; mov L0x55555557a5c8 ( 2842341)@sint32; mov L0x55555557a5cc ( 4055324)@sint32;
mov L0x55555557a5d0 (-3767016)@sint32; mov L0x55555557a5d4 (-2994039)@sint32; mov L0x55555557a5d8 (-1333058)@sint32; mov L0x55555557a5dc ( -451100)@sint32;
mov L0x55555557a5e0 (-1279661)@sint32; mov L0x55555557a5e4 ( 1500165)@sint32; mov L0x55555557a5e8 ( -542412)@sint32; mov L0x55555557a5ec (-2584293)@sint32;
mov L0x55555557a5f0 (-2013608)@sint32; mov L0x55555557a5f4 ( 1957272)@sint32; mov L0x55555557a5f8 (-3183426)@sint32; mov L0x55555557a5fc (  810149)@sint32;
mov L0x55555557a600 (-3038916)@sint32; mov L0x55555557a604 ( 2213111)@sint32; mov L0x55555557a608 ( -426683)@sint32; mov L0x55555557a60c (-1667432)@sint32;
mov L0x55555557a610 (-2939036)@sint32; mov L0x55555557a614 (  183443)@sint32; mov L0x55555557a618 ( -554416)@sint32; mov L0x55555557a61c ( 3937738)@sint32;
mov L0x55555557a620 ( 3407706)@sint32; mov L0x55555557a624 ( 2244091)@sint32; mov L0x55555557a628 ( 2434439)@sint32; mov L0x55555557a62c (-3759364)@sint32;
mov L0x55555557a630 ( 1859098)@sint32; mov L0x55555557a634 (-1613174)@sint32; mov L0x55555557a638 (-3122442)@sint32; mov L0x55555557a63c ( -525098)@sint32;
mov L0x55555557a640 (  286988)@sint32; mov L0x55555557a644 (-3342277)@sint32; mov L0x55555557a648 ( 2691481)@sint32; mov L0x55555557a64c ( 1247620)@sint32;
mov L0x55555557a650 ( 1250494)@sint32; mov L0x55555557a654 ( 1869119)@sint32; mov L0x55555557a658 ( 1237275)@sint32; mov L0x55555557a65c ( 1312455)@sint32;
mov L0x55555557a660 ( 1917081)@sint32; mov L0x55555557a664 (  777191)@sint32; mov L0x55555557a668 (-2831860)@sint32; mov L0x55555557a66c (-3724270)@sint32;
mov L0x55555557a670 ( 2432395)@sint32; mov L0x55555557a674 ( 3369112)@sint32; mov L0x55555557a678 (  162844)@sint32; mov L0x55555557a67c ( 1652634)@sint32;
mov L0x55555557a680 ( 3523897)@sint32; mov L0x55555557a684 ( -975884)@sint32; mov L0x55555557a688 ( 1723600)@sint32; mov L0x55555557a68c (-1104333)@sint32;
mov L0x55555557a690 (-2235985)@sint32; mov L0x55555557a694 ( -976891)@sint32; mov L0x55555557a698 ( 3919660)@sint32; mov L0x55555557a69c ( 1400424)@sint32;
mov L0x55555557a6a0 ( 2316500)@sint32; mov L0x55555557a6a4 (-2446433)@sint32; mov L0x55555557a6a8 (-1235728)@sint32; mov L0x55555557a6ac (-1197226)@sint32;
mov L0x55555557a6b0 (  909542)@sint32; mov L0x55555557a6b4 (  -43260)@sint32; mov L0x55555557a6b8 ( 2031748)@sint32; mov L0x55555557a6bc ( -768622)@sint32;
mov L0x55555557a6c0 (-2437823)@sint32; mov L0x55555557a6c4 ( 1735879)@sint32; mov L0x55555557a6c8 (-2590150)@sint32; mov L0x55555557a6cc ( 2486353)@sint32;
mov L0x55555557a6d0 ( 2635921)@sint32; mov L0x55555557a6d4 ( 1903435)@sint32; mov L0x55555557a6d8 (-3318210)@sint32; mov L0x55555557a6dc ( 3306115)@sint32;
mov L0x55555557a6e0 (-2546312)@sint32; mov L0x55555557a6e4 ( 2235880)@sint32; mov L0x55555557a6e8 (-1671176)@sint32; mov L0x55555557a6ec (  594136)@sint32;
mov L0x55555557a6f0 ( 2454455)@sint32; mov L0x55555557a6f4 (  185531)@sint32; mov L0x55555557a6f8 ( 1616392)@sint32; mov L0x55555557a6fc (-3694233)@sint32;
mov L0x55555557a700 ( 3866901)@sint32; mov L0x55555557a704 ( 1717735)@sint32; mov L0x55555557a708 (-1803090)@sint32; mov L0x55555557a70c ( -260646)@sint32;
mov L0x55555557a710 ( -420899)@sint32; mov L0x55555557a714 ( 1612842)@sint32; mov L0x55555557a718 (  -48306)@sint32; mov L0x55555557a71c ( -846154)@sint32;
mov L0x55555557a720 ( 3817976)@sint32; mov L0x55555557a724 (-3562462)@sint32; mov L0x55555557a728 ( 3513181)@sint32; mov L0x55555557a72c (-3193378)@sint32;
mov L0x55555557a730 (  819034)@sint32; mov L0x55555557a734 ( -522500)@sint32; mov L0x55555557a738 ( 3207046)@sint32; mov L0x55555557a73c (-3595838)@sint32;
mov L0x55555557a740 ( 4108315)@sint32; mov L0x55555557a744 (  203044)@sint32; mov L0x55555557a748 ( 1265009)@sint32; mov L0x55555557a74c ( 1595974)@sint32;
mov L0x55555557a750 (-3548272)@sint32; mov L0x55555557a754 (-1050970)@sint32; mov L0x55555557a758 (-1430225)@sint32; mov L0x55555557a75c (-1962642)@sint32;
mov L0x55555557a760 (-1374803)@sint32; mov L0x55555557a764 ( 3406031)@sint32; mov L0x55555557a768 (-1846953)@sint32; mov L0x55555557a76c (-3776993)@sint32;
mov L0x55555557a770 ( -164721)@sint32; mov L0x55555557a774 (-1207385)@sint32; mov L0x55555557a778 ( 3014001)@sint32; mov L0x55555557a77c (-1799107)@sint32;
mov L0x55555557a780 (  269760)@sint32; mov L0x55555557a784 (  472078)@sint32; mov L0x55555557a788 ( 1910376)@sint32; mov L0x55555557a78c (-3833893)@sint32;
mov L0x55555557a790 (-2286327)@sint32; mov L0x55555557a794 (-3545687)@sint32; mov L0x55555557a798 (-1362209)@sint32; mov L0x55555557a79c ( 1976782)@sint32;
(* INPUTS *)
mov L0x7fffffff44c0 f000; mov L0x7fffffff44c4 f001; mov L0x7fffffff44c8 f002; mov L0x7fffffff44cc f003;
mov L0x7fffffff44d0 f004; mov L0x7fffffff44d4 f005; mov L0x7fffffff44d8 f006; mov L0x7fffffff44dc f007;
mov L0x7fffffff44e0 f008; mov L0x7fffffff44e4 f009; mov L0x7fffffff44e8 f010; mov L0x7fffffff44ec f011;
mov L0x7fffffff44f0 f012; mov L0x7fffffff44f4 f013; mov L0x7fffffff44f8 f014; mov L0x7fffffff44fc f015;
mov L0x7fffffff4500 f016; mov L0x7fffffff4504 f017; mov L0x7fffffff4508 f018; mov L0x7fffffff450c f019;
mov L0x7fffffff4510 f020; mov L0x7fffffff4514 f021; mov L0x7fffffff4518 f022; mov L0x7fffffff451c f023;
mov L0x7fffffff4520 f024; mov L0x7fffffff4524 f025; mov L0x7fffffff4528 f026; mov L0x7fffffff452c f027;
mov L0x7fffffff4530 f028; mov L0x7fffffff4534 f029; mov L0x7fffffff4538 f030; mov L0x7fffffff453c f031;
mov L0x7fffffff4540 f032; mov L0x7fffffff4544 f033; mov L0x7fffffff4548 f034; mov L0x7fffffff454c f035;
mov L0x7fffffff4550 f036; mov L0x7fffffff4554 f037; mov L0x7fffffff4558 f038; mov L0x7fffffff455c f039;
mov L0x7fffffff4560 f040; mov L0x7fffffff4564 f041; mov L0x7fffffff4568 f042; mov L0x7fffffff456c f043;
mov L0x7fffffff4570 f044; mov L0x7fffffff4574 f045; mov L0x7fffffff4578 f046; mov L0x7fffffff457c f047;
mov L0x7fffffff4580 f048; mov L0x7fffffff4584 f049; mov L0x7fffffff4588 f050; mov L0x7fffffff458c f051;
mov L0x7fffffff4590 f052; mov L0x7fffffff4594 f053; mov L0x7fffffff4598 f054; mov L0x7fffffff459c f055;
mov L0x7fffffff45a0 f056; mov L0x7fffffff45a4 f057; mov L0x7fffffff45a8 f058; mov L0x7fffffff45ac f059;
mov L0x7fffffff45b0 f060; mov L0x7fffffff45b4 f061; mov L0x7fffffff45b8 f062; mov L0x7fffffff45bc f063;
mov L0x7fffffff45c0 f064; mov L0x7fffffff45c4 f065; mov L0x7fffffff45c8 f066; mov L0x7fffffff45cc f067;
mov L0x7fffffff45d0 f068; mov L0x7fffffff45d4 f069; mov L0x7fffffff45d8 f070; mov L0x7fffffff45dc f071;
mov L0x7fffffff45e0 f072; mov L0x7fffffff45e4 f073; mov L0x7fffffff45e8 f074; mov L0x7fffffff45ec f075;
mov L0x7fffffff45f0 f076; mov L0x7fffffff45f4 f077; mov L0x7fffffff45f8 f078; mov L0x7fffffff45fc f079;
mov L0x7fffffff4600 f080; mov L0x7fffffff4604 f081; mov L0x7fffffff4608 f082; mov L0x7fffffff460c f083;
mov L0x7fffffff4610 f084; mov L0x7fffffff4614 f085; mov L0x7fffffff4618 f086; mov L0x7fffffff461c f087;
mov L0x7fffffff4620 f088; mov L0x7fffffff4624 f089; mov L0x7fffffff4628 f090; mov L0x7fffffff462c f091;
mov L0x7fffffff4630 f092; mov L0x7fffffff4634 f093; mov L0x7fffffff4638 f094; mov L0x7fffffff463c f095;
mov L0x7fffffff4640 f096; mov L0x7fffffff4644 f097; mov L0x7fffffff4648 f098; mov L0x7fffffff464c f099;
mov L0x7fffffff4650 f100; mov L0x7fffffff4654 f101; mov L0x7fffffff4658 f102; mov L0x7fffffff465c f103;
mov L0x7fffffff4660 f104; mov L0x7fffffff4664 f105; mov L0x7fffffff4668 f106; mov L0x7fffffff466c f107;
mov L0x7fffffff4670 f108; mov L0x7fffffff4674 f109; mov L0x7fffffff4678 f110; mov L0x7fffffff467c f111;
mov L0x7fffffff4680 f112; mov L0x7fffffff4684 f113; mov L0x7fffffff4688 f114; mov L0x7fffffff468c f115;
mov L0x7fffffff4690 f116; mov L0x7fffffff4694 f117; mov L0x7fffffff4698 f118; mov L0x7fffffff469c f119;
mov L0x7fffffff46a0 f120; mov L0x7fffffff46a4 f121; mov L0x7fffffff46a8 f122; mov L0x7fffffff46ac f123;
mov L0x7fffffff46b0 f124; mov L0x7fffffff46b4 f125; mov L0x7fffffff46b8 f126; mov L0x7fffffff46bc f127;
mov L0x7fffffff46c0 f128; mov L0x7fffffff46c4 f129; mov L0x7fffffff46c8 f130; mov L0x7fffffff46cc f131;
mov L0x7fffffff46d0 f132; mov L0x7fffffff46d4 f133; mov L0x7fffffff46d8 f134; mov L0x7fffffff46dc f135;
mov L0x7fffffff46e0 f136; mov L0x7fffffff46e4 f137; mov L0x7fffffff46e8 f138; mov L0x7fffffff46ec f139;
mov L0x7fffffff46f0 f140; mov L0x7fffffff46f4 f141; mov L0x7fffffff46f8 f142; mov L0x7fffffff46fc f143;
mov L0x7fffffff4700 f144; mov L0x7fffffff4704 f145; mov L0x7fffffff4708 f146; mov L0x7fffffff470c f147;
mov L0x7fffffff4710 f148; mov L0x7fffffff4714 f149; mov L0x7fffffff4718 f150; mov L0x7fffffff471c f151;
mov L0x7fffffff4720 f152; mov L0x7fffffff4724 f153; mov L0x7fffffff4728 f154; mov L0x7fffffff472c f155;
mov L0x7fffffff4730 f156; mov L0x7fffffff4734 f157; mov L0x7fffffff4738 f158; mov L0x7fffffff473c f159;
mov L0x7fffffff4740 f160; mov L0x7fffffff4744 f161; mov L0x7fffffff4748 f162; mov L0x7fffffff474c f163;
mov L0x7fffffff4750 f164; mov L0x7fffffff4754 f165; mov L0x7fffffff4758 f166; mov L0x7fffffff475c f167;
mov L0x7fffffff4760 f168; mov L0x7fffffff4764 f169; mov L0x7fffffff4768 f170; mov L0x7fffffff476c f171;
mov L0x7fffffff4770 f172; mov L0x7fffffff4774 f173; mov L0x7fffffff4778 f174; mov L0x7fffffff477c f175;
mov L0x7fffffff4780 f176; mov L0x7fffffff4784 f177; mov L0x7fffffff4788 f178; mov L0x7fffffff478c f179;
mov L0x7fffffff4790 f180; mov L0x7fffffff4794 f181; mov L0x7fffffff4798 f182; mov L0x7fffffff479c f183;
mov L0x7fffffff47a0 f184; mov L0x7fffffff47a4 f185; mov L0x7fffffff47a8 f186; mov L0x7fffffff47ac f187;
mov L0x7fffffff47b0 f188; mov L0x7fffffff47b4 f189; mov L0x7fffffff47b8 f190; mov L0x7fffffff47bc f191;
mov L0x7fffffff47c0 f192; mov L0x7fffffff47c4 f193; mov L0x7fffffff47c8 f194; mov L0x7fffffff47cc f195;
mov L0x7fffffff47d0 f196; mov L0x7fffffff47d4 f197; mov L0x7fffffff47d8 f198; mov L0x7fffffff47dc f199;
mov L0x7fffffff47e0 f200; mov L0x7fffffff47e4 f201; mov L0x7fffffff47e8 f202; mov L0x7fffffff47ec f203;
mov L0x7fffffff47f0 f204; mov L0x7fffffff47f4 f205; mov L0x7fffffff47f8 f206; mov L0x7fffffff47fc f207;
mov L0x7fffffff4800 f208; mov L0x7fffffff4804 f209; mov L0x7fffffff4808 f210; mov L0x7fffffff480c f211;
mov L0x7fffffff4810 f212; mov L0x7fffffff4814 f213; mov L0x7fffffff4818 f214; mov L0x7fffffff481c f215;
mov L0x7fffffff4820 f216; mov L0x7fffffff4824 f217; mov L0x7fffffff4828 f218; mov L0x7fffffff482c f219;
mov L0x7fffffff4830 f220; mov L0x7fffffff4834 f221; mov L0x7fffffff4838 f222; mov L0x7fffffff483c f223;
mov L0x7fffffff4840 f224; mov L0x7fffffff4844 f225; mov L0x7fffffff4848 f226; mov L0x7fffffff484c f227;
mov L0x7fffffff4850 f228; mov L0x7fffffff4854 f229; mov L0x7fffffff4858 f230; mov L0x7fffffff485c f231;
mov L0x7fffffff4860 f232; mov L0x7fffffff4864 f233; mov L0x7fffffff4868 f234; mov L0x7fffffff486c f235;
mov L0x7fffffff4870 f236; mov L0x7fffffff4874 f237; mov L0x7fffffff4878 f238; mov L0x7fffffff487c f239;
mov L0x7fffffff4880 f240; mov L0x7fffffff4884 f241; mov L0x7fffffff4888 f242; mov L0x7fffffff488c f243;
mov L0x7fffffff4890 f244; mov L0x7fffffff4894 f245; mov L0x7fffffff4898 f246; mov L0x7fffffff489c f247;
mov L0x7fffffff48a0 f248; mov L0x7fffffff48a4 f249; mov L0x7fffffff48a8 f250; mov L0x7fffffff48ac f251;
mov L0x7fffffff48b0 f252; mov L0x7fffffff48b4 f253; mov L0x7fffffff48b8 f254; mov L0x7fffffff48bc f255;
(* INP_POLY *)
ghost X@bit, inp_poly@bit :
  inp_poly**2 = poly X [
    f000, f001, f002, f003, f004, f005, f006, f007,
    f008, f009, f010, f011, f012, f013, f014, f015,
    f016, f017, f018, f019, f020, f021, f022, f023,
    f024, f025, f026, f027, f028, f029, f030, f031,
    f032, f033, f034, f035, f036, f037, f038, f039,
    f040, f041, f042, f043, f044, f045, f046, f047,
    f048, f049, f050, f051, f052, f053, f054, f055,
    f056, f057, f058, f059, f060, f061, f062, f063,
    f064, f065, f066, f067, f068, f069, f070, f071,
    f072, f073, f074, f075, f076, f077, f078, f079,
    f080, f081, f082, f083, f084, f085, f086, f087,
    f088, f089, f090, f091, f092, f093, f094, f095,
    f096, f097, f098, f099, f100, f101, f102, f103,
    f104, f105, f106, f107, f108, f109, f110, f111,
    f112, f113, f114, f115, f116, f117, f118, f119,
    f120, f121, f122, f123, f124, f125, f126, f127,
    f128, f129, f130, f131, f132, f133, f134, f135,
    f136, f137, f138, f139, f140, f141, f142, f143,
    f144, f145, f146, f147, f148, f149, f150, f151,
    f152, f153, f154, f155, f156, f157, f158, f159,
    f160, f161, f162, f163, f164, f165, f166, f167,
    f168, f169, f170, f171, f172, f173, f174, f175,
    f176, f177, f178, f179, f180, f181, f182, f183,
    f184, f185, f186, f187, f188, f189, f190, f191,
    f192, f193, f194, f195, f196, f197, f198, f199,
    f200, f201, f202, f203, f204, f205, f206, f207,
    f208, f209, f210, f211, f212, f213, f214, f215,
    f216, f217, f218, f219, f220, f221, f222, f223,
    f224, f225, f226, f227, f228, f229, f230, f231,
    f232, f233, f234, f235, f236, f237, f238, f239,
    f240, f241, f242, f243, f244, f245, f246, f247,
    f248, f249, f250, f251, f252, f253, f254, f255
  ] &&& true;
(* #! -> SP = 0x7fffffff3c58 *)
#! 0x7fffffff3c58 = 0x7fffffff3c58;
(* vmovdqa (%rsi),%ymm0                            #! EA = L0x555555579de0; Value = 0x007fe001007fe001; PC = 0x555555572a7f *)
mov %ymm0 [L0x555555579de0, L0x555555579de4, L0x555555579de8, L0x555555579dec, L0x555555579df0, L0x555555579df4, L0x555555579df8, L0x555555579dfc];
(* vpbroadcastd 0x84(%rsi),%ymm1                   #! EA = L0x555555579e64; Value = 0x8cf871026d1f44f7; PC = 0x555555572a83 *)
broadcast %ymm1 8 [L0x555555579e64];
(* vpbroadcastd 0x524(%rsi),%ymm2                  #! EA = L0x55555557a304; Value = 0xffd83102000064f7; PC = 0x555555572a8c *)
broadcast %ymm2 8 [L0x55555557a304];
(* vmovdqa (%rdi),%ymm4                            #! EA = L0x7fffffff44c0; Value = 0xfffffffdfffffffc; PC = 0x555555572a95 *)
mov %ymm4 [L0x7fffffff44c0, L0x7fffffff44c4, L0x7fffffff44c8, L0x7fffffff44cc, L0x7fffffff44d0, L0x7fffffff44d4, L0x7fffffff44d8, L0x7fffffff44dc];
(* vmovdqa 0x80(%rdi),%ymm5                        #! EA = L0x7fffffff4540; Value = 0xfffffffcffffffff; PC = 0x555555572a99 *)
mov %ymm5 [L0x7fffffff4540, L0x7fffffff4544, L0x7fffffff4548, L0x7fffffff454c, L0x7fffffff4550, L0x7fffffff4554, L0x7fffffff4558, L0x7fffffff455c];
(* vmovdqa 0x100(%rdi),%ymm6                       #! EA = L0x7fffffff45c0; Value = 0xfffffffefffffffc; PC = 0x555555572aa1 *)
mov %ymm6 [L0x7fffffff45c0, L0x7fffffff45c4, L0x7fffffff45c8, L0x7fffffff45cc, L0x7fffffff45d0, L0x7fffffff45d4, L0x7fffffff45d8, L0x7fffffff45dc];
(* vmovdqa 0x180(%rdi),%ymm7                       #! EA = L0x7fffffff4640; Value = 0x0000000100000003; PC = 0x555555572aa9 *)
mov %ymm7 [L0x7fffffff4640, L0x7fffffff4644, L0x7fffffff4648, L0x7fffffff464c, L0x7fffffff4650, L0x7fffffff4654, L0x7fffffff4658, L0x7fffffff465c];
(* vmovdqa 0x200(%rdi),%ymm8                       #! EA = L0x7fffffff46c0; Value = 0xfffffffc00000004; PC = 0x555555572ab1 *)
mov %ymm8 [L0x7fffffff46c0, L0x7fffffff46c4, L0x7fffffff46c8, L0x7fffffff46cc, L0x7fffffff46d0, L0x7fffffff46d4, L0x7fffffff46d8, L0x7fffffff46dc];
(* vmovdqa 0x280(%rdi),%ymm9                       #! EA = L0x7fffffff4740; Value = 0x0000000000000001; PC = 0x555555572ab9 *)
mov %ymm9 [L0x7fffffff4740, L0x7fffffff4744, L0x7fffffff4748, L0x7fffffff474c, L0x7fffffff4750, L0x7fffffff4754, L0x7fffffff4758, L0x7fffffff475c];
(* vmovdqa 0x300(%rdi),%ymm10                      #! EA = L0x7fffffff47c0; Value = 0x00000001ffffffff; PC = 0x555555572ac1 *)
mov %ymm10 [L0x7fffffff47c0, L0x7fffffff47c4, L0x7fffffff47c8, L0x7fffffff47cc, L0x7fffffff47d0, L0x7fffffff47d4, L0x7fffffff47d8, L0x7fffffff47dc];
(* vmovdqa 0x380(%rdi),%ymm11                      #! EA = L0x7fffffff4840; Value = 0x00000000fffffffc; PC = 0x555555572ac9 *)
mov %ymm11 [L0x7fffffff4840, L0x7fffffff4844, L0x7fffffff4848, L0x7fffffff484c, L0x7fffffff4850, L0x7fffffff4854, L0x7fffffff4858, L0x7fffffff485c];
(* vpmuldq %ymm1,%ymm8,%ymm13                      #! PC = 0x555555572ad1 *)
mull %ymm13_h %ymm13_l (%ymm8[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm8,%ymm12                          #! PC = 0x555555572ad6 *)
mov %ymm12 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555572adb *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm8,%ymm8                       #! PC = 0x555555572ae0 *)
mull %ymm8_h %ymm8_l (%ymm8[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm8_ls@sint32[4] %ymm8_l;
mov %ymm8 [%ymm8_ls[0], %ymm8_h[0], %ymm8_ls[1], %ymm8_h[1], %ymm8_ls[2], %ymm8_h[2], %ymm8_ls[3], %ymm8_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555572ae5 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555572aea *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555572aef *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm8_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm8_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm8,%ymm8                           #! PC = 0x555555572af4 *)
mov %ymm8 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm8,%ymm8               #! PC = 0x555555572af9 *)
mov %ymm8 [%ymm8[0], %ymm12[1], %ymm8[2], %ymm12[3], %ymm8[4], %ymm12[5], %ymm8[6], %ymm12[7]];
(* vpsubd %ymm8,%ymm4,%ymm12                       #! PC = 0x555555572aff *)
sub %ymm12 %ymm4 %ymm8;
(* vpaddd %ymm8,%ymm4,%ymm4                        #! PC = 0x555555572b04 *)
add %ymm4 %ymm4 %ymm8;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555572b09 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555572b0e *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm8                      #! PC = 0x555555572b14 *)
add %ymm8 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm4,%ymm4                       #! PC = 0x555555572b19 *)
sub %ymm4 %ymm4 %ymm13;
(* vpmuldq %ymm1,%ymm9,%ymm13                      #! PC = 0x555555572b1e *)
mull %ymm13_h %ymm13_l (%ymm9[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm9,%ymm12                          #! PC = 0x555555572b23 *)
mov %ymm12 (%ymm9[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555572b28 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm9,%ymm9                       #! PC = 0x555555572b2d *)
mull %ymm9_h %ymm9_l (%ymm9[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm9_ls@sint32[4] %ymm9_l;
mov %ymm9 [%ymm9_ls[0], %ymm9_h[0], %ymm9_ls[1], %ymm9_h[1], %ymm9_ls[2], %ymm9_h[2], %ymm9_ls[3], %ymm9_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555572b32 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555572b37 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555572b3c *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm9_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm9_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm9,%ymm9                           #! PC = 0x555555572b41 *)
mov %ymm9 (%ymm9[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm9,%ymm9               #! PC = 0x555555572b46 *)
mov %ymm9 [%ymm9[0], %ymm12[1], %ymm9[2], %ymm12[3], %ymm9[4], %ymm12[5], %ymm9[6], %ymm12[7]];
(* vpsubd %ymm9,%ymm5,%ymm12                       #! PC = 0x555555572b4c *)
sub %ymm12 %ymm5 %ymm9;
(* vpaddd %ymm9,%ymm5,%ymm5                        #! PC = 0x555555572b51 *)
add %ymm5 %ymm5 %ymm9;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555572b56 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555572b5b *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm9                      #! PC = 0x555555572b61 *)
add %ymm9 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm5,%ymm5                       #! PC = 0x555555572b66 *)
sub %ymm5 %ymm5 %ymm13;
(* vpmuldq %ymm1,%ymm10,%ymm13                     #! PC = 0x555555572b6b *)
mull %ymm13_h %ymm13_l (%ymm10[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm10,%ymm12                         #! PC = 0x555555572b70 *)
mov %ymm12 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555572b75 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm10,%ymm10                     #! PC = 0x555555572b7a *)
mull %ymm10_h %ymm10_l (%ymm10[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm10_ls@sint32[4] %ymm10_l;
mov %ymm10 [%ymm10_ls[0], %ymm10_h[0], %ymm10_ls[1], %ymm10_h[1], %ymm10_ls[2], %ymm10_h[2], %ymm10_ls[3], %ymm10_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555572b7f *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555572b84 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555572b89 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm10_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm10_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm10,%ymm10                         #! PC = 0x555555572b8e *)
mov %ymm10 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm10,%ymm10             #! PC = 0x555555572b93 *)
mov %ymm10 [%ymm10[0], %ymm12[1], %ymm10[2], %ymm12[3], %ymm10[4], %ymm12[5], %ymm10[6], %ymm12[7]];
(* vpsubd %ymm10,%ymm6,%ymm12                      #! PC = 0x555555572b99 *)
sub %ymm12 %ymm6 %ymm10;
(* vpaddd %ymm10,%ymm6,%ymm6                       #! PC = 0x555555572b9e *)
add %ymm6 %ymm6 %ymm10;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555572ba3 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555572ba8 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm10                     #! PC = 0x555555572bae *)
add %ymm10 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm6,%ymm6                       #! PC = 0x555555572bb3 *)
sub %ymm6 %ymm6 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x555555572bb8 *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x555555572bbd *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555572bc2 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x555555572bc7 *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555572bcc *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555572bd1 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555572bd6 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x555555572bdb *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x555555572be0 *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm7,%ymm12                      #! PC = 0x555555572be6 *)
sub %ymm12 %ymm7 %ymm11;
(* vpaddd %ymm11,%ymm7,%ymm7                       #! PC = 0x555555572beb *)
add %ymm7 %ymm7 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555572bf0 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555572bf5 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x555555572bfb *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm7,%ymm7                       #! PC = 0x555555572c00 *)
sub %ymm7 %ymm7 %ymm13;
(* vpbroadcastd 0x88(%rsi),%ymm1                   #! EA = L0x555555579e68; Value = 0x8d1875038cf87102; PC = 0x555555572c05 *)
broadcast %ymm1 8 [L0x555555579e68];
(* vpbroadcastd 0x528(%rsi),%ymm2                  #! EA = L0x55555557a308; Value = 0xfff81503ffd83102; PC = 0x555555572c0e *)
broadcast %ymm2 8 [L0x55555557a308];
(* vpmuldq %ymm1,%ymm6,%ymm13                      #! PC = 0x555555572c17 *)
mull %ymm13_h %ymm13_l (%ymm6[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm6,%ymm12                          #! PC = 0x555555572c1c *)
mov %ymm12 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555572c20 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm6,%ymm6                       #! PC = 0x555555572c25 *)
mull %ymm6_h %ymm6_l (%ymm6[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm6_ls@sint32[4] %ymm6_l;
mov %ymm6 [%ymm6_ls[0], %ymm6_h[0], %ymm6_ls[1], %ymm6_h[1], %ymm6_ls[2], %ymm6_h[2], %ymm6_ls[3], %ymm6_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555572c2a *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555572c2f *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555572c34 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm6_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm6_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm6,%ymm6                           #! PC = 0x555555572c39 *)
mov %ymm6 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm6,%ymm6               #! PC = 0x555555572c3d *)
mov %ymm6 [%ymm6[0], %ymm12[1], %ymm6[2], %ymm12[3], %ymm6[4], %ymm12[5], %ymm6[6], %ymm12[7]];
(* vpsubd %ymm6,%ymm4,%ymm12                       #! PC = 0x555555572c43 *)
sub %ymm12 %ymm4 %ymm6;
(* vpaddd %ymm6,%ymm4,%ymm4                        #! PC = 0x555555572c47 *)
add %ymm4 %ymm4 %ymm6;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555572c4b *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555572c50 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm6                      #! PC = 0x555555572c56 *)
add %ymm6 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm4,%ymm4                       #! PC = 0x555555572c5b *)
sub %ymm4 %ymm4 %ymm13;
(* vpmuldq %ymm1,%ymm7,%ymm13                      #! PC = 0x555555572c60 *)
mull %ymm13_h %ymm13_l (%ymm7[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm7,%ymm12                          #! PC = 0x555555572c65 *)
mov %ymm12 (%ymm7[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555572c69 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm7,%ymm7                       #! PC = 0x555555572c6e *)
mull %ymm7_h %ymm7_l (%ymm7[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm7_ls@sint32[4] %ymm7_l;
mov %ymm7 [%ymm7_ls[0], %ymm7_h[0], %ymm7_ls[1], %ymm7_h[1], %ymm7_ls[2], %ymm7_h[2], %ymm7_ls[3], %ymm7_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555572c73 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555572c78 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555572c7d *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm7_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm7_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm7,%ymm7                           #! PC = 0x555555572c82 *)
mov %ymm7 (%ymm7[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm7,%ymm7               #! PC = 0x555555572c86 *)
mov %ymm7 [%ymm7[0], %ymm12[1], %ymm7[2], %ymm12[3], %ymm7[4], %ymm12[5], %ymm7[6], %ymm12[7]];
(* vpsubd %ymm7,%ymm5,%ymm12                       #! PC = 0x555555572c8c *)
sub %ymm12 %ymm5 %ymm7;
(* vpaddd %ymm7,%ymm5,%ymm5                        #! PC = 0x555555572c90 *)
add %ymm5 %ymm5 %ymm7;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555572c94 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555572c99 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm7                      #! PC = 0x555555572c9f *)
add %ymm7 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm5,%ymm5                       #! PC = 0x555555572ca4 *)
sub %ymm5 %ymm5 %ymm13;
(* vpbroadcastd 0x8c(%rsi),%ymm1                   #! EA = L0x555555579e6c; Value = 0x61cc1e448d187503; PC = 0x555555572ca9 *)
broadcast %ymm1 8 [L0x555555579e6c];
(* vpbroadcastd 0x52c(%rsi),%ymm2                  #! EA = L0x55555557a30c; Value = 0x00039e44fff81503; PC = 0x555555572cb2 *)
broadcast %ymm2 8 [L0x55555557a30c];
(* vpmuldq %ymm1,%ymm10,%ymm13                     #! PC = 0x555555572cbb *)
mull %ymm13_h %ymm13_l (%ymm10[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm10,%ymm12                         #! PC = 0x555555572cc0 *)
mov %ymm12 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555572cc5 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm10,%ymm10                     #! PC = 0x555555572cca *)
mull %ymm10_h %ymm10_l (%ymm10[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm10_ls@sint32[4] %ymm10_l;
mov %ymm10 [%ymm10_ls[0], %ymm10_h[0], %ymm10_ls[1], %ymm10_h[1], %ymm10_ls[2], %ymm10_h[2], %ymm10_ls[3], %ymm10_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555572ccf *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555572cd4 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555572cd9 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm10_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm10_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm10,%ymm10                         #! PC = 0x555555572cde *)
mov %ymm10 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm10,%ymm10             #! PC = 0x555555572ce3 *)
mov %ymm10 [%ymm10[0], %ymm12[1], %ymm10[2], %ymm12[3], %ymm10[4], %ymm12[5], %ymm10[6], %ymm12[7]];
(* vpsubd %ymm10,%ymm8,%ymm12                      #! PC = 0x555555572ce9 *)
sub %ymm12 %ymm8 %ymm10;
(* vpaddd %ymm10,%ymm8,%ymm8                       #! PC = 0x555555572cee *)
add %ymm8 %ymm8 %ymm10;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555572cf3 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555572cf8 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm10                     #! PC = 0x555555572cfe *)
add %ymm10 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm8,%ymm8                       #! PC = 0x555555572d03 *)
sub %ymm8 %ymm8 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x555555572d08 *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x555555572d0d *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555572d12 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x555555572d17 *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555572d1c *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555572d21 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555572d26 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x555555572d2b *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x555555572d30 *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm9,%ymm12                      #! PC = 0x555555572d36 *)
sub %ymm12 %ymm9 %ymm11;
(* vpaddd %ymm11,%ymm9,%ymm9                       #! PC = 0x555555572d3b *)
add %ymm9 %ymm9 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555572d40 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555572d45 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x555555572d4b *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm9,%ymm9                       #! PC = 0x555555572d50 *)
sub %ymm9 %ymm9 %ymm13;
(* vmovdqa %ymm4,(%rdi)                            #! EA = L0x7fffffff44c0; PC = 0x555555572d55 *)
mov [L0x7fffffff44c0, L0x7fffffff44c4, L0x7fffffff44c8, L0x7fffffff44cc, L0x7fffffff44d0, L0x7fffffff44d4, L0x7fffffff44d8, L0x7fffffff44dc] %ymm4;
(* vmovdqa %ymm5,0x80(%rdi)                        #! EA = L0x7fffffff4540; PC = 0x555555572d59 *)
mov [L0x7fffffff4540, L0x7fffffff4544, L0x7fffffff4548, L0x7fffffff454c, L0x7fffffff4550, L0x7fffffff4554, L0x7fffffff4558, L0x7fffffff455c] %ymm5;
(* vmovdqa %ymm6,0x100(%rdi)                       #! EA = L0x7fffffff45c0; PC = 0x555555572d61 *)
mov [L0x7fffffff45c0, L0x7fffffff45c4, L0x7fffffff45c8, L0x7fffffff45cc, L0x7fffffff45d0, L0x7fffffff45d4, L0x7fffffff45d8, L0x7fffffff45dc] %ymm6;
(* vmovdqa %ymm7,0x180(%rdi)                       #! EA = L0x7fffffff4640; PC = 0x555555572d69 *)
mov [L0x7fffffff4640, L0x7fffffff4644, L0x7fffffff4648, L0x7fffffff464c, L0x7fffffff4650, L0x7fffffff4654, L0x7fffffff4658, L0x7fffffff465c] %ymm7;
(* vmovdqa %ymm8,0x200(%rdi)                       #! EA = L0x7fffffff46c0; PC = 0x555555572d71 *)
mov [L0x7fffffff46c0, L0x7fffffff46c4, L0x7fffffff46c8, L0x7fffffff46cc, L0x7fffffff46d0, L0x7fffffff46d4, L0x7fffffff46d8, L0x7fffffff46dc] %ymm8;
(* vmovdqa %ymm9,0x280(%rdi)                       #! EA = L0x7fffffff4740; PC = 0x555555572d79 *)
mov [L0x7fffffff4740, L0x7fffffff4744, L0x7fffffff4748, L0x7fffffff474c, L0x7fffffff4750, L0x7fffffff4754, L0x7fffffff4758, L0x7fffffff475c] %ymm9;
(* vmovdqa %ymm10,0x300(%rdi)                      #! EA = L0x7fffffff47c0; PC = 0x555555572d81 *)
mov [L0x7fffffff47c0, L0x7fffffff47c4, L0x7fffffff47c8, L0x7fffffff47cc, L0x7fffffff47d0, L0x7fffffff47d4, L0x7fffffff47d8, L0x7fffffff47dc] %ymm10;
(* vmovdqa %ymm11,0x380(%rdi)                      #! EA = L0x7fffffff4840; PC = 0x555555572d89 *)
mov [L0x7fffffff4840, L0x7fffffff4844, L0x7fffffff4848, L0x7fffffff484c, L0x7fffffff4850, L0x7fffffff4854, L0x7fffffff4858, L0x7fffffff485c] %ymm11;
(* vpbroadcastd 0x84(%rsi),%ymm1                   #! EA = L0x555555579e64; Value = 0x8cf871026d1f44f7; PC = 0x555555572d91 *)
broadcast %ymm1 8 [L0x555555579e64];
(* vpbroadcastd 0x524(%rsi),%ymm2                  #! EA = L0x55555557a304; Value = 0xffd83102000064f7; PC = 0x555555572d9a *)
broadcast %ymm2 8 [L0x55555557a304];
(* vmovdqa 0x20(%rdi),%ymm4                        #! EA = L0x7fffffff44e0; Value = 0x0000000200000003; PC = 0x555555572da3 *)
mov %ymm4 [L0x7fffffff44e0, L0x7fffffff44e4, L0x7fffffff44e8, L0x7fffffff44ec, L0x7fffffff44f0, L0x7fffffff44f4, L0x7fffffff44f8, L0x7fffffff44fc];
(* vmovdqa 0xa0(%rdi),%ymm5                        #! EA = L0x7fffffff4560; Value = 0x0000000000000000; PC = 0x555555572da8 *)
mov %ymm5 [L0x7fffffff4560, L0x7fffffff4564, L0x7fffffff4568, L0x7fffffff456c, L0x7fffffff4570, L0x7fffffff4574, L0x7fffffff4578, L0x7fffffff457c];
(* vmovdqa 0x120(%rdi),%ymm6                       #! EA = L0x7fffffff45e0; Value = 0xfffffffc00000003; PC = 0x555555572db0 *)
mov %ymm6 [L0x7fffffff45e0, L0x7fffffff45e4, L0x7fffffff45e8, L0x7fffffff45ec, L0x7fffffff45f0, L0x7fffffff45f4, L0x7fffffff45f8, L0x7fffffff45fc];
(* vmovdqa 0x1a0(%rdi),%ymm7                       #! EA = L0x7fffffff4660; Value = 0xffffffff00000004; PC = 0x555555572db8 *)
mov %ymm7 [L0x7fffffff4660, L0x7fffffff4664, L0x7fffffff4668, L0x7fffffff466c, L0x7fffffff4670, L0x7fffffff4674, L0x7fffffff4678, L0x7fffffff467c];
(* vmovdqa 0x220(%rdi),%ymm8                       #! EA = L0x7fffffff46e0; Value = 0xfffffffcfffffffd; PC = 0x555555572dc0 *)
mov %ymm8 [L0x7fffffff46e0, L0x7fffffff46e4, L0x7fffffff46e8, L0x7fffffff46ec, L0x7fffffff46f0, L0x7fffffff46f4, L0x7fffffff46f8, L0x7fffffff46fc];
(* vmovdqa 0x2a0(%rdi),%ymm9                       #! EA = L0x7fffffff4760; Value = 0x00000004ffffffff; PC = 0x555555572dc8 *)
mov %ymm9 [L0x7fffffff4760, L0x7fffffff4764, L0x7fffffff4768, L0x7fffffff476c, L0x7fffffff4770, L0x7fffffff4774, L0x7fffffff4778, L0x7fffffff477c];
(* vmovdqa 0x320(%rdi),%ymm10                      #! EA = L0x7fffffff47e0; Value = 0xffffffff00000002; PC = 0x555555572dd0 *)
mov %ymm10 [L0x7fffffff47e0, L0x7fffffff47e4, L0x7fffffff47e8, L0x7fffffff47ec, L0x7fffffff47f0, L0x7fffffff47f4, L0x7fffffff47f8, L0x7fffffff47fc];
(* vmovdqa 0x3a0(%rdi),%ymm11                      #! EA = L0x7fffffff4860; Value = 0xfffffffffffffffe; PC = 0x555555572dd8 *)
mov %ymm11 [L0x7fffffff4860, L0x7fffffff4864, L0x7fffffff4868, L0x7fffffff486c, L0x7fffffff4870, L0x7fffffff4874, L0x7fffffff4878, L0x7fffffff487c];
(* vpmuldq %ymm1,%ymm8,%ymm13                      #! PC = 0x555555572de0 *)
mull %ymm13_h %ymm13_l (%ymm8[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm8,%ymm12                          #! PC = 0x555555572de5 *)
mov %ymm12 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555572dea *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm8,%ymm8                       #! PC = 0x555555572def *)
mull %ymm8_h %ymm8_l (%ymm8[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm8_ls@sint32[4] %ymm8_l;
mov %ymm8 [%ymm8_ls[0], %ymm8_h[0], %ymm8_ls[1], %ymm8_h[1], %ymm8_ls[2], %ymm8_h[2], %ymm8_ls[3], %ymm8_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555572df4 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555572df9 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555572dfe *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm8_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm8_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm8,%ymm8                           #! PC = 0x555555572e03 *)
mov %ymm8 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm8,%ymm8               #! PC = 0x555555572e08 *)
mov %ymm8 [%ymm8[0], %ymm12[1], %ymm8[2], %ymm12[3], %ymm8[4], %ymm12[5], %ymm8[6], %ymm12[7]];
(* vpsubd %ymm8,%ymm4,%ymm12                       #! PC = 0x555555572e0e *)
sub %ymm12 %ymm4 %ymm8;
(* vpaddd %ymm8,%ymm4,%ymm4                        #! PC = 0x555555572e13 *)
add %ymm4 %ymm4 %ymm8;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555572e18 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555572e1d *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm8                      #! PC = 0x555555572e23 *)
add %ymm8 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm4,%ymm4                       #! PC = 0x555555572e28 *)
sub %ymm4 %ymm4 %ymm13;
(* vpmuldq %ymm1,%ymm9,%ymm13                      #! PC = 0x555555572e2d *)
mull %ymm13_h %ymm13_l (%ymm9[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm9,%ymm12                          #! PC = 0x555555572e32 *)
mov %ymm12 (%ymm9[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555572e37 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm9,%ymm9                       #! PC = 0x555555572e3c *)
mull %ymm9_h %ymm9_l (%ymm9[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm9_ls@sint32[4] %ymm9_l;
mov %ymm9 [%ymm9_ls[0], %ymm9_h[0], %ymm9_ls[1], %ymm9_h[1], %ymm9_ls[2], %ymm9_h[2], %ymm9_ls[3], %ymm9_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555572e41 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555572e46 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555572e4b *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm9_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm9_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm9,%ymm9                           #! PC = 0x555555572e50 *)
mov %ymm9 (%ymm9[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm9,%ymm9               #! PC = 0x555555572e55 *)
mov %ymm9 [%ymm9[0], %ymm12[1], %ymm9[2], %ymm12[3], %ymm9[4], %ymm12[5], %ymm9[6], %ymm12[7]];
(* vpsubd %ymm9,%ymm5,%ymm12                       #! PC = 0x555555572e5b *)
sub %ymm12 %ymm5 %ymm9;
(* vpaddd %ymm9,%ymm5,%ymm5                        #! PC = 0x555555572e60 *)
add %ymm5 %ymm5 %ymm9;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555572e65 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555572e6a *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm9                      #! PC = 0x555555572e70 *)
add %ymm9 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm5,%ymm5                       #! PC = 0x555555572e75 *)
sub %ymm5 %ymm5 %ymm13;
(* vpmuldq %ymm1,%ymm10,%ymm13                     #! PC = 0x555555572e7a *)
mull %ymm13_h %ymm13_l (%ymm10[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm10,%ymm12                         #! PC = 0x555555572e7f *)
mov %ymm12 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555572e84 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm10,%ymm10                     #! PC = 0x555555572e89 *)
mull %ymm10_h %ymm10_l (%ymm10[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm10_ls@sint32[4] %ymm10_l;
mov %ymm10 [%ymm10_ls[0], %ymm10_h[0], %ymm10_ls[1], %ymm10_h[1], %ymm10_ls[2], %ymm10_h[2], %ymm10_ls[3], %ymm10_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555572e8e *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555572e93 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555572e98 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm10_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm10_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm10,%ymm10                         #! PC = 0x555555572e9d *)
mov %ymm10 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm10,%ymm10             #! PC = 0x555555572ea2 *)
mov %ymm10 [%ymm10[0], %ymm12[1], %ymm10[2], %ymm12[3], %ymm10[4], %ymm12[5], %ymm10[6], %ymm12[7]];
(* vpsubd %ymm10,%ymm6,%ymm12                      #! PC = 0x555555572ea8 *)
sub %ymm12 %ymm6 %ymm10;
(* vpaddd %ymm10,%ymm6,%ymm6                       #! PC = 0x555555572ead *)
add %ymm6 %ymm6 %ymm10;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555572eb2 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555572eb7 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm10                     #! PC = 0x555555572ebd *)
add %ymm10 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm6,%ymm6                       #! PC = 0x555555572ec2 *)
sub %ymm6 %ymm6 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x555555572ec7 *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x555555572ecc *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555572ed1 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x555555572ed6 *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555572edb *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555572ee0 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555572ee5 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x555555572eea *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x555555572eef *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm7,%ymm12                      #! PC = 0x555555572ef5 *)
sub %ymm12 %ymm7 %ymm11;
(* vpaddd %ymm11,%ymm7,%ymm7                       #! PC = 0x555555572efa *)
add %ymm7 %ymm7 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555572eff *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555572f04 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x555555572f0a *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm7,%ymm7                       #! PC = 0x555555572f0f *)
sub %ymm7 %ymm7 %ymm13;
(* vpbroadcastd 0x88(%rsi),%ymm1                   #! EA = L0x555555579e68; Value = 0x8d1875038cf87102; PC = 0x555555572f14 *)
broadcast %ymm1 8 [L0x555555579e68];
(* vpbroadcastd 0x528(%rsi),%ymm2                  #! EA = L0x55555557a308; Value = 0xfff81503ffd83102; PC = 0x555555572f1d *)
broadcast %ymm2 8 [L0x55555557a308];
(* vpmuldq %ymm1,%ymm6,%ymm13                      #! PC = 0x555555572f26 *)
mull %ymm13_h %ymm13_l (%ymm6[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm6,%ymm12                          #! PC = 0x555555572f2b *)
mov %ymm12 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555572f2f *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm6,%ymm6                       #! PC = 0x555555572f34 *)
mull %ymm6_h %ymm6_l (%ymm6[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm6_ls@sint32[4] %ymm6_l;
mov %ymm6 [%ymm6_ls[0], %ymm6_h[0], %ymm6_ls[1], %ymm6_h[1], %ymm6_ls[2], %ymm6_h[2], %ymm6_ls[3], %ymm6_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555572f39 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555572f3e *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555572f43 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm6_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm6_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm6,%ymm6                           #! PC = 0x555555572f48 *)
mov %ymm6 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm6,%ymm6               #! PC = 0x555555572f4c *)
mov %ymm6 [%ymm6[0], %ymm12[1], %ymm6[2], %ymm12[3], %ymm6[4], %ymm12[5], %ymm6[6], %ymm12[7]];
(* vpsubd %ymm6,%ymm4,%ymm12                       #! PC = 0x555555572f52 *)
sub %ymm12 %ymm4 %ymm6;
(* vpaddd %ymm6,%ymm4,%ymm4                        #! PC = 0x555555572f56 *)
add %ymm4 %ymm4 %ymm6;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555572f5a *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555572f5f *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm6                      #! PC = 0x555555572f65 *)
add %ymm6 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm4,%ymm4                       #! PC = 0x555555572f6a *)
sub %ymm4 %ymm4 %ymm13;
(* vpmuldq %ymm1,%ymm7,%ymm13                      #! PC = 0x555555572f6f *)
mull %ymm13_h %ymm13_l (%ymm7[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm7,%ymm12                          #! PC = 0x555555572f74 *)
mov %ymm12 (%ymm7[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555572f78 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm7,%ymm7                       #! PC = 0x555555572f7d *)
mull %ymm7_h %ymm7_l (%ymm7[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm7_ls@sint32[4] %ymm7_l;
mov %ymm7 [%ymm7_ls[0], %ymm7_h[0], %ymm7_ls[1], %ymm7_h[1], %ymm7_ls[2], %ymm7_h[2], %ymm7_ls[3], %ymm7_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555572f82 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555572f87 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555572f8c *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm7_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm7_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm7,%ymm7                           #! PC = 0x555555572f91 *)
mov %ymm7 (%ymm7[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm7,%ymm7               #! PC = 0x555555572f95 *)
mov %ymm7 [%ymm7[0], %ymm12[1], %ymm7[2], %ymm12[3], %ymm7[4], %ymm12[5], %ymm7[6], %ymm12[7]];
(* vpsubd %ymm7,%ymm5,%ymm12                       #! PC = 0x555555572f9b *)
sub %ymm12 %ymm5 %ymm7;
(* vpaddd %ymm7,%ymm5,%ymm5                        #! PC = 0x555555572f9f *)
add %ymm5 %ymm5 %ymm7;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555572fa3 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555572fa8 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm7                      #! PC = 0x555555572fae *)
add %ymm7 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm5,%ymm5                       #! PC = 0x555555572fb3 *)
sub %ymm5 %ymm5 %ymm13;
(* vpbroadcastd 0x8c(%rsi),%ymm1                   #! EA = L0x555555579e6c; Value = 0x61cc1e448d187503; PC = 0x555555572fb8 *)
broadcast %ymm1 8 [L0x555555579e6c];
(* vpbroadcastd 0x52c(%rsi),%ymm2                  #! EA = L0x55555557a30c; Value = 0x00039e44fff81503; PC = 0x555555572fc1 *)
broadcast %ymm2 8 [L0x55555557a30c];
(* vpmuldq %ymm1,%ymm10,%ymm13                     #! PC = 0x555555572fca *)
mull %ymm13_h %ymm13_l (%ymm10[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm10,%ymm12                         #! PC = 0x555555572fcf *)
mov %ymm12 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555572fd4 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm10,%ymm10                     #! PC = 0x555555572fd9 *)
mull %ymm10_h %ymm10_l (%ymm10[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm10_ls@sint32[4] %ymm10_l;
mov %ymm10 [%ymm10_ls[0], %ymm10_h[0], %ymm10_ls[1], %ymm10_h[1], %ymm10_ls[2], %ymm10_h[2], %ymm10_ls[3], %ymm10_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555572fde *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555572fe3 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555572fe8 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm10_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm10_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm10,%ymm10                         #! PC = 0x555555572fed *)
mov %ymm10 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm10,%ymm10             #! PC = 0x555555572ff2 *)
mov %ymm10 [%ymm10[0], %ymm12[1], %ymm10[2], %ymm12[3], %ymm10[4], %ymm12[5], %ymm10[6], %ymm12[7]];
(* vpsubd %ymm10,%ymm8,%ymm12                      #! PC = 0x555555572ff8 *)
sub %ymm12 %ymm8 %ymm10;
(* vpaddd %ymm10,%ymm8,%ymm8                       #! PC = 0x555555572ffd *)
add %ymm8 %ymm8 %ymm10;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555573002 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573007 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm10                     #! PC = 0x55555557300d *)
add %ymm10 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm8,%ymm8                       #! PC = 0x555555573012 *)
sub %ymm8 %ymm8 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x555555573017 *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x55555557301c *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555573021 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x555555573026 *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x55555557302b *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573030 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555573035 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x55555557303a *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x55555557303f *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm9,%ymm12                      #! PC = 0x555555573045 *)
sub %ymm12 %ymm9 %ymm11;
(* vpaddd %ymm11,%ymm9,%ymm9                       #! PC = 0x55555557304a *)
add %ymm9 %ymm9 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x55555557304f *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573054 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x55555557305a *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm9,%ymm9                       #! PC = 0x55555557305f *)
sub %ymm9 %ymm9 %ymm13;
(* vmovdqa %ymm4,0x20(%rdi)                        #! EA = L0x7fffffff44e0; PC = 0x555555573064 *)
mov [L0x7fffffff44e0, L0x7fffffff44e4, L0x7fffffff44e8, L0x7fffffff44ec, L0x7fffffff44f0, L0x7fffffff44f4, L0x7fffffff44f8, L0x7fffffff44fc] %ymm4;
(* vmovdqa %ymm5,0xa0(%rdi)                        #! EA = L0x7fffffff4560; PC = 0x555555573069 *)
mov [L0x7fffffff4560, L0x7fffffff4564, L0x7fffffff4568, L0x7fffffff456c, L0x7fffffff4570, L0x7fffffff4574, L0x7fffffff4578, L0x7fffffff457c] %ymm5;
(* vmovdqa %ymm6,0x120(%rdi)                       #! EA = L0x7fffffff45e0; PC = 0x555555573071 *)
mov [L0x7fffffff45e0, L0x7fffffff45e4, L0x7fffffff45e8, L0x7fffffff45ec, L0x7fffffff45f0, L0x7fffffff45f4, L0x7fffffff45f8, L0x7fffffff45fc] %ymm6;
(* vmovdqa %ymm7,0x1a0(%rdi)                       #! EA = L0x7fffffff4660; PC = 0x555555573079 *)
mov [L0x7fffffff4660, L0x7fffffff4664, L0x7fffffff4668, L0x7fffffff466c, L0x7fffffff4670, L0x7fffffff4674, L0x7fffffff4678, L0x7fffffff467c] %ymm7;
(* vmovdqa %ymm8,0x220(%rdi)                       #! EA = L0x7fffffff46e0; PC = 0x555555573081 *)
mov [L0x7fffffff46e0, L0x7fffffff46e4, L0x7fffffff46e8, L0x7fffffff46ec, L0x7fffffff46f0, L0x7fffffff46f4, L0x7fffffff46f8, L0x7fffffff46fc] %ymm8;
(* vmovdqa %ymm9,0x2a0(%rdi)                       #! EA = L0x7fffffff4760; PC = 0x555555573089 *)
mov [L0x7fffffff4760, L0x7fffffff4764, L0x7fffffff4768, L0x7fffffff476c, L0x7fffffff4770, L0x7fffffff4774, L0x7fffffff4778, L0x7fffffff477c] %ymm9;
(* vmovdqa %ymm10,0x320(%rdi)                      #! EA = L0x7fffffff47e0; PC = 0x555555573091 *)
mov [L0x7fffffff47e0, L0x7fffffff47e4, L0x7fffffff47e8, L0x7fffffff47ec, L0x7fffffff47f0, L0x7fffffff47f4, L0x7fffffff47f8, L0x7fffffff47fc] %ymm10;
(* vmovdqa %ymm11,0x3a0(%rdi)                      #! EA = L0x7fffffff4860; PC = 0x555555573099 *)
mov [L0x7fffffff4860, L0x7fffffff4864, L0x7fffffff4868, L0x7fffffff486c, L0x7fffffff4870, L0x7fffffff4874, L0x7fffffff4878, L0x7fffffff487c] %ymm11;
(* vpbroadcastd 0x84(%rsi),%ymm1                   #! EA = L0x555555579e64; Value = 0x8cf871026d1f44f7; PC = 0x5555555730a1 *)
broadcast %ymm1 8 [L0x555555579e64];
(* vpbroadcastd 0x524(%rsi),%ymm2                  #! EA = L0x55555557a304; Value = 0xffd83102000064f7; PC = 0x5555555730aa *)
broadcast %ymm2 8 [L0x55555557a304];
(* vmovdqa 0x40(%rdi),%ymm4                        #! EA = L0x7fffffff4500; Value = 0x0000000000000004; PC = 0x5555555730b3 *)
mov %ymm4 [L0x7fffffff4500, L0x7fffffff4504, L0x7fffffff4508, L0x7fffffff450c, L0x7fffffff4510, L0x7fffffff4514, L0x7fffffff4518, L0x7fffffff451c];
(* vmovdqa 0xc0(%rdi),%ymm5                        #! EA = L0x7fffffff4580; Value = 0x0000000100000000; PC = 0x5555555730b8 *)
mov %ymm5 [L0x7fffffff4580, L0x7fffffff4584, L0x7fffffff4588, L0x7fffffff458c, L0x7fffffff4590, L0x7fffffff4594, L0x7fffffff4598, L0x7fffffff459c];
(* vmovdqa 0x140(%rdi),%ymm6                       #! EA = L0x7fffffff4600; Value = 0x0000000100000003; PC = 0x5555555730c0 *)
mov %ymm6 [L0x7fffffff4600, L0x7fffffff4604, L0x7fffffff4608, L0x7fffffff460c, L0x7fffffff4610, L0x7fffffff4614, L0x7fffffff4618, L0x7fffffff461c];
(* vmovdqa 0x1c0(%rdi),%ymm7                       #! EA = L0x7fffffff4680; Value = 0x0000000200000004; PC = 0x5555555730c8 *)
mov %ymm7 [L0x7fffffff4680, L0x7fffffff4684, L0x7fffffff4688, L0x7fffffff468c, L0x7fffffff4690, L0x7fffffff4694, L0x7fffffff4698, L0x7fffffff469c];
(* vmovdqa 0x240(%rdi),%ymm8                       #! EA = L0x7fffffff4700; Value = 0x0000000000000003; PC = 0x5555555730d0 *)
mov %ymm8 [L0x7fffffff4700, L0x7fffffff4704, L0x7fffffff4708, L0x7fffffff470c, L0x7fffffff4710, L0x7fffffff4714, L0x7fffffff4718, L0x7fffffff471c];
(* vmovdqa 0x2c0(%rdi),%ymm9                       #! EA = L0x7fffffff4780; Value = 0x0000000300000000; PC = 0x5555555730d8 *)
mov %ymm9 [L0x7fffffff4780, L0x7fffffff4784, L0x7fffffff4788, L0x7fffffff478c, L0x7fffffff4790, L0x7fffffff4794, L0x7fffffff4798, L0x7fffffff479c];
(* vmovdqa 0x340(%rdi),%ymm10                      #! EA = L0x7fffffff4800; Value = 0xfffffffc00000004; PC = 0x5555555730e0 *)
mov %ymm10 [L0x7fffffff4800, L0x7fffffff4804, L0x7fffffff4808, L0x7fffffff480c, L0x7fffffff4810, L0x7fffffff4814, L0x7fffffff4818, L0x7fffffff481c];
(* vmovdqa 0x3c0(%rdi),%ymm11                      #! EA = L0x7fffffff4880; Value = 0xfffffffd00000000; PC = 0x5555555730e8 *)
mov %ymm11 [L0x7fffffff4880, L0x7fffffff4884, L0x7fffffff4888, L0x7fffffff488c, L0x7fffffff4890, L0x7fffffff4894, L0x7fffffff4898, L0x7fffffff489c];
(* vpmuldq %ymm1,%ymm8,%ymm13                      #! PC = 0x5555555730f0 *)
mull %ymm13_h %ymm13_l (%ymm8[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm8,%ymm12                          #! PC = 0x5555555730f5 *)
mov %ymm12 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x5555555730fa *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm8,%ymm8                       #! PC = 0x5555555730ff *)
mull %ymm8_h %ymm8_l (%ymm8[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm8_ls@sint32[4] %ymm8_l;
mov %ymm8 [%ymm8_ls[0], %ymm8_h[0], %ymm8_ls[1], %ymm8_h[1], %ymm8_ls[2], %ymm8_h[2], %ymm8_ls[3], %ymm8_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555573104 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573109 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x55555557310e *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm8_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm8_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm8,%ymm8                           #! PC = 0x555555573113 *)
mov %ymm8 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm8,%ymm8               #! PC = 0x555555573118 *)
mov %ymm8 [%ymm8[0], %ymm12[1], %ymm8[2], %ymm12[3], %ymm8[4], %ymm12[5], %ymm8[6], %ymm12[7]];
(* vpsubd %ymm8,%ymm4,%ymm12                       #! PC = 0x55555557311e *)
sub %ymm12 %ymm4 %ymm8;
(* vpaddd %ymm8,%ymm4,%ymm4                        #! PC = 0x555555573123 *)
add %ymm4 %ymm4 %ymm8;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555573128 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x55555557312d *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm8                      #! PC = 0x555555573133 *)
add %ymm8 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm4,%ymm4                       #! PC = 0x555555573138 *)
sub %ymm4 %ymm4 %ymm13;
(* vpmuldq %ymm1,%ymm9,%ymm13                      #! PC = 0x55555557313d *)
mull %ymm13_h %ymm13_l (%ymm9[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm9,%ymm12                          #! PC = 0x555555573142 *)
mov %ymm12 (%ymm9[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555573147 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm9,%ymm9                       #! PC = 0x55555557314c *)
mull %ymm9_h %ymm9_l (%ymm9[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm9_ls@sint32[4] %ymm9_l;
mov %ymm9 [%ymm9_ls[0], %ymm9_h[0], %ymm9_ls[1], %ymm9_h[1], %ymm9_ls[2], %ymm9_h[2], %ymm9_ls[3], %ymm9_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555573151 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573156 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x55555557315b *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm9_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm9_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm9,%ymm9                           #! PC = 0x555555573160 *)
mov %ymm9 (%ymm9[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm9,%ymm9               #! PC = 0x555555573165 *)
mov %ymm9 [%ymm9[0], %ymm12[1], %ymm9[2], %ymm12[3], %ymm9[4], %ymm12[5], %ymm9[6], %ymm12[7]];
(* vpsubd %ymm9,%ymm5,%ymm12                       #! PC = 0x55555557316b *)
sub %ymm12 %ymm5 %ymm9;
(* vpaddd %ymm9,%ymm5,%ymm5                        #! PC = 0x555555573170 *)
add %ymm5 %ymm5 %ymm9;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555573175 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x55555557317a *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm9                      #! PC = 0x555555573180 *)
add %ymm9 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm5,%ymm5                       #! PC = 0x555555573185 *)
sub %ymm5 %ymm5 %ymm13;
(* vpmuldq %ymm1,%ymm10,%ymm13                     #! PC = 0x55555557318a *)
mull %ymm13_h %ymm13_l (%ymm10[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm10,%ymm12                         #! PC = 0x55555557318f *)
mov %ymm12 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555573194 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm10,%ymm10                     #! PC = 0x555555573199 *)
mull %ymm10_h %ymm10_l (%ymm10[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm10_ls@sint32[4] %ymm10_l;
mov %ymm10 [%ymm10_ls[0], %ymm10_h[0], %ymm10_ls[1], %ymm10_h[1], %ymm10_ls[2], %ymm10_h[2], %ymm10_ls[3], %ymm10_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x55555557319e *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555731a3 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555731a8 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm10_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm10_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm10,%ymm10                         #! PC = 0x5555555731ad *)
mov %ymm10 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm10,%ymm10             #! PC = 0x5555555731b2 *)
mov %ymm10 [%ymm10[0], %ymm12[1], %ymm10[2], %ymm12[3], %ymm10[4], %ymm12[5], %ymm10[6], %ymm12[7]];
(* vpsubd %ymm10,%ymm6,%ymm12                      #! PC = 0x5555555731b8 *)
sub %ymm12 %ymm6 %ymm10;
(* vpaddd %ymm10,%ymm6,%ymm6                       #! PC = 0x5555555731bd *)
add %ymm6 %ymm6 %ymm10;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555731c2 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555731c7 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm10                     #! PC = 0x5555555731cd *)
add %ymm10 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm6,%ymm6                       #! PC = 0x5555555731d2 *)
sub %ymm6 %ymm6 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x5555555731d7 *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x5555555731dc *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x5555555731e1 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x5555555731e6 *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x5555555731eb *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555731f0 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555731f5 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x5555555731fa *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x5555555731ff *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm7,%ymm12                      #! PC = 0x555555573205 *)
sub %ymm12 %ymm7 %ymm11;
(* vpaddd %ymm11,%ymm7,%ymm7                       #! PC = 0x55555557320a *)
add %ymm7 %ymm7 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x55555557320f *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573214 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x55555557321a *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm7,%ymm7                       #! PC = 0x55555557321f *)
sub %ymm7 %ymm7 %ymm13;
(* vpbroadcastd 0x88(%rsi),%ymm1                   #! EA = L0x555555579e68; Value = 0x8d1875038cf87102; PC = 0x555555573224 *)
broadcast %ymm1 8 [L0x555555579e68];
(* vpbroadcastd 0x528(%rsi),%ymm2                  #! EA = L0x55555557a308; Value = 0xfff81503ffd83102; PC = 0x55555557322d *)
broadcast %ymm2 8 [L0x55555557a308];
(* vpmuldq %ymm1,%ymm6,%ymm13                      #! PC = 0x555555573236 *)
mull %ymm13_h %ymm13_l (%ymm6[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm6,%ymm12                          #! PC = 0x55555557323b *)
mov %ymm12 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x55555557323f *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm6,%ymm6                       #! PC = 0x555555573244 *)
mull %ymm6_h %ymm6_l (%ymm6[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm6_ls@sint32[4] %ymm6_l;
mov %ymm6 [%ymm6_ls[0], %ymm6_h[0], %ymm6_ls[1], %ymm6_h[1], %ymm6_ls[2], %ymm6_h[2], %ymm6_ls[3], %ymm6_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555573249 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x55555557324e *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555573253 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm6_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm6_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm6,%ymm6                           #! PC = 0x555555573258 *)
mov %ymm6 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm6,%ymm6               #! PC = 0x55555557325c *)
mov %ymm6 [%ymm6[0], %ymm12[1], %ymm6[2], %ymm12[3], %ymm6[4], %ymm12[5], %ymm6[6], %ymm12[7]];
(* vpsubd %ymm6,%ymm4,%ymm12                       #! PC = 0x555555573262 *)
sub %ymm12 %ymm4 %ymm6;
(* vpaddd %ymm6,%ymm4,%ymm4                        #! PC = 0x555555573266 *)
add %ymm4 %ymm4 %ymm6;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x55555557326a *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x55555557326f *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm6                      #! PC = 0x555555573275 *)
add %ymm6 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm4,%ymm4                       #! PC = 0x55555557327a *)
sub %ymm4 %ymm4 %ymm13;
(* vpmuldq %ymm1,%ymm7,%ymm13                      #! PC = 0x55555557327f *)
mull %ymm13_h %ymm13_l (%ymm7[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm7,%ymm12                          #! PC = 0x555555573284 *)
mov %ymm12 (%ymm7[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555573288 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm7,%ymm7                       #! PC = 0x55555557328d *)
mull %ymm7_h %ymm7_l (%ymm7[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm7_ls@sint32[4] %ymm7_l;
mov %ymm7 [%ymm7_ls[0], %ymm7_h[0], %ymm7_ls[1], %ymm7_h[1], %ymm7_ls[2], %ymm7_h[2], %ymm7_ls[3], %ymm7_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555573292 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573297 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x55555557329c *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm7_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm7_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm7,%ymm7                           #! PC = 0x5555555732a1 *)
mov %ymm7 (%ymm7[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm7,%ymm7               #! PC = 0x5555555732a5 *)
mov %ymm7 [%ymm7[0], %ymm12[1], %ymm7[2], %ymm12[3], %ymm7[4], %ymm12[5], %ymm7[6], %ymm12[7]];
(* vpsubd %ymm7,%ymm5,%ymm12                       #! PC = 0x5555555732ab *)
sub %ymm12 %ymm5 %ymm7;
(* vpaddd %ymm7,%ymm5,%ymm5                        #! PC = 0x5555555732af *)
add %ymm5 %ymm5 %ymm7;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555732b3 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555732b8 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm7                      #! PC = 0x5555555732be *)
add %ymm7 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm5,%ymm5                       #! PC = 0x5555555732c3 *)
sub %ymm5 %ymm5 %ymm13;
(* vpbroadcastd 0x8c(%rsi),%ymm1                   #! EA = L0x555555579e6c; Value = 0x61cc1e448d187503; PC = 0x5555555732c8 *)
broadcast %ymm1 8 [L0x555555579e6c];
(* vpbroadcastd 0x52c(%rsi),%ymm2                  #! EA = L0x55555557a30c; Value = 0x00039e44fff81503; PC = 0x5555555732d1 *)
broadcast %ymm2 8 [L0x55555557a30c];
(* vpmuldq %ymm1,%ymm10,%ymm13                     #! PC = 0x5555555732da *)
mull %ymm13_h %ymm13_l (%ymm10[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm10,%ymm12                         #! PC = 0x5555555732df *)
mov %ymm12 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x5555555732e4 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm10,%ymm10                     #! PC = 0x5555555732e9 *)
mull %ymm10_h %ymm10_l (%ymm10[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm10_ls@sint32[4] %ymm10_l;
mov %ymm10 [%ymm10_ls[0], %ymm10_h[0], %ymm10_ls[1], %ymm10_h[1], %ymm10_ls[2], %ymm10_h[2], %ymm10_ls[3], %ymm10_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x5555555732ee *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555732f3 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555732f8 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm10_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm10_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm10,%ymm10                         #! PC = 0x5555555732fd *)
mov %ymm10 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm10,%ymm10             #! PC = 0x555555573302 *)
mov %ymm10 [%ymm10[0], %ymm12[1], %ymm10[2], %ymm12[3], %ymm10[4], %ymm12[5], %ymm10[6], %ymm12[7]];
(* vpsubd %ymm10,%ymm8,%ymm12                      #! PC = 0x555555573308 *)
sub %ymm12 %ymm8 %ymm10;
(* vpaddd %ymm10,%ymm8,%ymm8                       #! PC = 0x55555557330d *)
add %ymm8 %ymm8 %ymm10;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555573312 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573317 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm10                     #! PC = 0x55555557331d *)
add %ymm10 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm8,%ymm8                       #! PC = 0x555555573322 *)
sub %ymm8 %ymm8 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x555555573327 *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x55555557332c *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555573331 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x555555573336 *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x55555557333b *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573340 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555573345 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x55555557334a *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x55555557334f *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm9,%ymm12                      #! PC = 0x555555573355 *)
sub %ymm12 %ymm9 %ymm11;
(* vpaddd %ymm11,%ymm9,%ymm9                       #! PC = 0x55555557335a *)
add %ymm9 %ymm9 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x55555557335f *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573364 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x55555557336a *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm9,%ymm9                       #! PC = 0x55555557336f *)
sub %ymm9 %ymm9 %ymm13;
(* vmovdqa %ymm4,0x40(%rdi)                        #! EA = L0x7fffffff4500; PC = 0x555555573374 *)
mov [L0x7fffffff4500, L0x7fffffff4504, L0x7fffffff4508, L0x7fffffff450c, L0x7fffffff4510, L0x7fffffff4514, L0x7fffffff4518, L0x7fffffff451c] %ymm4;
(* vmovdqa %ymm5,0xc0(%rdi)                        #! EA = L0x7fffffff4580; PC = 0x555555573379 *)
mov [L0x7fffffff4580, L0x7fffffff4584, L0x7fffffff4588, L0x7fffffff458c, L0x7fffffff4590, L0x7fffffff4594, L0x7fffffff4598, L0x7fffffff459c] %ymm5;
(* vmovdqa %ymm6,0x140(%rdi)                       #! EA = L0x7fffffff4600; PC = 0x555555573381 *)
mov [L0x7fffffff4600, L0x7fffffff4604, L0x7fffffff4608, L0x7fffffff460c, L0x7fffffff4610, L0x7fffffff4614, L0x7fffffff4618, L0x7fffffff461c] %ymm6;
(* vmovdqa %ymm7,0x1c0(%rdi)                       #! EA = L0x7fffffff4680; PC = 0x555555573389 *)
mov [L0x7fffffff4680, L0x7fffffff4684, L0x7fffffff4688, L0x7fffffff468c, L0x7fffffff4690, L0x7fffffff4694, L0x7fffffff4698, L0x7fffffff469c] %ymm7;
(* vmovdqa %ymm8,0x240(%rdi)                       #! EA = L0x7fffffff4700; PC = 0x555555573391 *)
mov [L0x7fffffff4700, L0x7fffffff4704, L0x7fffffff4708, L0x7fffffff470c, L0x7fffffff4710, L0x7fffffff4714, L0x7fffffff4718, L0x7fffffff471c] %ymm8;
(* vmovdqa %ymm9,0x2c0(%rdi)                       #! EA = L0x7fffffff4780; PC = 0x555555573399 *)
mov [L0x7fffffff4780, L0x7fffffff4784, L0x7fffffff4788, L0x7fffffff478c, L0x7fffffff4790, L0x7fffffff4794, L0x7fffffff4798, L0x7fffffff479c] %ymm9;
(* vmovdqa %ymm10,0x340(%rdi)                      #! EA = L0x7fffffff4800; PC = 0x5555555733a1 *)
mov [L0x7fffffff4800, L0x7fffffff4804, L0x7fffffff4808, L0x7fffffff480c, L0x7fffffff4810, L0x7fffffff4814, L0x7fffffff4818, L0x7fffffff481c] %ymm10;
(* vmovdqa %ymm11,0x3c0(%rdi)                      #! EA = L0x7fffffff4880; PC = 0x5555555733a9 *)
mov [L0x7fffffff4880, L0x7fffffff4884, L0x7fffffff4888, L0x7fffffff488c, L0x7fffffff4890, L0x7fffffff4894, L0x7fffffff4898, L0x7fffffff489c] %ymm11;
(* vpbroadcastd 0x84(%rsi),%ymm1                   #! EA = L0x555555579e64; Value = 0x8cf871026d1f44f7; PC = 0x5555555733b1 *)
broadcast %ymm1 8 [L0x555555579e64];
(* vpbroadcastd 0x524(%rsi),%ymm2                  #! EA = L0x55555557a304; Value = 0xffd83102000064f7; PC = 0x5555555733ba *)
broadcast %ymm2 8 [L0x55555557a304];
(* vmovdqa 0x60(%rdi),%ymm4                        #! EA = L0x7fffffff4520; Value = 0xfffffffd00000004; PC = 0x5555555733c3 *)
mov %ymm4 [L0x7fffffff4520, L0x7fffffff4524, L0x7fffffff4528, L0x7fffffff452c, L0x7fffffff4530, L0x7fffffff4534, L0x7fffffff4538, L0x7fffffff453c];
(* vmovdqa 0xe0(%rdi),%ymm5                        #! EA = L0x7fffffff45a0; Value = 0xfffffffe00000004; PC = 0x5555555733c8 *)
mov %ymm5 [L0x7fffffff45a0, L0x7fffffff45a4, L0x7fffffff45a8, L0x7fffffff45ac, L0x7fffffff45b0, L0x7fffffff45b4, L0x7fffffff45b8, L0x7fffffff45bc];
(* vmovdqa 0x160(%rdi),%ymm6                       #! EA = L0x7fffffff4620; Value = 0xfffffffe00000003; PC = 0x5555555733d0 *)
mov %ymm6 [L0x7fffffff4620, L0x7fffffff4624, L0x7fffffff4628, L0x7fffffff462c, L0x7fffffff4630, L0x7fffffff4634, L0x7fffffff4638, L0x7fffffff463c];
(* vmovdqa 0x1e0(%rdi),%ymm7                       #! EA = L0x7fffffff46a0; Value = 0xffffffffffffffff; PC = 0x5555555733d8 *)
mov %ymm7 [L0x7fffffff46a0, L0x7fffffff46a4, L0x7fffffff46a8, L0x7fffffff46ac, L0x7fffffff46b0, L0x7fffffff46b4, L0x7fffffff46b8, L0x7fffffff46bc];
(* vmovdqa 0x260(%rdi),%ymm8                       #! EA = L0x7fffffff4720; Value = 0x00000000fffffffd; PC = 0x5555555733e0 *)
mov %ymm8 [L0x7fffffff4720, L0x7fffffff4724, L0x7fffffff4728, L0x7fffffff472c, L0x7fffffff4730, L0x7fffffff4734, L0x7fffffff4738, L0x7fffffff473c];
(* vmovdqa 0x2e0(%rdi),%ymm9                       #! EA = L0x7fffffff47a0; Value = 0x0000000300000000; PC = 0x5555555733e8 *)
mov %ymm9 [L0x7fffffff47a0, L0x7fffffff47a4, L0x7fffffff47a8, L0x7fffffff47ac, L0x7fffffff47b0, L0x7fffffff47b4, L0x7fffffff47b8, L0x7fffffff47bc];
(* vmovdqa 0x360(%rdi),%ymm10                      #! EA = L0x7fffffff4820; Value = 0xfffffffe00000000; PC = 0x5555555733f0 *)
mov %ymm10 [L0x7fffffff4820, L0x7fffffff4824, L0x7fffffff4828, L0x7fffffff482c, L0x7fffffff4830, L0x7fffffff4834, L0x7fffffff4838, L0x7fffffff483c];
(* vmovdqa 0x3e0(%rdi),%ymm11                      #! EA = L0x7fffffff48a0; Value = 0x00000001fffffffd; PC = 0x5555555733f8 *)
mov %ymm11 [L0x7fffffff48a0, L0x7fffffff48a4, L0x7fffffff48a8, L0x7fffffff48ac, L0x7fffffff48b0, L0x7fffffff48b4, L0x7fffffff48b8, L0x7fffffff48bc];
(* vpmuldq %ymm1,%ymm8,%ymm13                      #! PC = 0x555555573400 *)
mull %ymm13_h %ymm13_l (%ymm8[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm8,%ymm12                          #! PC = 0x555555573405 *)
mov %ymm12 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x55555557340a *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm8,%ymm8                       #! PC = 0x55555557340f *)
mull %ymm8_h %ymm8_l (%ymm8[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm8_ls@sint32[4] %ymm8_l;
mov %ymm8 [%ymm8_ls[0], %ymm8_h[0], %ymm8_ls[1], %ymm8_h[1], %ymm8_ls[2], %ymm8_h[2], %ymm8_ls[3], %ymm8_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555573414 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573419 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x55555557341e *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm8_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm8_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm8,%ymm8                           #! PC = 0x555555573423 *)
mov %ymm8 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm8,%ymm8               #! PC = 0x555555573428 *)
mov %ymm8 [%ymm8[0], %ymm12[1], %ymm8[2], %ymm12[3], %ymm8[4], %ymm12[5], %ymm8[6], %ymm12[7]];
(* vpsubd %ymm8,%ymm4,%ymm12                       #! PC = 0x55555557342e *)
sub %ymm12 %ymm4 %ymm8;
(* vpaddd %ymm8,%ymm4,%ymm4                        #! PC = 0x555555573433 *)
add %ymm4 %ymm4 %ymm8;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555573438 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x55555557343d *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm8                      #! PC = 0x555555573443 *)
add %ymm8 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm4,%ymm4                       #! PC = 0x555555573448 *)
sub %ymm4 %ymm4 %ymm13;
(* vpmuldq %ymm1,%ymm9,%ymm13                      #! PC = 0x55555557344d *)
mull %ymm13_h %ymm13_l (%ymm9[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm9,%ymm12                          #! PC = 0x555555573452 *)
mov %ymm12 (%ymm9[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555573457 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm9,%ymm9                       #! PC = 0x55555557345c *)
mull %ymm9_h %ymm9_l (%ymm9[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm9_ls@sint32[4] %ymm9_l;
mov %ymm9 [%ymm9_ls[0], %ymm9_h[0], %ymm9_ls[1], %ymm9_h[1], %ymm9_ls[2], %ymm9_h[2], %ymm9_ls[3], %ymm9_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555573461 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573466 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x55555557346b *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm9_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm9_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm9,%ymm9                           #! PC = 0x555555573470 *)
mov %ymm9 (%ymm9[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm9,%ymm9               #! PC = 0x555555573475 *)
mov %ymm9 [%ymm9[0], %ymm12[1], %ymm9[2], %ymm12[3], %ymm9[4], %ymm12[5], %ymm9[6], %ymm12[7]];
(* vpsubd %ymm9,%ymm5,%ymm12                       #! PC = 0x55555557347b *)
sub %ymm12 %ymm5 %ymm9;
(* vpaddd %ymm9,%ymm5,%ymm5                        #! PC = 0x555555573480 *)
add %ymm5 %ymm5 %ymm9;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555573485 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x55555557348a *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm9                      #! PC = 0x555555573490 *)
add %ymm9 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm5,%ymm5                       #! PC = 0x555555573495 *)
sub %ymm5 %ymm5 %ymm13;
(* vpmuldq %ymm1,%ymm10,%ymm13                     #! PC = 0x55555557349a *)
mull %ymm13_h %ymm13_l (%ymm10[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm10,%ymm12                         #! PC = 0x55555557349f *)
mov %ymm12 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x5555555734a4 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm10,%ymm10                     #! PC = 0x5555555734a9 *)
mull %ymm10_h %ymm10_l (%ymm10[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm10_ls@sint32[4] %ymm10_l;
mov %ymm10 [%ymm10_ls[0], %ymm10_h[0], %ymm10_ls[1], %ymm10_h[1], %ymm10_ls[2], %ymm10_h[2], %ymm10_ls[3], %ymm10_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x5555555734ae *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555734b3 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555734b8 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm10_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm10_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm10,%ymm10                         #! PC = 0x5555555734bd *)
mov %ymm10 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm10,%ymm10             #! PC = 0x5555555734c2 *)
mov %ymm10 [%ymm10[0], %ymm12[1], %ymm10[2], %ymm12[3], %ymm10[4], %ymm12[5], %ymm10[6], %ymm12[7]];
(* vpsubd %ymm10,%ymm6,%ymm12                      #! PC = 0x5555555734c8 *)
sub %ymm12 %ymm6 %ymm10;
(* vpaddd %ymm10,%ymm6,%ymm6                       #! PC = 0x5555555734cd *)
add %ymm6 %ymm6 %ymm10;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555734d2 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555734d7 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm10                     #! PC = 0x5555555734dd *)
add %ymm10 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm6,%ymm6                       #! PC = 0x5555555734e2 *)
sub %ymm6 %ymm6 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x5555555734e7 *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x5555555734ec *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x5555555734f1 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x5555555734f6 *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x5555555734fb *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573500 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555573505 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x55555557350a *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x55555557350f *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm7,%ymm12                      #! PC = 0x555555573515 *)
sub %ymm12 %ymm7 %ymm11;
(* vpaddd %ymm11,%ymm7,%ymm7                       #! PC = 0x55555557351a *)
add %ymm7 %ymm7 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x55555557351f *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573524 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x55555557352a *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm7,%ymm7                       #! PC = 0x55555557352f *)
sub %ymm7 %ymm7 %ymm13;
(* vpbroadcastd 0x88(%rsi),%ymm1                   #! EA = L0x555555579e68; Value = 0x8d1875038cf87102; PC = 0x555555573534 *)
broadcast %ymm1 8 [L0x555555579e68];
(* vpbroadcastd 0x528(%rsi),%ymm2                  #! EA = L0x55555557a308; Value = 0xfff81503ffd83102; PC = 0x55555557353d *)
broadcast %ymm2 8 [L0x55555557a308];
(* vpmuldq %ymm1,%ymm6,%ymm13                      #! PC = 0x555555573546 *)
mull %ymm13_h %ymm13_l (%ymm6[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm6,%ymm12                          #! PC = 0x55555557354b *)
mov %ymm12 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x55555557354f *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm6,%ymm6                       #! PC = 0x555555573554 *)
mull %ymm6_h %ymm6_l (%ymm6[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm6_ls@sint32[4] %ymm6_l;
mov %ymm6 [%ymm6_ls[0], %ymm6_h[0], %ymm6_ls[1], %ymm6_h[1], %ymm6_ls[2], %ymm6_h[2], %ymm6_ls[3], %ymm6_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555573559 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x55555557355e *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555573563 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm6_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm6_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm6,%ymm6                           #! PC = 0x555555573568 *)
mov %ymm6 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm6,%ymm6               #! PC = 0x55555557356c *)
mov %ymm6 [%ymm6[0], %ymm12[1], %ymm6[2], %ymm12[3], %ymm6[4], %ymm12[5], %ymm6[6], %ymm12[7]];
(* vpsubd %ymm6,%ymm4,%ymm12                       #! PC = 0x555555573572 *)
sub %ymm12 %ymm4 %ymm6;
(* vpaddd %ymm6,%ymm4,%ymm4                        #! PC = 0x555555573576 *)
add %ymm4 %ymm4 %ymm6;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x55555557357a *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x55555557357f *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm6                      #! PC = 0x555555573585 *)
add %ymm6 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm4,%ymm4                       #! PC = 0x55555557358a *)
sub %ymm4 %ymm4 %ymm13;
(* vpmuldq %ymm1,%ymm7,%ymm13                      #! PC = 0x55555557358f *)
mull %ymm13_h %ymm13_l (%ymm7[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm7,%ymm12                          #! PC = 0x555555573594 *)
mov %ymm12 (%ymm7[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555573598 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm7,%ymm7                       #! PC = 0x55555557359d *)
mull %ymm7_h %ymm7_l (%ymm7[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm7_ls@sint32[4] %ymm7_l;
mov %ymm7 [%ymm7_ls[0], %ymm7_h[0], %ymm7_ls[1], %ymm7_h[1], %ymm7_ls[2], %ymm7_h[2], %ymm7_ls[3], %ymm7_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x5555555735a2 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555735a7 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555735ac *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm7_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm7_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm7,%ymm7                           #! PC = 0x5555555735b1 *)
mov %ymm7 (%ymm7[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm7,%ymm7               #! PC = 0x5555555735b5 *)
mov %ymm7 [%ymm7[0], %ymm12[1], %ymm7[2], %ymm12[3], %ymm7[4], %ymm12[5], %ymm7[6], %ymm12[7]];
(* vpsubd %ymm7,%ymm5,%ymm12                       #! PC = 0x5555555735bb *)
sub %ymm12 %ymm5 %ymm7;
(* vpaddd %ymm7,%ymm5,%ymm5                        #! PC = 0x5555555735bf *)
add %ymm5 %ymm5 %ymm7;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555735c3 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555735c8 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm7                      #! PC = 0x5555555735ce *)
add %ymm7 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm5,%ymm5                       #! PC = 0x5555555735d3 *)
sub %ymm5 %ymm5 %ymm13;
(* vpbroadcastd 0x8c(%rsi),%ymm1                   #! EA = L0x555555579e6c; Value = 0x61cc1e448d187503; PC = 0x5555555735d8 *)
broadcast %ymm1 8 [L0x555555579e6c];
(* vpbroadcastd 0x52c(%rsi),%ymm2                  #! EA = L0x55555557a30c; Value = 0x00039e44fff81503; PC = 0x5555555735e1 *)
broadcast %ymm2 8 [L0x55555557a30c];
(* vpmuldq %ymm1,%ymm10,%ymm13                     #! PC = 0x5555555735ea *)
mull %ymm13_h %ymm13_l (%ymm10[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm10,%ymm12                         #! PC = 0x5555555735ef *)
mov %ymm12 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x5555555735f4 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm10,%ymm10                     #! PC = 0x5555555735f9 *)
mull %ymm10_h %ymm10_l (%ymm10[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm10_ls@sint32[4] %ymm10_l;
mov %ymm10 [%ymm10_ls[0], %ymm10_h[0], %ymm10_ls[1], %ymm10_h[1], %ymm10_ls[2], %ymm10_h[2], %ymm10_ls[3], %ymm10_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x5555555735fe *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573603 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555573608 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm10_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm10_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm10,%ymm10                         #! PC = 0x55555557360d *)
mov %ymm10 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm10,%ymm10             #! PC = 0x555555573612 *)
mov %ymm10 [%ymm10[0], %ymm12[1], %ymm10[2], %ymm12[3], %ymm10[4], %ymm12[5], %ymm10[6], %ymm12[7]];
(* vpsubd %ymm10,%ymm8,%ymm12                      #! PC = 0x555555573618 *)
sub %ymm12 %ymm8 %ymm10;
(* vpaddd %ymm10,%ymm8,%ymm8                       #! PC = 0x55555557361d *)
add %ymm8 %ymm8 %ymm10;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555573622 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573627 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm10                     #! PC = 0x55555557362d *)
add %ymm10 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm8,%ymm8                       #! PC = 0x555555573632 *)
sub %ymm8 %ymm8 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x555555573637 *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x55555557363c *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555573641 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x555555573646 *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x55555557364b *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573650 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555573655 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x55555557365a *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x55555557365f *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm9,%ymm12                      #! PC = 0x555555573665 *)
sub %ymm12 %ymm9 %ymm11;
(* vpaddd %ymm11,%ymm9,%ymm9                       #! PC = 0x55555557366a *)
add %ymm9 %ymm9 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x55555557366f *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573674 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x55555557367a *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm9,%ymm9                       #! PC = 0x55555557367f *)
sub %ymm9 %ymm9 %ymm13;
(* vmovdqa %ymm4,0x60(%rdi)                        #! EA = L0x7fffffff4520; PC = 0x555555573684 *)
mov [L0x7fffffff4520, L0x7fffffff4524, L0x7fffffff4528, L0x7fffffff452c, L0x7fffffff4530, L0x7fffffff4534, L0x7fffffff4538, L0x7fffffff453c] %ymm4;
(* vmovdqa %ymm5,0xe0(%rdi)                        #! EA = L0x7fffffff45a0; PC = 0x555555573689 *)
mov [L0x7fffffff45a0, L0x7fffffff45a4, L0x7fffffff45a8, L0x7fffffff45ac, L0x7fffffff45b0, L0x7fffffff45b4, L0x7fffffff45b8, L0x7fffffff45bc] %ymm5;
(* vmovdqa %ymm6,0x160(%rdi)                       #! EA = L0x7fffffff4620; PC = 0x555555573691 *)
mov [L0x7fffffff4620, L0x7fffffff4624, L0x7fffffff4628, L0x7fffffff462c, L0x7fffffff4630, L0x7fffffff4634, L0x7fffffff4638, L0x7fffffff463c] %ymm6;
(* vmovdqa %ymm7,0x1e0(%rdi)                       #! EA = L0x7fffffff46a0; PC = 0x555555573699 *)
mov [L0x7fffffff46a0, L0x7fffffff46a4, L0x7fffffff46a8, L0x7fffffff46ac, L0x7fffffff46b0, L0x7fffffff46b4, L0x7fffffff46b8, L0x7fffffff46bc] %ymm7;
(* vmovdqa %ymm8,0x260(%rdi)                       #! EA = L0x7fffffff4720; PC = 0x5555555736a1 *)
mov [L0x7fffffff4720, L0x7fffffff4724, L0x7fffffff4728, L0x7fffffff472c, L0x7fffffff4730, L0x7fffffff4734, L0x7fffffff4738, L0x7fffffff473c] %ymm8;
(* vmovdqa %ymm9,0x2e0(%rdi)                       #! EA = L0x7fffffff47a0; PC = 0x5555555736a9 *)
mov [L0x7fffffff47a0, L0x7fffffff47a4, L0x7fffffff47a8, L0x7fffffff47ac, L0x7fffffff47b0, L0x7fffffff47b4, L0x7fffffff47b8, L0x7fffffff47bc] %ymm9;
(* vmovdqa %ymm10,0x360(%rdi)                      #! EA = L0x7fffffff4820; PC = 0x5555555736b1 *)
mov [L0x7fffffff4820, L0x7fffffff4824, L0x7fffffff4828, L0x7fffffff482c, L0x7fffffff4830, L0x7fffffff4834, L0x7fffffff4838, L0x7fffffff483c] %ymm10;
(* vmovdqa %ymm11,0x3e0(%rdi)                      #! EA = L0x7fffffff48a0; PC = 0x5555555736b9 *)
mov [L0x7fffffff48a0, L0x7fffffff48a4, L0x7fffffff48a8, L0x7fffffff48ac, L0x7fffffff48b0, L0x7fffffff48b4, L0x7fffffff48b8, L0x7fffffff48bc] %ymm11;

(* vmovdqa (%rdi),%ymm4                            #! EA = L0x7fffffff44c0; Value = 0xffa0c29200063cff; PC = 0x5555555736c1 *)
mov %ymm4 [L0x7fffffff44c0, L0x7fffffff44c4, L0x7fffffff44c8, L0x7fffffff44cc, L0x7fffffff44d0, L0x7fffffff44d4, L0x7fffffff44d8, L0x7fffffff44dc];
(* vmovdqa 0x20(%rdi),%ymm5                        #! EA = L0x7fffffff44e0; Value = 0xffbaccf900431104; PC = 0x5555555736c5 *)
mov %ymm5 [L0x7fffffff44e0, L0x7fffffff44e4, L0x7fffffff44e8, L0x7fffffff44ec, L0x7fffffff44f0, L0x7fffffff44f4, L0x7fffffff44f8, L0x7fffffff44fc];
(* vmovdqa 0x40(%rdi),%ymm6                        #! EA = L0x7fffffff4500; Value = 0xffd3bfc4ffeeafde; PC = 0x5555555736ca *)
mov %ymm6 [L0x7fffffff4500, L0x7fffffff4504, L0x7fffffff4508, L0x7fffffff450c, L0x7fffffff4510, L0x7fffffff4514, L0x7fffffff4518, L0x7fffffff451c];
(* vmovdqa 0x60(%rdi),%ymm7                        #! EA = L0x7fffffff4520; Value = 0x001a0a5f00502634; PC = 0x5555555736cf *)
mov %ymm7 [L0x7fffffff4520, L0x7fffffff4524, L0x7fffffff4528, L0x7fffffff452c, L0x7fffffff4530, L0x7fffffff4534, L0x7fffffff4538, L0x7fffffff453c];
(* vmovdqa 0x80(%rdi),%ymm8                        #! EA = L0x7fffffff4540; Value = 0x00397563ff904891; PC = 0x5555555736d4 *)
mov %ymm8 [L0x7fffffff4540, L0x7fffffff4544, L0x7fffffff4548, L0x7fffffff454c, L0x7fffffff4550, L0x7fffffff4554, L0x7fffffff4558, L0x7fffffff455c];
(* vmovdqa 0xa0(%rdi),%ymm9                        #! EA = L0x7fffffff4560; Value = 0x0032bd370029acc8; PC = 0x5555555736dc *)
mov %ymm9 [L0x7fffffff4560, L0x7fffffff4564, L0x7fffffff4568, L0x7fffffff456c, L0x7fffffff4570, L0x7fffffff4574, L0x7fffffff4578, L0x7fffffff457c];
(* vmovdqa 0xc0(%rdi),%ymm10                       #! EA = L0x7fffffff4580; Value = 0xffa31498ffe6159a; PC = 0x5555555736e4 *)
mov %ymm10 [L0x7fffffff4580, L0x7fffffff4584, L0x7fffffff4588, L0x7fffffff458c, L0x7fffffff4590, L0x7fffffff4594, L0x7fffffff4598, L0x7fffffff459c];
(* vmovdqa 0xe0(%rdi),%ymm11                       #! EA = L0x7fffffff45a0; Value = 0xffdc4a04001a1a64; PC = 0x5555555736ec *)
mov %ymm11 [L0x7fffffff45a0, L0x7fffffff45a4, L0x7fffffff45a8, L0x7fffffff45ac, L0x7fffffff45b0, L0x7fffffff45b4, L0x7fffffff45b8, L0x7fffffff45bc];
(* vpbroadcastd 0x90(%rsi),%ymm1                   #! EA = L0x555555579e70; Value = 0x5817211861cc1e44; PC = 0x5555555736f4 *)
broadcast %ymm1 8 [L0x555555579e70];
(* vpbroadcastd 0x530(%rsi),%ymm2                  #! EA = L0x55555557a310; Value = 0xfff4211800039e44; PC = 0x5555555736fd *)
broadcast %ymm2 8 [L0x55555557a310];
(* vpmuldq %ymm1,%ymm8,%ymm13                      #! PC = 0x555555573706 *)
mull %ymm13_h %ymm13_l (%ymm8[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm8,%ymm12                          #! PC = 0x55555557370b *)
mov %ymm12 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555573710 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm8,%ymm8                       #! PC = 0x555555573715 *)
mull %ymm8_h %ymm8_l (%ymm8[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm8_ls@sint32[4] %ymm8_l;
mov %ymm8 [%ymm8_ls[0], %ymm8_h[0], %ymm8_ls[1], %ymm8_h[1], %ymm8_ls[2], %ymm8_h[2], %ymm8_ls[3], %ymm8_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x55555557371a *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x55555557371f *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555573724 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm8_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm8_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm8,%ymm8                           #! PC = 0x555555573729 *)
mov %ymm8 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm8,%ymm8               #! PC = 0x55555557372e *)
mov %ymm8 [%ymm8[0], %ymm12[1], %ymm8[2], %ymm12[3], %ymm8[4], %ymm12[5], %ymm8[6], %ymm12[7]];
(* vpsubd %ymm8,%ymm4,%ymm12                       #! PC = 0x555555573734 *)
sub %ymm12 %ymm4 %ymm8;
(* vpaddd %ymm8,%ymm4,%ymm4                        #! PC = 0x555555573739 *)
add %ymm4 %ymm4 %ymm8;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x55555557373e *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573743 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm8                      #! PC = 0x555555573749 *)
add %ymm8 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm4,%ymm4                       #! PC = 0x55555557374e *)
sub %ymm4 %ymm4 %ymm13;
(* vpmuldq %ymm1,%ymm9,%ymm13                      #! PC = 0x555555573753 *)
mull %ymm13_h %ymm13_l (%ymm9[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm9,%ymm12                          #! PC = 0x555555573758 *)
mov %ymm12 (%ymm9[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x55555557375d *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm9,%ymm9                       #! PC = 0x555555573762 *)
mull %ymm9_h %ymm9_l (%ymm9[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm9_ls@sint32[4] %ymm9_l;
mov %ymm9 [%ymm9_ls[0], %ymm9_h[0], %ymm9_ls[1], %ymm9_h[1], %ymm9_ls[2], %ymm9_h[2], %ymm9_ls[3], %ymm9_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555573767 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x55555557376c *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555573771 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm9_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm9_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm9,%ymm9                           #! PC = 0x555555573776 *)
mov %ymm9 (%ymm9[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm9,%ymm9               #! PC = 0x55555557377b *)
mov %ymm9 [%ymm9[0], %ymm12[1], %ymm9[2], %ymm12[3], %ymm9[4], %ymm12[5], %ymm9[6], %ymm12[7]];
(* vpsubd %ymm9,%ymm5,%ymm12                       #! PC = 0x555555573781 *)
sub %ymm12 %ymm5 %ymm9;
(* vpaddd %ymm9,%ymm5,%ymm5                        #! PC = 0x555555573786 *)
add %ymm5 %ymm5 %ymm9;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x55555557378b *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573790 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm9                      #! PC = 0x555555573796 *)
add %ymm9 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm5,%ymm5                       #! PC = 0x55555557379b *)
sub %ymm5 %ymm5 %ymm13;
(* vpmuldq %ymm1,%ymm10,%ymm13                     #! PC = 0x5555555737a0 *)
mull %ymm13_h %ymm13_l (%ymm10[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm10,%ymm12                         #! PC = 0x5555555737a5 *)
mov %ymm12 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x5555555737aa *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm10,%ymm10                     #! PC = 0x5555555737af *)
mull %ymm10_h %ymm10_l (%ymm10[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm10_ls@sint32[4] %ymm10_l;
mov %ymm10 [%ymm10_ls[0], %ymm10_h[0], %ymm10_ls[1], %ymm10_h[1], %ymm10_ls[2], %ymm10_h[2], %ymm10_ls[3], %ymm10_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x5555555737b4 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555737b9 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555737be *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm10_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm10_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm10,%ymm10                         #! PC = 0x5555555737c3 *)
mov %ymm10 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm10,%ymm10             #! PC = 0x5555555737c8 *)
mov %ymm10 [%ymm10[0], %ymm12[1], %ymm10[2], %ymm12[3], %ymm10[4], %ymm12[5], %ymm10[6], %ymm12[7]];
(* vpsubd %ymm10,%ymm6,%ymm12                      #! PC = 0x5555555737ce *)
sub %ymm12 %ymm6 %ymm10;
(* vpaddd %ymm10,%ymm6,%ymm6                       #! PC = 0x5555555737d3 *)
add %ymm6 %ymm6 %ymm10;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555737d8 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555737dd *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm10                     #! PC = 0x5555555737e3 *)
add %ymm10 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm6,%ymm6                       #! PC = 0x5555555737e8 *)
sub %ymm6 %ymm6 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x5555555737ed *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x5555555737f2 *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x5555555737f7 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x5555555737fc *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555573801 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573806 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x55555557380b *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x555555573810 *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x555555573815 *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm7,%ymm12                      #! PC = 0x55555557381b *)
sub %ymm12 %ymm7 %ymm11;
(* vpaddd %ymm11,%ymm7,%ymm7                       #! PC = 0x555555573820 *)
add %ymm7 %ymm7 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555573825 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x55555557382a *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x555555573830 *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm7,%ymm7                       #! PC = 0x555555573835 *)
sub %ymm7 %ymm7 %ymm13;
(* vperm2i128 $0x20,%ymm8,%ymm4,%ymm3              #! PC = 0x55555557383a *)
mov %ymm3 (%ymm4[0, 1, 2, 3] ++ %ymm8[0, 1, 2, 3]);
(* vperm2i128 $0x31,%ymm8,%ymm4,%ymm8              #! PC = 0x555555573840 *)
mov %ymm8 (%ymm4[4, 5, 6, 7] ++ %ymm8[4, 5, 6, 7]);
(* vperm2i128 $0x20,%ymm9,%ymm5,%ymm4              #! PC = 0x555555573846 *)
mov %ymm4 (%ymm5[0, 1, 2, 3] ++ %ymm9[0, 1, 2, 3]);
(* vperm2i128 $0x31,%ymm9,%ymm5,%ymm9              #! PC = 0x55555557384c *)
mov %ymm9 (%ymm5[4, 5, 6, 7] ++ %ymm9[4, 5, 6, 7]);
(* vperm2i128 $0x20,%ymm10,%ymm6,%ymm5             #! PC = 0x555555573852 *)
mov %ymm5 (%ymm6[0, 1, 2, 3] ++ %ymm10[0, 1, 2, 3]);
(* vperm2i128 $0x31,%ymm10,%ymm6,%ymm10            #! PC = 0x555555573858 *)
mov %ymm10 (%ymm6[4, 5, 6, 7] ++ %ymm10[4, 5, 6, 7]);
(* vperm2i128 $0x20,%ymm11,%ymm7,%ymm6             #! PC = 0x55555557385e *)
mov %ymm6 (%ymm7[0, 1, 2, 3] ++ %ymm11[0, 1, 2, 3]);
(* vperm2i128 $0x31,%ymm11,%ymm7,%ymm11            #! PC = 0x555555573864 *)
mov %ymm11 (%ymm7[4, 5, 6, 7] ++ %ymm11[4, 5, 6, 7]);

(* vmovdqa 0xa0(%rsi),%ymm1                        #! EA = L0x555555579e80; Value = 0x12613e2b12613e2b; PC = 0x55555557386a *)
mov %ymm1 [L0x555555579e80, L0x555555579e84, L0x555555579e88, L0x555555579e8c, L0x555555579e90, L0x555555579e94, L0x555555579e98, L0x555555579e9c];
(* vmovdqa 0x540(%rsi),%ymm2                       #! EA = L0x55555557a320; Value = 0x001bde2b001bde2b; PC = 0x555555573872 *)
mov %ymm2 [L0x55555557a320, L0x55555557a324, L0x55555557a328, L0x55555557a32c, L0x55555557a330, L0x55555557a334, L0x55555557a338, L0x55555557a33c];
(* vpmuldq %ymm1,%ymm5,%ymm13                      #! PC = 0x55555557387a *)
mull %ymm13_h %ymm13_l (%ymm5[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm5,%ymm12                          #! PC = 0x55555557387f *)
mov %ymm12 (%ymm5[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555573883 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm5,%ymm5                       #! PC = 0x555555573888 *)
mull %ymm5_h %ymm5_l (%ymm5[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm5_ls@sint32[4] %ymm5_l;
mov %ymm5 [%ymm5_ls[0], %ymm5_h[0], %ymm5_ls[1], %ymm5_h[1], %ymm5_ls[2], %ymm5_h[2], %ymm5_ls[3], %ymm5_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x55555557388d *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573892 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555573897 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm5_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm5_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm5,%ymm5                           #! PC = 0x55555557389c *)
mov %ymm5 (%ymm5[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm5,%ymm5               #! PC = 0x5555555738a0 *)
mov %ymm5 [%ymm5[0], %ymm12[1], %ymm5[2], %ymm12[3], %ymm5[4], %ymm12[5], %ymm5[6], %ymm12[7]];
(* vpsubd %ymm5,%ymm3,%ymm12                       #! PC = 0x5555555738a6 *)
sub %ymm12 %ymm3 %ymm5;
(* vpaddd %ymm5,%ymm3,%ymm3                        #! PC = 0x5555555738aa *)
add %ymm3 %ymm3 %ymm5;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555738ae *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555738b3 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm5                      #! PC = 0x5555555738b9 *)
add %ymm5 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm3,%ymm3                       #! PC = 0x5555555738be *)
sub %ymm3 %ymm3 %ymm13;
(* vpmuldq %ymm1,%ymm10,%ymm13                     #! PC = 0x5555555738c3 *)
mull %ymm13_h %ymm13_l (%ymm10[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm10,%ymm12                         #! PC = 0x5555555738c8 *)
mov %ymm12 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x5555555738cd *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm10,%ymm10                     #! PC = 0x5555555738d2 *)
mull %ymm10_h %ymm10_l (%ymm10[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm10_ls@sint32[4] %ymm10_l;
mov %ymm10 [%ymm10_ls[0], %ymm10_h[0], %ymm10_ls[1], %ymm10_h[1], %ymm10_ls[2], %ymm10_h[2], %ymm10_ls[3], %ymm10_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x5555555738d7 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555738dc *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555738e1 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm10_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm10_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm10,%ymm10                         #! PC = 0x5555555738e6 *)
mov %ymm10 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm10,%ymm10             #! PC = 0x5555555738eb *)
mov %ymm10 [%ymm10[0], %ymm12[1], %ymm10[2], %ymm12[3], %ymm10[4], %ymm12[5], %ymm10[6], %ymm12[7]];
(* vpsubd %ymm10,%ymm8,%ymm12                      #! PC = 0x5555555738f1 *)
sub %ymm12 %ymm8 %ymm10;
(* vpaddd %ymm10,%ymm8,%ymm8                       #! PC = 0x5555555738f6 *)
add %ymm8 %ymm8 %ymm10;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555738fb *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573900 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm10                     #! PC = 0x555555573906 *)
add %ymm10 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm8,%ymm8                       #! PC = 0x55555557390b *)
sub %ymm8 %ymm8 %ymm13;
(* vpmuldq %ymm1,%ymm6,%ymm13                      #! PC = 0x555555573910 *)
mull %ymm13_h %ymm13_l (%ymm6[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm6,%ymm12                          #! PC = 0x555555573915 *)
mov %ymm12 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555573919 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm6,%ymm6                       #! PC = 0x55555557391e *)
mull %ymm6_h %ymm6_l (%ymm6[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm6_ls@sint32[4] %ymm6_l;
mov %ymm6 [%ymm6_ls[0], %ymm6_h[0], %ymm6_ls[1], %ymm6_h[1], %ymm6_ls[2], %ymm6_h[2], %ymm6_ls[3], %ymm6_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555573923 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573928 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x55555557392d *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm6_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm6_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm6,%ymm6                           #! PC = 0x555555573932 *)
mov %ymm6 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm6,%ymm6               #! PC = 0x555555573936 *)
mov %ymm6 [%ymm6[0], %ymm12[1], %ymm6[2], %ymm12[3], %ymm6[4], %ymm12[5], %ymm6[6], %ymm12[7]];
(* vpsubd %ymm6,%ymm4,%ymm12                       #! PC = 0x55555557393c *)
sub %ymm12 %ymm4 %ymm6;
(* vpaddd %ymm6,%ymm4,%ymm4                        #! PC = 0x555555573940 *)
add %ymm4 %ymm4 %ymm6;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555573944 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573949 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm6                      #! PC = 0x55555557394f *)
add %ymm6 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm4,%ymm4                       #! PC = 0x555555573954 *)
sub %ymm4 %ymm4 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x555555573959 *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x55555557395e *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555573963 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x555555573968 *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x55555557396d *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573972 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555573977 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x55555557397c *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x555555573981 *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm9,%ymm12                      #! PC = 0x555555573987 *)
sub %ymm12 %ymm9 %ymm11;
(* vpaddd %ymm11,%ymm9,%ymm9                       #! PC = 0x55555557398c *)
add %ymm9 %ymm9 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555573991 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573996 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x55555557399c *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm9,%ymm9                       #! PC = 0x5555555739a1 *)
sub %ymm9 %ymm9 %ymm13;
(* vpunpcklqdq %ymm5,%ymm3,%ymm7                   #! PC = 0x5555555739a6 *)
mov %ymm7 [%ymm3[0], %ymm3[1], %ymm5[0], %ymm5[1], %ymm3[4], %ymm3[5], %ymm5[4], %ymm5[5]];
(* vpunpckhqdq %ymm5,%ymm3,%ymm5                   #! PC = 0x5555555739aa *)
mov %ymm5 [%ymm3[2], %ymm3[3], %ymm5[2], %ymm5[3], %ymm3[6], %ymm3[7], %ymm5[6], %ymm5[7]];
(* vpunpcklqdq %ymm10,%ymm8,%ymm3                  #! PC = 0x5555555739ae *)
mov %ymm3 [%ymm8[0], %ymm8[1], %ymm10[0], %ymm10[1], %ymm8[4], %ymm8[5], %ymm10[4], %ymm10[5]];
(* vpunpckhqdq %ymm10,%ymm8,%ymm10                 #! PC = 0x5555555739b3 *)
mov %ymm10 [%ymm8[2], %ymm8[3], %ymm10[2], %ymm10[3], %ymm8[6], %ymm8[7], %ymm10[6], %ymm10[7]];
(* vpunpcklqdq %ymm6,%ymm4,%ymm8                   #! PC = 0x5555555739b8 *)
mov %ymm8 [%ymm4[0], %ymm4[1], %ymm6[0], %ymm6[1], %ymm4[4], %ymm4[5], %ymm6[4], %ymm6[5]];
(* vpunpckhqdq %ymm6,%ymm4,%ymm6                   #! PC = 0x5555555739bc *)
mov %ymm6 [%ymm4[2], %ymm4[3], %ymm6[2], %ymm6[3], %ymm4[6], %ymm4[7], %ymm6[6], %ymm6[7]];
(* vpunpcklqdq %ymm11,%ymm9,%ymm4                  #! PC = 0x5555555739c0 *)
mov %ymm4 [%ymm9[0], %ymm9[1], %ymm11[0], %ymm11[1], %ymm9[4], %ymm9[5], %ymm11[4], %ymm11[5]];
(* vpunpckhqdq %ymm11,%ymm9,%ymm11                 #! PC = 0x5555555739c5 *)
mov %ymm11 [%ymm9[2], %ymm9[3], %ymm11[2], %ymm11[3], %ymm9[6], %ymm9[7], %ymm11[6], %ymm11[7]];

(* vmovdqa 0x120(%rsi),%ymm1                       #! EA = L0x555555579f00; Value = 0x66f4965866f49658; PC = 0x5555555739ca *)
mov %ymm1 [L0x555555579f00, L0x555555579f04, L0x555555579f08, L0x555555579f0c, L0x555555579f10, L0x555555579f14, L0x555555579f18, L0x555555579f1c];
(* vmovdqa 0x5c0(%rsi),%ymm2                       #! EA = L0x55555557a3a0; Value = 0x0029965800299658; PC = 0x5555555739d2 *)
mov %ymm2 [L0x55555557a3a0, L0x55555557a3a4, L0x55555557a3a8, L0x55555557a3ac, L0x55555557a3b0, L0x55555557a3b4, L0x55555557a3b8, L0x55555557a3bc];
(* vpmuldq %ymm1,%ymm8,%ymm13                      #! PC = 0x5555555739da *)
mull %ymm13_h %ymm13_l (%ymm8[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm8,%ymm12                          #! PC = 0x5555555739df *)
mov %ymm12 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x5555555739e4 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm8,%ymm8                       #! PC = 0x5555555739e9 *)
mull %ymm8_h %ymm8_l (%ymm8[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm8_ls@sint32[4] %ymm8_l;
mov %ymm8 [%ymm8_ls[0], %ymm8_h[0], %ymm8_ls[1], %ymm8_h[1], %ymm8_ls[2], %ymm8_h[2], %ymm8_ls[3], %ymm8_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x5555555739ee *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555739f3 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555739f8 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm8_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm8_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm8,%ymm8                           #! PC = 0x5555555739fd *)
mov %ymm8 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm8,%ymm8               #! PC = 0x555555573a02 *)
mov %ymm8 [%ymm8[0], %ymm12[1], %ymm8[2], %ymm12[3], %ymm8[4], %ymm12[5], %ymm8[6], %ymm12[7]];
(* vpsubd %ymm8,%ymm7,%ymm12                       #! PC = 0x555555573a08 *)
sub %ymm12 %ymm7 %ymm8;
(* vpaddd %ymm8,%ymm7,%ymm7                        #! PC = 0x555555573a0d *)
add %ymm7 %ymm7 %ymm8;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555573a12 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573a17 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm8                      #! PC = 0x555555573a1d *)
add %ymm8 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm7,%ymm7                       #! PC = 0x555555573a22 *)
sub %ymm7 %ymm7 %ymm13;
(* vpmuldq %ymm1,%ymm6,%ymm13                      #! PC = 0x555555573a27 *)
mull %ymm13_h %ymm13_l (%ymm6[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm6,%ymm12                          #! PC = 0x555555573a2c *)
mov %ymm12 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555573a30 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm6,%ymm6                       #! PC = 0x555555573a35 *)
mull %ymm6_h %ymm6_l (%ymm6[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm6_ls@sint32[4] %ymm6_l;
mov %ymm6 [%ymm6_ls[0], %ymm6_h[0], %ymm6_ls[1], %ymm6_h[1], %ymm6_ls[2], %ymm6_h[2], %ymm6_ls[3], %ymm6_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555573a3a *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573a3f *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555573a44 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm6_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm6_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm6,%ymm6                           #! PC = 0x555555573a49 *)
mov %ymm6 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm6,%ymm6               #! PC = 0x555555573a4d *)
mov %ymm6 [%ymm6[0], %ymm12[1], %ymm6[2], %ymm12[3], %ymm6[4], %ymm12[5], %ymm6[6], %ymm12[7]];
(* vpsubd %ymm6,%ymm5,%ymm12                       #! PC = 0x555555573a53 *)
sub %ymm12 %ymm5 %ymm6;
(* vpaddd %ymm6,%ymm5,%ymm5                        #! PC = 0x555555573a57 *)
add %ymm5 %ymm5 %ymm6;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555573a5b *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573a60 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm6                      #! PC = 0x555555573a66 *)
add %ymm6 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm5,%ymm5                       #! PC = 0x555555573a6b *)
sub %ymm5 %ymm5 %ymm13;
(* vpmuldq %ymm1,%ymm4,%ymm13                      #! PC = 0x555555573a70 *)
mull %ymm13_h %ymm13_l (%ymm4[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm4,%ymm12                          #! PC = 0x555555573a75 *)
mov %ymm12 (%ymm4[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555573a79 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm4,%ymm4                       #! PC = 0x555555573a7e *)
mull %ymm4_h %ymm4_l (%ymm4[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm4_ls@sint32[4] %ymm4_l;
mov %ymm4 [%ymm4_ls[0], %ymm4_h[0], %ymm4_ls[1], %ymm4_h[1], %ymm4_ls[2], %ymm4_h[2], %ymm4_ls[3], %ymm4_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555573a83 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573a88 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555573a8d *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm4_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm4_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm4,%ymm4                           #! PC = 0x555555573a92 *)
mov %ymm4 (%ymm4[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm4,%ymm4               #! PC = 0x555555573a96 *)
mov %ymm4 [%ymm4[0], %ymm12[1], %ymm4[2], %ymm12[3], %ymm4[4], %ymm12[5], %ymm4[6], %ymm12[7]];
(* vpsubd %ymm4,%ymm3,%ymm12                       #! PC = 0x555555573a9c *)
sub %ymm12 %ymm3 %ymm4;
(* vpaddd %ymm4,%ymm3,%ymm3                        #! PC = 0x555555573aa0 *)
add %ymm3 %ymm3 %ymm4;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555573aa4 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573aa9 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm4                      #! PC = 0x555555573aaf *)
add %ymm4 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm3,%ymm3                       #! PC = 0x555555573ab4 *)
sub %ymm3 %ymm3 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x555555573ab9 *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x555555573abe *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555573ac3 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x555555573ac8 *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555573acd *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573ad2 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555573ad7 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x555555573adc *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x555555573ae1 *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm10,%ymm12                     #! PC = 0x555555573ae7 *)
sub %ymm12 %ymm10 %ymm11;
(* vpaddd %ymm11,%ymm10,%ymm10                     #! PC = 0x555555573aec *)
add %ymm10 %ymm10 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555573af1 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573af6 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x555555573afc *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm10,%ymm10                     #! PC = 0x555555573b01 *)
sub %ymm10 %ymm10 %ymm13;
(* vmovsldup %ymm8,%ymm9                           #! PC = 0x555555573b06 *)
mov %ymm9 (%ymm8[0, 0, 2, 2, 4, 4, 6, 6]);
(* vpblendd $0xaa,%ymm9,%ymm7,%ymm9                #! PC = 0x555555573b0b *)
mov %ymm9 [%ymm7[0], %ymm9[1], %ymm7[2], %ymm9[3], %ymm7[4], %ymm9[5], %ymm7[6], %ymm9[7]];
(* vpsrlq $0x20,%ymm7,%ymm7                        #! PC = 0x555555573b11 *)
mov %ymm7 [%ymm7[1], 0@sint32, %ymm7[3], 0@sint32, %ymm7[5], 0@sint32, %ymm7[7], 0@sint32];
(* vpblendd $0xaa,%ymm8,%ymm7,%ymm8                #! PC = 0x555555573b16 *)
mov %ymm8 [%ymm7[0], %ymm8[1], %ymm7[2], %ymm8[3], %ymm7[4], %ymm8[5], %ymm7[6], %ymm8[7]];
(* vmovsldup %ymm6,%ymm7                           #! PC = 0x555555573b1c *)
mov %ymm7 (%ymm6[0, 0, 2, 2, 4, 4, 6, 6]);
(* vpblendd $0xaa,%ymm7,%ymm5,%ymm7                #! PC = 0x555555573b20 *)
mov %ymm7 [%ymm5[0], %ymm7[1], %ymm5[2], %ymm7[3], %ymm5[4], %ymm7[5], %ymm5[6], %ymm7[7]];
(* vpsrlq $0x20,%ymm5,%ymm5                        #! PC = 0x555555573b26 *)
mov %ymm5 [%ymm5[1], 0@sint32, %ymm5[3], 0@sint32, %ymm5[5], 0@sint32, %ymm5[7], 0@sint32];
(* vpblendd $0xaa,%ymm6,%ymm5,%ymm6                #! PC = 0x555555573b2b *)
mov %ymm6 [%ymm5[0], %ymm6[1], %ymm5[2], %ymm6[3], %ymm5[4], %ymm6[5], %ymm5[6], %ymm6[7]];
(* vmovsldup %ymm4,%ymm5                           #! PC = 0x555555573b31 *)
mov %ymm5 (%ymm4[0, 0, 2, 2, 4, 4, 6, 6]);
(* vpblendd $0xaa,%ymm5,%ymm3,%ymm5                #! PC = 0x555555573b35 *)
mov %ymm5 [%ymm3[0], %ymm5[1], %ymm3[2], %ymm5[3], %ymm3[4], %ymm5[5], %ymm3[6], %ymm5[7]];
(* vpsrlq $0x20,%ymm3,%ymm3                        #! PC = 0x555555573b3b *)
mov %ymm3 [%ymm3[1], 0@sint32, %ymm3[3], 0@sint32, %ymm3[5], 0@sint32, %ymm3[7], 0@sint32];
(* vpblendd $0xaa,%ymm4,%ymm3,%ymm4                #! PC = 0x555555573b40 *)
mov %ymm4 [%ymm3[0], %ymm4[1], %ymm3[2], %ymm4[3], %ymm3[4], %ymm4[5], %ymm3[6], %ymm4[7]];
(* vmovsldup %ymm11,%ymm3                          #! PC = 0x555555573b46 *)
mov %ymm3 (%ymm11[0, 0, 2, 2, 4, 4, 6, 6]);
(* vpblendd $0xaa,%ymm3,%ymm10,%ymm3               #! PC = 0x555555573b4b *)
mov %ymm3 [%ymm10[0], %ymm3[1], %ymm10[2], %ymm3[3], %ymm10[4], %ymm3[5], %ymm10[6], %ymm3[7]];
(* vpsrlq $0x20,%ymm10,%ymm10                      #! PC = 0x555555573b51 *)
mov %ymm10 [%ymm10[1], 0@sint32, %ymm10[3], 0@sint32, %ymm10[5], 0@sint32, %ymm10[7], 0@sint32];
(* vpblendd $0xaa,%ymm11,%ymm10,%ymm11             #! PC = 0x555555573b57 *)
mov %ymm11 [%ymm10[0], %ymm11[1], %ymm10[2], %ymm11[3], %ymm10[4], %ymm11[5], %ymm10[6], %ymm11[7]];

(* vmovdqa 0x1a0(%rsi),%ymm1                       #! EA = L0x555555579f80; Value = 0x9ec5762091f62a67; PC = 0x555555573b5d *)
mov %ymm1 [L0x555555579f80, L0x555555579f84, L0x555555579f88, L0x555555579f8c, L0x555555579f90, L0x555555579f94, L0x555555579f98, L0x555555579f9c];
(* vmovdqa 0x640(%rsi),%ymm2                       #! EA = L0x55555557a420; Value = 0x0001762000294a67; PC = 0x555555573b65 *)
mov %ymm2 [L0x55555557a420, L0x55555557a424, L0x55555557a428, L0x55555557a42c, L0x55555557a430, L0x55555557a434, L0x55555557a438, L0x55555557a43c];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x555555573b6d *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x555555573b72 *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm5,%ymm13                      #! PC = 0x555555573b76 *)
mull %ymm13_h %ymm13_l (%ymm5[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm5,%ymm12                          #! PC = 0x555555573b7b *)
mov %ymm12 (%ymm5[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555573b7f *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm5,%ymm5                       #! PC = 0x555555573b84 *)
mull %ymm5_h %ymm5_l (%ymm5[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm5_ls@sint32[4] %ymm5_l;
mov %ymm5 [%ymm5_ls[0], %ymm5_h[0], %ymm5_ls[1], %ymm5_h[1], %ymm5_ls[2], %ymm5_h[2], %ymm5_ls[3], %ymm5_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555573b89 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573b8e *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555573b93 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm5_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm5_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm5,%ymm5                           #! PC = 0x555555573b98 *)
mov %ymm5 (%ymm5[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm5,%ymm5               #! PC = 0x555555573b9c *)
mov %ymm5 [%ymm5[0], %ymm12[1], %ymm5[2], %ymm12[3], %ymm5[4], %ymm12[5], %ymm5[6], %ymm12[7]];
(* vpsubd %ymm5,%ymm9,%ymm12                       #! PC = 0x555555573ba2 *)
sub %ymm12 %ymm9 %ymm5;
(* vpaddd %ymm5,%ymm9,%ymm9                        #! PC = 0x555555573ba6 *)
add %ymm9 %ymm9 %ymm5;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555573baa *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573baf *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm5                      #! PC = 0x555555573bb5 *)
add %ymm5 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm9,%ymm9                       #! PC = 0x555555573bba *)
sub %ymm9 %ymm9 %ymm13;
(* vpmuldq %ymm1,%ymm4,%ymm13                      #! PC = 0x555555573bbf *)
mull %ymm13_h %ymm13_l (%ymm4[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm4,%ymm12                          #! PC = 0x555555573bc4 *)
mov %ymm12 (%ymm4[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555573bc8 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm4,%ymm4                       #! PC = 0x555555573bcd *)
mull %ymm4_h %ymm4_l (%ymm4[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm4_ls@sint32[4] %ymm4_l;
mov %ymm4 [%ymm4_ls[0], %ymm4_h[0], %ymm4_ls[1], %ymm4_h[1], %ymm4_ls[2], %ymm4_h[2], %ymm4_ls[3], %ymm4_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555573bd2 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573bd7 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555573bdc *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm4_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm4_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm4,%ymm4                           #! PC = 0x555555573be1 *)
mov %ymm4 (%ymm4[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm4,%ymm4               #! PC = 0x555555573be5 *)
mov %ymm4 [%ymm4[0], %ymm12[1], %ymm4[2], %ymm12[3], %ymm4[4], %ymm12[5], %ymm4[6], %ymm12[7]];
(* vpsubd %ymm4,%ymm8,%ymm12                       #! PC = 0x555555573beb *)
sub %ymm12 %ymm8 %ymm4;
(* vpaddd %ymm4,%ymm8,%ymm8                        #! PC = 0x555555573bef *)
add %ymm8 %ymm8 %ymm4;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555573bf3 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573bf8 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm4                      #! PC = 0x555555573bfe *)
add %ymm4 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm8,%ymm8                       #! PC = 0x555555573c03 *)
sub %ymm8 %ymm8 %ymm13;
(* vpmuldq %ymm1,%ymm3,%ymm13                      #! PC = 0x555555573c08 *)
mull %ymm13_h %ymm13_l (%ymm3[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm3,%ymm12                          #! PC = 0x555555573c0d *)
mov %ymm12 (%ymm3[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555573c11 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm3,%ymm3                       #! PC = 0x555555573c16 *)
mull %ymm3_h %ymm3_l (%ymm3[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm3_ls@sint32[4] %ymm3_l;
mov %ymm3 [%ymm3_ls[0], %ymm3_h[0], %ymm3_ls[1], %ymm3_h[1], %ymm3_ls[2], %ymm3_h[2], %ymm3_ls[3], %ymm3_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555573c1b *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573c20 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555573c25 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm3_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm3_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm3,%ymm3                           #! PC = 0x555555573c2a *)
mov %ymm3 (%ymm3[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm3,%ymm3               #! PC = 0x555555573c2e *)
mov %ymm3 [%ymm3[0], %ymm12[1], %ymm3[2], %ymm12[3], %ymm3[4], %ymm12[5], %ymm3[6], %ymm12[7]];
(* vpsubd %ymm3,%ymm7,%ymm12                       #! PC = 0x555555573c34 *)
sub %ymm12 %ymm7 %ymm3;
(* vpaddd %ymm3,%ymm7,%ymm7                        #! PC = 0x555555573c38 *)
add %ymm7 %ymm7 %ymm3;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555573c3c *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573c41 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm3                      #! PC = 0x555555573c47 *)
add %ymm3 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm7,%ymm7                       #! PC = 0x555555573c4c *)
sub %ymm7 %ymm7 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x555555573c51 *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x555555573c56 *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555573c5b *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x555555573c60 *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555573c65 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573c6a *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555573c6f *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x555555573c74 *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x555555573c79 *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm6,%ymm12                      #! PC = 0x555555573c7f *)
sub %ymm12 %ymm6 %ymm11;
(* vpaddd %ymm11,%ymm6,%ymm6                       #! PC = 0x555555573c84 *)
add %ymm6 %ymm6 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555573c89 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573c8e *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x555555573c94 *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm6,%ymm6                       #! PC = 0x555555573c99 *)
sub %ymm6 %ymm6 %ymm13;

(* vmovdqa 0x220(%rsi),%ymm1                       #! EA = L0x55555557a000; Value = 0x5081c1cfa220a6e5; PC = 0x555555573c9e *)
mov %ymm1 [L0x55555557a000, L0x55555557a004, L0x55555557a008, L0x55555557a00c, L0x55555557a010, L0x55555557a014, L0x55555557a018, L0x55555557a01c];
(* vmovdqa 0x6c0(%rsi),%ymm2                       #! EA = L0x55555557a4a0; Value = 0xffc7e1cfffc406e5; PC = 0x555555573ca6 *)
mov %ymm2 [L0x55555557a4a0, L0x55555557a4a4, L0x55555557a4a8, L0x55555557a4ac, L0x55555557a4b0, L0x55555557a4b4, L0x55555557a4b8, L0x55555557a4bc];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x555555573cae *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x555555573cb3 *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm7,%ymm13                      #! PC = 0x555555573cb7 *)
mull %ymm13_h %ymm13_l (%ymm7[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm7,%ymm12                          #! PC = 0x555555573cbc *)
mov %ymm12 (%ymm7[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555573cc0 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm7,%ymm7                       #! PC = 0x555555573cc5 *)
mull %ymm7_h %ymm7_l (%ymm7[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm7_ls@sint32[4] %ymm7_l;
mov %ymm7 [%ymm7_ls[0], %ymm7_h[0], %ymm7_ls[1], %ymm7_h[1], %ymm7_ls[2], %ymm7_h[2], %ymm7_ls[3], %ymm7_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555573cca *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573ccf *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555573cd4 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm7_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm7_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm7,%ymm7                           #! PC = 0x555555573cd9 *)
mov %ymm7 (%ymm7[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm7,%ymm7               #! PC = 0x555555573cdd *)
mov %ymm7 [%ymm7[0], %ymm12[1], %ymm7[2], %ymm12[3], %ymm7[4], %ymm12[5], %ymm7[6], %ymm12[7]];
(* vpsubd %ymm7,%ymm9,%ymm12                       #! PC = 0x555555573ce3 *)
sub %ymm12 %ymm9 %ymm7;
(* vpaddd %ymm7,%ymm9,%ymm9                        #! PC = 0x555555573ce7 *)
add %ymm9 %ymm9 %ymm7;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555573ceb *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573cf0 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm7                      #! PC = 0x555555573cf6 *)
add %ymm7 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm9,%ymm9                       #! PC = 0x555555573cfb *)
sub %ymm9 %ymm9 %ymm13;
(* vpmuldq %ymm1,%ymm6,%ymm13                      #! PC = 0x555555573d00 *)
mull %ymm13_h %ymm13_l (%ymm6[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm6,%ymm12                          #! PC = 0x555555573d05 *)
mov %ymm12 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555573d09 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm6,%ymm6                       #! PC = 0x555555573d0e *)
mull %ymm6_h %ymm6_l (%ymm6[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm6_ls@sint32[4] %ymm6_l;
mov %ymm6 [%ymm6_ls[0], %ymm6_h[0], %ymm6_ls[1], %ymm6_h[1], %ymm6_ls[2], %ymm6_h[2], %ymm6_ls[3], %ymm6_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555573d13 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573d18 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555573d1d *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm6_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm6_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm6,%ymm6                           #! PC = 0x555555573d22 *)
mov %ymm6 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm6,%ymm6               #! PC = 0x555555573d26 *)
mov %ymm6 [%ymm6[0], %ymm12[1], %ymm6[2], %ymm12[3], %ymm6[4], %ymm12[5], %ymm6[6], %ymm12[7]];
(* vpsubd %ymm6,%ymm8,%ymm12                       #! PC = 0x555555573d2c *)
sub %ymm12 %ymm8 %ymm6;
(* vpaddd %ymm6,%ymm8,%ymm8                        #! PC = 0x555555573d30 *)
add %ymm8 %ymm8 %ymm6;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555573d34 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573d39 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm6                      #! PC = 0x555555573d3f *)
add %ymm6 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm8,%ymm8                       #! PC = 0x555555573d44 *)
sub %ymm8 %ymm8 %ymm13;
(* vmovdqa 0x2a0(%rsi),%ymm1                       #! EA = L0x55555557a080; Value = 0x8a54b819d8f8cc81; PC = 0x555555573d49 *)
mov %ymm1 [L0x55555557a080, L0x55555557a084, L0x55555557a088, L0x55555557a08c, L0x55555557a090, L0x55555557a094, L0x55555557a098, L0x55555557a09c];
(* vmovdqa 0x740(%rsi),%ymm2                       #! EA = L0x55555557a520; Value = 0xffd19819ffe8ac81; PC = 0x555555573d51 *)
mov %ymm2 [L0x55555557a520, L0x55555557a524, L0x55555557a528, L0x55555557a52c, L0x55555557a530, L0x55555557a534, L0x55555557a538, L0x55555557a53c];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x555555573d59 *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x555555573d5e *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm3,%ymm13                      #! PC = 0x555555573d62 *)
mull %ymm13_h %ymm13_l (%ymm3[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm3,%ymm12                          #! PC = 0x555555573d67 *)
mov %ymm12 (%ymm3[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555573d6b *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm3,%ymm3                       #! PC = 0x555555573d70 *)
mull %ymm3_h %ymm3_l (%ymm3[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm3_ls@sint32[4] %ymm3_l;
mov %ymm3 [%ymm3_ls[0], %ymm3_h[0], %ymm3_ls[1], %ymm3_h[1], %ymm3_ls[2], %ymm3_h[2], %ymm3_ls[3], %ymm3_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555573d75 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573d7a *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555573d7f *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm3_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm3_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm3,%ymm3                           #! PC = 0x555555573d84 *)
mov %ymm3 (%ymm3[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm3,%ymm3               #! PC = 0x555555573d88 *)
mov %ymm3 [%ymm3[0], %ymm12[1], %ymm3[2], %ymm12[3], %ymm3[4], %ymm12[5], %ymm3[6], %ymm12[7]];
(* vpsubd %ymm3,%ymm5,%ymm12                       #! PC = 0x555555573d8e *)
sub %ymm12 %ymm5 %ymm3;
(* vpaddd %ymm3,%ymm5,%ymm5                        #! PC = 0x555555573d92 *)
add %ymm5 %ymm5 %ymm3;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555573d96 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573d9b *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm3                      #! PC = 0x555555573da1 *)
add %ymm3 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm5,%ymm5                       #! PC = 0x555555573da6 *)
sub %ymm5 %ymm5 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x555555573dab *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x555555573db0 *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555573db5 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x555555573dba *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555573dbf *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573dc4 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555573dc9 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x555555573dce *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x555555573dd3 *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm4,%ymm12                      #! PC = 0x555555573dd9 *)
sub %ymm12 %ymm4 %ymm11;
(* vpaddd %ymm11,%ymm4,%ymm4                       #! PC = 0x555555573dde *)
add %ymm4 %ymm4 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555573de3 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573de8 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x555555573dee *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm4,%ymm4                       #! PC = 0x555555573df3 *)
sub %ymm4 %ymm4 %ymm13;

(* vmovdqa 0x320(%rsi),%ymm1                       #! EA = L0x55555557a100; Value = 0xaebb3f72fff24a93; PC = 0x555555573df8 *)
mov %ymm1 [L0x55555557a100, L0x55555557a104, L0x55555557a108, L0x55555557a10c, L0x55555557a110, L0x55555557a114, L0x55555557a118, L0x55555557a11c];
(* vmovdqa 0x7c0(%rsi),%ymm2                       #! EA = L0x55555557a5a0; Value = 0xffccff72001fea93; PC = 0x555555573e00 *)
mov %ymm2 [L0x55555557a5a0, L0x55555557a5a4, L0x55555557a5a8, L0x55555557a5ac, L0x55555557a5b0, L0x55555557a5b4, L0x55555557a5b8, L0x55555557a5bc];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x555555573e08 *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x555555573e0d *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm8,%ymm13                      #! PC = 0x555555573e11 *)
mull %ymm13_h %ymm13_l (%ymm8[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm8,%ymm12                          #! PC = 0x555555573e16 *)
mov %ymm12 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555573e1b *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm8,%ymm8                       #! PC = 0x555555573e20 *)
mull %ymm8_h %ymm8_l (%ymm8[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm8_ls@sint32[4] %ymm8_l;
mov %ymm8 [%ymm8_ls[0], %ymm8_h[0], %ymm8_ls[1], %ymm8_h[1], %ymm8_ls[2], %ymm8_h[2], %ymm8_ls[3], %ymm8_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555573e25 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573e2a *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555573e2f *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm8_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm8_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm8,%ymm8                           #! PC = 0x555555573e34 *)
mov %ymm8 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm8,%ymm8               #! PC = 0x555555573e39 *)
mov %ymm8 [%ymm8[0], %ymm12[1], %ymm8[2], %ymm12[3], %ymm8[4], %ymm12[5], %ymm8[6], %ymm12[7]];
(* vpsubd %ymm8,%ymm9,%ymm12                       #! PC = 0x555555573e3f *)
sub %ymm12 %ymm9 %ymm8;
(* vpaddd %ymm8,%ymm9,%ymm9                        #! PC = 0x555555573e44 *)
add %ymm9 %ymm9 %ymm8;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555573e49 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573e4e *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm8                      #! PC = 0x555555573e54 *)
add %ymm8 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm9,%ymm9                       #! PC = 0x555555573e59 *)
sub %ymm9 %ymm9 %ymm13;
(* vmovdqa 0x3a0(%rsi),%ymm1                       #! EA = L0x55555557a180; Value = 0x36619dfb3b1f3f5a; PC = 0x555555573e5e *)
mov %ymm1 [L0x55555557a180, L0x55555557a184, L0x55555557a188, L0x55555557a18c, L0x55555557a190, L0x55555557a194, L0x55555557a198, L0x55555557a19c];
(* vmovdqa 0x840(%rsi),%ymm2                       #! EA = L0x55555557a620; Value = 0x00223dfb0033ff5a; PC = 0x555555573e66 *)
mov %ymm2 [L0x55555557a620, L0x55555557a624, L0x55555557a628, L0x55555557a62c, L0x55555557a630, L0x55555557a634, L0x55555557a638, L0x55555557a63c];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x555555573e6e *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x555555573e73 *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm6,%ymm13                      #! PC = 0x555555573e77 *)
mull %ymm13_h %ymm13_l (%ymm6[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm6,%ymm12                          #! PC = 0x555555573e7c *)
mov %ymm12 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555573e80 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm6,%ymm6                       #! PC = 0x555555573e85 *)
mull %ymm6_h %ymm6_l (%ymm6[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm6_ls@sint32[4] %ymm6_l;
mov %ymm6 [%ymm6_ls[0], %ymm6_h[0], %ymm6_ls[1], %ymm6_h[1], %ymm6_ls[2], %ymm6_h[2], %ymm6_ls[3], %ymm6_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555573e8a *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573e8f *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555573e94 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm6_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm6_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm6,%ymm6                           #! PC = 0x555555573e99 *)
mov %ymm6 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm6,%ymm6               #! PC = 0x555555573e9d *)
mov %ymm6 [%ymm6[0], %ymm12[1], %ymm6[2], %ymm12[3], %ymm6[4], %ymm12[5], %ymm6[6], %ymm12[7]];
(* vpsubd %ymm6,%ymm7,%ymm12                       #! PC = 0x555555573ea3 *)
sub %ymm12 %ymm7 %ymm6;
(* vpaddd %ymm6,%ymm7,%ymm7                        #! PC = 0x555555573ea7 *)
add %ymm7 %ymm7 %ymm6;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555573eab *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573eb0 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm6                      #! PC = 0x555555573eb6 *)
add %ymm6 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm7,%ymm7                       #! PC = 0x555555573ebb *)
sub %ymm7 %ymm7 %ymm13;
(* vmovdqa 0x420(%rsi),%ymm1                       #! EA = L0x55555557a200; Value = 0x01ce8b9f513dd8d4; PC = 0x555555573ec0 *)
mov %ymm1 [L0x55555557a200, L0x55555557a204, L0x55555557a208, L0x55555557a20c, L0x55555557a210, L0x55555557a214, L0x55555557a218, L0x55555557a21c];
(* vmovdqa 0x8c0(%rsi),%ymm2                       #! EA = L0x55555557a6a0; Value = 0xffdaab9f002358d4; PC = 0x555555573ec8 *)
mov %ymm2 [L0x55555557a6a0, L0x55555557a6a4, L0x55555557a6a8, L0x55555557a6ac, L0x55555557a6b0, L0x55555557a6b4, L0x55555557a6b8, L0x55555557a6bc];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x555555573ed0 *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x555555573ed5 *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm4,%ymm13                      #! PC = 0x555555573ed9 *)
mull %ymm13_h %ymm13_l (%ymm4[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm4,%ymm12                          #! PC = 0x555555573ede *)
mov %ymm12 (%ymm4[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555573ee2 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm4,%ymm4                       #! PC = 0x555555573ee7 *)
mull %ymm4_h %ymm4_l (%ymm4[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm4_ls@sint32[4] %ymm4_l;
mov %ymm4 [%ymm4_ls[0], %ymm4_h[0], %ymm4_ls[1], %ymm4_h[1], %ymm4_ls[2], %ymm4_h[2], %ymm4_ls[3], %ymm4_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555573eec *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573ef1 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555573ef6 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm4_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm4_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm4,%ymm4                           #! PC = 0x555555573efb *)
mov %ymm4 (%ymm4[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm4,%ymm4               #! PC = 0x555555573eff *)
mov %ymm4 [%ymm4[0], %ymm12[1], %ymm4[2], %ymm12[3], %ymm4[4], %ymm12[5], %ymm4[6], %ymm12[7]];
(* vpsubd %ymm4,%ymm5,%ymm12                       #! PC = 0x555555573f05 *)
sub %ymm12 %ymm5 %ymm4;
(* vpaddd %ymm4,%ymm5,%ymm5                        #! PC = 0x555555573f09 *)
add %ymm5 %ymm5 %ymm4;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555573f0d *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573f12 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm4                      #! PC = 0x555555573f18 *)
add %ymm4 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm5,%ymm5                       #! PC = 0x555555573f1d *)
sub %ymm5 %ymm5 %ymm13;
(* vmovdqa 0x4a0(%rsi),%ymm1                       #! EA = L0x55555557a280; Value = 0xab4de4222c7941f8; PC = 0x555555573f22 *)
mov %ymm1 [L0x55555557a280, L0x55555557a284, L0x55555557a288, L0x55555557a28c, L0x55555557a290, L0x55555557a294, L0x55555557a298, L0x55555557a29c];
(* vmovdqa 0x940(%rsi),%ymm2                       #! EA = L0x55555557a720; Value = 0xffc9a422003a41f8; PC = 0x555555573f2a *)
mov %ymm2 [L0x55555557a720, L0x55555557a724, L0x55555557a728, L0x55555557a72c, L0x55555557a730, L0x55555557a734, L0x55555557a738, L0x55555557a73c];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x555555573f32 *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x555555573f37 *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x555555573f3b *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x555555573f40 *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555573f45 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x555555573f4a *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555573f4f *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555573f54 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555573f59 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x555555573f5e *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x555555573f63 *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm3,%ymm12                      #! PC = 0x555555573f69 *)
sub %ymm12 %ymm3 %ymm11;
(* vpaddd %ymm11,%ymm3,%ymm3                       #! PC = 0x555555573f6e *)
add %ymm3 %ymm3 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555573f73 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555573f78 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x555555573f7e *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm3,%ymm3                       #! PC = 0x555555573f83 *)
sub %ymm3 %ymm3 %ymm13;
(* vmovdqa %ymm9,(%rdi)                            #! EA = L0x7fffffff44c0; PC = 0x555555573f88 *)
mov [L0x7fffffff44c0, L0x7fffffff44c4, L0x7fffffff44c8, L0x7fffffff44cc, L0x7fffffff44d0, L0x7fffffff44d4, L0x7fffffff44d8, L0x7fffffff44dc] %ymm9;
(* vmovdqa %ymm8,0x20(%rdi)                        #! EA = L0x7fffffff44e0; PC = 0x555555573f8c *)
mov [L0x7fffffff44e0, L0x7fffffff44e4, L0x7fffffff44e8, L0x7fffffff44ec, L0x7fffffff44f0, L0x7fffffff44f4, L0x7fffffff44f8, L0x7fffffff44fc] %ymm8;
(* vmovdqa %ymm7,0x40(%rdi)                        #! EA = L0x7fffffff4500; PC = 0x555555573f91 *)
mov [L0x7fffffff4500, L0x7fffffff4504, L0x7fffffff4508, L0x7fffffff450c, L0x7fffffff4510, L0x7fffffff4514, L0x7fffffff4518, L0x7fffffff451c] %ymm7;
(* vmovdqa %ymm6,0x60(%rdi)                        #! EA = L0x7fffffff4520; PC = 0x555555573f96 *)
mov [L0x7fffffff4520, L0x7fffffff4524, L0x7fffffff4528, L0x7fffffff452c, L0x7fffffff4530, L0x7fffffff4534, L0x7fffffff4538, L0x7fffffff453c] %ymm6;
(* vmovdqa %ymm5,0x80(%rdi)                        #! EA = L0x7fffffff4540; PC = 0x555555573f9b *)
mov [L0x7fffffff4540, L0x7fffffff4544, L0x7fffffff4548, L0x7fffffff454c, L0x7fffffff4550, L0x7fffffff4554, L0x7fffffff4558, L0x7fffffff455c] %ymm5;
(* vmovdqa %ymm4,0xa0(%rdi)                        #! EA = L0x7fffffff4560; PC = 0x555555573fa3 *)
mov [L0x7fffffff4560, L0x7fffffff4564, L0x7fffffff4568, L0x7fffffff456c, L0x7fffffff4570, L0x7fffffff4574, L0x7fffffff4578, L0x7fffffff457c] %ymm4;
(* vmovdqa %ymm3,0xc0(%rdi)                        #! EA = L0x7fffffff4580; PC = 0x555555573fab *)
mov [L0x7fffffff4580, L0x7fffffff4584, L0x7fffffff4588, L0x7fffffff458c, L0x7fffffff4590, L0x7fffffff4594, L0x7fffffff4598, L0x7fffffff459c] %ymm3;
(* vmovdqa %ymm11,0xe0(%rdi)                       #! EA = L0x7fffffff45a0; PC = 0x555555573fb3 *)
mov [L0x7fffffff45a0, L0x7fffffff45a4, L0x7fffffff45a8, L0x7fffffff45ac, L0x7fffffff45b0, L0x7fffffff45b4, L0x7fffffff45b8, L0x7fffffff45bc] %ymm11;

(* vmovdqa 0x100(%rdi),%ymm4                       #! EA = L0x7fffffff45c0; Value = 0x0013cd5c00453305; PC = 0x555555573fbb *)
mov %ymm4 [L0x7fffffff45c0, L0x7fffffff45c4, L0x7fffffff45c8, L0x7fffffff45cc, L0x7fffffff45d0, L0x7fffffff45d4, L0x7fffffff45d8, L0x7fffffff45dc];
(* vmovdqa 0x120(%rdi),%ymm5                       #! EA = L0x7fffffff45e0; Value = 0xfff9c2ff00043afa; PC = 0x555555573fc3 *)
mov %ymm5 [L0x7fffffff45e0, L0x7fffffff45e4, L0x7fffffff45e8, L0x7fffffff45ec, L0x7fffffff45f0, L0x7fffffff45f4, L0x7fffffff45f8, L0x7fffffff45fc];
(* vmovdqa 0x140(%rdi),%ymm6                       #! EA = L0x7fffffff4600; Value = 0x002c403cffca0432; PC = 0x555555573fcb *)
mov %ymm6 [L0x7fffffff4600, L0x7fffffff4604, L0x7fffffff4608, L0x7fffffff460c, L0x7fffffff4610, L0x7fffffff4614, L0x7fffffff4618, L0x7fffffff461c];
(* vmovdqa 0x160(%rdi),%ymm7                       #! EA = L0x7fffffff4620; Value = 0xffe5f59bfff725cc; PC = 0x555555573fd3 *)
mov %ymm7 [L0x7fffffff4620, L0x7fffffff4624, L0x7fffffff4628, L0x7fffffff462c, L0x7fffffff4630, L0x7fffffff4634, L0x7fffffff4638, L0x7fffffff463c];
(* vmovdqa 0x180(%rdi),%ymm8                       #! EA = L0x7fffffff4640; Value = 0xffc68a950002b36f; PC = 0x555555573fdb *)
mov %ymm8 [L0x7fffffff4640, L0x7fffffff4644, L0x7fffffff4648, L0x7fffffff464c, L0x7fffffff4650, L0x7fffffff4654, L0x7fffffff4658, L0x7fffffff465c];
(* vmovdqa 0x1a0(%rdi),%ymm9                       #! EA = L0x7fffffff4660; Value = 0x0018b2d500435736; PC = 0x555555573fe3 *)
mov %ymm9 [L0x7fffffff4660, L0x7fffffff4664, L0x7fffffff4668, L0x7fffffff466c, L0x7fffffff4670, L0x7fffffff4674, L0x7fffffff4678, L0x7fffffff467c];
(* vmovdqa 0x1c0(%rdi),%ymm10                      #! EA = L0x7fffffff4680; Value = 0x00159f720019ea66; PC = 0x555555573feb *)
mov %ymm10 [L0x7fffffff4680, L0x7fffffff4684, L0x7fffffff4688, L0x7fffffff468c, L0x7fffffff4690, L0x7fffffff4694, L0x7fffffff4698, L0x7fffffff469c];
(* vmovdqa 0x1e0(%rdi),%ymm11                      #! EA = L0x7fffffff46a0; Value = 0xffdc6a00ffe5e5a4; PC = 0x555555573ff3 *)
mov %ymm11 [L0x7fffffff46a0, L0x7fffffff46a4, L0x7fffffff46a8, L0x7fffffff46ac, L0x7fffffff46b0, L0x7fffffff46b4, L0x7fffffff46b8, L0x7fffffff46bc];
(* vpbroadcastd 0x94(%rsi),%ymm1                   #! EA = L0x555555579e74; Value = 0x6017a12858172118; PC = 0x555555573ffb *)
broadcast %ymm1 8 [L0x555555579e74];
(* vpbroadcastd 0x534(%rsi),%ymm2                  #! EA = L0x55555557a314; Value = 0xfff2a128fff42118; PC = 0x555555574004 *)
broadcast %ymm2 8 [L0x55555557a314];
(* vpmuldq %ymm1,%ymm8,%ymm13                      #! PC = 0x55555557400d *)
mull %ymm13_h %ymm13_l (%ymm8[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm8,%ymm12                          #! PC = 0x555555574012 *)
mov %ymm12 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555574017 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm8,%ymm8                       #! PC = 0x55555557401c *)
mull %ymm8_h %ymm8_l (%ymm8[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm8_ls@sint32[4] %ymm8_l;
mov %ymm8 [%ymm8_ls[0], %ymm8_h[0], %ymm8_ls[1], %ymm8_h[1], %ymm8_ls[2], %ymm8_h[2], %ymm8_ls[3], %ymm8_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555574021 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574026 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x55555557402b *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm8_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm8_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm8,%ymm8                           #! PC = 0x555555574030 *)
mov %ymm8 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm8,%ymm8               #! PC = 0x555555574035 *)
mov %ymm8 [%ymm8[0], %ymm12[1], %ymm8[2], %ymm12[3], %ymm8[4], %ymm12[5], %ymm8[6], %ymm12[7]];
(* vpsubd %ymm8,%ymm4,%ymm12                       #! PC = 0x55555557403b *)
sub %ymm12 %ymm4 %ymm8;
(* vpaddd %ymm8,%ymm4,%ymm4                        #! PC = 0x555555574040 *)
add %ymm4 %ymm4 %ymm8;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574045 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x55555557404a *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm8                      #! PC = 0x555555574050 *)
add %ymm8 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm4,%ymm4                       #! PC = 0x555555574055 *)
sub %ymm4 %ymm4 %ymm13;
(* vpmuldq %ymm1,%ymm9,%ymm13                      #! PC = 0x55555557405a *)
mull %ymm13_h %ymm13_l (%ymm9[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm9,%ymm12                          #! PC = 0x55555557405f *)
mov %ymm12 (%ymm9[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555574064 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm9,%ymm9                       #! PC = 0x555555574069 *)
mull %ymm9_h %ymm9_l (%ymm9[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm9_ls@sint32[4] %ymm9_l;
mov %ymm9 [%ymm9_ls[0], %ymm9_h[0], %ymm9_ls[1], %ymm9_h[1], %ymm9_ls[2], %ymm9_h[2], %ymm9_ls[3], %ymm9_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x55555557406e *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574073 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555574078 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm9_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm9_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm9,%ymm9                           #! PC = 0x55555557407d *)
mov %ymm9 (%ymm9[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm9,%ymm9               #! PC = 0x555555574082 *)
mov %ymm9 [%ymm9[0], %ymm12[1], %ymm9[2], %ymm12[3], %ymm9[4], %ymm12[5], %ymm9[6], %ymm12[7]];
(* vpsubd %ymm9,%ymm5,%ymm12                       #! PC = 0x555555574088 *)
sub %ymm12 %ymm5 %ymm9;
(* vpaddd %ymm9,%ymm5,%ymm5                        #! PC = 0x55555557408d *)
add %ymm5 %ymm5 %ymm9;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574092 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555574097 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm9                      #! PC = 0x55555557409d *)
add %ymm9 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm5,%ymm5                       #! PC = 0x5555555740a2 *)
sub %ymm5 %ymm5 %ymm13;
(* vpmuldq %ymm1,%ymm10,%ymm13                     #! PC = 0x5555555740a7 *)
mull %ymm13_h %ymm13_l (%ymm10[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm10,%ymm12                         #! PC = 0x5555555740ac *)
mov %ymm12 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x5555555740b1 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm10,%ymm10                     #! PC = 0x5555555740b6 *)
mull %ymm10_h %ymm10_l (%ymm10[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm10_ls@sint32[4] %ymm10_l;
mov %ymm10 [%ymm10_ls[0], %ymm10_h[0], %ymm10_ls[1], %ymm10_h[1], %ymm10_ls[2], %ymm10_h[2], %ymm10_ls[3], %ymm10_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x5555555740bb *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555740c0 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555740c5 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm10_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm10_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm10,%ymm10                         #! PC = 0x5555555740ca *)
mov %ymm10 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm10,%ymm10             #! PC = 0x5555555740cf *)
mov %ymm10 [%ymm10[0], %ymm12[1], %ymm10[2], %ymm12[3], %ymm10[4], %ymm12[5], %ymm10[6], %ymm12[7]];
(* vpsubd %ymm10,%ymm6,%ymm12                      #! PC = 0x5555555740d5 *)
sub %ymm12 %ymm6 %ymm10;
(* vpaddd %ymm10,%ymm6,%ymm6                       #! PC = 0x5555555740da *)
add %ymm6 %ymm6 %ymm10;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555740df *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555740e4 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm10                     #! PC = 0x5555555740ea *)
add %ymm10 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm6,%ymm6                       #! PC = 0x5555555740ef *)
sub %ymm6 %ymm6 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x5555555740f4 *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x5555555740f9 *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x5555555740fe *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x555555574103 *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555574108 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x55555557410d *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555574112 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x555555574117 *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x55555557411c *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm7,%ymm12                      #! PC = 0x555555574122 *)
sub %ymm12 %ymm7 %ymm11;
(* vpaddd %ymm11,%ymm7,%ymm7                       #! PC = 0x555555574127 *)
add %ymm7 %ymm7 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x55555557412c *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555574131 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x555555574137 *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm7,%ymm7                       #! PC = 0x55555557413c *)
sub %ymm7 %ymm7 %ymm13;
(* vperm2i128 $0x20,%ymm8,%ymm4,%ymm3              #! PC = 0x555555574141 *)
mov %ymm3 (%ymm4[0, 1, 2, 3] ++ %ymm8[0, 1, 2, 3]);
(* vperm2i128 $0x31,%ymm8,%ymm4,%ymm8              #! PC = 0x555555574147 *)
mov %ymm8 (%ymm4[4, 5, 6, 7] ++ %ymm8[4, 5, 6, 7]);
(* vperm2i128 $0x20,%ymm9,%ymm5,%ymm4              #! PC = 0x55555557414d *)
mov %ymm4 (%ymm5[0, 1, 2, 3] ++ %ymm9[0, 1, 2, 3]);
(* vperm2i128 $0x31,%ymm9,%ymm5,%ymm9              #! PC = 0x555555574153 *)
mov %ymm9 (%ymm5[4, 5, 6, 7] ++ %ymm9[4, 5, 6, 7]);
(* vperm2i128 $0x20,%ymm10,%ymm6,%ymm5             #! PC = 0x555555574159 *)
mov %ymm5 (%ymm6[0, 1, 2, 3] ++ %ymm10[0, 1, 2, 3]);
(* vperm2i128 $0x31,%ymm10,%ymm6,%ymm10            #! PC = 0x55555557415f *)
mov %ymm10 (%ymm6[4, 5, 6, 7] ++ %ymm10[4, 5, 6, 7]);
(* vperm2i128 $0x20,%ymm11,%ymm7,%ymm6             #! PC = 0x555555574165 *)
mov %ymm6 (%ymm7[0, 1, 2, 3] ++ %ymm11[0, 1, 2, 3]);
(* vperm2i128 $0x31,%ymm11,%ymm7,%ymm11            #! PC = 0x55555557416b *)
mov %ymm11 (%ymm7[4, 5, 6, 7] ++ %ymm11[4, 5, 6, 7]);

(* vmovdqa 0xc0(%rsi),%ymm1                        #! EA = L0x555555579ea0; Value = 0xae1024adae1024ad; PC = 0x555555574171 *)
mov %ymm1 [L0x555555579ea0, L0x555555579ea4, L0x555555579ea8, L0x555555579eac, L0x555555579eb0, L0x555555579eb4, L0x555555579eb8, L0x555555579ebc];
(* vmovdqa 0x560(%rsi),%ymm2                       #! EA = L0x55555557a340; Value = 0xfffa84adfffa84ad; PC = 0x555555574179 *)
mov %ymm2 [L0x55555557a340, L0x55555557a344, L0x55555557a348, L0x55555557a34c, L0x55555557a350, L0x55555557a354, L0x55555557a358, L0x55555557a35c];
(* vpmuldq %ymm1,%ymm5,%ymm13                      #! PC = 0x555555574181 *)
mull %ymm13_h %ymm13_l (%ymm5[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm5,%ymm12                          #! PC = 0x555555574186 *)
mov %ymm12 (%ymm5[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x55555557418a *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm5,%ymm5                       #! PC = 0x55555557418f *)
mull %ymm5_h %ymm5_l (%ymm5[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm5_ls@sint32[4] %ymm5_l;
mov %ymm5 [%ymm5_ls[0], %ymm5_h[0], %ymm5_ls[1], %ymm5_h[1], %ymm5_ls[2], %ymm5_h[2], %ymm5_ls[3], %ymm5_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555574194 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574199 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x55555557419e *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm5_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm5_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm5,%ymm5                           #! PC = 0x5555555741a3 *)
mov %ymm5 (%ymm5[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm5,%ymm5               #! PC = 0x5555555741a7 *)
mov %ymm5 [%ymm5[0], %ymm12[1], %ymm5[2], %ymm12[3], %ymm5[4], %ymm12[5], %ymm5[6], %ymm12[7]];
(* vpsubd %ymm5,%ymm3,%ymm12                       #! PC = 0x5555555741ad *)
sub %ymm12 %ymm3 %ymm5;
(* vpaddd %ymm5,%ymm3,%ymm3                        #! PC = 0x5555555741b1 *)
add %ymm3 %ymm3 %ymm5;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555741b5 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555741ba *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm5                      #! PC = 0x5555555741c0 *)
add %ymm5 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm3,%ymm3                       #! PC = 0x5555555741c5 *)
sub %ymm3 %ymm3 %ymm13;
(* vpmuldq %ymm1,%ymm10,%ymm13                     #! PC = 0x5555555741ca *)
mull %ymm13_h %ymm13_l (%ymm10[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm10,%ymm12                         #! PC = 0x5555555741cf *)
mov %ymm12 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x5555555741d4 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm10,%ymm10                     #! PC = 0x5555555741d9 *)
mull %ymm10_h %ymm10_l (%ymm10[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm10_ls@sint32[4] %ymm10_l;
mov %ymm10 [%ymm10_ls[0], %ymm10_h[0], %ymm10_ls[1], %ymm10_h[1], %ymm10_ls[2], %ymm10_h[2], %ymm10_ls[3], %ymm10_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x5555555741de *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555741e3 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555741e8 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm10_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm10_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm10,%ymm10                         #! PC = 0x5555555741ed *)
mov %ymm10 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm10,%ymm10             #! PC = 0x5555555741f2 *)
mov %ymm10 [%ymm10[0], %ymm12[1], %ymm10[2], %ymm12[3], %ymm10[4], %ymm12[5], %ymm10[6], %ymm12[7]];
(* vpsubd %ymm10,%ymm8,%ymm12                      #! PC = 0x5555555741f8 *)
sub %ymm12 %ymm8 %ymm10;
(* vpaddd %ymm10,%ymm8,%ymm8                       #! PC = 0x5555555741fd *)
add %ymm8 %ymm8 %ymm10;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574202 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555574207 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm10                     #! PC = 0x55555557420d *)
add %ymm10 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm8,%ymm8                       #! PC = 0x555555574212 *)
sub %ymm8 %ymm8 %ymm13;
(* vpmuldq %ymm1,%ymm6,%ymm13                      #! PC = 0x555555574217 *)
mull %ymm13_h %ymm13_l (%ymm6[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm6,%ymm12                          #! PC = 0x55555557421c *)
mov %ymm12 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555574220 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm6,%ymm6                       #! PC = 0x555555574225 *)
mull %ymm6_h %ymm6_l (%ymm6[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm6_ls@sint32[4] %ymm6_l;
mov %ymm6 [%ymm6_ls[0], %ymm6_h[0], %ymm6_ls[1], %ymm6_h[1], %ymm6_ls[2], %ymm6_h[2], %ymm6_ls[3], %ymm6_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x55555557422a *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x55555557422f *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555574234 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm6_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm6_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm6,%ymm6                           #! PC = 0x555555574239 *)
mov %ymm6 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm6,%ymm6               #! PC = 0x55555557423d *)
mov %ymm6 [%ymm6[0], %ymm12[1], %ymm6[2], %ymm12[3], %ymm6[4], %ymm12[5], %ymm6[6], %ymm12[7]];
(* vpsubd %ymm6,%ymm4,%ymm12                       #! PC = 0x555555574243 *)
sub %ymm12 %ymm4 %ymm6;
(* vpaddd %ymm6,%ymm4,%ymm4                        #! PC = 0x555555574247 *)
add %ymm4 %ymm4 %ymm6;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x55555557424b *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555574250 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm6                      #! PC = 0x555555574256 *)
add %ymm6 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm4,%ymm4                       #! PC = 0x55555557425b *)
sub %ymm4 %ymm4 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x555555574260 *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x555555574265 *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x55555557426a *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x55555557426f *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555574274 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574279 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x55555557427e *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x555555574283 *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x555555574288 *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm9,%ymm12                      #! PC = 0x55555557428e *)
sub %ymm12 %ymm9 %ymm11;
(* vpaddd %ymm11,%ymm9,%ymm9                       #! PC = 0x555555574293 *)
add %ymm9 %ymm9 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574298 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x55555557429d *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x5555555742a3 *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm9,%ymm9                       #! PC = 0x5555555742a8 *)
sub %ymm9 %ymm9 %ymm13;
(* vpunpcklqdq %ymm5,%ymm3,%ymm7                   #! PC = 0x5555555742ad *)
mov %ymm7 [%ymm3[0], %ymm3[1], %ymm5[0], %ymm5[1], %ymm3[4], %ymm3[5], %ymm5[4], %ymm5[5]];
(* vpunpckhqdq %ymm5,%ymm3,%ymm5                   #! PC = 0x5555555742b1 *)
mov %ymm5 [%ymm3[2], %ymm3[3], %ymm5[2], %ymm5[3], %ymm3[6], %ymm3[7], %ymm5[6], %ymm5[7]];
(* vpunpcklqdq %ymm10,%ymm8,%ymm3                  #! PC = 0x5555555742b5 *)
mov %ymm3 [%ymm8[0], %ymm8[1], %ymm10[0], %ymm10[1], %ymm8[4], %ymm8[5], %ymm10[4], %ymm10[5]];
(* vpunpckhqdq %ymm10,%ymm8,%ymm10                 #! PC = 0x5555555742ba *)
mov %ymm10 [%ymm8[2], %ymm8[3], %ymm10[2], %ymm10[3], %ymm8[6], %ymm8[7], %ymm10[6], %ymm10[7]];
(* vpunpcklqdq %ymm6,%ymm4,%ymm8                   #! PC = 0x5555555742bf *)
mov %ymm8 [%ymm4[0], %ymm4[1], %ymm6[0], %ymm6[1], %ymm4[4], %ymm4[5], %ymm6[4], %ymm6[5]];
(* vpunpckhqdq %ymm6,%ymm4,%ymm6                   #! PC = 0x5555555742c3 *)
mov %ymm6 [%ymm4[2], %ymm4[3], %ymm6[2], %ymm6[3], %ymm4[6], %ymm4[7], %ymm6[6], %ymm6[7]];
(* vpunpcklqdq %ymm11,%ymm9,%ymm4                  #! PC = 0x5555555742c7 *)
mov %ymm4 [%ymm9[0], %ymm9[1], %ymm11[0], %ymm11[1], %ymm9[4], %ymm9[5], %ymm11[4], %ymm11[5]];
(* vpunpckhqdq %ymm11,%ymm9,%ymm11                 #! PC = 0x5555555742cc *)
mov %ymm11 [%ymm9[2], %ymm9[3], %ymm11[2], %ymm11[3], %ymm9[6], %ymm9[7], %ymm11[6], %ymm11[7]];

(* vmovdqa 0x140(%rsi),%ymm1                       #! EA = L0x555555579f20; Value = 0x6ba99d906ba99d90; PC = 0x5555555742d1 *)
mov %ymm1 [L0x555555579f20, L0x555555579f24, L0x555555579f28, L0x555555579f2c, L0x555555579f30, L0x555555579f34, L0x555555579f38, L0x555555579f3c];
(* vmovdqa 0x5e0(%rsi),%ymm2                       #! EA = L0x55555557a3c0; Value = 0xfff79d90fff79d90; PC = 0x5555555742d9 *)
mov %ymm2 [L0x55555557a3c0, L0x55555557a3c4, L0x55555557a3c8, L0x55555557a3cc, L0x55555557a3d0, L0x55555557a3d4, L0x55555557a3d8, L0x55555557a3dc];
(* vpmuldq %ymm1,%ymm8,%ymm13                      #! PC = 0x5555555742e1 *)
mull %ymm13_h %ymm13_l (%ymm8[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm8,%ymm12                          #! PC = 0x5555555742e6 *)
mov %ymm12 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x5555555742eb *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm8,%ymm8                       #! PC = 0x5555555742f0 *)
mull %ymm8_h %ymm8_l (%ymm8[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm8_ls@sint32[4] %ymm8_l;
mov %ymm8 [%ymm8_ls[0], %ymm8_h[0], %ymm8_ls[1], %ymm8_h[1], %ymm8_ls[2], %ymm8_h[2], %ymm8_ls[3], %ymm8_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x5555555742f5 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555742fa *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555742ff *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm8_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm8_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm8,%ymm8                           #! PC = 0x555555574304 *)
mov %ymm8 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm8,%ymm8               #! PC = 0x555555574309 *)
mov %ymm8 [%ymm8[0], %ymm12[1], %ymm8[2], %ymm12[3], %ymm8[4], %ymm12[5], %ymm8[6], %ymm12[7]];
(* vpsubd %ymm8,%ymm7,%ymm12                       #! PC = 0x55555557430f *)
sub %ymm12 %ymm7 %ymm8;
(* vpaddd %ymm8,%ymm7,%ymm7                        #! PC = 0x555555574314 *)
add %ymm7 %ymm7 %ymm8;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574319 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x55555557431e *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm8                      #! PC = 0x555555574324 *)
add %ymm8 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm7,%ymm7                       #! PC = 0x555555574329 *)
sub %ymm7 %ymm7 %ymm13;
(* vpmuldq %ymm1,%ymm6,%ymm13                      #! PC = 0x55555557432e *)
mull %ymm13_h %ymm13_l (%ymm6[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm6,%ymm12                          #! PC = 0x555555574333 *)
mov %ymm12 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555574337 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm6,%ymm6                       #! PC = 0x55555557433c *)
mull %ymm6_h %ymm6_l (%ymm6[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm6_ls@sint32[4] %ymm6_l;
mov %ymm6 [%ymm6_ls[0], %ymm6_h[0], %ymm6_ls[1], %ymm6_h[1], %ymm6_ls[2], %ymm6_h[2], %ymm6_ls[3], %ymm6_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555574341 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574346 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x55555557434b *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm6_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm6_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm6,%ymm6                           #! PC = 0x555555574350 *)
mov %ymm6 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm6,%ymm6               #! PC = 0x555555574354 *)
mov %ymm6 [%ymm6[0], %ymm12[1], %ymm6[2], %ymm12[3], %ymm6[4], %ymm12[5], %ymm6[6], %ymm12[7]];
(* vpsubd %ymm6,%ymm5,%ymm12                       #! PC = 0x55555557435a *)
sub %ymm12 %ymm5 %ymm6;
(* vpaddd %ymm6,%ymm5,%ymm5                        #! PC = 0x55555557435e *)
add %ymm5 %ymm5 %ymm6;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574362 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555574367 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm6                      #! PC = 0x55555557436d *)
add %ymm6 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm5,%ymm5                       #! PC = 0x555555574372 *)
sub %ymm5 %ymm5 %ymm13;
(* vpmuldq %ymm1,%ymm4,%ymm13                      #! PC = 0x555555574377 *)
mull %ymm13_h %ymm13_l (%ymm4[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm4,%ymm12                          #! PC = 0x55555557437c *)
mov %ymm12 (%ymm4[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555574380 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm4,%ymm4                       #! PC = 0x555555574385 *)
mull %ymm4_h %ymm4_l (%ymm4[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm4_ls@sint32[4] %ymm4_l;
mov %ymm4 [%ymm4_ls[0], %ymm4_h[0], %ymm4_ls[1], %ymm4_h[1], %ymm4_ls[2], %ymm4_h[2], %ymm4_ls[3], %ymm4_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x55555557438a *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x55555557438f *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555574394 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm4_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm4_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm4,%ymm4                           #! PC = 0x555555574399 *)
mov %ymm4 (%ymm4[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm4,%ymm4               #! PC = 0x55555557439d *)
mov %ymm4 [%ymm4[0], %ymm12[1], %ymm4[2], %ymm12[3], %ymm4[4], %ymm12[5], %ymm4[6], %ymm12[7]];
(* vpsubd %ymm4,%ymm3,%ymm12                       #! PC = 0x5555555743a3 *)
sub %ymm12 %ymm3 %ymm4;
(* vpaddd %ymm4,%ymm3,%ymm3                        #! PC = 0x5555555743a7 *)
add %ymm3 %ymm3 %ymm4;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555743ab *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555743b0 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm4                      #! PC = 0x5555555743b6 *)
add %ymm4 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm3,%ymm3                       #! PC = 0x5555555743bb *)
sub %ymm3 %ymm3 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x5555555743c0 *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x5555555743c5 *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x5555555743ca *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x5555555743cf *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x5555555743d4 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555743d9 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555743de *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x5555555743e3 *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x5555555743e8 *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm10,%ymm12                     #! PC = 0x5555555743ee *)
sub %ymm12 %ymm10 %ymm11;
(* vpaddd %ymm11,%ymm10,%ymm10                     #! PC = 0x5555555743f3 *)
add %ymm10 %ymm10 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555743f8 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555743fd *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x555555574403 *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm10,%ymm10                     #! PC = 0x555555574408 *)
sub %ymm10 %ymm10 %ymm13;
(* vmovsldup %ymm8,%ymm9                           #! PC = 0x55555557440d *)
mov %ymm9 (%ymm8[0, 0, 2, 2, 4, 4, 6, 6]);
(* vpblendd $0xaa,%ymm9,%ymm7,%ymm9                #! PC = 0x555555574412 *)
mov %ymm9 [%ymm7[0], %ymm9[1], %ymm7[2], %ymm9[3], %ymm7[4], %ymm9[5], %ymm7[6], %ymm9[7]];
(* vpsrlq $0x20,%ymm7,%ymm7                        #! PC = 0x555555574418 *)
mov %ymm7 [%ymm7[1], 0@sint32, %ymm7[3], 0@sint32, %ymm7[5], 0@sint32, %ymm7[7], 0@sint32];
(* vpblendd $0xaa,%ymm8,%ymm7,%ymm8                #! PC = 0x55555557441d *)
mov %ymm8 [%ymm7[0], %ymm8[1], %ymm7[2], %ymm8[3], %ymm7[4], %ymm8[5], %ymm7[6], %ymm8[7]];
(* vmovsldup %ymm6,%ymm7                           #! PC = 0x555555574423 *)
mov %ymm7 (%ymm6[0, 0, 2, 2, 4, 4, 6, 6]);
(* vpblendd $0xaa,%ymm7,%ymm5,%ymm7                #! PC = 0x555555574427 *)
mov %ymm7 [%ymm5[0], %ymm7[1], %ymm5[2], %ymm7[3], %ymm5[4], %ymm7[5], %ymm5[6], %ymm7[7]];
(* vpsrlq $0x20,%ymm5,%ymm5                        #! PC = 0x55555557442d *)
mov %ymm5 [%ymm5[1], 0@sint32, %ymm5[3], 0@sint32, %ymm5[5], 0@sint32, %ymm5[7], 0@sint32];
(* vpblendd $0xaa,%ymm6,%ymm5,%ymm6                #! PC = 0x555555574432 *)
mov %ymm6 [%ymm5[0], %ymm6[1], %ymm5[2], %ymm6[3], %ymm5[4], %ymm6[5], %ymm5[6], %ymm6[7]];
(* vmovsldup %ymm4,%ymm5                           #! PC = 0x555555574438 *)
mov %ymm5 (%ymm4[0, 0, 2, 2, 4, 4, 6, 6]);
(* vpblendd $0xaa,%ymm5,%ymm3,%ymm5                #! PC = 0x55555557443c *)
mov %ymm5 [%ymm3[0], %ymm5[1], %ymm3[2], %ymm5[3], %ymm3[4], %ymm5[5], %ymm3[6], %ymm5[7]];
(* vpsrlq $0x20,%ymm3,%ymm3                        #! PC = 0x555555574442 *)
mov %ymm3 [%ymm3[1], 0@sint32, %ymm3[3], 0@sint32, %ymm3[5], 0@sint32, %ymm3[7], 0@sint32];
(* vpblendd $0xaa,%ymm4,%ymm3,%ymm4                #! PC = 0x555555574447 *)
mov %ymm4 [%ymm3[0], %ymm4[1], %ymm3[2], %ymm4[3], %ymm3[4], %ymm4[5], %ymm3[6], %ymm4[7]];
(* vmovsldup %ymm11,%ymm3                          #! PC = 0x55555557444d *)
mov %ymm3 (%ymm11[0, 0, 2, 2, 4, 4, 6, 6]);
(* vpblendd $0xaa,%ymm3,%ymm10,%ymm3               #! PC = 0x555555574452 *)
mov %ymm3 [%ymm10[0], %ymm3[1], %ymm10[2], %ymm3[3], %ymm10[4], %ymm3[5], %ymm10[6], %ymm3[7]];
(* vpsrlq $0x20,%ymm10,%ymm10                      #! PC = 0x555555574458 *)
mov %ymm10 [%ymm10[1], 0@sint32, %ymm10[3], 0@sint32, %ymm10[5], 0@sint32, %ymm10[7], 0@sint32];
(* vpblendd $0xaa,%ymm11,%ymm10,%ymm11             #! PC = 0x55555557445e *)
mov %ymm11 [%ymm10[0], %ymm11[1], %ymm10[2], %ymm11[3], %ymm10[4], %ymm11[5], %ymm10[6], %ymm11[7]];

(* vmovdqa 0x1c0(%rsi),%ymm1                       #! EA = L0x555555579fa0; Value = 0xe3a10e7cf3f5b585; PC = 0x555555574464 *)
mov %ymm1 [L0x555555579fa0, L0x555555579fa4, L0x555555579fa8, L0x555555579fac, L0x555555579fb0, L0x555555579fb4, L0x555555579fb8, L0x555555579fbc];
(* vmovdqa 0x660(%rsi),%ymm2                       #! EA = L0x55555557a440; Value = 0xffd18e7cffc51585; PC = 0x55555557446c *)
mov %ymm2 [L0x55555557a440, L0x55555557a444, L0x55555557a448, L0x55555557a44c, L0x55555557a450, L0x55555557a454, L0x55555557a458, L0x55555557a45c];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x555555574474 *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x555555574479 *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm5,%ymm13                      #! PC = 0x55555557447d *)
mull %ymm13_h %ymm13_l (%ymm5[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm5,%ymm12                          #! PC = 0x555555574482 *)
mov %ymm12 (%ymm5[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555574486 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm5,%ymm5                       #! PC = 0x55555557448b *)
mull %ymm5_h %ymm5_l (%ymm5[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm5_ls@sint32[4] %ymm5_l;
mov %ymm5 [%ymm5_ls[0], %ymm5_h[0], %ymm5_ls[1], %ymm5_h[1], %ymm5_ls[2], %ymm5_h[2], %ymm5_ls[3], %ymm5_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555574490 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574495 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x55555557449a *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm5_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm5_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm5,%ymm5                           #! PC = 0x55555557449f *)
mov %ymm5 (%ymm5[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm5,%ymm5               #! PC = 0x5555555744a3 *)
mov %ymm5 [%ymm5[0], %ymm12[1], %ymm5[2], %ymm12[3], %ymm5[4], %ymm12[5], %ymm5[6], %ymm12[7]];
(* vpsubd %ymm5,%ymm9,%ymm12                       #! PC = 0x5555555744a9 *)
sub %ymm12 %ymm9 %ymm5;
(* vpaddd %ymm5,%ymm9,%ymm9                        #! PC = 0x5555555744ad *)
add %ymm9 %ymm9 %ymm5;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555744b1 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555744b6 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm5                      #! PC = 0x5555555744bc *)
add %ymm5 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm9,%ymm9                       #! PC = 0x5555555744c1 *)
sub %ymm9 %ymm9 %ymm13;
(* vpmuldq %ymm1,%ymm4,%ymm13                      #! PC = 0x5555555744c6 *)
mull %ymm13_h %ymm13_l (%ymm4[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm4,%ymm12                          #! PC = 0x5555555744cb *)
mov %ymm12 (%ymm4[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x5555555744cf *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm4,%ymm4                       #! PC = 0x5555555744d4 *)
mull %ymm4_h %ymm4_l (%ymm4[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm4_ls@sint32[4] %ymm4_l;
mov %ymm4 [%ymm4_ls[0], %ymm4_h[0], %ymm4_ls[1], %ymm4_h[1], %ymm4_ls[2], %ymm4_h[2], %ymm4_ls[3], %ymm4_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x5555555744d9 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555744de *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555744e3 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm4_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm4_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm4,%ymm4                           #! PC = 0x5555555744e8 *)
mov %ymm4 (%ymm4[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm4,%ymm4               #! PC = 0x5555555744ec *)
mov %ymm4 [%ymm4[0], %ymm12[1], %ymm4[2], %ymm12[3], %ymm4[4], %ymm12[5], %ymm4[6], %ymm12[7]];
(* vpsubd %ymm4,%ymm8,%ymm12                       #! PC = 0x5555555744f2 *)
sub %ymm12 %ymm8 %ymm4;
(* vpaddd %ymm4,%ymm8,%ymm8                        #! PC = 0x5555555744f6 *)
add %ymm8 %ymm8 %ymm4;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555744fa *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555744ff *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm4                      #! PC = 0x555555574505 *)
add %ymm4 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm8,%ymm8                       #! PC = 0x55555557450a *)
sub %ymm8 %ymm8 %ymm13;
(* vpmuldq %ymm1,%ymm3,%ymm13                      #! PC = 0x55555557450f *)
mull %ymm13_h %ymm13_l (%ymm3[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm3,%ymm12                          #! PC = 0x555555574514 *)
mov %ymm12 (%ymm3[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555574518 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm3,%ymm3                       #! PC = 0x55555557451d *)
mull %ymm3_h %ymm3_l (%ymm3[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm3_ls@sint32[4] %ymm3_l;
mov %ymm3 [%ymm3_ls[0], %ymm3_h[0], %ymm3_ls[1], %ymm3_h[1], %ymm3_ls[2], %ymm3_h[2], %ymm3_ls[3], %ymm3_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555574522 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574527 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x55555557452c *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm3_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm3_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm3,%ymm3                           #! PC = 0x555555574531 *)
mov %ymm3 (%ymm3[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm3,%ymm3               #! PC = 0x555555574535 *)
mov %ymm3 [%ymm3[0], %ymm12[1], %ymm3[2], %ymm12[3], %ymm3[4], %ymm12[5], %ymm3[6], %ymm12[7]];
(* vpsubd %ymm3,%ymm7,%ymm12                       #! PC = 0x55555557453b *)
sub %ymm12 %ymm7 %ymm3;
(* vpaddd %ymm3,%ymm7,%ymm7                        #! PC = 0x55555557453f *)
add %ymm7 %ymm7 %ymm3;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574543 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555574548 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm3                      #! PC = 0x55555557454e *)
add %ymm3 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm7,%ymm7                       #! PC = 0x555555574553 *)
sub %ymm7 %ymm7 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x555555574558 *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x55555557455d *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555574562 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x555555574567 *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x55555557456c *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574571 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555574576 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x55555557457b *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x555555574580 *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm6,%ymm12                      #! PC = 0x555555574586 *)
sub %ymm12 %ymm6 %ymm11;
(* vpaddd %ymm11,%ymm6,%ymm6                       #! PC = 0x55555557458b *)
add %ymm6 %ymm6 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574590 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555574595 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x55555557459b *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm6,%ymm6                       #! PC = 0x5555555745a0 *)
sub %ymm6 %ymm6 %ymm13;

(* vmovdqa 0x240(%rsi),%ymm1                       #! EA = L0x55555557a020; Value = 0xdce7c638d15250f2; PC = 0x5555555745a5 *)
mov %ymm1 [L0x55555557a020, L0x55555557a024, L0x55555557a028, L0x55555557a02c, L0x55555557a030, L0x55555557a034, L0x55555557a038, L0x55555557a03c];
(* vmovdqa 0x6e0(%rsi),%ymm2                       #! EA = L0x55555557a4c0; Value = 0x0020c638003410f2; PC = 0x5555555745ad *)
mov %ymm2 [L0x55555557a4c0, L0x55555557a4c4, L0x55555557a4c8, L0x55555557a4cc, L0x55555557a4d0, L0x55555557a4d4, L0x55555557a4d8, L0x55555557a4dc];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x5555555745b5 *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x5555555745ba *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm7,%ymm13                      #! PC = 0x5555555745be *)
mull %ymm13_h %ymm13_l (%ymm7[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm7,%ymm12                          #! PC = 0x5555555745c3 *)
mov %ymm12 (%ymm7[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x5555555745c7 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm7,%ymm7                       #! PC = 0x5555555745cc *)
mull %ymm7_h %ymm7_l (%ymm7[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm7_ls@sint32[4] %ymm7_l;
mov %ymm7 [%ymm7_ls[0], %ymm7_h[0], %ymm7_ls[1], %ymm7_h[1], %ymm7_ls[2], %ymm7_h[2], %ymm7_ls[3], %ymm7_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x5555555745d1 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555745d6 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555745db *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm7_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm7_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm7,%ymm7                           #! PC = 0x5555555745e0 *)
mov %ymm7 (%ymm7[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm7,%ymm7               #! PC = 0x5555555745e4 *)
mov %ymm7 [%ymm7[0], %ymm12[1], %ymm7[2], %ymm12[3], %ymm7[4], %ymm12[5], %ymm7[6], %ymm12[7]];
(* vpsubd %ymm7,%ymm9,%ymm12                       #! PC = 0x5555555745ea *)
sub %ymm12 %ymm9 %ymm7;
(* vpaddd %ymm7,%ymm9,%ymm9                        #! PC = 0x5555555745ee *)
add %ymm9 %ymm9 %ymm7;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555745f2 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555745f7 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm7                      #! PC = 0x5555555745fd *)
add %ymm7 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm9,%ymm9                       #! PC = 0x555555574602 *)
sub %ymm9 %ymm9 %ymm13;
(* vpmuldq %ymm1,%ymm6,%ymm13                      #! PC = 0x555555574607 *)
mull %ymm13_h %ymm13_l (%ymm6[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm6,%ymm12                          #! PC = 0x55555557460c *)
mov %ymm12 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555574610 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm6,%ymm6                       #! PC = 0x555555574615 *)
mull %ymm6_h %ymm6_l (%ymm6[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm6_ls@sint32[4] %ymm6_l;
mov %ymm6 [%ymm6_ls[0], %ymm6_h[0], %ymm6_ls[1], %ymm6_h[1], %ymm6_ls[2], %ymm6_h[2], %ymm6_ls[3], %ymm6_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x55555557461a *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x55555557461f *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555574624 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm6_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm6_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm6,%ymm6                           #! PC = 0x555555574629 *)
mov %ymm6 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm6,%ymm6               #! PC = 0x55555557462d *)
mov %ymm6 [%ymm6[0], %ymm12[1], %ymm6[2], %ymm12[3], %ymm6[4], %ymm12[5], %ymm6[6], %ymm12[7]];
(* vpsubd %ymm6,%ymm8,%ymm12                       #! PC = 0x555555574633 *)
sub %ymm12 %ymm8 %ymm6;
(* vpaddd %ymm6,%ymm8,%ymm8                        #! PC = 0x555555574637 *)
add %ymm8 %ymm8 %ymm6;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x55555557463b *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555574640 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm6                      #! PC = 0x555555574646 *)
add %ymm6 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm8,%ymm8                       #! PC = 0x55555557464b *)
sub %ymm8 %ymm8 %ymm13;
(* vmovdqa 0x2c0(%rsi),%ymm1                       #! EA = L0x55555557a0a0; Value = 0x5a7d4e9ff1419e85; PC = 0x555555574650 *)
mov %ymm1 [L0x55555557a0a0, L0x55555557a0a4, L0x55555557a0a8, L0x55555557a0ac, L0x55555557a0b0, L0x55555557a0b4, L0x55555557a0b8, L0x55555557a0bc];
(* vmovdqa 0x760(%rsi),%ymm2                       #! EA = L0x55555557a540; Value = 0x00296e9ffff0fe85; PC = 0x555555574658 *)
mov %ymm2 [L0x55555557a540, L0x55555557a544, L0x55555557a548, L0x55555557a54c, L0x55555557a550, L0x55555557a554, L0x55555557a558, L0x55555557a55c];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x555555574660 *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x555555574665 *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm3,%ymm13                      #! PC = 0x555555574669 *)
mull %ymm13_h %ymm13_l (%ymm3[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm3,%ymm12                          #! PC = 0x55555557466e *)
mov %ymm12 (%ymm3[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555574672 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm3,%ymm3                       #! PC = 0x555555574677 *)
mull %ymm3_h %ymm3_l (%ymm3[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm3_ls@sint32[4] %ymm3_l;
mov %ymm3 [%ymm3_ls[0], %ymm3_h[0], %ymm3_ls[1], %ymm3_h[1], %ymm3_ls[2], %ymm3_h[2], %ymm3_ls[3], %ymm3_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x55555557467c *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574681 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555574686 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm3_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm3_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm3,%ymm3                           #! PC = 0x55555557468b *)
mov %ymm3 (%ymm3[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm3,%ymm3               #! PC = 0x55555557468f *)
mov %ymm3 [%ymm3[0], %ymm12[1], %ymm3[2], %ymm12[3], %ymm3[4], %ymm12[5], %ymm3[6], %ymm12[7]];
(* vpsubd %ymm3,%ymm5,%ymm12                       #! PC = 0x555555574695 *)
sub %ymm12 %ymm5 %ymm3;
(* vpaddd %ymm3,%ymm5,%ymm5                        #! PC = 0x555555574699 *)
add %ymm5 %ymm5 %ymm3;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x55555557469d *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555746a2 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm3                      #! PC = 0x5555555746a8 *)
add %ymm3 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm5,%ymm5                       #! PC = 0x5555555746ad *)
sub %ymm5 %ymm5 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x5555555746b2 *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x5555555746b7 *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x5555555746bc *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x5555555746c1 *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x5555555746c6 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555746cb *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555746d0 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x5555555746d5 *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x5555555746da *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm4,%ymm12                      #! PC = 0x5555555746e0 *)
sub %ymm12 %ymm4 %ymm11;
(* vpaddd %ymm11,%ymm4,%ymm4                       #! PC = 0x5555555746e5 *)
add %ymm4 %ymm4 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555746ea *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555746ef *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x5555555746f5 *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm4,%ymm4                       #! PC = 0x5555555746fa *)
sub %ymm4 %ymm4 %ymm13;

(* vmovdqa 0x340(%rsi),%ymm1                       #! EA = L0x55555557a120; Value = 0xb71152e77ea85919; PC = 0x5555555746ff *)
mov %ymm1 [L0x55555557a120, L0x55555557a124, L0x55555557a128, L0x55555557a12c, L0x55555557a130, L0x55555557a134, L0x55555557a138, L0x55555557a13c];
(* vmovdqa 0x7e0(%rsi),%ymm2                       #! EA = L0x55555557a5c0; Value = 0x003472e700053919; PC = 0x555555574707 *)
mov %ymm2 [L0x55555557a5c0, L0x55555557a5c4, L0x55555557a5c8, L0x55555557a5cc, L0x55555557a5d0, L0x55555557a5d4, L0x55555557a5d8, L0x55555557a5dc];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x55555557470f *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x555555574714 *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm8,%ymm13                      #! PC = 0x555555574718 *)
mull %ymm13_h %ymm13_l (%ymm8[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm8,%ymm12                          #! PC = 0x55555557471d *)
mov %ymm12 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555574722 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm8,%ymm8                       #! PC = 0x555555574727 *)
mull %ymm8_h %ymm8_l (%ymm8[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm8_ls@sint32[4] %ymm8_l;
mov %ymm8 [%ymm8_ls[0], %ymm8_h[0], %ymm8_ls[1], %ymm8_h[1], %ymm8_ls[2], %ymm8_h[2], %ymm8_ls[3], %ymm8_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x55555557472c *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574731 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555574736 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm8_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm8_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm8,%ymm8                           #! PC = 0x55555557473b *)
mov %ymm8 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm8,%ymm8               #! PC = 0x555555574740 *)
mov %ymm8 [%ymm8[0], %ymm12[1], %ymm8[2], %ymm12[3], %ymm8[4], %ymm12[5], %ymm8[6], %ymm12[7]];
(* vpsubd %ymm8,%ymm9,%ymm12                       #! PC = 0x555555574746 *)
sub %ymm12 %ymm9 %ymm8;
(* vpaddd %ymm8,%ymm9,%ymm9                        #! PC = 0x55555557474b *)
add %ymm9 %ymm9 %ymm8;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574750 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555574755 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm8                      #! PC = 0x55555557475b *)
add %ymm8 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm9,%ymm9                       #! PC = 0x555555574760 *)
sub %ymm9 %ymm9 %ymm13;
(* vmovdqa 0x3c0(%rsi),%ymm1                       #! EA = L0x55555557a1a0; Value = 0x6e54603b3625e10c; PC = 0x555555574765 *)
mov %ymm1 [L0x55555557a1a0, L0x55555557a1a4, L0x55555557a1a8, L0x55555557a1ac, L0x55555557a1b0, L0x55555557a1b4, L0x55555557a1b8, L0x55555557a1bc];
(* vmovdqa 0x860(%rsi),%ymm2                       #! EA = L0x55555557a640; Value = 0xffcd003b0004610c; PC = 0x55555557476d *)
mov %ymm2 [L0x55555557a640, L0x55555557a644, L0x55555557a648, L0x55555557a64c, L0x55555557a650, L0x55555557a654, L0x55555557a658, L0x55555557a65c];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x555555574775 *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x55555557477a *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm6,%ymm13                      #! PC = 0x55555557477e *)
mull %ymm13_h %ymm13_l (%ymm6[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm6,%ymm12                          #! PC = 0x555555574783 *)
mov %ymm12 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555574787 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm6,%ymm6                       #! PC = 0x55555557478c *)
mull %ymm6_h %ymm6_l (%ymm6[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm6_ls@sint32[4] %ymm6_l;
mov %ymm6 [%ymm6_ls[0], %ymm6_h[0], %ymm6_ls[1], %ymm6_h[1], %ymm6_ls[2], %ymm6_h[2], %ymm6_ls[3], %ymm6_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555574791 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574796 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x55555557479b *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm6_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm6_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm6,%ymm6                           #! PC = 0x5555555747a0 *)
mov %ymm6 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm6,%ymm6               #! PC = 0x5555555747a4 *)
mov %ymm6 [%ymm6[0], %ymm12[1], %ymm6[2], %ymm12[3], %ymm6[4], %ymm12[5], %ymm6[6], %ymm12[7]];
(* vpsubd %ymm6,%ymm7,%ymm12                       #! PC = 0x5555555747aa *)
sub %ymm12 %ymm7 %ymm6;
(* vpaddd %ymm6,%ymm7,%ymm7                        #! PC = 0x5555555747ae *)
add %ymm7 %ymm7 %ymm6;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555747b2 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555747b7 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm6                      #! PC = 0x5555555747bd *)
add %ymm6 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm7,%ymm7                       #! PC = 0x5555555747c2 *)
sub %ymm7 %ymm7 %ymm13;
(* vmovdqa 0x440(%rsi),%ymm1                       #! EA = L0x55555557a220; Value = 0x08335cc7bd02ed41; PC = 0x5555555747c7 *)
mov %ymm1 [L0x55555557a220, L0x55555557a224, L0x55555557a228, L0x55555557a22c, L0x55555557a230, L0x55555557a234, L0x55555557a238, L0x55555557a23c];
(* vmovdqa 0x8e0(%rsi),%ymm2                       #! EA = L0x55555557a6c0; Value = 0x001a7cc7ffdacd41; PC = 0x5555555747cf *)
mov %ymm2 [L0x55555557a6c0, L0x55555557a6c4, L0x55555557a6c8, L0x55555557a6cc, L0x55555557a6d0, L0x55555557a6d4, L0x55555557a6d8, L0x55555557a6dc];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x5555555747d7 *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x5555555747dc *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm4,%ymm13                      #! PC = 0x5555555747e0 *)
mull %ymm13_h %ymm13_l (%ymm4[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm4,%ymm12                          #! PC = 0x5555555747e5 *)
mov %ymm12 (%ymm4[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x5555555747e9 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm4,%ymm4                       #! PC = 0x5555555747ee *)
mull %ymm4_h %ymm4_l (%ymm4[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm4_ls@sint32[4] %ymm4_l;
mov %ymm4 [%ymm4_ls[0], %ymm4_h[0], %ymm4_ls[1], %ymm4_h[1], %ymm4_ls[2], %ymm4_h[2], %ymm4_ls[3], %ymm4_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x5555555747f3 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555747f8 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555747fd *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm4_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm4_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm4,%ymm4                           #! PC = 0x555555574802 *)
mov %ymm4 (%ymm4[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm4,%ymm4               #! PC = 0x555555574806 *)
mov %ymm4 [%ymm4[0], %ymm12[1], %ymm4[2], %ymm12[3], %ymm4[4], %ymm12[5], %ymm4[6], %ymm12[7]];
(* vpsubd %ymm4,%ymm5,%ymm12                       #! PC = 0x55555557480c *)
sub %ymm12 %ymm5 %ymm4;
(* vpaddd %ymm4,%ymm5,%ymm5                        #! PC = 0x555555574810 *)
add %ymm5 %ymm5 %ymm4;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574814 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555574819 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm4                      #! PC = 0x55555557481f *)
add %ymm4 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm5,%ymm5                       #! PC = 0x555555574824 *)
sub %ymm5 %ymm5 %ymm13;
(* vmovdqa 0x4c0(%rsi),%ymm1                       #! EA = L0x55555557a2a0; Value = 0x6127992434c2101b; PC = 0x555555574829 *)
mov %ymm1 [L0x55555557a2a0, L0x55555557a2a4, L0x55555557a2a8, L0x55555557a2ac, L0x55555557a2b0, L0x55555557a2b4, L0x55555557a2b8, L0x55555557a2bc];
(* vmovdqa 0x960(%rsi),%ymm2                       #! EA = L0x55555557a740; Value = 0x00031924003eb01b; PC = 0x555555574831 *)
mov %ymm2 [L0x55555557a740, L0x55555557a744, L0x55555557a748, L0x55555557a74c, L0x55555557a750, L0x55555557a754, L0x55555557a758, L0x55555557a75c];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x555555574839 *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x55555557483e *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x555555574842 *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x555555574847 *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x55555557484c *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x555555574851 *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555574856 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x55555557485b *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555574860 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x555555574865 *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x55555557486a *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm3,%ymm12                      #! PC = 0x555555574870 *)
sub %ymm12 %ymm3 %ymm11;
(* vpaddd %ymm11,%ymm3,%ymm3                       #! PC = 0x555555574875 *)
add %ymm3 %ymm3 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x55555557487a *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x55555557487f *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x555555574885 *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm3,%ymm3                       #! PC = 0x55555557488a *)
sub %ymm3 %ymm3 %ymm13;
(* vmovdqa %ymm9,0x100(%rdi)                       #! EA = L0x7fffffff45c0; PC = 0x55555557488f *)
mov [L0x7fffffff45c0, L0x7fffffff45c4, L0x7fffffff45c8, L0x7fffffff45cc, L0x7fffffff45d0, L0x7fffffff45d4, L0x7fffffff45d8, L0x7fffffff45dc] %ymm9;
(* vmovdqa %ymm8,0x120(%rdi)                       #! EA = L0x7fffffff45e0; PC = 0x555555574897 *)
mov [L0x7fffffff45e0, L0x7fffffff45e4, L0x7fffffff45e8, L0x7fffffff45ec, L0x7fffffff45f0, L0x7fffffff45f4, L0x7fffffff45f8, L0x7fffffff45fc] %ymm8;
(* vmovdqa %ymm7,0x140(%rdi)                       #! EA = L0x7fffffff4600; PC = 0x55555557489f *)
mov [L0x7fffffff4600, L0x7fffffff4604, L0x7fffffff4608, L0x7fffffff460c, L0x7fffffff4610, L0x7fffffff4614, L0x7fffffff4618, L0x7fffffff461c] %ymm7;
(* vmovdqa %ymm6,0x160(%rdi)                       #! EA = L0x7fffffff4620; PC = 0x5555555748a7 *)
mov [L0x7fffffff4620, L0x7fffffff4624, L0x7fffffff4628, L0x7fffffff462c, L0x7fffffff4630, L0x7fffffff4634, L0x7fffffff4638, L0x7fffffff463c] %ymm6;
(* vmovdqa %ymm5,0x180(%rdi)                       #! EA = L0x7fffffff4640; PC = 0x5555555748af *)
mov [L0x7fffffff4640, L0x7fffffff4644, L0x7fffffff4648, L0x7fffffff464c, L0x7fffffff4650, L0x7fffffff4654, L0x7fffffff4658, L0x7fffffff465c] %ymm5;
(* vmovdqa %ymm4,0x1a0(%rdi)                       #! EA = L0x7fffffff4660; PC = 0x5555555748b7 *)
mov [L0x7fffffff4660, L0x7fffffff4664, L0x7fffffff4668, L0x7fffffff466c, L0x7fffffff4670, L0x7fffffff4674, L0x7fffffff4678, L0x7fffffff467c] %ymm4;
(* vmovdqa %ymm3,0x1c0(%rdi)                       #! EA = L0x7fffffff4680; PC = 0x5555555748bf *)
mov [L0x7fffffff4680, L0x7fffffff4684, L0x7fffffff4688, L0x7fffffff468c, L0x7fffffff4690, L0x7fffffff4694, L0x7fffffff4698, L0x7fffffff469c] %ymm3;
(* vmovdqa %ymm11,0x1e0(%rdi)                      #! EA = L0x7fffffff46a0; PC = 0x5555555748c7 *)
mov [L0x7fffffff46a0, L0x7fffffff46a4, L0x7fffffff46a8, L0x7fffffff46ac, L0x7fffffff46b0, L0x7fffffff46b4, L0x7fffffff46b8, L0x7fffffff46bc] %ymm11;

(* vmovdqa 0x200(%rdi),%ymm4                       #! EA = L0x7fffffff46c0; Value = 0xffec6298ffbafced; PC = 0x5555555748cf *)
mov %ymm4 [L0x7fffffff46c0, L0x7fffffff46c4, L0x7fffffff46c8, L0x7fffffff46cc, L0x7fffffff46d0, L0x7fffffff46d4, L0x7fffffff46d8, L0x7fffffff46dc];
(* vmovdqa 0x220(%rdi),%ymm5                       #! EA = L0x7fffffff46e0; Value = 0x00066cfffffbb50e; PC = 0x5555555748d7 *)
mov %ymm5 [L0x7fffffff46e0, L0x7fffffff46e4, L0x7fffffff46e8, L0x7fffffff46ec, L0x7fffffff46f0, L0x7fffffff46f4, L0x7fffffff46f8, L0x7fffffff46fc];
(* vmovdqa 0x240(%rdi),%ymm6                       #! EA = L0x7fffffff4700; Value = 0xffd36fce00360bd4; PC = 0x5555555748df *)
mov %ymm6 [L0x7fffffff4700, L0x7fffffff4704, L0x7fffffff4708, L0x7fffffff470c, L0x7fffffff4710, L0x7fffffff4714, L0x7fffffff4718, L0x7fffffff471c];
(* vmovdqa 0x260(%rdi),%ymm7                       #! EA = L0x7fffffff4720; Value = 0x001a0a5f0008aa42; PC = 0x5555555748e7 *)
mov %ymm7 [L0x7fffffff4720, L0x7fffffff4724, L0x7fffffff4728, L0x7fffffff472c, L0x7fffffff4730, L0x7fffffff4734, L0x7fffffff4738, L0x7fffffff473c];
(* vmovdqa 0x280(%rdi),%ymm8                       #! EA = L0x7fffffff4740; Value = 0x00396565fffcdc9d; PC = 0x5555555748ef *)
mov %ymm8 [L0x7fffffff4740, L0x7fffffff4744, L0x7fffffff4748, L0x7fffffff474c, L0x7fffffff4750, L0x7fffffff4754, L0x7fffffff4758, L0x7fffffff475c];
(* vmovdqa 0x2a0(%rdi),%ymm9                       #! EA = L0x7fffffff4760; Value = 0xffe74d2bffbc48d6; PC = 0x5555555748f7 *)
mov %ymm9 [L0x7fffffff4760, L0x7fffffff4764, L0x7fffffff4768, L0x7fffffff476c, L0x7fffffff4770, L0x7fffffff4774, L0x7fffffff4778, L0x7fffffff477c];
(* vmovdqa 0x2c0(%rdi),%ymm10                      #! EA = L0x7fffffff4780; Value = 0xffea109affe5d5a2; PC = 0x5555555748ff *)
mov %ymm10 [L0x7fffffff4780, L0x7fffffff4784, L0x7fffffff4788, L0x7fffffff478c, L0x7fffffff4790, L0x7fffffff4794, L0x7fffffff4798, L0x7fffffff479c];
(* vmovdqa 0x2e0(%rdi),%ymm11                      #! EA = L0x7fffffff47a0; Value = 0x0023b5f80019fa68; PC = 0x555555574907 *)
mov %ymm11 [L0x7fffffff47a0, L0x7fffffff47a4, L0x7fffffff47a8, L0x7fffffff47ac, L0x7fffffff47b0, L0x7fffffff47b4, L0x7fffffff47b8, L0x7fffffff47bc];
(* vpbroadcastd 0x98(%rsi),%ymm1                   #! EA = L0x555555579e78; Value = 0x61cb9e246017a128; PC = 0x55555557490f *)
broadcast %ymm1 8 [L0x555555579e78];
(* vpbroadcastd 0x538(%rsi),%ymm2                  #! EA = L0x55555557a318; Value = 0x00071e24fff2a128; PC = 0x555555574918 *)
broadcast %ymm2 8 [L0x55555557a318];
(* vpmuldq %ymm1,%ymm8,%ymm13                      #! PC = 0x555555574921 *)
mull %ymm13_h %ymm13_l (%ymm8[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm8,%ymm12                          #! PC = 0x555555574926 *)
mov %ymm12 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x55555557492b *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm8,%ymm8                       #! PC = 0x555555574930 *)
mull %ymm8_h %ymm8_l (%ymm8[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm8_ls@sint32[4] %ymm8_l;
mov %ymm8 [%ymm8_ls[0], %ymm8_h[0], %ymm8_ls[1], %ymm8_h[1], %ymm8_ls[2], %ymm8_h[2], %ymm8_ls[3], %ymm8_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555574935 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x55555557493a *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x55555557493f *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm8_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm8_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm8,%ymm8                           #! PC = 0x555555574944 *)
mov %ymm8 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm8,%ymm8               #! PC = 0x555555574949 *)
mov %ymm8 [%ymm8[0], %ymm12[1], %ymm8[2], %ymm12[3], %ymm8[4], %ymm12[5], %ymm8[6], %ymm12[7]];
(* vpsubd %ymm8,%ymm4,%ymm12                       #! PC = 0x55555557494f *)
sub %ymm12 %ymm4 %ymm8;
(* vpaddd %ymm8,%ymm4,%ymm4                        #! PC = 0x555555574954 *)
add %ymm4 %ymm4 %ymm8;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574959 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x55555557495e *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm8                      #! PC = 0x555555574964 *)
add %ymm8 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm4,%ymm4                       #! PC = 0x555555574969 *)
sub %ymm4 %ymm4 %ymm13;
(* vpmuldq %ymm1,%ymm9,%ymm13                      #! PC = 0x55555557496e *)
mull %ymm13_h %ymm13_l (%ymm9[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm9,%ymm12                          #! PC = 0x555555574973 *)
mov %ymm12 (%ymm9[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555574978 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm9,%ymm9                       #! PC = 0x55555557497d *)
mull %ymm9_h %ymm9_l (%ymm9[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm9_ls@sint32[4] %ymm9_l;
mov %ymm9 [%ymm9_ls[0], %ymm9_h[0], %ymm9_ls[1], %ymm9_h[1], %ymm9_ls[2], %ymm9_h[2], %ymm9_ls[3], %ymm9_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555574982 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574987 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x55555557498c *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm9_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm9_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm9,%ymm9                           #! PC = 0x555555574991 *)
mov %ymm9 (%ymm9[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm9,%ymm9               #! PC = 0x555555574996 *)
mov %ymm9 [%ymm9[0], %ymm12[1], %ymm9[2], %ymm12[3], %ymm9[4], %ymm12[5], %ymm9[6], %ymm12[7]];
(* vpsubd %ymm9,%ymm5,%ymm12                       #! PC = 0x55555557499c *)
sub %ymm12 %ymm5 %ymm9;
(* vpaddd %ymm9,%ymm5,%ymm5                        #! PC = 0x5555555749a1 *)
add %ymm5 %ymm5 %ymm9;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555749a6 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555749ab *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm9                      #! PC = 0x5555555749b1 *)
add %ymm9 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm5,%ymm5                       #! PC = 0x5555555749b6 *)
sub %ymm5 %ymm5 %ymm13;
(* vpmuldq %ymm1,%ymm10,%ymm13                     #! PC = 0x5555555749bb *)
mull %ymm13_h %ymm13_l (%ymm10[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm10,%ymm12                         #! PC = 0x5555555749c0 *)
mov %ymm12 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x5555555749c5 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm10,%ymm10                     #! PC = 0x5555555749ca *)
mull %ymm10_h %ymm10_l (%ymm10[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm10_ls@sint32[4] %ymm10_l;
mov %ymm10 [%ymm10_ls[0], %ymm10_h[0], %ymm10_ls[1], %ymm10_h[1], %ymm10_ls[2], %ymm10_h[2], %ymm10_ls[3], %ymm10_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x5555555749cf *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555749d4 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555749d9 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm10_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm10_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm10,%ymm10                         #! PC = 0x5555555749de *)
mov %ymm10 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm10,%ymm10             #! PC = 0x5555555749e3 *)
mov %ymm10 [%ymm10[0], %ymm12[1], %ymm10[2], %ymm12[3], %ymm10[4], %ymm12[5], %ymm10[6], %ymm12[7]];
(* vpsubd %ymm10,%ymm6,%ymm12                      #! PC = 0x5555555749e9 *)
sub %ymm12 %ymm6 %ymm10;
(* vpaddd %ymm10,%ymm6,%ymm6                       #! PC = 0x5555555749ee *)
add %ymm6 %ymm6 %ymm10;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555749f3 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555749f8 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm10                     #! PC = 0x5555555749fe *)
add %ymm10 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm6,%ymm6                       #! PC = 0x555555574a03 *)
sub %ymm6 %ymm6 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x555555574a08 *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x555555574a0d *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555574a12 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x555555574a17 *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555574a1c *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574a21 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555574a26 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x555555574a2b *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x555555574a30 *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm7,%ymm12                      #! PC = 0x555555574a36 *)
sub %ymm12 %ymm7 %ymm11;
(* vpaddd %ymm11,%ymm7,%ymm7                       #! PC = 0x555555574a3b *)
add %ymm7 %ymm7 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574a40 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555574a45 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x555555574a4b *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm7,%ymm7                       #! PC = 0x555555574a50 *)
sub %ymm7 %ymm7 %ymm13;
(* vperm2i128 $0x20,%ymm8,%ymm4,%ymm3              #! PC = 0x555555574a55 *)
mov %ymm3 (%ymm4[0, 1, 2, 3] ++ %ymm8[0, 1, 2, 3]);
(* vperm2i128 $0x31,%ymm8,%ymm4,%ymm8              #! PC = 0x555555574a5b *)
mov %ymm8 (%ymm4[4, 5, 6, 7] ++ %ymm8[4, 5, 6, 7]);
(* vperm2i128 $0x20,%ymm9,%ymm5,%ymm4              #! PC = 0x555555574a61 *)
mov %ymm4 (%ymm5[0, 1, 2, 3] ++ %ymm9[0, 1, 2, 3]);
(* vperm2i128 $0x31,%ymm9,%ymm5,%ymm9              #! PC = 0x555555574a67 *)
mov %ymm9 (%ymm5[4, 5, 6, 7] ++ %ymm9[4, 5, 6, 7]);
(* vperm2i128 $0x20,%ymm10,%ymm6,%ymm5             #! PC = 0x555555574a6d *)
mov %ymm5 (%ymm6[0, 1, 2, 3] ++ %ymm10[0, 1, 2, 3]);
(* vperm2i128 $0x31,%ymm10,%ymm6,%ymm10            #! PC = 0x555555574a73 *)
mov %ymm10 (%ymm6[4, 5, 6, 7] ++ %ymm10[4, 5, 6, 7]);
(* vperm2i128 $0x20,%ymm11,%ymm7,%ymm6             #! PC = 0x555555574a79 *)
mov %ymm6 (%ymm7[0, 1, 2, 3] ++ %ymm11[0, 1, 2, 3]);
(* vperm2i128 $0x31,%ymm11,%ymm7,%ymm11            #! PC = 0x555555574a7f *)
mov %ymm11 (%ymm7[4, 5, 6, 7] ++ %ymm11[4, 5, 6, 7]);

(* vmovdqa 0xe0(%rsi),%ymm1                        #! EA = L0x555555579ec0; Value = 0x8cfe3a758cfe3a75; PC = 0x555555574a85 *)
mov %ymm1 [L0x555555579ec0, L0x555555579ec4, L0x555555579ec8, L0x555555579ecc, L0x555555579ed0, L0x555555579ed4, L0x555555579ed8, L0x555555579edc];
(* vmovdqa 0x580(%rsi),%ymm2                       #! EA = L0x55555557a360; Value = 0x002f9a75002f9a75; PC = 0x555555574a8d *)
mov %ymm2 [L0x55555557a360, L0x55555557a364, L0x55555557a368, L0x55555557a36c, L0x55555557a370, L0x55555557a374, L0x55555557a378, L0x55555557a37c];
(* vpmuldq %ymm1,%ymm5,%ymm13                      #! PC = 0x555555574a95 *)
mull %ymm13_h %ymm13_l (%ymm5[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm5,%ymm12                          #! PC = 0x555555574a9a *)
mov %ymm12 (%ymm5[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555574a9e *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm5,%ymm5                       #! PC = 0x555555574aa3 *)
mull %ymm5_h %ymm5_l (%ymm5[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm5_ls@sint32[4] %ymm5_l;
mov %ymm5 [%ymm5_ls[0], %ymm5_h[0], %ymm5_ls[1], %ymm5_h[1], %ymm5_ls[2], %ymm5_h[2], %ymm5_ls[3], %ymm5_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555574aa8 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574aad *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555574ab2 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm5_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm5_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm5,%ymm5                           #! PC = 0x555555574ab7 *)
mov %ymm5 (%ymm5[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm5,%ymm5               #! PC = 0x555555574abb *)
mov %ymm5 [%ymm5[0], %ymm12[1], %ymm5[2], %ymm12[3], %ymm5[4], %ymm12[5], %ymm5[6], %ymm12[7]];
(* vpsubd %ymm5,%ymm3,%ymm12                       #! PC = 0x555555574ac1 *)
sub %ymm12 %ymm3 %ymm5;
(* vpaddd %ymm5,%ymm3,%ymm3                        #! PC = 0x555555574ac5 *)
add %ymm3 %ymm3 %ymm5;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574ac9 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555574ace *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm5                      #! PC = 0x555555574ad4 *)
add %ymm5 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm3,%ymm3                       #! PC = 0x555555574ad9 *)
sub %ymm3 %ymm3 %ymm13;
(* vpmuldq %ymm1,%ymm10,%ymm13                     #! PC = 0x555555574ade *)
mull %ymm13_h %ymm13_l (%ymm10[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm10,%ymm12                         #! PC = 0x555555574ae3 *)
mov %ymm12 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555574ae8 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm10,%ymm10                     #! PC = 0x555555574aed *)
mull %ymm10_h %ymm10_l (%ymm10[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm10_ls@sint32[4] %ymm10_l;
mov %ymm10 [%ymm10_ls[0], %ymm10_h[0], %ymm10_ls[1], %ymm10_h[1], %ymm10_ls[2], %ymm10_h[2], %ymm10_ls[3], %ymm10_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555574af2 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574af7 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555574afc *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm10_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm10_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm10,%ymm10                         #! PC = 0x555555574b01 *)
mov %ymm10 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm10,%ymm10             #! PC = 0x555555574b06 *)
mov %ymm10 [%ymm10[0], %ymm12[1], %ymm10[2], %ymm12[3], %ymm10[4], %ymm12[5], %ymm10[6], %ymm12[7]];
(* vpsubd %ymm10,%ymm8,%ymm12                      #! PC = 0x555555574b0c *)
sub %ymm12 %ymm8 %ymm10;
(* vpaddd %ymm10,%ymm8,%ymm8                       #! PC = 0x555555574b11 *)
add %ymm8 %ymm8 %ymm10;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574b16 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555574b1b *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm10                     #! PC = 0x555555574b21 *)
add %ymm10 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm8,%ymm8                       #! PC = 0x555555574b26 *)
sub %ymm8 %ymm8 %ymm13;
(* vpmuldq %ymm1,%ymm6,%ymm13                      #! PC = 0x555555574b2b *)
mull %ymm13_h %ymm13_l (%ymm6[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm6,%ymm12                          #! PC = 0x555555574b30 *)
mov %ymm12 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555574b34 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm6,%ymm6                       #! PC = 0x555555574b39 *)
mull %ymm6_h %ymm6_l (%ymm6[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm6_ls@sint32[4] %ymm6_l;
mov %ymm6 [%ymm6_ls[0], %ymm6_h[0], %ymm6_ls[1], %ymm6_h[1], %ymm6_ls[2], %ymm6_h[2], %ymm6_ls[3], %ymm6_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555574b3e *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574b43 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555574b48 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm6_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm6_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm6,%ymm6                           #! PC = 0x555555574b4d *)
mov %ymm6 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm6,%ymm6               #! PC = 0x555555574b51 *)
mov %ymm6 [%ymm6[0], %ymm12[1], %ymm6[2], %ymm12[3], %ymm6[4], %ymm12[5], %ymm6[6], %ymm12[7]];
(* vpsubd %ymm6,%ymm4,%ymm12                       #! PC = 0x555555574b57 *)
sub %ymm12 %ymm4 %ymm6;
(* vpaddd %ymm6,%ymm4,%ymm4                        #! PC = 0x555555574b5b *)
add %ymm4 %ymm4 %ymm6;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574b5f *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555574b64 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm6                      #! PC = 0x555555574b6a *)
add %ymm6 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm4,%ymm4                       #! PC = 0x555555574b6f *)
sub %ymm4 %ymm4 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x555555574b74 *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x555555574b79 *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555574b7e *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x555555574b83 *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555574b88 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574b8d *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555574b92 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x555555574b97 *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x555555574b9c *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm9,%ymm12                      #! PC = 0x555555574ba2 *)
sub %ymm12 %ymm9 %ymm11;
(* vpaddd %ymm11,%ymm9,%ymm9                       #! PC = 0x555555574ba7 *)
add %ymm9 %ymm9 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574bac *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555574bb1 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x555555574bb7 *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm9,%ymm9                       #! PC = 0x555555574bbc *)
sub %ymm9 %ymm9 %ymm13;
(* vpunpcklqdq %ymm5,%ymm3,%ymm7                   #! PC = 0x555555574bc1 *)
mov %ymm7 [%ymm3[0], %ymm3[1], %ymm5[0], %ymm5[1], %ymm3[4], %ymm3[5], %ymm5[4], %ymm5[5]];
(* vpunpckhqdq %ymm5,%ymm3,%ymm5                   #! PC = 0x555555574bc5 *)
mov %ymm5 [%ymm3[2], %ymm3[3], %ymm5[2], %ymm5[3], %ymm3[6], %ymm3[7], %ymm5[6], %ymm5[7]];
(* vpunpcklqdq %ymm10,%ymm8,%ymm3                  #! PC = 0x555555574bc9 *)
mov %ymm3 [%ymm8[0], %ymm8[1], %ymm10[0], %ymm10[1], %ymm8[4], %ymm8[5], %ymm10[4], %ymm10[5]];
(* vpunpckhqdq %ymm10,%ymm8,%ymm10                 #! PC = 0x555555574bce *)
mov %ymm10 [%ymm8[2], %ymm8[3], %ymm10[2], %ymm10[3], %ymm8[6], %ymm8[7], %ymm10[6], %ymm10[7]];
(* vpunpcklqdq %ymm6,%ymm4,%ymm8                   #! PC = 0x555555574bd3 *)
mov %ymm8 [%ymm4[0], %ymm4[1], %ymm6[0], %ymm6[1], %ymm4[4], %ymm4[5], %ymm6[4], %ymm6[5]];
(* vpunpckhqdq %ymm6,%ymm4,%ymm6                   #! PC = 0x555555574bd7 *)
mov %ymm6 [%ymm4[2], %ymm4[3], %ymm6[2], %ymm6[3], %ymm4[6], %ymm4[7], %ymm6[6], %ymm6[7]];
(* vpunpcklqdq %ymm11,%ymm9,%ymm4                  #! PC = 0x555555574bdb *)
mov %ymm4 [%ymm9[0], %ymm9[1], %ymm11[0], %ymm11[1], %ymm9[4], %ymm9[5], %ymm11[4], %ymm11[5]];
(* vpunpckhqdq %ymm11,%ymm9,%ymm11                 #! PC = 0x555555574be0 *)
mov %ymm11 [%ymm9[2], %ymm9[3], %ymm11[2], %ymm11[3], %ymm9[6], %ymm9[7], %ymm11[6], %ymm11[7]];

(* vmovdqa 0x160(%rsi),%ymm1                       #! EA = L0x555555579f40; Value = 0x629a6dd6629a6dd6; PC = 0x555555574be5 *)
mov %ymm1 [L0x555555579f40, L0x555555579f44, L0x555555579f48, L0x555555579f4c, L0x555555579f50, L0x555555579f54, L0x555555579f58, L0x555555579f5c];
(* vmovdqa 0x600(%rsi),%ymm2                       #! EA = L0x55555557a3e0; Value = 0xffdfadd6ffdfadd6; PC = 0x555555574bed *)
mov %ymm2 [L0x55555557a3e0, L0x55555557a3e4, L0x55555557a3e8, L0x55555557a3ec, L0x55555557a3f0, L0x55555557a3f4, L0x55555557a3f8, L0x55555557a3fc];
(* vpmuldq %ymm1,%ymm8,%ymm13                      #! PC = 0x555555574bf5 *)
mull %ymm13_h %ymm13_l (%ymm8[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm8,%ymm12                          #! PC = 0x555555574bfa *)
mov %ymm12 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555574bff *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm8,%ymm8                       #! PC = 0x555555574c04 *)
mull %ymm8_h %ymm8_l (%ymm8[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm8_ls@sint32[4] %ymm8_l;
mov %ymm8 [%ymm8_ls[0], %ymm8_h[0], %ymm8_ls[1], %ymm8_h[1], %ymm8_ls[2], %ymm8_h[2], %ymm8_ls[3], %ymm8_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555574c09 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574c0e *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555574c13 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm8_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm8_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm8,%ymm8                           #! PC = 0x555555574c18 *)
mov %ymm8 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm8,%ymm8               #! PC = 0x555555574c1d *)
mov %ymm8 [%ymm8[0], %ymm12[1], %ymm8[2], %ymm12[3], %ymm8[4], %ymm12[5], %ymm8[6], %ymm12[7]];
(* vpsubd %ymm8,%ymm7,%ymm12                       #! PC = 0x555555574c23 *)
sub %ymm12 %ymm7 %ymm8;
(* vpaddd %ymm8,%ymm7,%ymm7                        #! PC = 0x555555574c28 *)
add %ymm7 %ymm7 %ymm8;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574c2d *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555574c32 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm8                      #! PC = 0x555555574c38 *)
add %ymm8 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm7,%ymm7                       #! PC = 0x555555574c3d *)
sub %ymm7 %ymm7 %ymm13;
(* vpmuldq %ymm1,%ymm6,%ymm13                      #! PC = 0x555555574c42 *)
mull %ymm13_h %ymm13_l (%ymm6[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm6,%ymm12                          #! PC = 0x555555574c47 *)
mov %ymm12 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555574c4b *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm6,%ymm6                       #! PC = 0x555555574c50 *)
mull %ymm6_h %ymm6_l (%ymm6[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm6_ls@sint32[4] %ymm6_l;
mov %ymm6 [%ymm6_ls[0], %ymm6_h[0], %ymm6_ls[1], %ymm6_h[1], %ymm6_ls[2], %ymm6_h[2], %ymm6_ls[3], %ymm6_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555574c55 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574c5a *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555574c5f *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm6_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm6_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm6,%ymm6                           #! PC = 0x555555574c64 *)
mov %ymm6 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm6,%ymm6               #! PC = 0x555555574c68 *)
mov %ymm6 [%ymm6[0], %ymm12[1], %ymm6[2], %ymm12[3], %ymm6[4], %ymm12[5], %ymm6[6], %ymm12[7]];
(* vpsubd %ymm6,%ymm5,%ymm12                       #! PC = 0x555555574c6e *)
sub %ymm12 %ymm5 %ymm6;
(* vpaddd %ymm6,%ymm5,%ymm5                        #! PC = 0x555555574c72 *)
add %ymm5 %ymm5 %ymm6;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574c76 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555574c7b *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm6                      #! PC = 0x555555574c81 *)
add %ymm6 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm5,%ymm5                       #! PC = 0x555555574c86 *)
sub %ymm5 %ymm5 %ymm13;
(* vpmuldq %ymm1,%ymm4,%ymm13                      #! PC = 0x555555574c8b *)
mull %ymm13_h %ymm13_l (%ymm4[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm4,%ymm12                          #! PC = 0x555555574c90 *)
mov %ymm12 (%ymm4[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555574c94 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm4,%ymm4                       #! PC = 0x555555574c99 *)
mull %ymm4_h %ymm4_l (%ymm4[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm4_ls@sint32[4] %ymm4_l;
mov %ymm4 [%ymm4_ls[0], %ymm4_h[0], %ymm4_ls[1], %ymm4_h[1], %ymm4_ls[2], %ymm4_h[2], %ymm4_ls[3], %ymm4_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555574c9e *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574ca3 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555574ca8 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm4_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm4_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm4,%ymm4                           #! PC = 0x555555574cad *)
mov %ymm4 (%ymm4[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm4,%ymm4               #! PC = 0x555555574cb1 *)
mov %ymm4 [%ymm4[0], %ymm12[1], %ymm4[2], %ymm12[3], %ymm4[4], %ymm12[5], %ymm4[6], %ymm12[7]];
(* vpsubd %ymm4,%ymm3,%ymm12                       #! PC = 0x555555574cb7 *)
sub %ymm12 %ymm3 %ymm4;
(* vpaddd %ymm4,%ymm3,%ymm3                        #! PC = 0x555555574cbb *)
add %ymm3 %ymm3 %ymm4;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574cbf *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555574cc4 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm4                      #! PC = 0x555555574cca *)
add %ymm4 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm3,%ymm3                       #! PC = 0x555555574ccf *)
sub %ymm3 %ymm3 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x555555574cd4 *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x555555574cd9 *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555574cde *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x555555574ce3 *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555574ce8 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574ced *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555574cf2 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x555555574cf7 *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x555555574cfc *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm10,%ymm12                     #! PC = 0x555555574d02 *)
sub %ymm12 %ymm10 %ymm11;
(* vpaddd %ymm11,%ymm10,%ymm10                     #! PC = 0x555555574d07 *)
add %ymm10 %ymm10 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574d0c *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555574d11 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x555555574d17 *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm10,%ymm10                     #! PC = 0x555555574d1c *)
sub %ymm10 %ymm10 %ymm13;
(* vmovsldup %ymm8,%ymm9                           #! PC = 0x555555574d21 *)
mov %ymm9 (%ymm8[0, 0, 2, 2, 4, 4, 6, 6]);
(* vpblendd $0xaa,%ymm9,%ymm7,%ymm9                #! PC = 0x555555574d26 *)
mov %ymm9 [%ymm7[0], %ymm9[1], %ymm7[2], %ymm9[3], %ymm7[4], %ymm9[5], %ymm7[6], %ymm9[7]];
(* vpsrlq $0x20,%ymm7,%ymm7                        #! PC = 0x555555574d2c *)
mov %ymm7 [%ymm7[1], 0@sint32, %ymm7[3], 0@sint32, %ymm7[5], 0@sint32, %ymm7[7], 0@sint32];
(* vpblendd $0xaa,%ymm8,%ymm7,%ymm8                #! PC = 0x555555574d31 *)
mov %ymm8 [%ymm7[0], %ymm8[1], %ymm7[2], %ymm8[3], %ymm7[4], %ymm8[5], %ymm7[6], %ymm8[7]];
(* vmovsldup %ymm6,%ymm7                           #! PC = 0x555555574d37 *)
mov %ymm7 (%ymm6[0, 0, 2, 2, 4, 4, 6, 6]);
(* vpblendd $0xaa,%ymm7,%ymm5,%ymm7                #! PC = 0x555555574d3b *)
mov %ymm7 [%ymm5[0], %ymm7[1], %ymm5[2], %ymm7[3], %ymm5[4], %ymm7[5], %ymm5[6], %ymm7[7]];
(* vpsrlq $0x20,%ymm5,%ymm5                        #! PC = 0x555555574d41 *)
mov %ymm5 [%ymm5[1], 0@sint32, %ymm5[3], 0@sint32, %ymm5[5], 0@sint32, %ymm5[7], 0@sint32];
(* vpblendd $0xaa,%ymm6,%ymm5,%ymm6                #! PC = 0x555555574d46 *)
mov %ymm6 [%ymm5[0], %ymm6[1], %ymm5[2], %ymm6[3], %ymm5[4], %ymm6[5], %ymm5[6], %ymm6[7]];
(* vmovsldup %ymm4,%ymm5                           #! PC = 0x555555574d4c *)
mov %ymm5 (%ymm4[0, 0, 2, 2, 4, 4, 6, 6]);
(* vpblendd $0xaa,%ymm5,%ymm3,%ymm5                #! PC = 0x555555574d50 *)
mov %ymm5 [%ymm3[0], %ymm5[1], %ymm3[2], %ymm5[3], %ymm3[4], %ymm5[5], %ymm3[6], %ymm5[7]];
(* vpsrlq $0x20,%ymm3,%ymm3                        #! PC = 0x555555574d56 *)
mov %ymm3 [%ymm3[1], 0@sint32, %ymm3[3], 0@sint32, %ymm3[5], 0@sint32, %ymm3[7], 0@sint32];
(* vpblendd $0xaa,%ymm4,%ymm3,%ymm4                #! PC = 0x555555574d5b *)
mov %ymm4 [%ymm3[0], %ymm4[1], %ymm3[2], %ymm4[3], %ymm3[4], %ymm4[5], %ymm3[6], %ymm4[7]];
(* vmovsldup %ymm11,%ymm3                          #! PC = 0x555555574d61 *)
mov %ymm3 (%ymm11[0, 0, 2, 2, 4, 4, 6, 6]);
(* vpblendd $0xaa,%ymm3,%ymm10,%ymm3               #! PC = 0x555555574d66 *)
mov %ymm3 [%ymm10[0], %ymm3[1], %ymm10[2], %ymm3[3], %ymm10[4], %ymm3[5], %ymm10[6], %ymm3[7]];
(* vpsrlq $0x20,%ymm10,%ymm10                      #! PC = 0x555555574d6c *)
mov %ymm10 [%ymm10[1], 0@sint32, %ymm10[3], 0@sint32, %ymm10[5], 0@sint32, %ymm10[7], 0@sint32];
(* vpblendd $0xaa,%ymm11,%ymm10,%ymm11             #! PC = 0x555555574d72 *)
mov %ymm11 [%ymm10[0], %ymm11[1], %ymm10[2], %ymm11[3], %ymm10[4], %ymm11[5], %ymm10[6], %ymm11[7]];

(* vmovdqa 0x1e0(%rsi),%ymm1                       #! EA = L0x555555579fc0; Value = 0xca41aad697adb23d; PC = 0x555555574d78 *)
mov %ymm1 [L0x555555579fc0, L0x555555579fc4, L0x555555579fc8, L0x555555579fcc, L0x555555579fd0, L0x555555579fd4, L0x555555579fd8, L0x555555579fdc];
(* vmovdqa 0x680(%rsi),%ymm2                       #! EA = L0x55555557a460; Value = 0xffe6ead6ffe6123d; PC = 0x555555574d80 *)
mov %ymm2 [L0x55555557a460, L0x55555557a464, L0x55555557a468, L0x55555557a46c, L0x55555557a470, L0x55555557a474, L0x55555557a478, L0x55555557a47c];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x555555574d88 *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x555555574d8d *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm5,%ymm13                      #! PC = 0x555555574d91 *)
mull %ymm13_h %ymm13_l (%ymm5[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm5,%ymm12                          #! PC = 0x555555574d96 *)
mov %ymm12 (%ymm5[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555574d9a *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm5,%ymm5                       #! PC = 0x555555574d9f *)
mull %ymm5_h %ymm5_l (%ymm5[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm5_ls@sint32[4] %ymm5_l;
mov %ymm5 [%ymm5_ls[0], %ymm5_h[0], %ymm5_ls[1], %ymm5_h[1], %ymm5_ls[2], %ymm5_h[2], %ymm5_ls[3], %ymm5_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555574da4 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574da9 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555574dae *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm5_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm5_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm5,%ymm5                           #! PC = 0x555555574db3 *)
mov %ymm5 (%ymm5[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm5,%ymm5               #! PC = 0x555555574db7 *)
mov %ymm5 [%ymm5[0], %ymm12[1], %ymm5[2], %ymm12[3], %ymm5[4], %ymm12[5], %ymm5[6], %ymm12[7]];
(* vpsubd %ymm5,%ymm9,%ymm12                       #! PC = 0x555555574dbd *)
sub %ymm12 %ymm9 %ymm5;
(* vpaddd %ymm5,%ymm9,%ymm9                        #! PC = 0x555555574dc1 *)
add %ymm9 %ymm9 %ymm5;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574dc5 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555574dca *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm5                      #! PC = 0x555555574dd0 *)
add %ymm5 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm9,%ymm9                       #! PC = 0x555555574dd5 *)
sub %ymm9 %ymm9 %ymm13;
(* vpmuldq %ymm1,%ymm4,%ymm13                      #! PC = 0x555555574dda *)
mull %ymm13_h %ymm13_l (%ymm4[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm4,%ymm12                          #! PC = 0x555555574ddf *)
mov %ymm12 (%ymm4[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555574de3 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm4,%ymm4                       #! PC = 0x555555574de8 *)
mull %ymm4_h %ymm4_l (%ymm4[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm4_ls@sint32[4] %ymm4_l;
mov %ymm4 [%ymm4_ls[0], %ymm4_h[0], %ymm4_ls[1], %ymm4_h[1], %ymm4_ls[2], %ymm4_h[2], %ymm4_ls[3], %ymm4_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555574ded *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574df2 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555574df7 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm4_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm4_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm4,%ymm4                           #! PC = 0x555555574dfc *)
mov %ymm4 (%ymm4[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm4,%ymm4               #! PC = 0x555555574e00 *)
mov %ymm4 [%ymm4[0], %ymm12[1], %ymm4[2], %ymm12[3], %ymm4[4], %ymm12[5], %ymm4[6], %ymm12[7]];
(* vpsubd %ymm4,%ymm8,%ymm12                       #! PC = 0x555555574e06 *)
sub %ymm12 %ymm8 %ymm4;
(* vpaddd %ymm4,%ymm8,%ymm8                        #! PC = 0x555555574e0a *)
add %ymm8 %ymm8 %ymm4;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574e0e *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555574e13 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm4                      #! PC = 0x555555574e19 *)
add %ymm4 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm8,%ymm8                       #! PC = 0x555555574e1e *)
sub %ymm8 %ymm8 %ymm13;
(* vpmuldq %ymm1,%ymm3,%ymm13                      #! PC = 0x555555574e23 *)
mull %ymm13_h %ymm13_l (%ymm3[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm3,%ymm12                          #! PC = 0x555555574e28 *)
mov %ymm12 (%ymm3[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555574e2c *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm3,%ymm3                       #! PC = 0x555555574e31 *)
mull %ymm3_h %ymm3_l (%ymm3[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm3_ls@sint32[4] %ymm3_l;
mov %ymm3 [%ymm3_ls[0], %ymm3_h[0], %ymm3_ls[1], %ymm3_h[1], %ymm3_ls[2], %ymm3_h[2], %ymm3_ls[3], %ymm3_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555574e36 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574e3b *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555574e40 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm3_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm3_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm3,%ymm3                           #! PC = 0x555555574e45 *)
mov %ymm3 (%ymm3[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm3,%ymm3               #! PC = 0x555555574e49 *)
mov %ymm3 [%ymm3[0], %ymm12[1], %ymm3[2], %ymm12[3], %ymm3[4], %ymm12[5], %ymm3[6], %ymm12[7]];
(* vpsubd %ymm3,%ymm7,%ymm12                       #! PC = 0x555555574e4f *)
sub %ymm12 %ymm7 %ymm3;
(* vpaddd %ymm3,%ymm7,%ymm7                        #! PC = 0x555555574e53 *)
add %ymm7 %ymm7 %ymm3;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574e57 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555574e5c *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm3                      #! PC = 0x555555574e62 *)
add %ymm3 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm7,%ymm7                       #! PC = 0x555555574e67 *)
sub %ymm7 %ymm7 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x555555574e6c *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x555555574e71 *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555574e76 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x555555574e7b *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555574e80 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574e85 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555574e8a *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x555555574e8f *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x555555574e94 *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm6,%ymm12                      #! PC = 0x555555574e9a *)
sub %ymm12 %ymm6 %ymm11;
(* vpaddd %ymm11,%ymm6,%ymm6                       #! PC = 0x555555574e9f *)
add %ymm6 %ymm6 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574ea4 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555574ea9 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x555555574eaf *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm6,%ymm6                       #! PC = 0x555555574eb4 *)
sub %ymm6 %ymm6 %ymm13;

(* vmovdqa 0x260(%rsi),%ymm1                       #! EA = L0x55555557a040; Value = 0x490aa4174eca1be9; PC = 0x555555574eb9 *)
mov %ymm1 [L0x55555557a040, L0x55555557a044, L0x55555557a048, L0x55555557a04c, L0x55555557a050, L0x55555557a054, L0x55555557a058, L0x55555557a05c];
(* vmovdqa 0x700(%rsi),%ymm2                       #! EA = L0x55555557a4e0; Value = 0x0007c417ffccfbe9; PC = 0x555555574ec1 *)
mov %ymm2 [L0x55555557a4e0, L0x55555557a4e4, L0x55555557a4e8, L0x55555557a4ec, L0x55555557a4f0, L0x55555557a4f4, L0x55555557a4f8, L0x55555557a4fc];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x555555574ec9 *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x555555574ece *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm7,%ymm13                      #! PC = 0x555555574ed2 *)
mull %ymm13_h %ymm13_l (%ymm7[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm7,%ymm12                          #! PC = 0x555555574ed7 *)
mov %ymm12 (%ymm7[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555574edb *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm7,%ymm7                       #! PC = 0x555555574ee0 *)
mull %ymm7_h %ymm7_l (%ymm7[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm7_ls@sint32[4] %ymm7_l;
mov %ymm7 [%ymm7_ls[0], %ymm7_h[0], %ymm7_ls[1], %ymm7_h[1], %ymm7_ls[2], %ymm7_h[2], %ymm7_ls[3], %ymm7_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555574ee5 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574eea *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555574eef *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm7_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm7_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm7,%ymm7                           #! PC = 0x555555574ef4 *)
mov %ymm7 (%ymm7[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm7,%ymm7               #! PC = 0x555555574ef8 *)
mov %ymm7 [%ymm7[0], %ymm12[1], %ymm7[2], %ymm12[3], %ymm7[4], %ymm12[5], %ymm7[6], %ymm12[7]];
(* vpsubd %ymm7,%ymm9,%ymm12                       #! PC = 0x555555574efe *)
sub %ymm12 %ymm9 %ymm7;
(* vpaddd %ymm7,%ymm9,%ymm9                        #! PC = 0x555555574f02 *)
add %ymm9 %ymm9 %ymm7;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574f06 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555574f0b *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm7                      #! PC = 0x555555574f11 *)
add %ymm7 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm9,%ymm9                       #! PC = 0x555555574f16 *)
sub %ymm9 %ymm9 %ymm13;
(* vpmuldq %ymm1,%ymm6,%ymm13                      #! PC = 0x555555574f1b *)
mull %ymm13_h %ymm13_l (%ymm6[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm6,%ymm12                          #! PC = 0x555555574f20 *)
mov %ymm12 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555574f24 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm6,%ymm6                       #! PC = 0x555555574f29 *)
mull %ymm6_h %ymm6_l (%ymm6[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm6_ls@sint32[4] %ymm6_l;
mov %ymm6 [%ymm6_ls[0], %ymm6_h[0], %ymm6_ls[1], %ymm6_h[1], %ymm6_ls[2], %ymm6_h[2], %ymm6_ls[3], %ymm6_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555574f2e *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574f33 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555574f38 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm6_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm6_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm6,%ymm6                           #! PC = 0x555555574f3d *)
mov %ymm6 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm6,%ymm6               #! PC = 0x555555574f41 *)
mov %ymm6 [%ymm6[0], %ymm12[1], %ymm6[2], %ymm12[3], %ymm6[4], %ymm12[5], %ymm6[6], %ymm12[7]];
(* vpsubd %ymm6,%ymm8,%ymm12                       #! PC = 0x555555574f47 *)
sub %ymm12 %ymm8 %ymm6;
(* vpaddd %ymm6,%ymm8,%ymm8                        #! PC = 0x555555574f4b *)
add %ymm8 %ymm8 %ymm6;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574f4f *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555574f54 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm6                      #! PC = 0x555555574f5a *)
add %ymm6 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm8,%ymm8                       #! PC = 0x555555574f5f *)
sub %ymm8 %ymm8 %ymm13;
(* vmovdqa 0x2e0(%rsi),%ymm1                       #! EA = L0x55555557a0c0; Value = 0x44e04588c9620af0; PC = 0x555555574f64 *)
mov %ymm1 [L0x55555557a0c0, L0x55555557a0c4, L0x55555557a0c8, L0x55555557a0cc, L0x55555557a0d0, L0x55555557a0d4, L0x55555557a0d8, L0x55555557a0dc];
(* vmovdqa 0x780(%rsi),%ymm2                       #! EA = L0x55555557a560; Value = 0x002f458800040af0; PC = 0x555555574f6c *)
mov %ymm2 [L0x55555557a560, L0x55555557a564, L0x55555557a568, L0x55555557a56c, L0x55555557a570, L0x55555557a574, L0x55555557a578, L0x55555557a57c];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x555555574f74 *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x555555574f79 *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm3,%ymm13                      #! PC = 0x555555574f7d *)
mull %ymm13_h %ymm13_l (%ymm3[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm3,%ymm12                          #! PC = 0x555555574f82 *)
mov %ymm12 (%ymm3[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555574f86 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm3,%ymm3                       #! PC = 0x555555574f8b *)
mull %ymm3_h %ymm3_l (%ymm3[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm3_ls@sint32[4] %ymm3_l;
mov %ymm3 [%ymm3_ls[0], %ymm3_h[0], %ymm3_ls[1], %ymm3_h[1], %ymm3_ls[2], %ymm3_h[2], %ymm3_ls[3], %ymm3_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555574f90 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574f95 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555574f9a *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm3_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm3_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm3,%ymm3                           #! PC = 0x555555574f9f *)
mov %ymm3 (%ymm3[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm3,%ymm3               #! PC = 0x555555574fa3 *)
mov %ymm3 [%ymm3[0], %ymm12[1], %ymm3[2], %ymm12[3], %ymm3[4], %ymm12[5], %ymm3[6], %ymm12[7]];
(* vpsubd %ymm3,%ymm5,%ymm12                       #! PC = 0x555555574fa9 *)
sub %ymm12 %ymm5 %ymm3;
(* vpaddd %ymm3,%ymm5,%ymm5                        #! PC = 0x555555574fad *)
add %ymm5 %ymm5 %ymm3;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574fb1 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555574fb6 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm3                      #! PC = 0x555555574fbc *)
add %ymm3 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm5,%ymm5                       #! PC = 0x555555574fc1 *)
sub %ymm5 %ymm5 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x555555574fc6 *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x555555574fcb *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555574fd0 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x555555574fd5 *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555574fda *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555574fdf *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555574fe4 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x555555574fe9 *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x555555574fee *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm4,%ymm12                      #! PC = 0x555555574ff4 *)
sub %ymm12 %ymm4 %ymm11;
(* vpaddd %ymm11,%ymm4,%ymm4                       #! PC = 0x555555574ff9 *)
add %ymm4 %ymm4 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555574ffe *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555575003 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x555555575009 *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm4,%ymm4                       #! PC = 0x55555557500e *)
sub %ymm4 %ymm4 %ymm13;

(* vmovdqa 0x360(%rsi),%ymm1                       #! EA = L0x55555557a140; Value = 0xee1784053196d953; PC = 0x555555575013 *)
mov %ymm1 [L0x55555557a140, L0x55555557a144, L0x55555557a148, L0x55555557a14c, L0x55555557a150, L0x55555557a154, L0x55555557a158, L0x55555557a15c];
(* vmovdqa 0x800(%rsi),%ymm2                       #! EA = L0x55555557a5e0; Value = 0x0016e405ffec7953; PC = 0x55555557501b *)
mov %ymm2 [L0x55555557a5e0, L0x55555557a5e4, L0x55555557a5e8, L0x55555557a5ec, L0x55555557a5f0, L0x55555557a5f4, L0x55555557a5f8, L0x55555557a5fc];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x555555575023 *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x555555575028 *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm8,%ymm13                      #! PC = 0x55555557502c *)
mull %ymm13_h %ymm13_l (%ymm8[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm8,%ymm12                          #! PC = 0x555555575031 *)
mov %ymm12 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555575036 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm8,%ymm8                       #! PC = 0x55555557503b *)
mull %ymm8_h %ymm8_l (%ymm8[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm8_ls@sint32[4] %ymm8_l;
mov %ymm8 [%ymm8_ls[0], %ymm8_h[0], %ymm8_ls[1], %ymm8_h[1], %ymm8_ls[2], %ymm8_h[2], %ymm8_ls[3], %ymm8_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555575040 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555575045 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x55555557504a *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm8_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm8_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm8,%ymm8                           #! PC = 0x55555557504f *)
mov %ymm8 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm8,%ymm8               #! PC = 0x555555575054 *)
mov %ymm8 [%ymm8[0], %ymm12[1], %ymm8[2], %ymm12[3], %ymm8[4], %ymm12[5], %ymm8[6], %ymm12[7]];
(* vpsubd %ymm8,%ymm9,%ymm12                       #! PC = 0x55555557505a *)
sub %ymm12 %ymm9 %ymm8;
(* vpaddd %ymm8,%ymm9,%ymm9                        #! PC = 0x55555557505f *)
add %ymm9 %ymm9 %ymm8;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555575064 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555575069 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm8                      #! PC = 0x55555557506f *)
add %ymm8 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm9,%ymm9                       #! PC = 0x555555575074 *)
sub %ymm9 %ymm9 %ymm13;
(* vmovdqa 0x3e0(%rsi),%ymm1                       #! EA = L0x55555557a1c0; Value = 0x2408bbe7bfb06099; PC = 0x555555575079 *)
mov %ymm1 [L0x55555557a1c0, L0x55555557a1c4, L0x55555557a1c8, L0x55555557a1cc, L0x55555557a1d0, L0x55555557a1d4, L0x55555557a1d8, L0x55555557a1dc];
(* vmovdqa 0x880(%rsi),%ymm2                       #! EA = L0x55555557a660; Value = 0x000bdbe7001d4099; PC = 0x555555575081 *)
mov %ymm2 [L0x55555557a660, L0x55555557a664, L0x55555557a668, L0x55555557a66c, L0x55555557a670, L0x55555557a674, L0x55555557a678, L0x55555557a67c];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x555555575089 *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x55555557508e *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm6,%ymm13                      #! PC = 0x555555575092 *)
mull %ymm13_h %ymm13_l (%ymm6[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm6,%ymm12                          #! PC = 0x555555575097 *)
mov %ymm12 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x55555557509b *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm6,%ymm6                       #! PC = 0x5555555750a0 *)
mull %ymm6_h %ymm6_l (%ymm6[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm6_ls@sint32[4] %ymm6_l;
mov %ymm6 [%ymm6_ls[0], %ymm6_h[0], %ymm6_ls[1], %ymm6_h[1], %ymm6_ls[2], %ymm6_h[2], %ymm6_ls[3], %ymm6_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x5555555750a5 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555750aa *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555750af *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm6_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm6_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm6,%ymm6                           #! PC = 0x5555555750b4 *)
mov %ymm6 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm6,%ymm6               #! PC = 0x5555555750b8 *)
mov %ymm6 [%ymm6[0], %ymm12[1], %ymm6[2], %ymm12[3], %ymm6[4], %ymm12[5], %ymm6[6], %ymm12[7]];
(* vpsubd %ymm6,%ymm7,%ymm12                       #! PC = 0x5555555750be *)
sub %ymm12 %ymm7 %ymm6;
(* vpaddd %ymm6,%ymm7,%ymm7                        #! PC = 0x5555555750c2 *)
add %ymm7 %ymm7 %ymm6;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555750c6 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555750cb *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm6                      #! PC = 0x5555555750d1 *)
add %ymm6 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm7,%ymm7                       #! PC = 0x5555555750d6 *)
sub %ymm7 %ymm7 %ymm13;
(* vmovdqa 0x460(%rsi),%ymm1                       #! EA = L0x55555557a240; Value = 0xefdf1de848882578; PC = 0x5555555750db *)
mov %ymm1 [L0x55555557a240, L0x55555557a244, L0x55555557a248, L0x55555557a24c, L0x55555557a250, L0x55555557a254, L0x55555557a258, L0x55555557a25c];
(* vmovdqa 0x900(%rsi),%ymm2                       #! EA = L0x55555557a6e0; Value = 0x00221de8ffd92578; PC = 0x5555555750e3 *)
mov %ymm2 [L0x55555557a6e0, L0x55555557a6e4, L0x55555557a6e8, L0x55555557a6ec, L0x55555557a6f0, L0x55555557a6f4, L0x55555557a6f8, L0x55555557a6fc];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x5555555750eb *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x5555555750f0 *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm4,%ymm13                      #! PC = 0x5555555750f4 *)
mull %ymm13_h %ymm13_l (%ymm4[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm4,%ymm12                          #! PC = 0x5555555750f9 *)
mov %ymm12 (%ymm4[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x5555555750fd *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm4,%ymm4                       #! PC = 0x555555575102 *)
mull %ymm4_h %ymm4_l (%ymm4[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm4_ls@sint32[4] %ymm4_l;
mov %ymm4 [%ymm4_ls[0], %ymm4_h[0], %ymm4_ls[1], %ymm4_h[1], %ymm4_ls[2], %ymm4_h[2], %ymm4_ls[3], %ymm4_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555575107 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x55555557510c *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555575111 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm4_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm4_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm4,%ymm4                           #! PC = 0x555555575116 *)
mov %ymm4 (%ymm4[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm4,%ymm4               #! PC = 0x55555557511a *)
mov %ymm4 [%ymm4[0], %ymm12[1], %ymm4[2], %ymm12[3], %ymm4[4], %ymm12[5], %ymm4[6], %ymm12[7]];
(* vpsubd %ymm4,%ymm5,%ymm12                       #! PC = 0x555555575120 *)
sub %ymm12 %ymm5 %ymm4;
(* vpaddd %ymm4,%ymm5,%ymm5                        #! PC = 0x555555575124 *)
add %ymm5 %ymm5 %ymm4;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555575128 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x55555557512d *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm4                      #! PC = 0x555555575133 *)
add %ymm4 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm5,%ymm5                       #! PC = 0x555555575138 *)
sub %ymm5 %ymm5 %ymm13;
(* vmovdqa 0x4e0(%rsi),%ymm1                       #! EA = L0x55555557a2c0; Value = 0x53cdd8cf3e20a5ad; PC = 0x55555557513d *)
mov %ymm1 [L0x55555557a2c0, L0x55555557a2c4, L0x55555557a2c8, L0x55555557a2cc, L0x55555557a2d0, L0x55555557a2d4, L0x55555557a2d8, L0x55555557a2dc];
(* vmovdqa 0x980(%rsi),%ymm2                       #! EA = L0x55555557a760; Value = 0x0033f8cfffeb05ad; PC = 0x555555575145 *)
mov %ymm2 [L0x55555557a760, L0x55555557a764, L0x55555557a768, L0x55555557a76c, L0x55555557a770, L0x55555557a774, L0x55555557a778, L0x55555557a77c];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x55555557514d *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x555555575152 *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x555555575156 *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x55555557515b *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555575160 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x555555575165 *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x55555557516a *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x55555557516f *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555575174 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x555555575179 *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x55555557517e *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm3,%ymm12                      #! PC = 0x555555575184 *)
sub %ymm12 %ymm3 %ymm11;
(* vpaddd %ymm11,%ymm3,%ymm3                       #! PC = 0x555555575189 *)
add %ymm3 %ymm3 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x55555557518e *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555575193 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x555555575199 *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm3,%ymm3                       #! PC = 0x55555557519e *)
sub %ymm3 %ymm3 %ymm13;
(* vmovdqa %ymm9,0x200(%rdi)                       #! EA = L0x7fffffff46c0; PC = 0x5555555751a3 *)
mov [L0x7fffffff46c0, L0x7fffffff46c4, L0x7fffffff46c8, L0x7fffffff46cc, L0x7fffffff46d0, L0x7fffffff46d4, L0x7fffffff46d8, L0x7fffffff46dc] %ymm9;
(* vmovdqa %ymm8,0x220(%rdi)                       #! EA = L0x7fffffff46e0; PC = 0x5555555751ab *)
mov [L0x7fffffff46e0, L0x7fffffff46e4, L0x7fffffff46e8, L0x7fffffff46ec, L0x7fffffff46f0, L0x7fffffff46f4, L0x7fffffff46f8, L0x7fffffff46fc] %ymm8;
(* vmovdqa %ymm7,0x240(%rdi)                       #! EA = L0x7fffffff4700; PC = 0x5555555751b3 *)
mov [L0x7fffffff4700, L0x7fffffff4704, L0x7fffffff4708, L0x7fffffff470c, L0x7fffffff4710, L0x7fffffff4714, L0x7fffffff4718, L0x7fffffff471c] %ymm7;
(* vmovdqa %ymm6,0x260(%rdi)                       #! EA = L0x7fffffff4720; PC = 0x5555555751bb *)
mov [L0x7fffffff4720, L0x7fffffff4724, L0x7fffffff4728, L0x7fffffff472c, L0x7fffffff4730, L0x7fffffff4734, L0x7fffffff4738, L0x7fffffff473c] %ymm6;
(* vmovdqa %ymm5,0x280(%rdi)                       #! EA = L0x7fffffff4740; PC = 0x5555555751c3 *)
mov [L0x7fffffff4740, L0x7fffffff4744, L0x7fffffff4748, L0x7fffffff474c, L0x7fffffff4750, L0x7fffffff4754, L0x7fffffff4758, L0x7fffffff475c] %ymm5;
(* vmovdqa %ymm4,0x2a0(%rdi)                       #! EA = L0x7fffffff4760; PC = 0x5555555751cb *)
mov [L0x7fffffff4760, L0x7fffffff4764, L0x7fffffff4768, L0x7fffffff476c, L0x7fffffff4770, L0x7fffffff4774, L0x7fffffff4778, L0x7fffffff477c] %ymm4;
(* vmovdqa %ymm3,0x2c0(%rdi)                       #! EA = L0x7fffffff4780; PC = 0x5555555751d3 *)
mov [L0x7fffffff4780, L0x7fffffff4784, L0x7fffffff4788, L0x7fffffff478c, L0x7fffffff4790, L0x7fffffff4794, L0x7fffffff4798, L0x7fffffff479c] %ymm3;
(* vmovdqa %ymm11,0x2e0(%rdi)                      #! EA = L0x7fffffff47a0; PC = 0x5555555751db *)
mov [L0x7fffffff47a0, L0x7fffffff47a4, L0x7fffffff47a8, L0x7fffffff47ac, L0x7fffffff47b0, L0x7fffffff47b4, L0x7fffffff47b8, L0x7fffffff47bc] %ymm11;

(* vmovdqa 0x300(%rdi),%ymm4                       #! EA = L0x7fffffff47c0; Value = 0x005f0d6efff992ff; PC = 0x5555555751e3 *)
mov %ymm4 [L0x7fffffff47c0, L0x7fffffff47c4, L0x7fffffff47c8, L0x7fffffff47cc, L0x7fffffff47d0, L0x7fffffff47d4, L0x7fffffff47d8, L0x7fffffff47dc];
(* vmovdqa 0x320(%rdi),%ymm5                       #! EA = L0x7fffffff47e0; Value = 0x00450311ffbcff00; PC = 0x5555555751eb *)
mov %ymm5 [L0x7fffffff47e0, L0x7fffffff47e4, L0x7fffffff47e8, L0x7fffffff47ec, L0x7fffffff47f0, L0x7fffffff47f4, L0x7fffffff47f8, L0x7fffffff47fc];
(* vmovdqa 0x340(%rdi),%ymm6                       #! EA = L0x7fffffff4800; Value = 0x002c90320011402c; PC = 0x5555555751f3 *)
mov %ymm6 [L0x7fffffff4800, L0x7fffffff4804, L0x7fffffff4808, L0x7fffffff480c, L0x7fffffff4810, L0x7fffffff4814, L0x7fffffff4818, L0x7fffffff481c];
(* vmovdqa 0x360(%rdi),%ymm7                       #! EA = L0x7fffffff4820; Value = 0xffe5f59bffb009ce; PC = 0x5555555751fb *)
mov %ymm7 [L0x7fffffff4820, L0x7fffffff4824, L0x7fffffff4828, L0x7fffffff482c, L0x7fffffff4830, L0x7fffffff4834, L0x7fffffff4838, L0x7fffffff483c];
(* vmovdqa 0x380(%rdi),%ymm8                       #! EA = L0x7fffffff4840; Value = 0xffc69a930070275f; PC = 0x555555575203 *)
mov %ymm8 [L0x7fffffff4840, L0x7fffffff4844, L0x7fffffff4848, L0x7fffffff484c, L0x7fffffff4850, L0x7fffffff4854, L0x7fffffff4858, L0x7fffffff485c];
(* vmovdqa 0x3a0(%rdi),%ymm9                       #! EA = L0x7fffffff4860; Value = 0xffcd42c9ffd6b32c; PC = 0x55555557520b *)
mov %ymm9 [L0x7fffffff4860, L0x7fffffff4864, L0x7fffffff4868, L0x7fffffff486c, L0x7fffffff4870, L0x7fffffff4874, L0x7fffffff4878, L0x7fffffff487c];
(* vmovdqa 0x3c0(%rdi),%ymm10                      #! EA = L0x7fffffff4880; Value = 0x005d3b60001a2a5e; PC = 0x555555575213 *)
mov %ymm10 [L0x7fffffff4880, L0x7fffffff4884, L0x7fffffff4888, L0x7fffffff488c, L0x7fffffff4890, L0x7fffffff4894, L0x7fffffff4898, L0x7fffffff489c];
(* vmovdqa 0x3e0(%rdi),%ymm11                      #! EA = L0x7fffffff48a0; Value = 0x002395fcffe605a0; PC = 0x55555557521b *)
mov %ymm11 [L0x7fffffff48a0, L0x7fffffff48a4, L0x7fffffff48a8, L0x7fffffff48ac, L0x7fffffff48b0, L0x7fffffff48b4, L0x7fffffff48b8, L0x7fffffff48bc];
(* vpbroadcastd 0x9c(%rsi),%ymm1                   #! EA = L0x555555579e7c; Value = 0x12613e2b61cb9e24; PC = 0x555555575223 *)
broadcast %ymm1 8 [L0x555555579e7c];
(* vpbroadcastd 0x53c(%rsi),%ymm2                  #! EA = L0x55555557a31c; Value = 0x001bde2b00071e24; PC = 0x55555557522c *)
broadcast %ymm2 8 [L0x55555557a31c];
(* vpmuldq %ymm1,%ymm8,%ymm13                      #! PC = 0x555555575235 *)
mull %ymm13_h %ymm13_l (%ymm8[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm8,%ymm12                          #! PC = 0x55555557523a *)
mov %ymm12 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x55555557523f *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm8,%ymm8                       #! PC = 0x555555575244 *)
mull %ymm8_h %ymm8_l (%ymm8[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm8_ls@sint32[4] %ymm8_l;
mov %ymm8 [%ymm8_ls[0], %ymm8_h[0], %ymm8_ls[1], %ymm8_h[1], %ymm8_ls[2], %ymm8_h[2], %ymm8_ls[3], %ymm8_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555575249 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x55555557524e *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555575253 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm8_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm8_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm8,%ymm8                           #! PC = 0x555555575258 *)
mov %ymm8 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm8,%ymm8               #! PC = 0x55555557525d *)
mov %ymm8 [%ymm8[0], %ymm12[1], %ymm8[2], %ymm12[3], %ymm8[4], %ymm12[5], %ymm8[6], %ymm12[7]];
(* vpsubd %ymm8,%ymm4,%ymm12                       #! PC = 0x555555575263 *)
sub %ymm12 %ymm4 %ymm8;
(* vpaddd %ymm8,%ymm4,%ymm4                        #! PC = 0x555555575268 *)
add %ymm4 %ymm4 %ymm8;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x55555557526d *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555575272 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm8                      #! PC = 0x555555575278 *)
add %ymm8 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm4,%ymm4                       #! PC = 0x55555557527d *)
sub %ymm4 %ymm4 %ymm13;
(* vpmuldq %ymm1,%ymm9,%ymm13                      #! PC = 0x555555575282 *)
mull %ymm13_h %ymm13_l (%ymm9[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm9,%ymm12                          #! PC = 0x555555575287 *)
mov %ymm12 (%ymm9[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x55555557528c *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm9,%ymm9                       #! PC = 0x555555575291 *)
mull %ymm9_h %ymm9_l (%ymm9[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm9_ls@sint32[4] %ymm9_l;
mov %ymm9 [%ymm9_ls[0], %ymm9_h[0], %ymm9_ls[1], %ymm9_h[1], %ymm9_ls[2], %ymm9_h[2], %ymm9_ls[3], %ymm9_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555575296 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x55555557529b *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555752a0 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm9_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm9_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm9,%ymm9                           #! PC = 0x5555555752a5 *)
mov %ymm9 (%ymm9[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm9,%ymm9               #! PC = 0x5555555752aa *)
mov %ymm9 [%ymm9[0], %ymm12[1], %ymm9[2], %ymm12[3], %ymm9[4], %ymm12[5], %ymm9[6], %ymm12[7]];
(* vpsubd %ymm9,%ymm5,%ymm12                       #! PC = 0x5555555752b0 *)
sub %ymm12 %ymm5 %ymm9;
(* vpaddd %ymm9,%ymm5,%ymm5                        #! PC = 0x5555555752b5 *)
add %ymm5 %ymm5 %ymm9;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555752ba *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555752bf *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm9                      #! PC = 0x5555555752c5 *)
add %ymm9 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm5,%ymm5                       #! PC = 0x5555555752ca *)
sub %ymm5 %ymm5 %ymm13;
(* vpmuldq %ymm1,%ymm10,%ymm13                     #! PC = 0x5555555752cf *)
mull %ymm13_h %ymm13_l (%ymm10[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm10,%ymm12                         #! PC = 0x5555555752d4 *)
mov %ymm12 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x5555555752d9 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm10,%ymm10                     #! PC = 0x5555555752de *)
mull %ymm10_h %ymm10_l (%ymm10[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm10_ls@sint32[4] %ymm10_l;
mov %ymm10 [%ymm10_ls[0], %ymm10_h[0], %ymm10_ls[1], %ymm10_h[1], %ymm10_ls[2], %ymm10_h[2], %ymm10_ls[3], %ymm10_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x5555555752e3 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555752e8 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555752ed *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm10_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm10_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm10,%ymm10                         #! PC = 0x5555555752f2 *)
mov %ymm10 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm10,%ymm10             #! PC = 0x5555555752f7 *)
mov %ymm10 [%ymm10[0], %ymm12[1], %ymm10[2], %ymm12[3], %ymm10[4], %ymm12[5], %ymm10[6], %ymm12[7]];
(* vpsubd %ymm10,%ymm6,%ymm12                      #! PC = 0x5555555752fd *)
sub %ymm12 %ymm6 %ymm10;
(* vpaddd %ymm10,%ymm6,%ymm6                       #! PC = 0x555555575302 *)
add %ymm6 %ymm6 %ymm10;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555575307 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x55555557530c *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm10                     #! PC = 0x555555575312 *)
add %ymm10 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm6,%ymm6                       #! PC = 0x555555575317 *)
sub %ymm6 %ymm6 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x55555557531c *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x555555575321 *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555575326 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x55555557532b *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555575330 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555575335 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x55555557533a *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x55555557533f *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x555555575344 *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm7,%ymm12                      #! PC = 0x55555557534a *)
sub %ymm12 %ymm7 %ymm11;
(* vpaddd %ymm11,%ymm7,%ymm7                       #! PC = 0x55555557534f *)
add %ymm7 %ymm7 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555575354 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555575359 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x55555557535f *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm7,%ymm7                       #! PC = 0x555555575364 *)
sub %ymm7 %ymm7 %ymm13;
(* vperm2i128 $0x20,%ymm8,%ymm4,%ymm3              #! PC = 0x555555575369 *)
mov %ymm3 (%ymm4[0, 1, 2, 3] ++ %ymm8[0, 1, 2, 3]);
(* vperm2i128 $0x31,%ymm8,%ymm4,%ymm8              #! PC = 0x55555557536f *)
mov %ymm8 (%ymm4[4, 5, 6, 7] ++ %ymm8[4, 5, 6, 7]);
(* vperm2i128 $0x20,%ymm9,%ymm5,%ymm4              #! PC = 0x555555575375 *)
mov %ymm4 (%ymm5[0, 1, 2, 3] ++ %ymm9[0, 1, 2, 3]);
(* vperm2i128 $0x31,%ymm9,%ymm5,%ymm9              #! PC = 0x55555557537b *)
mov %ymm9 (%ymm5[4, 5, 6, 7] ++ %ymm9[4, 5, 6, 7]);
(* vperm2i128 $0x20,%ymm10,%ymm6,%ymm5             #! PC = 0x555555575381 *)
mov %ymm5 (%ymm6[0, 1, 2, 3] ++ %ymm10[0, 1, 2, 3]);
(* vperm2i128 $0x31,%ymm10,%ymm6,%ymm10            #! PC = 0x555555575387 *)
mov %ymm10 (%ymm6[4, 5, 6, 7] ++ %ymm10[4, 5, 6, 7]);
(* vperm2i128 $0x20,%ymm11,%ymm7,%ymm6             #! PC = 0x55555557538d *)
mov %ymm6 (%ymm7[0, 1, 2, 3] ++ %ymm11[0, 1, 2, 3]);
(* vperm2i128 $0x31,%ymm11,%ymm7,%ymm11            #! PC = 0x555555575393 *)
mov %ymm11 (%ymm7[4, 5, 6, 7] ++ %ymm11[4, 5, 6, 7]);

(* vmovdqa 0x100(%rsi),%ymm1                       #! EA = L0x555555579ee0; Value = 0xeef89a49eef89a49; PC = 0x555555575399 *)
mov %ymm1 [L0x555555579ee0, L0x555555579ee4, L0x555555579ee8, L0x555555579eec, L0x555555579ef0, L0x555555579ef4, L0x555555579ef8, L0x555555579efc];
(* vmovdqa 0x5a0(%rsi),%ymm2                       #! EA = L0x55555557a380; Value = 0x002f7a49002f7a49; PC = 0x5555555753a1 *)
mov %ymm2 [L0x55555557a380, L0x55555557a384, L0x55555557a388, L0x55555557a38c, L0x55555557a390, L0x55555557a394, L0x55555557a398, L0x55555557a39c];
(* vpmuldq %ymm1,%ymm5,%ymm13                      #! PC = 0x5555555753a9 *)
mull %ymm13_h %ymm13_l (%ymm5[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm5,%ymm12                          #! PC = 0x5555555753ae *)
mov %ymm12 (%ymm5[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x5555555753b2 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm5,%ymm5                       #! PC = 0x5555555753b7 *)
mull %ymm5_h %ymm5_l (%ymm5[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm5_ls@sint32[4] %ymm5_l;
mov %ymm5 [%ymm5_ls[0], %ymm5_h[0], %ymm5_ls[1], %ymm5_h[1], %ymm5_ls[2], %ymm5_h[2], %ymm5_ls[3], %ymm5_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x5555555753bc *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555753c1 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555753c6 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm5_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm5_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm5,%ymm5                           #! PC = 0x5555555753cb *)
mov %ymm5 (%ymm5[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm5,%ymm5               #! PC = 0x5555555753cf *)
mov %ymm5 [%ymm5[0], %ymm12[1], %ymm5[2], %ymm12[3], %ymm5[4], %ymm12[5], %ymm5[6], %ymm12[7]];
(* vpsubd %ymm5,%ymm3,%ymm12                       #! PC = 0x5555555753d5 *)
sub %ymm12 %ymm3 %ymm5;
(* vpaddd %ymm5,%ymm3,%ymm3                        #! PC = 0x5555555753d9 *)
add %ymm3 %ymm3 %ymm5;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555753dd *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555753e2 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm5                      #! PC = 0x5555555753e8 *)
add %ymm5 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm3,%ymm3                       #! PC = 0x5555555753ed *)
sub %ymm3 %ymm3 %ymm13;
(* vpmuldq %ymm1,%ymm10,%ymm13                     #! PC = 0x5555555753f2 *)
mull %ymm13_h %ymm13_l (%ymm10[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm10,%ymm12                         #! PC = 0x5555555753f7 *)
mov %ymm12 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x5555555753fc *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm10,%ymm10                     #! PC = 0x555555575401 *)
mull %ymm10_h %ymm10_l (%ymm10[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm10_ls@sint32[4] %ymm10_l;
mov %ymm10 [%ymm10_ls[0], %ymm10_h[0], %ymm10_ls[1], %ymm10_h[1], %ymm10_ls[2], %ymm10_h[2], %ymm10_ls[3], %ymm10_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555575406 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x55555557540b *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555575410 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm10_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm10_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm10,%ymm10                         #! PC = 0x555555575415 *)
mov %ymm10 (%ymm10[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm10,%ymm10             #! PC = 0x55555557541a *)
mov %ymm10 [%ymm10[0], %ymm12[1], %ymm10[2], %ymm12[3], %ymm10[4], %ymm12[5], %ymm10[6], %ymm12[7]];
(* vpsubd %ymm10,%ymm8,%ymm12                      #! PC = 0x555555575420 *)
sub %ymm12 %ymm8 %ymm10;
(* vpaddd %ymm10,%ymm8,%ymm8                       #! PC = 0x555555575425 *)
add %ymm8 %ymm8 %ymm10;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x55555557542a *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x55555557542f *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm10                     #! PC = 0x555555575435 *)
add %ymm10 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm8,%ymm8                       #! PC = 0x55555557543a *)
sub %ymm8 %ymm8 %ymm13;
(* vpmuldq %ymm1,%ymm6,%ymm13                      #! PC = 0x55555557543f *)
mull %ymm13_h %ymm13_l (%ymm6[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm6,%ymm12                          #! PC = 0x555555575444 *)
mov %ymm12 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555575448 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm6,%ymm6                       #! PC = 0x55555557544d *)
mull %ymm6_h %ymm6_l (%ymm6[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm6_ls@sint32[4] %ymm6_l;
mov %ymm6 [%ymm6_ls[0], %ymm6_h[0], %ymm6_ls[1], %ymm6_h[1], %ymm6_ls[2], %ymm6_h[2], %ymm6_ls[3], %ymm6_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555575452 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555575457 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x55555557545c *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm6_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm6_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm6,%ymm6                           #! PC = 0x555555575461 *)
mov %ymm6 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm6,%ymm6               #! PC = 0x555555575465 *)
mov %ymm6 [%ymm6[0], %ymm12[1], %ymm6[2], %ymm12[3], %ymm6[4], %ymm12[5], %ymm6[6], %ymm12[7]];
(* vpsubd %ymm6,%ymm4,%ymm12                       #! PC = 0x55555557546b *)
sub %ymm12 %ymm4 %ymm6;
(* vpaddd %ymm6,%ymm4,%ymm4                        #! PC = 0x55555557546f *)
add %ymm4 %ymm4 %ymm6;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555575473 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555575478 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm6                      #! PC = 0x55555557547e *)
add %ymm6 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm4,%ymm4                       #! PC = 0x555555575483 *)
sub %ymm4 %ymm4 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x555555575488 *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x55555557548d *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555575492 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x555555575497 *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x55555557549c *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555754a1 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555754a6 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x5555555754ab *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x5555555754b0 *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm9,%ymm12                      #! PC = 0x5555555754b6 *)
sub %ymm12 %ymm9 %ymm11;
(* vpaddd %ymm11,%ymm9,%ymm9                       #! PC = 0x5555555754bb *)
add %ymm9 %ymm9 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555754c0 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555754c5 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x5555555754cb *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm9,%ymm9                       #! PC = 0x5555555754d0 *)
sub %ymm9 %ymm9 %ymm13;
(* vpunpcklqdq %ymm5,%ymm3,%ymm7                   #! PC = 0x5555555754d5 *)
mov %ymm7 [%ymm3[0], %ymm3[1], %ymm5[0], %ymm5[1], %ymm3[4], %ymm3[5], %ymm5[4], %ymm5[5]];
(* vpunpckhqdq %ymm5,%ymm3,%ymm5                   #! PC = 0x5555555754d9 *)
mov %ymm5 [%ymm3[2], %ymm3[3], %ymm5[2], %ymm5[3], %ymm3[6], %ymm3[7], %ymm5[6], %ymm5[7]];
(* vpunpcklqdq %ymm10,%ymm8,%ymm3                  #! PC = 0x5555555754dd *)
mov %ymm3 [%ymm8[0], %ymm8[1], %ymm10[0], %ymm10[1], %ymm8[4], %ymm8[5], %ymm10[4], %ymm10[5]];
(* vpunpckhqdq %ymm10,%ymm8,%ymm10                 #! PC = 0x5555555754e2 *)
mov %ymm10 [%ymm8[2], %ymm8[3], %ymm10[2], %ymm10[3], %ymm8[6], %ymm8[7], %ymm10[6], %ymm10[7]];
(* vpunpcklqdq %ymm6,%ymm4,%ymm8                   #! PC = 0x5555555754e7 *)
mov %ymm8 [%ymm4[0], %ymm4[1], %ymm6[0], %ymm6[1], %ymm4[4], %ymm4[5], %ymm6[4], %ymm6[5]];
(* vpunpckhqdq %ymm6,%ymm4,%ymm6                   #! PC = 0x5555555754eb *)
mov %ymm6 [%ymm4[2], %ymm4[3], %ymm6[2], %ymm6[3], %ymm4[6], %ymm4[7], %ymm6[6], %ymm6[7]];
(* vpunpcklqdq %ymm11,%ymm9,%ymm4                  #! PC = 0x5555555754ef *)
mov %ymm4 [%ymm9[0], %ymm9[1], %ymm11[0], %ymm11[1], %ymm9[4], %ymm9[5], %ymm11[4], %ymm11[5]];
(* vpunpckhqdq %ymm11,%ymm9,%ymm11                 #! PC = 0x5555555754f4 *)
mov %ymm11 [%ymm9[2], %ymm9[3], %ymm11[2], %ymm11[3], %ymm9[6], %ymm9[7], %ymm11[6], %ymm11[7]];

(* vmovdqa 0x180(%rsi),%ymm1                       #! EA = L0x555555579f60; Value = 0x13a1703513a17035; PC = 0x5555555754f9 *)
mov %ymm1 [L0x555555579f60, L0x555555579f64, L0x555555579f68, L0x555555579f6c, L0x555555579f70, L0x555555579f74, L0x555555579f78, L0x555555579f7c];
(* vmovdqa 0x620(%rsi),%ymm2                       #! EA = L0x55555557a400; Value = 0x001ad035001ad035; PC = 0x555555575501 *)
mov %ymm2 [L0x55555557a400, L0x55555557a404, L0x55555557a408, L0x55555557a40c, L0x55555557a410, L0x55555557a414, L0x55555557a418, L0x55555557a41c];
(* vpmuldq %ymm1,%ymm8,%ymm13                      #! PC = 0x555555575509 *)
mull %ymm13_h %ymm13_l (%ymm8[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm8,%ymm12                          #! PC = 0x55555557550e *)
mov %ymm12 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x555555575513 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm8,%ymm8                       #! PC = 0x555555575518 *)
mull %ymm8_h %ymm8_l (%ymm8[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm8_ls@sint32[4] %ymm8_l;
mov %ymm8 [%ymm8_ls[0], %ymm8_h[0], %ymm8_ls[1], %ymm8_h[1], %ymm8_ls[2], %ymm8_h[2], %ymm8_ls[3], %ymm8_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x55555557551d *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555575522 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555575527 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm8_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm8_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm8,%ymm8                           #! PC = 0x55555557552c *)
mov %ymm8 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm8,%ymm8               #! PC = 0x555555575531 *)
mov %ymm8 [%ymm8[0], %ymm12[1], %ymm8[2], %ymm12[3], %ymm8[4], %ymm12[5], %ymm8[6], %ymm12[7]];
(* vpsubd %ymm8,%ymm7,%ymm12                       #! PC = 0x555555575537 *)
sub %ymm12 %ymm7 %ymm8;
(* vpaddd %ymm8,%ymm7,%ymm7                        #! PC = 0x55555557553c *)
add %ymm7 %ymm7 %ymm8;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555575541 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555575546 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm8                      #! PC = 0x55555557554c *)
add %ymm8 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm7,%ymm7                       #! PC = 0x555555575551 *)
sub %ymm7 %ymm7 %ymm13;
(* vpmuldq %ymm1,%ymm6,%ymm13                      #! PC = 0x555555575556 *)
mull %ymm13_h %ymm13_l (%ymm6[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm6,%ymm12                          #! PC = 0x55555557555b *)
mov %ymm12 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x55555557555f *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm6,%ymm6                       #! PC = 0x555555575564 *)
mull %ymm6_h %ymm6_l (%ymm6[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm6_ls@sint32[4] %ymm6_l;
mov %ymm6 [%ymm6_ls[0], %ymm6_h[0], %ymm6_ls[1], %ymm6_h[1], %ymm6_ls[2], %ymm6_h[2], %ymm6_ls[3], %ymm6_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x555555575569 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x55555557556e *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555575573 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm6_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm6_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm6,%ymm6                           #! PC = 0x555555575578 *)
mov %ymm6 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm6,%ymm6               #! PC = 0x55555557557c *)
mov %ymm6 [%ymm6[0], %ymm12[1], %ymm6[2], %ymm12[3], %ymm6[4], %ymm12[5], %ymm6[6], %ymm12[7]];
(* vpsubd %ymm6,%ymm5,%ymm12                       #! PC = 0x555555575582 *)
sub %ymm12 %ymm5 %ymm6;
(* vpaddd %ymm6,%ymm5,%ymm5                        #! PC = 0x555555575586 *)
add %ymm5 %ymm5 %ymm6;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x55555557558a *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x55555557558f *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm6                      #! PC = 0x555555575595 *)
add %ymm6 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm5,%ymm5                       #! PC = 0x55555557559a *)
sub %ymm5 %ymm5 %ymm13;
(* vpmuldq %ymm1,%ymm4,%ymm13                      #! PC = 0x55555557559f *)
mull %ymm13_h %ymm13_l (%ymm4[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm4,%ymm12                          #! PC = 0x5555555755a4 *)
mov %ymm12 (%ymm4[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x5555555755a8 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm4,%ymm4                       #! PC = 0x5555555755ad *)
mull %ymm4_h %ymm4_l (%ymm4[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm4_ls@sint32[4] %ymm4_l;
mov %ymm4 [%ymm4_ls[0], %ymm4_h[0], %ymm4_ls[1], %ymm4_h[1], %ymm4_ls[2], %ymm4_h[2], %ymm4_ls[3], %ymm4_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x5555555755b2 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555755b7 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555755bc *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm4_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm4_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm4,%ymm4                           #! PC = 0x5555555755c1 *)
mov %ymm4 (%ymm4[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm4,%ymm4               #! PC = 0x5555555755c5 *)
mov %ymm4 [%ymm4[0], %ymm12[1], %ymm4[2], %ymm12[3], %ymm4[4], %ymm12[5], %ymm4[6], %ymm12[7]];
(* vpsubd %ymm4,%ymm3,%ymm12                       #! PC = 0x5555555755cb *)
sub %ymm12 %ymm3 %ymm4;
(* vpaddd %ymm4,%ymm3,%ymm3                        #! PC = 0x5555555755cf *)
add %ymm3 %ymm3 %ymm4;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555755d3 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555755d8 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm4                      #! PC = 0x5555555755de *)
add %ymm4 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm3,%ymm3                       #! PC = 0x5555555755e3 *)
sub %ymm3 %ymm3 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x5555555755e8 *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x5555555755ed *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm12,%ymm14                     #! PC = 0x5555555755f2 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x5555555755f7 *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm2,%ymm12,%ymm12                     #! PC = 0x5555555755fc *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555575601 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555575606 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x55555557560b *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x555555575610 *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm10,%ymm12                     #! PC = 0x555555575616 *)
sub %ymm12 %ymm10 %ymm11;
(* vpaddd %ymm11,%ymm10,%ymm10                     #! PC = 0x55555557561b *)
add %ymm10 %ymm10 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555575620 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555575625 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x55555557562b *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm10,%ymm10                     #! PC = 0x555555575630 *)
sub %ymm10 %ymm10 %ymm13;
(* vmovsldup %ymm8,%ymm9                           #! PC = 0x555555575635 *)
mov %ymm9 (%ymm8[0, 0, 2, 2, 4, 4, 6, 6]);
(* vpblendd $0xaa,%ymm9,%ymm7,%ymm9                #! PC = 0x55555557563a *)
mov %ymm9 [%ymm7[0], %ymm9[1], %ymm7[2], %ymm9[3], %ymm7[4], %ymm9[5], %ymm7[6], %ymm9[7]];
(* vpsrlq $0x20,%ymm7,%ymm7                        #! PC = 0x555555575640 *)
mov %ymm7 [%ymm7[1], 0@sint32, %ymm7[3], 0@sint32, %ymm7[5], 0@sint32, %ymm7[7], 0@sint32];
(* vpblendd $0xaa,%ymm8,%ymm7,%ymm8                #! PC = 0x555555575645 *)
mov %ymm8 [%ymm7[0], %ymm8[1], %ymm7[2], %ymm8[3], %ymm7[4], %ymm8[5], %ymm7[6], %ymm8[7]];
(* vmovsldup %ymm6,%ymm7                           #! PC = 0x55555557564b *)
mov %ymm7 (%ymm6[0, 0, 2, 2, 4, 4, 6, 6]);
(* vpblendd $0xaa,%ymm7,%ymm5,%ymm7                #! PC = 0x55555557564f *)
mov %ymm7 [%ymm5[0], %ymm7[1], %ymm5[2], %ymm7[3], %ymm5[4], %ymm7[5], %ymm5[6], %ymm7[7]];
(* vpsrlq $0x20,%ymm5,%ymm5                        #! PC = 0x555555575655 *)
mov %ymm5 [%ymm5[1], 0@sint32, %ymm5[3], 0@sint32, %ymm5[5], 0@sint32, %ymm5[7], 0@sint32];
(* vpblendd $0xaa,%ymm6,%ymm5,%ymm6                #! PC = 0x55555557565a *)
mov %ymm6 [%ymm5[0], %ymm6[1], %ymm5[2], %ymm6[3], %ymm5[4], %ymm6[5], %ymm5[6], %ymm6[7]];
(* vmovsldup %ymm4,%ymm5                           #! PC = 0x555555575660 *)
mov %ymm5 (%ymm4[0, 0, 2, 2, 4, 4, 6, 6]);
(* vpblendd $0xaa,%ymm5,%ymm3,%ymm5                #! PC = 0x555555575664 *)
mov %ymm5 [%ymm3[0], %ymm5[1], %ymm3[2], %ymm5[3], %ymm3[4], %ymm5[5], %ymm3[6], %ymm5[7]];
(* vpsrlq $0x20,%ymm3,%ymm3                        #! PC = 0x55555557566a *)
mov %ymm3 [%ymm3[1], 0@sint32, %ymm3[3], 0@sint32, %ymm3[5], 0@sint32, %ymm3[7], 0@sint32];
(* vpblendd $0xaa,%ymm4,%ymm3,%ymm4                #! PC = 0x55555557566f *)
mov %ymm4 [%ymm3[0], %ymm4[1], %ymm3[2], %ymm4[3], %ymm3[4], %ymm4[5], %ymm3[6], %ymm4[7]];
(* vmovsldup %ymm11,%ymm3                          #! PC = 0x555555575675 *)
mov %ymm3 (%ymm11[0, 0, 2, 2, 4, 4, 6, 6]);
(* vpblendd $0xaa,%ymm3,%ymm10,%ymm3               #! PC = 0x55555557567a *)
mov %ymm3 [%ymm10[0], %ymm3[1], %ymm10[2], %ymm3[3], %ymm10[4], %ymm3[5], %ymm10[6], %ymm3[7]];
(* vpsrlq $0x20,%ymm10,%ymm10                      #! PC = 0x555555575680 *)
mov %ymm10 [%ymm10[1], 0@sint32, %ymm10[3], 0@sint32, %ymm10[5], 0@sint32, %ymm10[7], 0@sint32];
(* vpblendd $0xaa,%ymm11,%ymm10,%ymm11             #! PC = 0x555555575686 *)
mov %ymm11 [%ymm10[0], %ymm11[1], %ymm10[2], %ymm11[3], %ymm10[4], %ymm11[5], %ymm10[6], %ymm11[7]];

(* vmovdqa 0x200(%rsi),%ymm1                       #! EA = L0x555555579fe0; Value = 0x9e7b5b9a588163a8; PC = 0x55555557568c *)
mov %ymm1 [L0x555555579fe0, L0x555555579fe4, L0x555555579fe8, L0x555555579fec, L0x555555579ff0, L0x555555579ff4, L0x555555579ff8, L0x555555579ffc];
(* vmovdqa 0x6a0(%rsi),%ymm2                       #! EA = L0x55555557a480; Value = 0x00081b9a000c63a8; PC = 0x555555575694 *)
mov %ymm2 [L0x55555557a480, L0x55555557a484, L0x55555557a488, L0x55555557a48c, L0x55555557a490, L0x55555557a494, L0x55555557a498, L0x55555557a49c];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x55555557569c *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x5555555756a1 *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm5,%ymm13                      #! PC = 0x5555555756a5 *)
mull %ymm13_h %ymm13_l (%ymm5[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm5,%ymm12                          #! PC = 0x5555555756aa *)
mov %ymm12 (%ymm5[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x5555555756ae *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm5,%ymm5                       #! PC = 0x5555555756b3 *)
mull %ymm5_h %ymm5_l (%ymm5[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm5_ls@sint32[4] %ymm5_l;
mov %ymm5 [%ymm5_ls[0], %ymm5_h[0], %ymm5_ls[1], %ymm5_h[1], %ymm5_ls[2], %ymm5_h[2], %ymm5_ls[3], %ymm5_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x5555555756b8 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555756bd *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555756c2 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm5_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm5_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm5,%ymm5                           #! PC = 0x5555555756c7 *)
mov %ymm5 (%ymm5[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm5,%ymm5               #! PC = 0x5555555756cb *)
mov %ymm5 [%ymm5[0], %ymm12[1], %ymm5[2], %ymm12[3], %ymm5[4], %ymm12[5], %ymm5[6], %ymm12[7]];
(* vpsubd %ymm5,%ymm9,%ymm12                       #! PC = 0x5555555756d1 *)
sub %ymm12 %ymm9 %ymm5;
(* vpaddd %ymm5,%ymm9,%ymm9                        #! PC = 0x5555555756d5 *)
add %ymm9 %ymm9 %ymm5;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555756d9 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555756de *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm5                      #! PC = 0x5555555756e4 *)
add %ymm5 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm9,%ymm9                       #! PC = 0x5555555756e9 *)
sub %ymm9 %ymm9 %ymm13;
(* vpmuldq %ymm1,%ymm4,%ymm13                      #! PC = 0x5555555756ee *)
mull %ymm13_h %ymm13_l (%ymm4[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm4,%ymm12                          #! PC = 0x5555555756f3 *)
mov %ymm12 (%ymm4[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x5555555756f7 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm4,%ymm4                       #! PC = 0x5555555756fc *)
mull %ymm4_h %ymm4_l (%ymm4[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm4_ls@sint32[4] %ymm4_l;
mov %ymm4 [%ymm4_ls[0], %ymm4_h[0], %ymm4_ls[1], %ymm4_h[1], %ymm4_ls[2], %ymm4_h[2], %ymm4_ls[3], %ymm4_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555575701 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555575706 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x55555557570b *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm4_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm4_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm4,%ymm4                           #! PC = 0x555555575710 *)
mov %ymm4 (%ymm4[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm4,%ymm4               #! PC = 0x555555575714 *)
mov %ymm4 [%ymm4[0], %ymm12[1], %ymm4[2], %ymm12[3], %ymm4[4], %ymm12[5], %ymm4[6], %ymm12[7]];
(* vpsubd %ymm4,%ymm8,%ymm12                       #! PC = 0x55555557571a *)
sub %ymm12 %ymm8 %ymm4;
(* vpaddd %ymm4,%ymm8,%ymm8                        #! PC = 0x55555557571e *)
add %ymm8 %ymm8 %ymm4;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555575722 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555575727 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm4                      #! PC = 0x55555557572d *)
add %ymm4 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm8,%ymm8                       #! PC = 0x555555575732 *)
sub %ymm8 %ymm8 %ymm13;
(* vpmuldq %ymm1,%ymm3,%ymm13                      #! PC = 0x555555575737 *)
mull %ymm13_h %ymm13_l (%ymm3[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm3,%ymm12                          #! PC = 0x55555557573c *)
mov %ymm12 (%ymm3[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555575740 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm3,%ymm3                       #! PC = 0x555555575745 *)
mull %ymm3_h %ymm3_l (%ymm3[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm3_ls@sint32[4] %ymm3_l;
mov %ymm3 [%ymm3_ls[0], %ymm3_h[0], %ymm3_ls[1], %ymm3_h[1], %ymm3_ls[2], %ymm3_h[2], %ymm3_ls[3], %ymm3_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x55555557574a *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x55555557574f *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555575754 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm3_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm3_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm3,%ymm3                           #! PC = 0x555555575759 *)
mov %ymm3 (%ymm3[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm3,%ymm3               #! PC = 0x55555557575d *)
mov %ymm3 [%ymm3[0], %ymm12[1], %ymm3[2], %ymm12[3], %ymm3[4], %ymm12[5], %ymm3[6], %ymm12[7]];
(* vpsubd %ymm3,%ymm7,%ymm12                       #! PC = 0x555555575763 *)
sub %ymm12 %ymm7 %ymm3;
(* vpaddd %ymm3,%ymm7,%ymm7                        #! PC = 0x555555575767 *)
add %ymm7 %ymm7 %ymm3;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x55555557576b *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555575770 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm3                      #! PC = 0x555555575776 *)
add %ymm3 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm7,%ymm7                       #! PC = 0x55555557577b *)
sub %ymm7 %ymm7 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x555555575780 *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x555555575785 *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x55555557578a *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x55555557578f *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555575794 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555575799 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x55555557579e *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x5555555757a3 *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x5555555757a8 *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm6,%ymm12                      #! PC = 0x5555555757ae *)
sub %ymm12 %ymm6 %ymm11;
(* vpaddd %ymm11,%ymm6,%ymm6                       #! PC = 0x5555555757b3 *)
add %ymm6 %ymm6 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555757b8 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555757bd *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x5555555757c3 *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm6,%ymm6                       #! PC = 0x5555555757c8 *)
sub %ymm6 %ymm6 %ymm13;

(* vmovdqa 0x280(%rsi),%ymm1                       #! EA = L0x55555557a060; Value = 0xae0876c2d690646c; PC = 0x5555555757cd *)
mov %ymm1 [L0x55555557a060, L0x55555557a064, L0x55555557a068, L0x55555557a06c, L0x55555557a070, L0x55555557a074, L0x55555557a078, L0x55555557a07c];
(* vmovdqa 0x720(%rsi),%ymm2                       #! EA = L0x55555557a500; Value = 0x003036c20002e46c; PC = 0x5555555757d5 *)
mov %ymm2 [L0x55555557a500, L0x55555557a504, L0x55555557a508, L0x55555557a50c, L0x55555557a510, L0x55555557a514, L0x55555557a518, L0x55555557a51c];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x5555555757dd *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x5555555757e2 *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm7,%ymm13                      #! PC = 0x5555555757e6 *)
mull %ymm13_h %ymm13_l (%ymm7[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm7,%ymm12                          #! PC = 0x5555555757eb *)
mov %ymm12 (%ymm7[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x5555555757ef *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm7,%ymm7                       #! PC = 0x5555555757f4 *)
mull %ymm7_h %ymm7_l (%ymm7[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm7_ls@sint32[4] %ymm7_l;
mov %ymm7 [%ymm7_ls[0], %ymm7_h[0], %ymm7_ls[1], %ymm7_h[1], %ymm7_ls[2], %ymm7_h[2], %ymm7_ls[3], %ymm7_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x5555555757f9 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555757fe *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555575803 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm7_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm7_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm7,%ymm7                           #! PC = 0x555555575808 *)
mov %ymm7 (%ymm7[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm7,%ymm7               #! PC = 0x55555557580c *)
mov %ymm7 [%ymm7[0], %ymm12[1], %ymm7[2], %ymm12[3], %ymm7[4], %ymm12[5], %ymm7[6], %ymm12[7]];
(* vpsubd %ymm7,%ymm9,%ymm12                       #! PC = 0x555555575812 *)
sub %ymm12 %ymm9 %ymm7;
(* vpaddd %ymm7,%ymm9,%ymm9                        #! PC = 0x555555575816 *)
add %ymm9 %ymm9 %ymm7;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x55555557581a *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x55555557581f *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm7                      #! PC = 0x555555575825 *)
add %ymm7 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm9,%ymm9                       #! PC = 0x55555557582a *)
sub %ymm9 %ymm9 %ymm13;
(* vpmuldq %ymm1,%ymm6,%ymm13                      #! PC = 0x55555557582f *)
mull %ymm13_h %ymm13_l (%ymm6[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm6,%ymm12                          #! PC = 0x555555575834 *)
mov %ymm12 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555575838 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm6,%ymm6                       #! PC = 0x55555557583d *)
mull %ymm6_h %ymm6_l (%ymm6[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm6_ls@sint32[4] %ymm6_l;
mov %ymm6 [%ymm6_ls[0], %ymm6_h[0], %ymm6_ls[1], %ymm6_h[1], %ymm6_ls[2], %ymm6_h[2], %ymm6_ls[3], %ymm6_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555575842 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555575847 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x55555557584c *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm6_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm6_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm6,%ymm6                           #! PC = 0x555555575851 *)
mov %ymm6 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm6,%ymm6               #! PC = 0x555555575855 *)
mov %ymm6 [%ymm6[0], %ymm12[1], %ymm6[2], %ymm12[3], %ymm6[4], %ymm12[5], %ymm6[6], %ymm12[7]];
(* vpsubd %ymm6,%ymm8,%ymm12                       #! PC = 0x55555557585b *)
sub %ymm12 %ymm8 %ymm6;
(* vpaddd %ymm6,%ymm8,%ymm8                        #! PC = 0x55555557585f *)
add %ymm8 %ymm8 %ymm6;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555575863 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555575868 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm6                      #! PC = 0x55555557586e *)
add %ymm6 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm8,%ymm8                       #! PC = 0x555555575873 *)
sub %ymm8 %ymm8 %ymm13;
(* vmovdqa 0x300(%rsi),%ymm1                       #! EA = L0x55555557a0e0; Value = 0x54e27ff654cac808; PC = 0x555555575878 *)
mov %ymm1 [L0x55555557a0e0, L0x55555557a0e4, L0x55555557a0e8, L0x55555557a0ec, L0x55555557a0f0, L0x55555557a0f4, L0x55555557a0f8, L0x55555557a0fc];
(* vmovdqa 0x7a0(%rsi),%ymm2                       #! EA = L0x55555557a580; Value = 0xffe3bff6ffc9c808; PC = 0x555555575880 *)
mov %ymm2 [L0x55555557a580, L0x55555557a584, L0x55555557a588, L0x55555557a58c, L0x55555557a590, L0x55555557a594, L0x55555557a598, L0x55555557a59c];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x555555575888 *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x55555557588d *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm3,%ymm13                      #! PC = 0x555555575891 *)
mull %ymm13_h %ymm13_l (%ymm3[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm3,%ymm12                          #! PC = 0x555555575896 *)
mov %ymm12 (%ymm3[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x55555557589a *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm3,%ymm3                       #! PC = 0x55555557589f *)
mull %ymm3_h %ymm3_l (%ymm3[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm3_ls@sint32[4] %ymm3_l;
mov %ymm3 [%ymm3_ls[0], %ymm3_h[0], %ymm3_ls[1], %ymm3_h[1], %ymm3_ls[2], %ymm3_h[2], %ymm3_ls[3], %ymm3_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x5555555758a4 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555758a9 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555758ae *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm3_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm3_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm3,%ymm3                           #! PC = 0x5555555758b3 *)
mov %ymm3 (%ymm3[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm3,%ymm3               #! PC = 0x5555555758b7 *)
mov %ymm3 [%ymm3[0], %ymm12[1], %ymm3[2], %ymm12[3], %ymm3[4], %ymm12[5], %ymm3[6], %ymm12[7]];
(* vpsubd %ymm3,%ymm5,%ymm12                       #! PC = 0x5555555758bd *)
sub %ymm12 %ymm5 %ymm3;
(* vpaddd %ymm3,%ymm5,%ymm5                        #! PC = 0x5555555758c1 *)
add %ymm5 %ymm5 %ymm3;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555758c5 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555758ca *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm3                      #! PC = 0x5555555758d0 *)
add %ymm3 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm5,%ymm5                       #! PC = 0x5555555758d5 *)
sub %ymm5 %ymm5 %ymm13;
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x5555555758da *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x5555555758df *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x5555555758e4 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x5555555758e9 *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x5555555758ee *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555758f3 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555758f8 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x5555555758fd *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x555555575902 *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm4,%ymm12                      #! PC = 0x555555575908 *)
sub %ymm12 %ymm4 %ymm11;
(* vpaddd %ymm11,%ymm4,%ymm4                       #! PC = 0x55555557590d *)
add %ymm4 %ymm4 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555575912 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555575917 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x55555557591d *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm4,%ymm4                       #! PC = 0x555555575922 *)
sub %ymm4 %ymm4 %ymm13;

(* vmovdqa 0x380(%rsi),%ymm1                       #! EA = L0x55555557a160; Value = 0x9940a4f785f9213c; PC = 0x555555575927 *)
mov %ymm1 [L0x55555557a160, L0x55555557a164, L0x55555557a168, L0x55555557a16c, L0x55555557a170, L0x55555557a174, L0x55555557a178, L0x55555557a17c];
(* vmovdqa 0x820(%rsi),%ymm2                       #! EA = L0x55555557a600; Value = 0x0021c4f7ffd1a13c; PC = 0x55555557592f *)
mov %ymm2 [L0x55555557a600, L0x55555557a604, L0x55555557a608, L0x55555557a60c, L0x55555557a610, L0x55555557a614, L0x55555557a618, L0x55555557a61c];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x555555575937 *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x55555557593c *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm8,%ymm13                      #! PC = 0x555555575940 *)
mull %ymm13_h %ymm13_l (%ymm8[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm8,%ymm12                          #! PC = 0x555555575945 *)
mov %ymm12 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x55555557594a *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm8,%ymm8                       #! PC = 0x55555557594f *)
mull %ymm8_h %ymm8_l (%ymm8[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm8_ls@sint32[4] %ymm8_l;
mov %ymm8 [%ymm8_ls[0], %ymm8_h[0], %ymm8_ls[1], %ymm8_h[1], %ymm8_ls[2], %ymm8_h[2], %ymm8_ls[3], %ymm8_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555575954 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555575959 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x55555557595e *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm8_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm8_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm8,%ymm8                           #! PC = 0x555555575963 *)
mov %ymm8 (%ymm8[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm8,%ymm8               #! PC = 0x555555575968 *)
mov %ymm8 [%ymm8[0], %ymm12[1], %ymm8[2], %ymm12[3], %ymm8[4], %ymm12[5], %ymm8[6], %ymm12[7]];
(* vpsubd %ymm8,%ymm9,%ymm12                       #! PC = 0x55555557596e *)
sub %ymm12 %ymm9 %ymm8;
(* vpaddd %ymm8,%ymm9,%ymm9                        #! PC = 0x555555575973 *)
add %ymm9 %ymm9 %ymm8;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555575978 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x55555557597d *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm8                      #! PC = 0x555555575983 *)
add %ymm8 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm9,%ymm9                       #! PC = 0x555555575988 *)
sub %ymm9 %ymm9 %ymm13;
(* vmovdqa 0x400(%rsi),%ymm1                       #! EA = L0x55555557a1e0; Value = 0xf96f9bf4005ce539; PC = 0x55555557598d *)
mov %ymm1 [L0x55555557a1e0, L0x55555557a1e4, L0x55555557a1e8, L0x55555557a1ec, L0x55555557a1f0, L0x55555557a1f4, L0x55555557a1f8, L0x55555557a1fc];
(* vmovdqa 0x8a0(%rsi),%ymm2                       #! EA = L0x55555557a680; Value = 0xfff11bf40035c539; PC = 0x555555575995 *)
mov %ymm2 [L0x55555557a680, L0x55555557a684, L0x55555557a688, L0x55555557a68c, L0x55555557a690, L0x55555557a694, L0x55555557a698, L0x55555557a69c];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x55555557599d *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x5555555759a2 *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm6,%ymm13                      #! PC = 0x5555555759a6 *)
mull %ymm13_h %ymm13_l (%ymm6[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm6,%ymm12                          #! PC = 0x5555555759ab *)
mov %ymm12 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x5555555759af *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm6,%ymm6                       #! PC = 0x5555555759b4 *)
mull %ymm6_h %ymm6_l (%ymm6[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm6_ls@sint32[4] %ymm6_l;
mov %ymm6 [%ymm6_ls[0], %ymm6_h[0], %ymm6_ls[1], %ymm6_h[1], %ymm6_ls[2], %ymm6_h[2], %ymm6_ls[3], %ymm6_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x5555555759b9 *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x5555555759be *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x5555555759c3 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm6_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm6_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm6,%ymm6                           #! PC = 0x5555555759c8 *)
mov %ymm6 (%ymm6[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm6,%ymm6               #! PC = 0x5555555759cc *)
mov %ymm6 [%ymm6[0], %ymm12[1], %ymm6[2], %ymm12[3], %ymm6[4], %ymm12[5], %ymm6[6], %ymm12[7]];
(* vpsubd %ymm6,%ymm7,%ymm12                       #! PC = 0x5555555759d2 *)
sub %ymm12 %ymm7 %ymm6;
(* vpaddd %ymm6,%ymm7,%ymm7                        #! PC = 0x5555555759d6 *)
add %ymm7 %ymm7 %ymm6;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x5555555759da *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x5555555759df *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm6                      #! PC = 0x5555555759e5 *)
add %ymm6 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm7,%ymm7                       #! PC = 0x5555555759ea *)
sub %ymm7 %ymm7 %ymm13;
(* vmovdqa 0x480(%rsi),%ymm1                       #! EA = L0x55555557a260; Value = 0xef5715e729dda115; PC = 0x5555555759ef *)
mov %ymm1 [L0x55555557a260, L0x55555557a264, L0x55555557a268, L0x55555557a26c, L0x55555557a270, L0x55555557a274, L0x55555557a278, L0x55555557a27c];
(* vmovdqa 0x920(%rsi),%ymm2                       #! EA = L0x55555557a700; Value = 0x001a35e7003b0115; PC = 0x5555555759f7 *)
mov %ymm2 [L0x55555557a700, L0x55555557a704, L0x55555557a708, L0x55555557a70c, L0x55555557a710, L0x55555557a714, L0x55555557a718, L0x55555557a71c];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x5555555759ff *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x555555575a04 *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm4,%ymm13                      #! PC = 0x555555575a08 *)
mull %ymm13_h %ymm13_l (%ymm4[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm4,%ymm12                          #! PC = 0x555555575a0d *)
mov %ymm12 (%ymm4[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555575a11 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm4,%ymm4                       #! PC = 0x555555575a16 *)
mull %ymm4_h %ymm4_l (%ymm4[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm4_ls@sint32[4] %ymm4_l;
mov %ymm4 [%ymm4_ls[0], %ymm4_h[0], %ymm4_ls[1], %ymm4_h[1], %ymm4_ls[2], %ymm4_h[2], %ymm4_ls[3], %ymm4_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555575a1b *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555575a20 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555575a25 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm4_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm4_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm4,%ymm4                           #! PC = 0x555555575a2a *)
mov %ymm4 (%ymm4[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm4,%ymm4               #! PC = 0x555555575a2e *)
mov %ymm4 [%ymm4[0], %ymm12[1], %ymm4[2], %ymm12[3], %ymm4[4], %ymm12[5], %ymm4[6], %ymm12[7]];
(* vpsubd %ymm4,%ymm5,%ymm12                       #! PC = 0x555555575a34 *)
sub %ymm12 %ymm5 %ymm4;
(* vpaddd %ymm4,%ymm5,%ymm5                        #! PC = 0x555555575a38 *)
add %ymm5 %ymm5 %ymm4;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555575a3c *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555575a41 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm4                      #! PC = 0x555555575a47 *)
add %ymm4 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm5,%ymm5                       #! PC = 0x555555575a4c *)
sub %ymm5 %ymm5 %ymm13;
(* vmovdqa 0x500(%rsi),%ymm1                       #! EA = L0x55555557a2e0; Value = 0x1788f40ea3bc1dc0; PC = 0x555555575a51 *)
mov %ymm1 [L0x55555557a2e0, L0x55555557a2e4, L0x55555557a2e8, L0x55555557a2ec, L0x55555557a2f0, L0x55555557a2f4, L0x55555557a2f8, L0x55555557a2fc];
(* vmovdqa 0x9a0(%rsi),%ymm2                       #! EA = L0x55555557a780; Value = 0x0007340e00041dc0; PC = 0x555555575a59 *)
mov %ymm2 [L0x55555557a780, L0x55555557a784, L0x55555557a788, L0x55555557a78c, L0x55555557a790, L0x55555557a794, L0x55555557a798, L0x55555557a79c];
(* vpsrlq $0x20,%ymm1,%ymm10                       #! PC = 0x555555575a61 *)
mov %ymm10 [%ymm1[1], 0@sint32, %ymm1[3], 0@sint32, %ymm1[5], 0@sint32, %ymm1[7], 0@sint32];
(* vmovshdup %ymm2,%ymm15                          #! PC = 0x555555575a66 *)
mov %ymm15 (%ymm2[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm1,%ymm11,%ymm13                     #! PC = 0x555555575a6a *)
mull %ymm13_h %ymm13_l (%ymm11[0, 2, 4, 6]) (%ymm1[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vmovshdup %ymm11,%ymm12                         #! PC = 0x555555575a6f *)
mov %ymm12 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpmuldq %ymm10,%ymm12,%ymm14                    #! PC = 0x555555575a74 *)
mull %ymm14_h %ymm14_l (%ymm12[0, 2, 4, 6]) (%ymm10[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
(* vpmuldq %ymm2,%ymm11,%ymm11                     #! PC = 0x555555575a79 *)
mull %ymm11_h %ymm11_l (%ymm11[0, 2, 4, 6]) (%ymm2[0, 2, 4, 6]);
cast %ymm11_ls@sint32[4] %ymm11_l;
mov %ymm11 [%ymm11_ls[0], %ymm11_h[0], %ymm11_ls[1], %ymm11_h[1], %ymm11_ls[2], %ymm11_h[2], %ymm11_ls[3], %ymm11_h[3]];
(* vpmuldq %ymm15,%ymm12,%ymm12                    #! PC = 0x555555575a7e *)
mull %ymm12_h %ymm12_l (%ymm12[0, 2, 4, 6]) (%ymm15[0, 2, 4, 6]);
cast %ymm12_ls@sint32[4] %ymm12_l;
mov %ymm12 [%ymm12_ls[0], %ymm12_h[0], %ymm12_ls[1], %ymm12_h[1], %ymm12_ls[2], %ymm12_h[2], %ymm12_ls[3], %ymm12_h[3]];
(* vpmuldq %ymm0,%ymm13,%ymm13                     #! PC = 0x555555575a83 *)
mull %ymm13_h %ymm13_l (%ymm13[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm13_ls@sint32[4] %ymm13_l;
mov %ymm13 [%ymm13_ls[0], %ymm13_h[0], %ymm13_ls[1], %ymm13_h[1], %ymm13_ls[2], %ymm13_h[2], %ymm13_ls[3], %ymm13_h[3]];
(* vpmuldq %ymm0,%ymm14,%ymm14                     #! PC = 0x555555575a88 *)
mull %ymm14_h %ymm14_l (%ymm14[0, 2, 4, 6]) (%ymm0[0, 2, 4, 6]);
cast %ymm14_ls@sint32[4] %ymm14_l;
mov %ymm14 [%ymm14_ls[0], %ymm14_h[0], %ymm14_ls[1], %ymm14_h[1], %ymm14_ls[2], %ymm14_h[2], %ymm14_ls[3], %ymm14_h[3]];
assert and [ eqmod %ymm11_l %ymm13_l [ 2**32, 2**32, 2**32, 2**32 ], eqmod %ymm12_l %ymm14_l [2**32, 2**32, 2**32, 2**32 ] ] &&& true;
assume and [ eq %ymm11_l %ymm13_l, eq %ymm12_l %ymm14_l ] &&& true;
(* vmovshdup %ymm11,%ymm11                         #! PC = 0x555555575a8d *)
mov %ymm11 (%ymm11[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm12,%ymm11,%ymm11             #! PC = 0x555555575a92 *)
mov %ymm11 [%ymm11[0], %ymm12[1], %ymm11[2], %ymm12[3], %ymm11[4], %ymm12[5], %ymm11[6], %ymm12[7]];
(* vpsubd %ymm11,%ymm3,%ymm12                      #! PC = 0x555555575a98 *)
sub %ymm12 %ymm3 %ymm11;
(* vpaddd %ymm11,%ymm3,%ymm3                       #! PC = 0x555555575a9d *)
add %ymm3 %ymm3 %ymm11;
(* vmovshdup %ymm13,%ymm13                         #! PC = 0x555555575aa2 *)
mov %ymm13 (%ymm13[1, 1, 3, 3, 5, 5, 7, 7]);
(* vpblendd $0xaa,%ymm14,%ymm13,%ymm13             #! PC = 0x555555575aa7 *)
mov %ymm13 [%ymm13[0], %ymm14[1], %ymm13[2], %ymm14[3], %ymm13[4], %ymm14[5], %ymm13[6], %ymm14[7]];
(* vpaddd %ymm13,%ymm12,%ymm11                     #! PC = 0x555555575aad *)
add %ymm11 %ymm12 %ymm13;
(* vpsubd %ymm13,%ymm3,%ymm3                       #! PC = 0x555555575ab2 *)
sub %ymm3 %ymm3 %ymm13;
(* vmovdqa %ymm9,0x300(%rdi)                       #! EA = L0x7fffffff47c0; PC = 0x555555575ab7 *)
mov [L0x7fffffff47c0, L0x7fffffff47c4, L0x7fffffff47c8, L0x7fffffff47cc, L0x7fffffff47d0, L0x7fffffff47d4, L0x7fffffff47d8, L0x7fffffff47dc] %ymm9;
(* vmovdqa %ymm8,0x320(%rdi)                       #! EA = L0x7fffffff47e0; PC = 0x555555575abf *)
mov [L0x7fffffff47e0, L0x7fffffff47e4, L0x7fffffff47e8, L0x7fffffff47ec, L0x7fffffff47f0, L0x7fffffff47f4, L0x7fffffff47f8, L0x7fffffff47fc] %ymm8;
(* vmovdqa %ymm7,0x340(%rdi)                       #! EA = L0x7fffffff4800; PC = 0x555555575ac7 *)
mov [L0x7fffffff4800, L0x7fffffff4804, L0x7fffffff4808, L0x7fffffff480c, L0x7fffffff4810, L0x7fffffff4814, L0x7fffffff4818, L0x7fffffff481c] %ymm7;
(* vmovdqa %ymm6,0x360(%rdi)                       #! EA = L0x7fffffff4820; PC = 0x555555575acf *)
mov [L0x7fffffff4820, L0x7fffffff4824, L0x7fffffff4828, L0x7fffffff482c, L0x7fffffff4830, L0x7fffffff4834, L0x7fffffff4838, L0x7fffffff483c] %ymm6;
(* vmovdqa %ymm5,0x380(%rdi)                       #! EA = L0x7fffffff4840; PC = 0x555555575ad7 *)
mov [L0x7fffffff4840, L0x7fffffff4844, L0x7fffffff4848, L0x7fffffff484c, L0x7fffffff4850, L0x7fffffff4854, L0x7fffffff4858, L0x7fffffff485c] %ymm5;
(* vmovdqa %ymm4,0x3a0(%rdi)                       #! EA = L0x7fffffff4860; PC = 0x555555575adf *)
mov [L0x7fffffff4860, L0x7fffffff4864, L0x7fffffff4868, L0x7fffffff486c, L0x7fffffff4870, L0x7fffffff4874, L0x7fffffff4878, L0x7fffffff487c] %ymm4;
(* vmovdqa %ymm3,0x3c0(%rdi)                       #! EA = L0x7fffffff4880; PC = 0x555555575ae7 *)
mov [L0x7fffffff4880, L0x7fffffff4884, L0x7fffffff4888, L0x7fffffff488c, L0x7fffffff4890, L0x7fffffff4894, L0x7fffffff4898, L0x7fffffff489c] %ymm3;
(* vmovdqa %ymm11,0x3e0(%rdi)                      #! EA = L0x7fffffff48a0; PC = 0x555555575aef *)
mov [L0x7fffffff48a0, L0x7fffffff48a4, L0x7fffffff48a8, L0x7fffffff48ac, L0x7fffffff48b0, L0x7fffffff48b4, L0x7fffffff48b8, L0x7fffffff48bc] %ymm11;

(* #! <- SP = 0x7fffffff3c58 *)
#! 0x7fffffff3c58 = 0x7fffffff3c58;
(* #retq                                           #! PC = 0x555555575af7 *)
#retq                                           #! 0x555555575af7 = 0x555555575af7;
{
  and [
    eqmod (inp_poly**2) L0x7fffffff44c0 [8380417, X - (1753)],
    eqmod (inp_poly**2) L0x7fffffff44e0 [8380417, X - (-1753)],
    eqmod (inp_poly**2) L0x7fffffff4500 [8380417, X - (-1935420)],
    eqmod (inp_poly**2) L0x7fffffff4520 [8380417, X - (1935420)],
    eqmod (inp_poly**2) L0x7fffffff4540 [8380417, X - (-2659525)],
    eqmod (inp_poly**2) L0x7fffffff4560 [8380417, X - (2659525)],
    eqmod (inp_poly**2) L0x7fffffff4580 [8380417, X - (-1455890)],
    eqmod (inp_poly**2) L0x7fffffff45a0 [8380417, X - (1455890)],
    eqmod (inp_poly**2) L0x7fffffff44c4 [8380417, X - (2660408)],
    eqmod (inp_poly**2) L0x7fffffff44e4 [8380417, X - (-2660408)],
    eqmod (inp_poly**2) L0x7fffffff4504 [8380417, X - (-1780227)],
    eqmod (inp_poly**2) L0x7fffffff4524 [8380417, X - (1780227)],
    eqmod (inp_poly**2) L0x7fffffff4544 [8380417, X - (-59148)],
    eqmod (inp_poly**2) L0x7fffffff4564 [8380417, X - (59148)],
    eqmod (inp_poly**2) L0x7fffffff4584 [8380417, X - (2772600)],
    eqmod (inp_poly**2) L0x7fffffff45a4 [8380417, X - (-2772600)],
    eqmod (inp_poly**2) L0x7fffffff44c8 [8380417, X - (1182243)],
    eqmod (inp_poly**2) L0x7fffffff44e8 [8380417, X - (-1182243)],
    eqmod (inp_poly**2) L0x7fffffff4508 [8380417, X - (87208)],
    eqmod (inp_poly**2) L0x7fffffff4528 [8380417, X - (-87208)],
    eqmod (inp_poly**2) L0x7fffffff4548 [8380417, X - (636927)],
    eqmod (inp_poly**2) L0x7fffffff4568 [8380417, X - (-636927)],
    eqmod (inp_poly**2) L0x7fffffff4588 [8380417, X - (-3965306)],
    eqmod (inp_poly**2) L0x7fffffff45a8 [8380417, X - (3965306)],
    eqmod (inp_poly**2) L0x7fffffff44cc [8380417, X - (-3956745)],
    eqmod (inp_poly**2) L0x7fffffff44ec [8380417, X - (3956745)],
    eqmod (inp_poly**2) L0x7fffffff450c [8380417, X - (-2296397)],
    eqmod (inp_poly**2) L0x7fffffff452c [8380417, X - (2296397)],
    eqmod (inp_poly**2) L0x7fffffff454c [8380417, X - (-3284915)],
    eqmod (inp_poly**2) L0x7fffffff456c [8380417, X - (3284915)],
    eqmod (inp_poly**2) L0x7fffffff458c [8380417, X - (-3716946)],
    eqmod (inp_poly**2) L0x7fffffff45ac [8380417, X - (3716946)],
    eqmod (inp_poly**2) L0x7fffffff44d0 [8380417, X - (-27812)],
    eqmod (inp_poly**2) L0x7fffffff44f0 [8380417, X - (27812)],
    eqmod (inp_poly**2) L0x7fffffff4510 [8380417, X - (822541)],
    eqmod (inp_poly**2) L0x7fffffff4530 [8380417, X - (-822541)],
    eqmod (inp_poly**2) L0x7fffffff4550 [8380417, X - (1009365)],
    eqmod (inp_poly**2) L0x7fffffff4570 [8380417, X - (-1009365)],
    eqmod (inp_poly**2) L0x7fffffff4590 [8380417, X - (-2454145)],
    eqmod (inp_poly**2) L0x7fffffff45b0 [8380417, X - (2454145)],
    eqmod (inp_poly**2) L0x7fffffff44d4 [8380417, X - (-1979497)],
    eqmod (inp_poly**2) L0x7fffffff44f4 [8380417, X - (1979497)],
    eqmod (inp_poly**2) L0x7fffffff4514 [8380417, X - (1596822)],
    eqmod (inp_poly**2) L0x7fffffff4534 [8380417, X - (-1596822)],
    eqmod (inp_poly**2) L0x7fffffff4554 [8380417, X - (-3956944)],
    eqmod (inp_poly**2) L0x7fffffff4574 [8380417, X - (3956944)],
    eqmod (inp_poly**2) L0x7fffffff4594 [8380417, X - (-3759465)],
    eqmod (inp_poly**2) L0x7fffffff45b4 [8380417, X - (3759465)],
    eqmod (inp_poly**2) L0x7fffffff44d8 [8380417, X - (-1685153)],
    eqmod (inp_poly**2) L0x7fffffff44f8 [8380417, X - (1685153)],
    eqmod (inp_poly**2) L0x7fffffff4518 [8380417, X - (-3410568)],
    eqmod (inp_poly**2) L0x7fffffff4538 [8380417, X - (3410568)],
    eqmod (inp_poly**2) L0x7fffffff4558 [8380417, X - (2678278)],
    eqmod (inp_poly**2) L0x7fffffff4578 [8380417, X - (-2678278)],
    eqmod (inp_poly**2) L0x7fffffff4598 [8380417, X - (-3768948)],
    eqmod (inp_poly**2) L0x7fffffff45b8 [8380417, X - (3768948)],
    eqmod (inp_poly**2) L0x7fffffff44dc [8380417, X - (-3551006)],
    eqmod (inp_poly**2) L0x7fffffff44fc [8380417, X - (3551006)],
    eqmod (inp_poly**2) L0x7fffffff451c [8380417, X - (635956)],
    eqmod (inp_poly**2) L0x7fffffff453c [8380417, X - (-635956)],
    eqmod (inp_poly**2) L0x7fffffff455c [8380417, X - (-250446)],
    eqmod (inp_poly**2) L0x7fffffff457c [8380417, X - (250446)],
    eqmod (inp_poly**2) L0x7fffffff459c [8380417, X - (-2455377)],
    eqmod (inp_poly**2) L0x7fffffff45bc [8380417, X - (2455377)],
    eqmod (inp_poly**2) L0x7fffffff45c0 [8380417, X - (-4146264)],
    eqmod (inp_poly**2) L0x7fffffff45e0 [8380417, X - (4146264)],
    eqmod (inp_poly**2) L0x7fffffff4600 [8380417, X - (-1772588)],
    eqmod (inp_poly**2) L0x7fffffff4620 [8380417, X - (1772588)],
    eqmod (inp_poly**2) L0x7fffffff4640 [8380417, X - (2192938)],
    eqmod (inp_poly**2) L0x7fffffff4660 [8380417, X - (-2192938)],
    eqmod (inp_poly**2) L0x7fffffff4680 [8380417, X - (-1727088)],
    eqmod (inp_poly**2) L0x7fffffff46a0 [8380417, X - (1727088)],
    eqmod (inp_poly**2) L0x7fffffff45c4 [8380417, X - (2387513)],
    eqmod (inp_poly**2) L0x7fffffff45e4 [8380417, X - (-2387513)],
    eqmod (inp_poly**2) L0x7fffffff4604 [8380417, X - (-3611750)],
    eqmod (inp_poly**2) L0x7fffffff4624 [8380417, X - (3611750)],
    eqmod (inp_poly**2) L0x7fffffff4644 [8380417, X - (-268456)],
    eqmod (inp_poly**2) L0x7fffffff4664 [8380417, X - (268456)],
    eqmod (inp_poly**2) L0x7fffffff4684 [8380417, X - (-3180456)],
    eqmod (inp_poly**2) L0x7fffffff46a4 [8380417, X - (3180456)],
    eqmod (inp_poly**2) L0x7fffffff45c8 [8380417, X - (3747250)],
    eqmod (inp_poly**2) L0x7fffffff45e8 [8380417, X - (-3747250)],
    eqmod (inp_poly**2) L0x7fffffff4608 [8380417, X - (2296099)],
    eqmod (inp_poly**2) L0x7fffffff4628 [8380417, X - (-2296099)],
    eqmod (inp_poly**2) L0x7fffffff4648 [8380417, X - (1239911)],
    eqmod (inp_poly**2) L0x7fffffff4668 [8380417, X - (-1239911)],
    eqmod (inp_poly**2) L0x7fffffff4688 [8380417, X - (-3838479)],
    eqmod (inp_poly**2) L0x7fffffff46a8 [8380417, X - (3838479)],
    eqmod (inp_poly**2) L0x7fffffff45cc [8380417, X - (3195676)],
    eqmod (inp_poly**2) L0x7fffffff45ec [8380417, X - (-3195676)],
    eqmod (inp_poly**2) L0x7fffffff460c [8380417, X - (2642980)],
    eqmod (inp_poly**2) L0x7fffffff462c [8380417, X - (-2642980)],
    eqmod (inp_poly**2) L0x7fffffff464c [8380417, X - (1254190)],
    eqmod (inp_poly**2) L0x7fffffff466c [8380417, X - (-1254190)],
    eqmod (inp_poly**2) L0x7fffffff468c [8380417, X - (-12417)],
    eqmod (inp_poly**2) L0x7fffffff46ac [8380417, X - (12417)],
    eqmod (inp_poly**2) L0x7fffffff45d0 [8380417, X - (2998219)],
    eqmod (inp_poly**2) L0x7fffffff45f0 [8380417, X - (-2998219)],
    eqmod (inp_poly**2) L0x7fffffff4610 [8380417, X - (141835)],
    eqmod (inp_poly**2) L0x7fffffff4630 [8380417, X - (-141835)],
    eqmod (inp_poly**2) L0x7fffffff4650 [8380417, X - (-89301)],
    eqmod (inp_poly**2) L0x7fffffff4670 [8380417, X - (89301)],
    eqmod (inp_poly**2) L0x7fffffff4690 [8380417, X - (2513018)],
    eqmod (inp_poly**2) L0x7fffffff46b0 [8380417, X - (-2513018)],
    eqmod (inp_poly**2) L0x7fffffff45d4 [8380417, X - (-1354892)],
    eqmod (inp_poly**2) L0x7fffffff45f4 [8380417, X - (1354892)],
    eqmod (inp_poly**2) L0x7fffffff4614 [8380417, X - (613238)],
    eqmod (inp_poly**2) L0x7fffffff4634 [8380417, X - (-613238)],
    eqmod (inp_poly**2) L0x7fffffff4654 [8380417, X - (-1310261)],
    eqmod (inp_poly**2) L0x7fffffff4674 [8380417, X - (1310261)],
    eqmod (inp_poly**2) L0x7fffffff4694 [8380417, X - (-2218467)],
    eqmod (inp_poly**2) L0x7fffffff46b4 [8380417, X - (2218467)],
    eqmod (inp_poly**2) L0x7fffffff45d8 [8380417, X - (-458740)],
    eqmod (inp_poly**2) L0x7fffffff45f8 [8380417, X - (458740)],
    eqmod (inp_poly**2) L0x7fffffff4618 [8380417, X - (-1921994)],
    eqmod (inp_poly**2) L0x7fffffff4638 [8380417, X - (1921994)],
    eqmod (inp_poly**2) L0x7fffffff4658 [8380417, X - (4040196)],
    eqmod (inp_poly**2) L0x7fffffff4678 [8380417, X - (-4040196)],
    eqmod (inp_poly**2) L0x7fffffff4698 [8380417, X - (-3472069)],
    eqmod (inp_poly**2) L0x7fffffff46b8 [8380417, X - (3472069)],
    eqmod (inp_poly**2) L0x7fffffff45dc [8380417, X - (2039144)],
    eqmod (inp_poly**2) L0x7fffffff45fc [8380417, X - (-2039144)],
    eqmod (inp_poly**2) L0x7fffffff461c [8380417, X - (-1879878)],
    eqmod (inp_poly**2) L0x7fffffff463c [8380417, X - (1879878)],
    eqmod (inp_poly**2) L0x7fffffff465c [8380417, X - (-818761)],
    eqmod (inp_poly**2) L0x7fffffff467c [8380417, X - (818761)],
    eqmod (inp_poly**2) L0x7fffffff469c [8380417, X - (-2178965)],
    eqmod (inp_poly**2) L0x7fffffff46bc [8380417, X - (2178965)],
    eqmod (inp_poly**2) L0x7fffffff46c0 [8380417, X - (-1623354)],
    eqmod (inp_poly**2) L0x7fffffff46e0 [8380417, X - (1623354)],
    eqmod (inp_poly**2) L0x7fffffff4700 [8380417, X - (2105286)],
    eqmod (inp_poly**2) L0x7fffffff4720 [8380417, X - (-2105286)],
    eqmod (inp_poly**2) L0x7fffffff4740 [8380417, X - (-2374402)],
    eqmod (inp_poly**2) L0x7fffffff4760 [8380417, X - (2374402)],
    eqmod (inp_poly**2) L0x7fffffff4780 [8380417, X - (-2033807)],
    eqmod (inp_poly**2) L0x7fffffff47a0 [8380417, X - (2033807)],
    eqmod (inp_poly**2) L0x7fffffff46c4 [8380417, X - (586241)],
    eqmod (inp_poly**2) L0x7fffffff46e4 [8380417, X - (-586241)],
    eqmod (inp_poly**2) L0x7fffffff4704 [8380417, X - (-1179613)],
    eqmod (inp_poly**2) L0x7fffffff4724 [8380417, X - (1179613)],
    eqmod (inp_poly**2) L0x7fffffff4744 [8380417, X - (527981)],
    eqmod (inp_poly**2) L0x7fffffff4764 [8380417, X - (-527981)],
    eqmod (inp_poly**2) L0x7fffffff4784 [8380417, X - (-2743411)],
    eqmod (inp_poly**2) L0x7fffffff47a4 [8380417, X - (2743411)],
    eqmod (inp_poly**2) L0x7fffffff46c8 [8380417, X - (-1476985)],
    eqmod (inp_poly**2) L0x7fffffff46e8 [8380417, X - (1476985)],
    eqmod (inp_poly**2) L0x7fffffff4708 [8380417, X - (1994046)],
    eqmod (inp_poly**2) L0x7fffffff4728 [8380417, X - (-1994046)],
    eqmod (inp_poly**2) L0x7fffffff4748 [8380417, X - (2491325)],
    eqmod (inp_poly**2) L0x7fffffff4768 [8380417, X - (-2491325)],
    eqmod (inp_poly**2) L0x7fffffff4788 [8380417, X - (-1393159)],
    eqmod (inp_poly**2) L0x7fffffff47a8 [8380417, X - (1393159)],
    eqmod (inp_poly**2) L0x7fffffff46cc [8380417, X - (507927)],
    eqmod (inp_poly**2) L0x7fffffff46ec [8380417, X - (-507927)],
    eqmod (inp_poly**2) L0x7fffffff470c [8380417, X - (-1187885)],
    eqmod (inp_poly**2) L0x7fffffff472c [8380417, X - (1187885)],
    eqmod (inp_poly**2) L0x7fffffff474c [8380417, X - (-724804)],
    eqmod (inp_poly**2) L0x7fffffff476c [8380417, X - (724804)],
    eqmod (inp_poly**2) L0x7fffffff478c [8380417, X - (-1834526)],
    eqmod (inp_poly**2) L0x7fffffff47ac [8380417, X - (1834526)],
    eqmod (inp_poly**2) L0x7fffffff46d0 [8380417, X - (-3033742)],
    eqmod (inp_poly**2) L0x7fffffff46f0 [8380417, X - (3033742)],
    eqmod (inp_poly**2) L0x7fffffff4710 [8380417, X - (-338420)],
    eqmod (inp_poly**2) L0x7fffffff4730 [8380417, X - (338420)],
    eqmod (inp_poly**2) L0x7fffffff4750 [8380417, X - (2647994)],
    eqmod (inp_poly**2) L0x7fffffff4770 [8380417, X - (-2647994)],
    eqmod (inp_poly**2) L0x7fffffff4790 [8380417, X - (3009748)],
    eqmod (inp_poly**2) L0x7fffffff47b0 [8380417, X - (-3009748)],
    eqmod (inp_poly**2) L0x7fffffff46d4 [8380417, X - (-2612853)],
    eqmod (inp_poly**2) L0x7fffffff46f4 [8380417, X - (2612853)],
    eqmod (inp_poly**2) L0x7fffffff4714 [8380417, X - (4148469)],
    eqmod (inp_poly**2) L0x7fffffff4734 [8380417, X - (-4148469)],
    eqmod (inp_poly**2) L0x7fffffff4754 [8380417, X - (749577)],
    eqmod (inp_poly**2) L0x7fffffff4774 [8380417, X - (-749577)],
    eqmod (inp_poly**2) L0x7fffffff4794 [8380417, X - (-4022750)],
    eqmod (inp_poly**2) L0x7fffffff47b4 [8380417, X - (4022750)],
    eqmod (inp_poly**2) L0x7fffffff46d8 [8380417, X - (3980599)],
    eqmod (inp_poly**2) L0x7fffffff46f8 [8380417, X - (-3980599)],
    eqmod (inp_poly**2) L0x7fffffff4718 [8380417, X - (2569011)],
    eqmod (inp_poly**2) L0x7fffffff4738 [8380417, X - (-2569011)],
    eqmod (inp_poly**2) L0x7fffffff4758 [8380417, X - (-1615530)],
    eqmod (inp_poly**2) L0x7fffffff4778 [8380417, X - (1615530)],
    eqmod (inp_poly**2) L0x7fffffff4798 [8380417, X - (1723229)],
    eqmod (inp_poly**2) L0x7fffffff47b8 [8380417, X - (-1723229)],
    eqmod (inp_poly**2) L0x7fffffff46dc [8380417, X - (1665318)],
    eqmod (inp_poly**2) L0x7fffffff46fc [8380417, X - (-1665318)],
    eqmod (inp_poly**2) L0x7fffffff471c [8380417, X - (2028038)],
    eqmod (inp_poly**2) L0x7fffffff473c [8380417, X - (-2028038)],
    eqmod (inp_poly**2) L0x7fffffff475c [8380417, X - (1163598)],
    eqmod (inp_poly**2) L0x7fffffff477c [8380417, X - (-1163598)],
    eqmod (inp_poly**2) L0x7fffffff479c [8380417, X - (-3369273)],
    eqmod (inp_poly**2) L0x7fffffff47bc [8380417, X - (3369273)],
    eqmod (inp_poly**2) L0x7fffffff47c0 [8380417, X - (3994671)],
    eqmod (inp_poly**2) L0x7fffffff47e0 [8380417, X - (-3994671)],
    eqmod (inp_poly**2) L0x7fffffff4800 [8380417, X - (-11879)],
    eqmod (inp_poly**2) L0x7fffffff4820 [8380417, X - (11879)],
    eqmod (inp_poly**2) L0x7fffffff4840 [8380417, X - (-1370517)],
    eqmod (inp_poly**2) L0x7fffffff4860 [8380417, X - (1370517)],
    eqmod (inp_poly**2) L0x7fffffff4880 [8380417, X - (3020393)],
    eqmod (inp_poly**2) L0x7fffffff48a0 [8380417, X - (-3020393)],
    eqmod (inp_poly**2) L0x7fffffff47c4 [8380417, X - (3363542)],
    eqmod (inp_poly**2) L0x7fffffff47e4 [8380417, X - (-3363542)],
    eqmod (inp_poly**2) L0x7fffffff4804 [8380417, X - (214880)],
    eqmod (inp_poly**2) L0x7fffffff4824 [8380417, X - (-214880)],
    eqmod (inp_poly**2) L0x7fffffff4844 [8380417, X - (545376)],
    eqmod (inp_poly**2) L0x7fffffff4864 [8380417, X - (-545376)],
    eqmod (inp_poly**2) L0x7fffffff4884 [8380417, X - (-770441)],
    eqmod (inp_poly**2) L0x7fffffff48a4 [8380417, X - (770441)],
    eqmod (inp_poly**2) L0x7fffffff47c8 [8380417, X - (3105558)],
    eqmod (inp_poly**2) L0x7fffffff47e8 [8380417, X - (-3105558)],
    eqmod (inp_poly**2) L0x7fffffff4808 [8380417, X - (-1103344)],
    eqmod (inp_poly**2) L0x7fffffff4828 [8380417, X - (1103344)],
    eqmod (inp_poly**2) L0x7fffffff4848 [8380417, X - (508145)],
    eqmod (inp_poly**2) L0x7fffffff4868 [8380417, X - (-508145)],
    eqmod (inp_poly**2) L0x7fffffff4888 [8380417, X - (-553718)],
    eqmod (inp_poly**2) L0x7fffffff48a8 [8380417, X - (553718)],
    eqmod (inp_poly**2) L0x7fffffff47cc [8380417, X - (860144)],
    eqmod (inp_poly**2) L0x7fffffff47ec [8380417, X - (-860144)],
    eqmod (inp_poly**2) L0x7fffffff480c [8380417, X - (3430436)],
    eqmod (inp_poly**2) L0x7fffffff482c [8380417, X - (-3430436)],
    eqmod (inp_poly**2) L0x7fffffff484c [8380417, X - (140244)],
    eqmod (inp_poly**2) L0x7fffffff486c [8380417, X - (-140244)],
    eqmod (inp_poly**2) L0x7fffffff488c [8380417, X - (-1514152)],
    eqmod (inp_poly**2) L0x7fffffff48ac [8380417, X - (1514152)],
    eqmod (inp_poly**2) L0x7fffffff47d0 [8380417, X - (-2185084)],
    eqmod (inp_poly**2) L0x7fffffff47f0 [8380417, X - (2185084)],
    eqmod (inp_poly**2) L0x7fffffff4810 [8380417, X - (3123762)],
    eqmod (inp_poly**2) L0x7fffffff4830 [8380417, X - (-3123762)],
    eqmod (inp_poly**2) L0x7fffffff4850 [8380417, X - (2358373)],
    eqmod (inp_poly**2) L0x7fffffff4870 [8380417, X - (-2358373)],
    eqmod (inp_poly**2) L0x7fffffff4890 [8380417, X - (-2193087)],
    eqmod (inp_poly**2) L0x7fffffff48b0 [8380417, X - (2193087)],
    eqmod (inp_poly**2) L0x7fffffff47d4 [8380417, X - (-3014420)],
    eqmod (inp_poly**2) L0x7fffffff47f4 [8380417, X - (3014420)],
    eqmod (inp_poly**2) L0x7fffffff4814 [8380417, X - (-1716814)],
    eqmod (inp_poly**2) L0x7fffffff4834 [8380417, X - (1716814)],
    eqmod (inp_poly**2) L0x7fffffff4854 [8380417, X - (2926054)],
    eqmod (inp_poly**2) L0x7fffffff4874 [8380417, X - (-2926054)],
    eqmod (inp_poly**2) L0x7fffffff4894 [8380417, X - (-392707)],
    eqmod (inp_poly**2) L0x7fffffff48b4 [8380417, X - (392707)],
    eqmod (inp_poly**2) L0x7fffffff47d8 [8380417, X - (-303005)],
    eqmod (inp_poly**2) L0x7fffffff47f8 [8380417, X - (303005)],
    eqmod (inp_poly**2) L0x7fffffff4818 [8380417, X - (3531229)],
    eqmod (inp_poly**2) L0x7fffffff4838 [8380417, X - (-3531229)],
    eqmod (inp_poly**2) L0x7fffffff4858 [8380417, X - (-3974485)],
    eqmod (inp_poly**2) L0x7fffffff4878 [8380417, X - (3974485)],
    eqmod (inp_poly**2) L0x7fffffff4898 [8380417, X - (-3773731)],
    eqmod (inp_poly**2) L0x7fffffff48b8 [8380417, X - (3773731)],
    eqmod (inp_poly**2) L0x7fffffff47dc [8380417, X - (1900052)],
    eqmod (inp_poly**2) L0x7fffffff47fc [8380417, X - (-1900052)],
    eqmod (inp_poly**2) L0x7fffffff481c [8380417, X - (-781875)],
    eqmod (inp_poly**2) L0x7fffffff483c [8380417, X - (781875)],
    eqmod (inp_poly**2) L0x7fffffff485c [8380417, X - (1054478)],
    eqmod (inp_poly**2) L0x7fffffff487c [8380417, X - (-1054478)],
    eqmod (inp_poly**2) L0x7fffffff489c [8380417, X - (-731434)],
    eqmod (inp_poly**2) L0x7fffffff48bc [8380417, X - (731434)]
  ]
  &&&
  and [
    (-75423753)@32 <s L0x7fffffff44c0, L0x7fffffff44c0 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff44c4, L0x7fffffff44c4 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff44c8, L0x7fffffff44c8 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff44cc, L0x7fffffff44cc <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff44d0, L0x7fffffff44d0 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff44d4, L0x7fffffff44d4 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff44d8, L0x7fffffff44d8 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff44dc, L0x7fffffff44dc <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff44e0, L0x7fffffff44e0 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff44e4, L0x7fffffff44e4 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff44e8, L0x7fffffff44e8 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff44ec, L0x7fffffff44ec <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff44f0, L0x7fffffff44f0 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff44f4, L0x7fffffff44f4 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff44f8, L0x7fffffff44f8 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff44fc, L0x7fffffff44fc <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4500, L0x7fffffff4500 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4504, L0x7fffffff4504 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4508, L0x7fffffff4508 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff450c, L0x7fffffff450c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4510, L0x7fffffff4510 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4514, L0x7fffffff4514 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4518, L0x7fffffff4518 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff451c, L0x7fffffff451c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4520, L0x7fffffff4520 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4524, L0x7fffffff4524 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4528, L0x7fffffff4528 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff452c, L0x7fffffff452c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4530, L0x7fffffff4530 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4534, L0x7fffffff4534 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4538, L0x7fffffff4538 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff453c, L0x7fffffff453c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4540, L0x7fffffff4540 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4544, L0x7fffffff4544 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4548, L0x7fffffff4548 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff454c, L0x7fffffff454c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4550, L0x7fffffff4550 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4554, L0x7fffffff4554 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4558, L0x7fffffff4558 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff455c, L0x7fffffff455c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4560, L0x7fffffff4560 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4564, L0x7fffffff4564 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4568, L0x7fffffff4568 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff456c, L0x7fffffff456c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4570, L0x7fffffff4570 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4574, L0x7fffffff4574 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4578, L0x7fffffff4578 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff457c, L0x7fffffff457c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4580, L0x7fffffff4580 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4584, L0x7fffffff4584 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4588, L0x7fffffff4588 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff458c, L0x7fffffff458c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4590, L0x7fffffff4590 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4594, L0x7fffffff4594 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4598, L0x7fffffff4598 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff459c, L0x7fffffff459c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff45a0, L0x7fffffff45a0 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff45a4, L0x7fffffff45a4 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff45a8, L0x7fffffff45a8 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff45ac, L0x7fffffff45ac <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff45b0, L0x7fffffff45b0 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff45b4, L0x7fffffff45b4 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff45b8, L0x7fffffff45b8 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff45bc, L0x7fffffff45bc <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff45c0, L0x7fffffff45c0 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff45c4, L0x7fffffff45c4 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff45c8, L0x7fffffff45c8 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff45cc, L0x7fffffff45cc <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff45d0, L0x7fffffff45d0 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff45d4, L0x7fffffff45d4 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff45d8, L0x7fffffff45d8 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff45dc, L0x7fffffff45dc <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff45e0, L0x7fffffff45e0 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff45e4, L0x7fffffff45e4 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff45e8, L0x7fffffff45e8 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff45ec, L0x7fffffff45ec <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff45f0, L0x7fffffff45f0 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff45f4, L0x7fffffff45f4 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff45f8, L0x7fffffff45f8 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff45fc, L0x7fffffff45fc <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4600, L0x7fffffff4600 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4604, L0x7fffffff4604 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4608, L0x7fffffff4608 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff460c, L0x7fffffff460c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4610, L0x7fffffff4610 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4614, L0x7fffffff4614 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4618, L0x7fffffff4618 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff461c, L0x7fffffff461c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4620, L0x7fffffff4620 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4624, L0x7fffffff4624 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4628, L0x7fffffff4628 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff462c, L0x7fffffff462c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4630, L0x7fffffff4630 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4634, L0x7fffffff4634 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4638, L0x7fffffff4638 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff463c, L0x7fffffff463c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4640, L0x7fffffff4640 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4644, L0x7fffffff4644 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4648, L0x7fffffff4648 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff464c, L0x7fffffff464c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4650, L0x7fffffff4650 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4654, L0x7fffffff4654 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4658, L0x7fffffff4658 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff465c, L0x7fffffff465c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4660, L0x7fffffff4660 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4664, L0x7fffffff4664 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4668, L0x7fffffff4668 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff466c, L0x7fffffff466c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4670, L0x7fffffff4670 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4674, L0x7fffffff4674 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4678, L0x7fffffff4678 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff467c, L0x7fffffff467c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4680, L0x7fffffff4680 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4684, L0x7fffffff4684 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4688, L0x7fffffff4688 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff468c, L0x7fffffff468c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4690, L0x7fffffff4690 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4694, L0x7fffffff4694 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4698, L0x7fffffff4698 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff469c, L0x7fffffff469c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff46a0, L0x7fffffff46a0 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff46a4, L0x7fffffff46a4 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff46a8, L0x7fffffff46a8 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff46ac, L0x7fffffff46ac <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff46b0, L0x7fffffff46b0 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff46b4, L0x7fffffff46b4 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff46b8, L0x7fffffff46b8 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff46bc, L0x7fffffff46bc <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff46c0, L0x7fffffff46c0 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff46c4, L0x7fffffff46c4 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff46c8, L0x7fffffff46c8 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff46cc, L0x7fffffff46cc <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff46d0, L0x7fffffff46d0 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff46d4, L0x7fffffff46d4 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff46d8, L0x7fffffff46d8 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff46dc, L0x7fffffff46dc <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff46e0, L0x7fffffff46e0 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff46e4, L0x7fffffff46e4 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff46e8, L0x7fffffff46e8 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff46ec, L0x7fffffff46ec <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff46f0, L0x7fffffff46f0 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff46f4, L0x7fffffff46f4 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff46f8, L0x7fffffff46f8 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff46fc, L0x7fffffff46fc <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4700, L0x7fffffff4700 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4704, L0x7fffffff4704 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4708, L0x7fffffff4708 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff470c, L0x7fffffff470c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4710, L0x7fffffff4710 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4714, L0x7fffffff4714 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4718, L0x7fffffff4718 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff471c, L0x7fffffff471c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4720, L0x7fffffff4720 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4724, L0x7fffffff4724 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4728, L0x7fffffff4728 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff472c, L0x7fffffff472c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4730, L0x7fffffff4730 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4734, L0x7fffffff4734 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4738, L0x7fffffff4738 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff473c, L0x7fffffff473c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4740, L0x7fffffff4740 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4744, L0x7fffffff4744 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4748, L0x7fffffff4748 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff474c, L0x7fffffff474c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4750, L0x7fffffff4750 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4754, L0x7fffffff4754 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4758, L0x7fffffff4758 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff475c, L0x7fffffff475c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4760, L0x7fffffff4760 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4764, L0x7fffffff4764 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4768, L0x7fffffff4768 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff476c, L0x7fffffff476c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4770, L0x7fffffff4770 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4774, L0x7fffffff4774 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4778, L0x7fffffff4778 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff477c, L0x7fffffff477c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4780, L0x7fffffff4780 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4784, L0x7fffffff4784 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4788, L0x7fffffff4788 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff478c, L0x7fffffff478c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4790, L0x7fffffff4790 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4794, L0x7fffffff4794 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4798, L0x7fffffff4798 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff479c, L0x7fffffff479c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff47a0, L0x7fffffff47a0 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff47a4, L0x7fffffff47a4 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff47a8, L0x7fffffff47a8 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff47ac, L0x7fffffff47ac <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff47b0, L0x7fffffff47b0 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff47b4, L0x7fffffff47b4 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff47b8, L0x7fffffff47b8 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff47bc, L0x7fffffff47bc <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff47c0, L0x7fffffff47c0 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff47c4, L0x7fffffff47c4 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff47c8, L0x7fffffff47c8 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff47cc, L0x7fffffff47cc <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff47d0, L0x7fffffff47d0 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff47d4, L0x7fffffff47d4 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff47d8, L0x7fffffff47d8 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff47dc, L0x7fffffff47dc <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff47e0, L0x7fffffff47e0 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff47e4, L0x7fffffff47e4 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff47e8, L0x7fffffff47e8 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff47ec, L0x7fffffff47ec <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff47f0, L0x7fffffff47f0 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff47f4, L0x7fffffff47f4 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff47f8, L0x7fffffff47f8 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff47fc, L0x7fffffff47fc <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4800, L0x7fffffff4800 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4804, L0x7fffffff4804 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4808, L0x7fffffff4808 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff480c, L0x7fffffff480c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4810, L0x7fffffff4810 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4814, L0x7fffffff4814 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4818, L0x7fffffff4818 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff481c, L0x7fffffff481c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4820, L0x7fffffff4820 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4824, L0x7fffffff4824 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4828, L0x7fffffff4828 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff482c, L0x7fffffff482c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4830, L0x7fffffff4830 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4834, L0x7fffffff4834 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4838, L0x7fffffff4838 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff483c, L0x7fffffff483c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4840, L0x7fffffff4840 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4844, L0x7fffffff4844 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4848, L0x7fffffff4848 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff484c, L0x7fffffff484c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4850, L0x7fffffff4850 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4854, L0x7fffffff4854 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4858, L0x7fffffff4858 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff485c, L0x7fffffff485c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4860, L0x7fffffff4860 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4864, L0x7fffffff4864 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4868, L0x7fffffff4868 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff486c, L0x7fffffff486c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4870, L0x7fffffff4870 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4874, L0x7fffffff4874 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4878, L0x7fffffff4878 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff487c, L0x7fffffff487c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4880, L0x7fffffff4880 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4884, L0x7fffffff4884 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4888, L0x7fffffff4888 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff488c, L0x7fffffff488c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff4890, L0x7fffffff4890 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4894, L0x7fffffff4894 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff4898, L0x7fffffff4898 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff489c, L0x7fffffff489c <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff48a0, L0x7fffffff48a0 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff48a4, L0x7fffffff48a4 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff48a8, L0x7fffffff48a8 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff48ac, L0x7fffffff48ac <s (75423753)@32,
    (-75423753)@32 <s L0x7fffffff48b0, L0x7fffffff48b0 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff48b4, L0x7fffffff48b4 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff48b8, L0x7fffffff48b8 <s (75423753)@32, (-75423753)@32 <s L0x7fffffff48bc, L0x7fffffff48bc <s (75423753)@32
  ]
}
