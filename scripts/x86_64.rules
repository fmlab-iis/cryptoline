# address patterns

#! $1c(%r$2c) = %%EA
#! (%r$1c) = %%EA
#! $1c(%rbp) = %%EA
#! (%rbp) = %%EA
#! $1c(%rbx) = %%EA
#! (%rbx) = %%EA
#! $1c(%rcx) = %%EA
#! (%rcx) = %%EA
#! $1c(%rdx) = %%EA
#! (%rdx) = %%EA
#! $1c(%rdi) = %%EA
#! (%rdi) = %%EA
#! $1c(%rsi) = %%EA
#! (%rsi) = %%EA
#! -$1c(%rip) = %%EA
#! $1c(%rip) = %%EA
#! $1c(%rsp) = %%EA
#! (%rsp) = %%EA

# registers

#! %r$1c = %%r$1c
#! %rax = %%rax
#! %rbx = %%rbx
#! %rdi = %%rdi
#! %rcx = %%rcx
#! %rdx = %%rdx
#! %rbp = %%rbp
#! %rsi = %%rsi
#! %xmm$1c = %%xmm$1c
#! %ymm$1c = %%ymm$1c

# scalar instructions

#! add $1v, $2v -> adds carry $2v $2v $1v
#! add $1c, $2v -> adds carry $2v $2v $1c@uint64
#! add $1ea, $2v -> adds carry $2v $2v $1ea
#! adc $1v, $2v -> adcs carry $2v $2v $1v carry
#! adc $1c, $2v -> adcs carry $2v $2v $1c@uint64 carry
#! adc $1ea, $2v -> adcs carry $2v $2v $1ea carry
#! adcx $1v, $2v -> adcs carry $2v $2v $1v carry
#! adox $1v, $2v -> adcs overflow $2v $2v $1v overflow
#! and $1v, $2v -> and $2v@uint64 $2v $1v
#! and $1c, $2v -> and $2v@uint64 $2v $1c@uint64
#! andn $1v, $2v, $3v -> not $2v_n@uint64 $2v; and $3v@uint64 $2v_n $1v
#! cmovb $1v, $2v -> cmov $2v carry $1v $2v
#! cmove $1v, $2v -> cmov $2v zero $1v $2v
#! cmovne $1v, $2v -> cmov $2v zero $2v $1v
#! dec $1v -> subb overflow $1v $1v 1@uint64
#! imul $1v, $2v -> mull dontcare $2v $1v $2v
#! lea $1ea, $2v -> mov $2v, $1ea
#! mov $1c, $2v -> mov $2v $1c@uint64
#! mov $1c, $2ea -> mov $2ea $1c@uint64
#! mov $1v, $2v -> mov $2v $1v
#! mov $1ea, $2v -> mov $2v $1ea
#! mov $1v, $2ea -> mov $2ea $1v
#! movabs $1c, $2v -> mov $2v $1c@uint64
#! mul $1v -> mull rdx rax rax $1v;\nsubc carry dc rdx 1@uint64; mov overflow carry
#! mulq $1v -> mull rdx rax rax $1v
#! mulq $1ea -> mull rdx rax rax $1ea
#! mulx $1v, $2v, $3v -> mull $3v $2v rdx $1v
#! sub $1v, $2v -> subb carry $2v $2v $1v
#! sub $1c, $2v -> subb carry $2v $2v $1c@uint64
#! sub $1ea, $2v -> subb carry $2v $2v $1ea
#! sub $1v, $2ea -> subb carry $2ea $2ea $1v
#! sbb $1v, $2v -> sbbs carry $2v $2v $1v carry
#! sbb $1c, $2v -> sbbs carry $2v $2v $1c@uint64 carry
#! sbb $1ea, $2v -> sbbs carry $2v $2v $1ea
#! shlx $1c, $2v, $3v -> split dc $3v $2v (64-$1c);\nshl $3v $3v $1c@uint64
#! shl $1c, $2v -> split dc $2v $2v (64-$1c);\nshl $2v $2v $1c@uint64
#! shrx $1c, $2v, $3v -> split $3v dc $2v $1c
#! shr $1c, $2v -> split $2v dc $2v $1c
#! test $1v, $1v -> subb zero dontcare $1v 1@uint64
#! test \$0x1, $1v -> and tmp@uint64 0x1@uint64 $1v;\nsubb zero dontcare tmp 0x1@uint64
#! xor $1v, $1v -> mov $1v 0@uint64;\nclear carry;\nclear overflow

# vector instructions
# w = 16 bits, d = 32 bits, q = 64 bits

#! movdqa $1xmm, $2xmm -> mov %$2xmm %$1xmm@uint64[2]
#! movdqa $1ea, $2xmm -> mov %$2xmm@uint64[2] [$1ea, $1ea[+8]]
#! movdqu $1ea, $2xmm -> mov %$2xmm@uint64[2] [$1ea, $1ea[+8]]
#! movdqu $1xmm, $2ea -> mov [$2ea, $2ea[+8]] %$1xmm@uint64[2]
#! movddup $1ea, $2xmm -> mov %$2xmm@uint64[2] [ $1ea, $1ea ]
#! paddd $1xmm, $2xmm -> adds %dc %$2xmm@uint32[4] %$2xmm %$1xmm
#! palignr \$0x4, $1xmm, $2xmm -> mov %$2xmm@uint32[4] (%$1xmm[1, 2, 3] ++ %$2xmm[0])
#! palignr \$0x8, $1xmm, $2xmm -> mov %$2xmm@uint64[2] [%$1xmm[1], %$2xmm[0]]
#! pextrq \$0x0, $1xmm, $2v -> mov %pextrq@uint64[2] %$1xmmq; mov [$2v, _] %pextrq
#! pshufd \$0xe, $1xmm, $2xmm -> mov %$2xmm@uint32[4] (%$1xmm[2, 3, 0, 0])
#! pshufd \$0x1b, $1xmm, $2xmm -> mov %$2xmm@uint32[4] (%$1xmm[3, 2, 1, 0])
#! pshufd \$0xb1, $1xmm, $2xmm -> mov %$2xmm@uint32[4] (%$1xmm[1, 0, 3, 2])
#! psllq $1c, $2xmm -> split %dc %$2xmmL %$2xmm@uint64[2] `64 - $1c`;\nbroadcast %off 2 [$1c@uint64]; shl %$2xmm %$2xmmL %off
#! psllq $1c, $2ymm -> split %dc %$2ymmL %$2ymm@uint64[4] `64 - $1c`;\nbroadcast %off 4 [$1c@uint64]; shl %$2ymm %$2ymmL %off
#! psrlq $1c, $2xmm -> split %$2xmm %dc %$2xmm@uint64[2] $1c
#! psrlq $1c, $2ymm -> split %$2ymm %dc %$2ymm@uint64[4] $1c
#! punpcklqdq $1xmm, $2xmm -> mov %$2xmm@uint64[2] [%$2xmm[0], %$1xmm[0]]
#! punpckhqdq $1xmm, $2xmm -> mov %$2xmm@uint64[2] [%$2xmm[1], %$1xmm[1]]
#! vmovapd $1ea, $2ymm -> mov %$2ymm@uint64[4] [$1ea,$1ea[+8],$1ea[+16],$1ea[+24]]
#! vmovapd $1ymm, $2ea -> mov [$2ea,$2ea[+8],$2ea[+16],$2ea[+24]] %$1ymm@uint64[4]
#! vmovdqa $1ea, $2ymm -> mov %$2ymm@uint64[4] [$1ea,$1ea[+8],$1ea[+16],$1ea[+24]]
#! vmovdqa $1ymm, $2ea -> mov [$2ea,$2ea[+8],$2ea[+16],$2ea[+24]] %$1ymm@uint64[4]
#! vmovdqu $1ea, $2ymm -> mov %$2ymm@uint64[4] [$1ea, $1ea[+8], $1ea[+16], $1ea[+24]]
#! vmovsldup $1ymm, $2ymm -> mov %$2ymm@sint32[8] (%$1ymm[0, 0, 2, 2, 4, 4, 6, 6])
#! vmovshdup $1ymm, $2ymm -> mov %$2ymm@sint32[8] (%$1ymm[1, 1, 3, 3, 5, 5, 7, 7])
#! vmovupd $1ea, $2ymm -> mov %$2ymm@uint64[4] [$1ea,$1ea[+8],$1ea[+16],$1ea[+24]]
#! vmovupd $1ymm, $2ea -> mov [$2ea,$2ea[+8],$2ea[+16],$2ea[+24]] %$1ymm@uint64[4]
#! vpaddd $1ymm, $2ymm, $3ymm -> add %$3ymm@sint32[8] %$2ymm %$1ymm
#! vpaddw $1ymm, $2ymm, $3ymm -> add %$3ymm@sint16[16] %$1ymm %$2ymm
#! vpand $1xmm, $2xmm, $3xmm -> and %$3xmm@uint64[2] %$2xmm %$1xmm
#! vpand $1ymm, $2ymm, $3ymm -> and %$3ymm@uint64[4] %$2ymm %$1ymm
#! vpblendd \$0xaa, $1ymm, $2ymm, $3ymm -> mov %$3ymm@sint32[8] [%$2ymm[0], %$1ymm[1], %$2ymm[2], %$1ymm[3], %$2ymm[4], %$1ymm[5], %$2ymm[6], %$1ymm[7]]
#! vpblendw \$0xaa, $2ymm, $1ymm, $3ymm -> mov %$3ymm@sint16[16] [%$1ymm[0], %$2ymm[0], %$1ymm[2], %$2ymm[2], %$1ymm[4], %$2ymm[4], %$1ymm[6], %$2ymm[6], %$1ymm[8], %$2ymm[8], %$1ymm[10], %$2ymm[10], %$1ymm[12], %$2ymm[12], %$1ymm[14], %$2ymm[14]]
#! vpbroadcastd $1ea, $2ymm -> broadcast %$2ymm@uint32[8] 8 [$1ea]
#! vperm2i128 \$0x20, $1ymm, $2ymm, $3ymm -> mov %$3ymm@sint32[8] (%$2ymm[0, 1, 2, 3] ++ %$1ymm[0, 1, 2, 3])
#! vperm2i128 \$0x31, $1ymm, $2ymm, $3ymm -> mov %$3ymm@sint32[8] (%$2ymm[4, 5, 6, 7] ++ %$1ymm[4, 5, 6, 7])
#! vpmuldq $1ymm, $2ymm, $3ymm -> mulj %$3ymm@sint64[4] (%$2ymm[0, 2, 4, 6]) (%$1ymm[0, 2, 4, 6])
#! vpmullw $1ymm, $2ymm, $3ymm -> smull %dc %$3ymm@uint16[16] %$1ymm %$2ymm
#! vpmulhw $1ymm, $2ymm, $3ymm -> smull %$3ymm@sint16[16] %dc %$1ymm %$2ymm
#! vpor $1xmm, $2xmm, $3xmm -> or %$3xmm@uint64[2] %$2xmm %$1xmm
#! vpor $1ymm, $2ymm, $3ymm -> or %$3ymm@uint64[4] %$2ymm %$1ymm
#! vpxor $1xmm, $2xmm, $3xmm -> xor %$3xmm@uint64[2] %$2xmm %$1xmm
#! vpxor $1ymm, $2ymm, $3ymm -> xor %$3ymm@uint64[4] %$2ymm %$1ymm
#! vpslld $1c, $2xmm, $3xmm -> split %dc %$2xmmL %$2xmm@uint32[4] `32 - $1c`;\nbroadcast %off 4 [$1c@uint32]; shl %$3xmm %$2xmmL %off
#! vpslld $1c, $2ymm, $3ymm -> split %dc %$2ymmL %$2ymm@uint32[8] `32 - $1c`;\nbroadcast %off 8 [$1c@uint32]; shl %$3ymm %$2ymmL %off
#! vpsrld $1c, $2xmm, $3xmm -> split %$3xmm %dc %$2xmm@uint32[4] $1c
#! vpsrld $1c, $2ymm, $3ymm -> split %$3ymm %dc %$2ymm@uint32[8] $1c
#! vpsllq $1c, $2xmm, $3xmm -> split %dc %$2xmmL %$2xmm@uint64[2] `64 - $1c`;\nbroadcast %off 2 [$1c@uint64]; shl %$3xmm %$2xmmL %off
#! vpsllq $1c, $2ymm, $3ymm -> split %dc %$2ymmL %$2ymm@uint64[4] `64 - $1c`;\nbroadcast %off 4 [$1c@uint64]; shl %$3ymm %$2ymmL %off
#! vpsrlq $1c, $2xmm, $3xmm -> split %$3xmm %dc %$2xmm@uint64[2] $1c
#! vpsrlq $1c, $2ymm, $3ymm -> split %$3ymm %dc %$2ymm@uint64[4] $1c
#! vpsllw $1c, $2xmm, $3xmm -> split %dc %$2xmmL %$2xmm@uint16[8] `16 - $1c`;\nbroadcast %off 8 [$1c@uint16]; shl %$3xmm %$2xmmL %off
#! vpsllw $1c, $2ymm, $3ymm -> split %dc %$2ymmL %$2ymm@uint16[16] `16 - $1c`;\nbroadcast %off 16 [$1c@uint16]; shl %$3ymm %$2ymmL %off
#! vpsraw $1c, $2xmm, $3xmm -> ssplit %$3xmm %dc %$2xmm@sint16[8] $1c
#! vpsraw $1c, $2ymm, $3ymm -> ssplit %$3ymm %dc %$2ymm@sint16[16] $1c
#! vpsrlw $1c, $2xmm, $3xmm -> split %$3xmm %dc %$2xmm@uint16[8] $1c
#! vpsrlw $1c, $2ymm, $3ymm -> split %$3ymm %dc %$2ymm@uint16[16] $1c
#! vpsubd $1ymm, $2ymm, $3ymm -> sub %$3ymm@sint32[8] %$2ymm %$1ymm
#! vpsubw $1ymm, $2ymm, $3ymm -> sub %$3ymm@sint16[16] %$2ymm %$1ymm
#! vpunpcklqdq $1xmm, $2xmm, $3xmm -> mov %$3xmm@uint64[2] [%$2xmm[0], %$1xmm[0]]
#! vpunpcklqdq $1ymm, $2ymm, $3ymm -> mov %$3ymm@uint64[4] [%$2ymm[0], %$1ymm[0], %$2ymm[2], %$1ymm[2]]
#! vpunpckhqdq $1xmm, $2xmm, $3xmm -> mov %$3xmm@uint64[2] [%$2xmm[1], %$1xmm[1]]
#! vpunpckhqdq $1ymm, $2ymm, $3ymm -> mov %$3ymm@uint64[4] [%$2ymm[1], %$1ymm[1], %$2ymm[3], %$1ymm[3]]
